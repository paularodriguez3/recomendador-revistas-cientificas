[
  {
    "title": "A new Copy-Move forgery detection method using LIOP",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103661",
    "abstract": "The most prevalent type of digital image falsification occurs when a portion of a image is copied and pasted onto another section of the same image. Falsification of the image made in this way is called copy-move forgery (CMF). This study presents a new and effective approach for copy-move forgery detection (CMFD) using the Local Intensity Order Pattern (LIOP) to overcome the restrictions of existing CMFD techniques. The input image is first converted to a YCbCr color space and then split into Y, Cb, and Cr color channels. The LIOP features are then extracted from each color channel and all the features are combined. The feature vectors are ordered lexicographically and related features are detected by comparing the LIOP features. Although the LIOP feature has rarely been used in CMFD prior to this study, the success rate of the proposed method is high. In addition, since the channels are not correlated to each other in the YCbCr color space, each color channel is considered as a gray image, and the success rate is increased by combining the features extracted from each of the color channels. The proposed approach was assessed using the CoMoFoD and GRIP datasets. Experimental findings demonstrated that the suggested method was successful and displayed robustness in post-processing attacks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032200181X",
    "keywords": [
      "Computer science",
      "Computer security"
    ],
    "authors": [
      {
        "surname": "Aydın",
        "given_name": "Yıldız"
      }
    ]
  },
  {
    "title": "Stereo image quality assessment considering the difference of statistical feature in early visual pathway",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103643",
    "abstract": "Human visual theory is closely related to stereo image quality assessment (SIQA), which determines whether the evaluation results of SIQA method can keep good consistency with subjective perception. Many SIQA methods are not fully based on human visual theory, so there is still room for improvement. The research on the visual system tends to the dorsal and ventral pathways, which ignores the information differences in the early visual pathways. It is worth noting that the ON and OFF receptive fields in retinal ganglion cells (RGCs) respond asymmetrically to the statistical features of images. Inspired by this, in this paper, we propose an SIQA method based on monocular and binocular visual features, which takes into account the difference of ON and OFF response features in early visual pathways. Moreover, the different information interaction mechanisms of visual cortex are used to fuse the response maps information of left and right images. Final, monocular and binocular features are extracted and sent to support vector regression (SVR) for quality prediction. Experimental results show that the proposed method is superior to several mainstream SIQA metrics on four publicly available stereo image databases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001638",
    "keywords": [
      "Artificial intelligence",
      "Binocular vision",
      "Computer science",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Depth perception",
      "Electrical engineering",
      "Engineering",
      "Feature (linguistics)",
      "Fuse (electrical)",
      "Human visual system model",
      "Image (mathematics)",
      "Linguistics",
      "Monocular",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Perception",
      "Philosophy",
      "Psychology",
      "Receptive field",
      "Stereopsis",
      "Visual cortex",
      "Visual perception"
    ],
    "authors": [
      {
        "surname": "Chang",
        "given_name": "Yongli"
      },
      {
        "surname": "Li",
        "given_name": "Sumei"
      },
      {
        "surname": "Jin",
        "given_name": "Jie"
      },
      {
        "surname": "Liu",
        "given_name": "Anqi"
      },
      {
        "surname": "Xiang",
        "given_name": "Wei"
      }
    ]
  },
  {
    "title": "Unbiased feature generating for generalized zero-shot learning",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103657",
    "abstract": "Generalized zero-shot learning (GZSL) aims at training a model on seen data to recognize objects from both seen and unseen classes. Existing generated-based methods show encouraging performance by directly generating unseen samples. However, due to insufficient exploration of unseen label space and limited class-wise semantic descriptions, existing methods still face the bias problem. In this paper, we divide the bias problem into seen-biased and neighbor-biased problems and propose a GZSL method named Unbiased Feature Generating. For the seen-biased problem, we train a classifier in complete label space by introducing the discriminative information contained in fake unseen samples. For the neighbor-biased problem, we generate untypical samples and refine the classification boundaries among neighbor classes. The classifier in complete label space and generator are trained in an iterative process to complement each other. The experimental results on four widely used datasets verify our method achieves encouraging performance compared with the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001778",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Discriminative model",
      "Feature vector",
      "Generator (circuit theory)",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "k-nearest neighbors algorithm"
    ],
    "authors": [
      {
        "surname": "Niu",
        "given_name": "Chang"
      },
      {
        "surname": "Shang",
        "given_name": "Junyuan"
      },
      {
        "surname": "Huang",
        "given_name": "Junchu"
      },
      {
        "surname": "Yang",
        "given_name": "Junmei"
      },
      {
        "surname": "Song",
        "given_name": "Yuting"
      },
      {
        "surname": "Zhou",
        "given_name": "Zhiheng"
      },
      {
        "surname": "Zhou",
        "given_name": "Guoxu"
      }
    ]
  },
  {
    "title": "A comprehensive survey and mathematical insights towards video summarization",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103670",
    "abstract": "Video Summarization is a technique to reduce the original raw video into a short video summary. Video summarization automates the task of acquiring key frames/segments from the video and combining them to generate a video summary. This paper provides a framework for summarization based on different criteria and also compares different literature work related to video summarization. The framework deals with formulating model for video summarization based on different criteria. Based on target audience/ viewership, number of videos, type of output intended, type of video summary and summarization factor; a model generating video summarization framework is proposed. The paper examines significant research works in the area of video summarization to present a comprehensive review against the framework. Different techniques, perspectives and modalities are considered to preserve the diversity of survey. This paper examines important mathematical formulations to provide meaningful insights for video summarization model creation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001900",
    "keywords": [
      "Applied psychology",
      "Automatic summarization",
      "Computer science",
      "Data science",
      "Information retrieval",
      "Multimedia",
      "Psychology",
      "Survey research"
    ],
    "authors": [
      {
        "surname": "Narwal",
        "given_name": "Pulkit"
      },
      {
        "surname": "Duhan",
        "given_name": "Neelam"
      },
      {
        "surname": "Kumar Bhatia",
        "given_name": "Komal"
      }
    ]
  },
  {
    "title": "Multi-scale and multi-patch transformer for sandstorm image enhancement",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103662",
    "abstract": "Sandstorm is a meteorological phenomenon common in arid and semi-arid regions. A sandstorm can carry large volumes of sand unexpectedly, which leads to severe color deviations and significantly degraded visibility when an image is taken in such a scenario. However, existing image enhancement methods cannot enhance sandstorm images well due to the challenging degradations and the scarcity of sandstorm training data. In this paper, we propose a Transformer with rotary position embedding to perform sandstorm image enhancement via building multi-scale and multi-patch dependencies. Our key insights in this work are 1) a multi-scale Transformer can globally eliminate the color deviations of sandstorm images via aggregating global information, 2) a multi-patch Transformer can recover local details well via learning the spatial variant degradations, and 3) a U-shape Transformer with rotary position embedding as the core unit of multi-scale and multi-patch Transformer can effectively build the long-range dependencies. We also contribute a real-world Sandstorm Image Enhancement (SIE) dataset including 1,400 sandstorm images with different degrees of degradations and various scenes. Experiments performed on synthetic images and real-world sandstorm images demonstrate that our proposed method not only obtains visually pleasing results but also outperforms state-of-the-art methods qualitatively and quantitatively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001821",
    "keywords": [
      "Artificial intelligence",
      "Cartography",
      "Computer science",
      "Electrical engineering",
      "Engineering",
      "Geography",
      "Image (mathematics)",
      "Scale (ratio)",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Liang",
        "given_name": "Pengwei"
      },
      {
        "surname": "Ding",
        "given_name": "Wenyu"
      },
      {
        "surname": "Fan",
        "given_name": "Lu"
      },
      {
        "surname": "Wang",
        "given_name": "Haoyu"
      },
      {
        "surname": "Li",
        "given_name": "Zihong"
      },
      {
        "surname": "Yang",
        "given_name": "Fan"
      },
      {
        "surname": "Wang",
        "given_name": "Bo"
      },
      {
        "surname": "Li",
        "given_name": "Chongyi"
      }
    ]
  },
  {
    "title": "Adaptive weight multi-channel center similar deep hashing",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103642",
    "abstract": "To increase the richness of the extracted text modality feature information and deeply explore the semantic similarity between the modalities. In this paper, we propose a novel method, named adaptive weight multi-channel center similar deep hashing (AMCDH). The algorithm first utilizes three channels with different configurations to extract feature information from the text modality; and then adds them according to the learned weight ratio to increase the richness of the information. We also introduce the Jaccard coefficient to measure the semantic similarity level between modalities from 0 to 1, and utilize it as the penalty coefficient of the cross-entropy loss function to increase its role in backpropagation. Besides, we propose a method of constructing center similarity, which makes the hash codes of similar data pairs close to the same center point, and dissimilar data pairs are scattered at different center points to generate high-quality hash codes. Extensive experimental evaluations on four benchmark datasets show that the performance of our proposed model AMCDH is significantly better than other competing baselines. The code can be obtained from https://github.com/DaveLiu6/AMCDH.git.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001626",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer security",
      "Data mining",
      "Entropy (arrow of time)",
      "Geodesy",
      "Geography",
      "Hash function",
      "Image (mathematics)",
      "Jaccard index",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Xinghua"
      },
      {
        "surname": "Cao",
        "given_name": "Guitao"
      },
      {
        "surname": "Lin",
        "given_name": "Qiubin"
      },
      {
        "surname": "Cao",
        "given_name": "Wenming"
      }
    ]
  },
  {
    "title": "A comprehensive benchmark analysis for sand dust image reconstruction",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103638",
    "abstract": "Recently, numerous sand dust removal algorithms have been proposed. To our best knowledge, however, most methods evaluated their performance in a no-reference way using few selected real-world images from the internet. It is unclear how to quantitatively analyze the performance of the algorithms in a supervised way. Moreover, due to the absence of large-scale datasets, there are no well-known sand dust reconstruction report algorithms up till now. To bridge the gaps, we presented a comprehensive perceptual study and analysis of real-world sandstorm images, then constructed a Sand-dust Image Reconstruction Benchmark(SIRB) for training Convolutional Neural Networks(CNNs) and evaluating the algorithm’s performance. We adopted the existing image transformation neural network trained on SIRB as the baseline to illustrate the generalization of SIRB for training CNNs. Finally, we conducted a comprehensive evaluation to demonstrate the performance and limitations of the sandstorm enhancement algorithms, which shed light on future research in sandstorm image reconstruction.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001584",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Cartography",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Geography",
      "Image (mathematics)",
      "Machine learning",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Si",
        "given_name": "Yazhong"
      },
      {
        "surname": "Yang",
        "given_name": "Fan"
      },
      {
        "surname": "Guo",
        "given_name": "Ya"
      },
      {
        "surname": "Zhang",
        "given_name": "Wei"
      },
      {
        "surname": "Yang",
        "given_name": "Yipu"
      }
    ]
  },
  {
    "title": "Single underwater image haze removal with a learning-based approach to blurriness estimation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103656",
    "abstract": "Underwater images are usually degraded due to light scattering and absorption. To recover the scene radiance of degraded underwater images, a new haze removal method is presented by incorporating a learning-based approach to blurriness estimation with the image formation model. Firstly, the image blurriness is estimated with a linear model trained on a set of selected grayscale images, the average Gaussian images and blurriness images. With the estimated image blurriness, three intermediate background lights (BLs) are computed to obtain the synthesized BL. Then the scene depth is calculated by using the estimated image blurriness and BL to construct a transmission map and restore the scene radiance. Compared with other haze removal methods, haze in degraded underwater images can be removed more accurately with our proposed method. Moreover, visual inspection, quantitative evaluation and application test demonstrate that our method is superior to the compared methods and beneficial to high-level vision tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001766",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Geology",
      "Grayscale",
      "Haze",
      "Image (mathematics)",
      "Meteorology",
      "Oceanography",
      "Physics",
      "Radiance",
      "Remote sensing",
      "Underwater"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Jian"
      },
      {
        "surname": "Wu",
        "given_name": "Hao-Tian"
      },
      {
        "surname": "Lu",
        "given_name": "Lu"
      },
      {
        "surname": "Luo",
        "given_name": "Xiangyang"
      },
      {
        "surname": "Hu",
        "given_name": "Jiankun"
      }
    ]
  },
  {
    "title": "Blind deblurring with fractional-order calculus and local minimal pixel prior",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103645",
    "abstract": "Fractional-order calculus is an extension of integer order calculus.In signal processing, fractional-order calculus can non-linearly enhance the low-frequency signal and suppress the high-frequency signal. In this paper, a new fractional-order local minimum pixel prior (FOLMP) is proposed by combining fractional-order calculus with the local minimum pixel prior. The FOLMP of the sharp images includes fewer non-zero pixels than the blur images. A new blur kernel estimation algorithm is proposed by combining L 0 regularized FOLMP with the maximum posterior probability. Furthermore, thekernel similarity is employed to adjust the iteration times to accelerate the computational efficiency. Comparative experiments show that the proposed algorithm can perform better on different types of datasets than the most advanced algorithms. In addition, non-overlapping image patches are adopted to compute the FOLMP, and the kernel similarity is used to suppress excessive iterations.Therefore, the proposed algorithm is several times or even tens of times more efficient than the classical prior-based methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001651",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Artificial intelligence",
      "Computer science",
      "Deblurring",
      "Discrete mathematics",
      "Fractional calculus",
      "Image (mathematics)",
      "Image processing",
      "Image restoration",
      "Kernel (algebra)",
      "Mathematical optimization",
      "Mathematics",
      "Pixel",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Jing"
      },
      {
        "surname": "Tan",
        "given_name": "Jieqing"
      },
      {
        "surname": "Ge",
        "given_name": "Xianyu"
      },
      {
        "surname": "Hu",
        "given_name": "Dandan"
      },
      {
        "surname": "He",
        "given_name": "Lei"
      }
    ]
  },
  {
    "title": "Skeleton-based deep pose feature learning for action quality assessment on figure skating videos",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103625",
    "abstract": "Most of the existing Action Quality Assessment (AQA) methods for scoring sports videos have deeply researched how to evaluate the single action or several sequential-defined actions that performed in short-term sport videos, such as diving, vault, etc. They attempted to extract features directly from RGB videos through 3D ConvNets, which makes the features mixed with ambiguous scene information. To investigate the effectiveness of deep pose feature learning on automatically evaluating the complicated activities in long-duration sports videos, such as figure skating and artistic gymnastic, we propose a skeleton-based deep pose feature learning method to address this problem. For pose feature extraction, a spatial–temporal pose extraction module (STPE) is built to capture the subtle changes of human body movements and obtain the detail representations for skeletal data in space and time dimensions. For temporal information representation, an inter-action temporal relation extraction module (ATRE) is implemented by recurrent neural network to model the dynamic temporal structure of skeletal subsequences. We evaluate the proposed method on figure skating activity of MIT-skate and FIS-V datasets. The experimental results show that the proposed method is more effective than RGB video-based deep feature learning methods, including SENet and C3D. Significant performance progress has been achieved for the Spearman Rank Correlation (SRC) on MIT-Skate dataset. On FIS-V dataset, for the Total Element Score (TES) and the Program Component Score (PCS), better SRC and MSE have been achieved between the predicted scores against the judge’s ones when compared with SENet and C3D feature methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001456",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Feature (linguistics)",
      "Feature extraction",
      "Feature learning",
      "Law",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Pose",
      "RGB color model",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Huiying"
      },
      {
        "surname": "Lei",
        "given_name": "Qing"
      },
      {
        "surname": "Zhang",
        "given_name": "Hongbo"
      },
      {
        "surname": "Du",
        "given_name": "Jixiang"
      },
      {
        "surname": "Gao",
        "given_name": "Shangce"
      }
    ]
  },
  {
    "title": "From synthetic to natural — single natural image dehazing deep networks using synthetic dataset domain randomization",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103636",
    "abstract": "Image dehazing methods aim to solve the problem of poor visibility in images due to haze. Techniques proposed for image dehazing in literature focus on image priors, haze lines or data driven statistical models. Variations of the classical methods relying on prior model or haze line model use no-reference image quality metrics to prove their dehazing performance. Recently developed deep learning models rely on huge amounts of hazy, haze-free pairs for training, and uses PSNR and SSIM like image reconstruction metrics to show their performance. These methods perform poorly on no-reference image quality assessments and also dehazes poorly at the depths of the image. These methods though can be optimized for memory usage and are faster. This work presents a deep learning model (Feature Fusion Attention Network) trained on a domain randomized synthetic dataset generated in simulation. The proposed model achieves the highest scores on blind image assessments through the gradient rationing technique for a deep learning-based approach by a significant margin. The images were evaluated on full-reference metrics as well and obtained favorable results. This approach also yields one of the highest edge sharpness obtained after dehazing. The training procedure adopted to obtain significant gains on real-world dehazing, without using any real-world data is also detailed in this paper.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001560",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Domain (mathematical analysis)",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Image quality",
      "Linguistics",
      "Machine learning",
      "Margin (machine learning)",
      "Mathematical analysis",
      "Mathematics",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Visibility"
    ],
    "authors": [
      {
        "surname": "Shamsuddin",
        "given_name": "Abdul Fathaah"
      },
      {
        "surname": "Ragunathan",
        "given_name": "Krupasankari"
      },
      {
        "surname": "P.",
        "given_name": "Abhijith"
      },
      {
        "surname": "Raja Sekar P.M.",
        "given_name": "Deepak"
      },
      {
        "surname": "Sankaran",
        "given_name": "Praveen"
      }
    ]
  },
  {
    "title": "CNN-based first quantization estimation of double compressed JPEG images",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103635",
    "abstract": "Multiple JPEG compressions leave artifacts in digital images: residual traces that could be exploited in forensics investigations to recover information about the device employed for acquisition or image editing software. In this paper, a novel First Quantization Estimation (FQE) algorithm based on convolutional neural networks (CNNs) is proposed. In particular, a solution based on an ensemble of CNNs was developed in conjunction with specific regularization strategies exploiting assumptions about neighboring element values of the quantization matrix to be inferred. Mostly designed to work in the aligned case, the solution was tested in challenging scenarios involving different input patch sizes, quantization matrices (both standard and custom) and datasets (i.e., RAISE and UCID collections). Comparisons with state-of-the-art solutions confirmed the effectiveness of the presented solution demonstrating for the first time to cover the widest combinations of parameters of double JPEG compressions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001559",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Image (mathematics)",
      "Image compression",
      "Image processing",
      "JPEG",
      "JPEG 2000",
      "Programming language",
      "Quantization (signal processing)",
      "Reference software",
      "Regularization (linguistics)",
      "Residual",
      "Software"
    ],
    "authors": [
      {
        "surname": "Battiato",
        "given_name": "Sebastiano"
      },
      {
        "surname": "Giudice",
        "given_name": "Oliver"
      },
      {
        "surname": "Guarnera",
        "given_name": "Francesco"
      },
      {
        "surname": "Puglisi",
        "given_name": "Giovanni"
      }
    ]
  },
  {
    "title": "Efficient graph attentional network for 3D object detection from Frustum-based LiDAR point clouds",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103667",
    "abstract": "LiDAR-based 3D object detection is important for autonomous driving scene perception, but point clouds produced by LiDAR are irregular and unstructured in nature, and cannot be adopted by the conventional Convolutional Neural Networks (CNN). Recently, Graph Convolutional Networks (GCN) has been proved as an ideal way to handle non-Euclidean structure data, as well as for point cloud processing. However, GCN involves massive computation for searching adjacent nodes, and the heavy computational cost limits its applications in processing large-scale LiDAR point cloud in autonomous driving. In this work, we adopt a frustum-based point cloud-image fusion scheme to reduce the amount of LiDAR point clouds, thus making the GCN-based large-scale LiDAR point clouds feature learning feasible. On this basis, we propose an efficient graph attentional network to accomplish the goal of 3D object detection in autonomous driving, which can learn features from raw LiDAR point cloud directly without any conversions. We evaluate the model on the public KITTI benchmark dataset, the 3D detection mAP is 63.72% on KITTI Cars, Pedestrian and Cyclists, and the inference speed achieves 7.9 fps on a single GPU, which is faster than other methods of the same type.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001870",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Frustum",
      "Geology",
      "Geometry",
      "Graph",
      "Lidar",
      "Linguistics",
      "Mathematics",
      "Object detection",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Point cloud",
      "Remote sensing",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Liang",
        "given_name": "Zhenming"
      },
      {
        "surname": "Huang",
        "given_name": "Yingping"
      },
      {
        "surname": "Liu",
        "given_name": "Zhenwei"
      }
    ]
  },
  {
    "title": "Improving small objects detection using transformer",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103620",
    "abstract": "General artificial intelligence counteracts the inductive bias of an algorithm and tunes the algorithm for out-of-distribution generalization. A conspicuous impact of the inductive bias is an unceasing trend in improving deep learning performance. Although a quintessential attention-based object detection technique, DETR, shows better accuracy than its predecessors, its accuracy deteriorates for detecting small-sized (in-perspective) objects. This study examines the inductive bias of DETR and proposes a normalized inductive bias for object detection using data fusion, SOF-DETR. A technique of lazy-fusion of features is introduced in SOF-DETR, which sustains deep contextual information of objects present in an image. The features from multiple subsequent deep layers are fused for object queries that learn long and short-distance spatial association in an image using the attention mechanism. Experimental results on the MS COCO and Udacity Self Driving Car datasets assert the effectiveness of the added normalized inductive bias and feature fusion techniques, showing increased COCO mAP scores on small-sized objects.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001432",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Electrical engineering",
      "Engineering",
      "Generalization",
      "Inductive bias",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Multi-task learning",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Perspective (graphical)",
      "Sensor fusion",
      "Systems engineering",
      "Task (project management)",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Dubey",
        "given_name": "Shikha"
      },
      {
        "surname": "Olimov",
        "given_name": "Farrukh"
      },
      {
        "surname": "Rafique",
        "given_name": "Muhammad Aasim"
      },
      {
        "surname": "Jeon",
        "given_name": "Moongu"
      }
    ]
  },
  {
    "title": "Bi-projection for 360°image object detection bridged by RoI Searcher",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103660",
    "abstract": "Object detection on 360°images is a vital component of 3D environment perception. The existing methods either treat panoramic images (usually represented in equirectangular projection—ERP) as normal FoV images and endure the distortions or project them into the less-distortion format and narrow the FoV, leading to unsatisfactory performance in practical applications. To solve this problem, we propose a dual-projection 360°object detection network named Bip R-CNN, consisting of three modules: a bi-projection feature extractor, a cross-projection region-of-interest (RoI) searcher, and a classification and regression predictor. Specifically, we extract the equirectangular and corresponding dual-cubemap features simultaneously from the input images. Besides, Projection-Inter Feature Fusion and Projection-Intra Feature Fusion are designed to allow the mutual interaction between the bi-projective features and promote the integration of features at different scales, respectively. In the proposed cross-projection RoI Searcher, we search for the bounding box (BBox) locations on cubemap from the corresponding ERP spherical proposals, bridging the RoIs of two different projection formats at feature level. Finally, the cube proposals are used to detect objects in the last predictor module. Considering the scarceness of the existing panoramic dataset (only indoor scenes), we propose an efficient approach to convert conventional datasets into annotated panoramic datasets without manual intervention, increasing the diversity of panoramic datasets. Extensive experiments are conducted on the synthetic and real-world datasets with spherical criteria, demonstrating our superiority to other state-of-the-art solutions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001808",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Linguistics",
      "Minimum bounding box",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Projection (relational algebra)"
    ],
    "authors": [
      {
        "surname": "Zheng",
        "given_name": "Zishuo"
      },
      {
        "surname": "Lin",
        "given_name": "Chunyu"
      },
      {
        "surname": "Nie",
        "given_name": "Lang"
      },
      {
        "surname": "Liao",
        "given_name": "Kang"
      },
      {
        "surname": "Zhao",
        "given_name": "Yao"
      }
    ]
  },
  {
    "title": "GCENet: Global contextual exploration network for RGB-D salient object detection",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103680",
    "abstract": "Representing contextual features at multiple scales is important for RGB-D SOD. Recently, due to advances in backbone convolutional neural networks (CNNs) revealing stronger multi-scale representation ability, many methods achieved comprising performance. However, most of them represent multi-scale features in a layer-wise manner, which ignores the fine-grained global contextual cues in a single layer. In this paper, we propose a novel global contextual exploration network (GCENet) to explore the performance gain of multi-scale contextual features in a fine-grained manner. Concretely, a cross-modal contextual feature module (CCFM) is proposed to represent the multi-scale contextual features at a single fine-grained level, which can enlarge the range of receptive fields for each network layer. Furthermore, we design a multi-scale feature decoder (MFD) that integrates fused features from CCFM in a top-down way. Extensive experiments on five benchmark datasets demonstrate that the proposed GCENet outperforms the other state-of-the-art (SOTA) RGB-D SOD methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322002000",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Cartography",
      "Chemistry",
      "Computer science",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Feature learning",
      "Geodesy",
      "Geography",
      "Law",
      "Layer (electronics)",
      "Linguistics",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "RGB color model",
      "Representation (politics)",
      "Salient",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "Xia",
        "given_name": "Chenxing"
      },
      {
        "surname": "Duan",
        "given_name": "Songsong"
      },
      {
        "surname": "Gao",
        "given_name": "Xiuju"
      },
      {
        "surname": "Sun",
        "given_name": "Yanguang"
      },
      {
        "surname": "Huang",
        "given_name": "Rongmei"
      },
      {
        "surname": "Ge",
        "given_name": "Bin"
      }
    ]
  },
  {
    "title": "Supporting workspace awareness in remote assistance through a flexible multi-camera system and Augmented Reality awareness cues",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103655",
    "abstract": "Workspace awareness is critical for remote assistance with physical tasks, yet it remains difficult to facilitate. For example, if the remote helper is limited to the single viewpoint provided by the worker’s hand-held or head-mounted camera, she lacks the ability to gain an overview of the workspace. This may be addressed by granting the helper view-independence, e.g., through a multi-camera system. However, it can be cumbersome to set up and calibrate multiple cameras, and it can be challenging for the local worker to identify the current viewpoint of the remote helper. We present CueCam, a multi-camera remote assistance system that supports mutual workspace awareness through a flexible ad-hoc camera calibration and various Augmented Reality cues that communicate the helper’s viewpoint and focus. In particular, we propose visual cues presented through a head-mounted Augmented Reality display (Virtual Hand, Color Cue), and sound cues emitted from the cameras’ physical locations (Spatial Sound). Findings from a lab study indicate that all proposed cues effectively support the worker’s awareness of helper’s location and focus, while the Color Cue demonstrated superiority in task performance and preference ratings during a search task.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001754",
    "keywords": [
      "Artificial intelligence",
      "Augmented reality",
      "Computer science",
      "Computer vision",
      "Engineering",
      "Focus (optics)",
      "Human–computer interaction",
      "Optics",
      "Physics",
      "Programming language",
      "Robot",
      "Set (abstract data type)",
      "Systems engineering",
      "Task (project management)",
      "Workspace"
    ],
    "authors": [
      {
        "surname": "Rasmussen",
        "given_name": "Troels"
      },
      {
        "surname": "Feuchtner",
        "given_name": "Tiare"
      },
      {
        "surname": "Huang",
        "given_name": "Weidong"
      },
      {
        "surname": "Grønbæk",
        "given_name": "Kaj"
      }
    ]
  },
  {
    "title": "Meta-transfer-adjustment learning for few-shot learning",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103678",
    "abstract": "Deep neural network models with strong feature extraction capacity are prone to overfitting and fail to adapt quickly to new tasks with few samples. Gradient-based meta-learning approaches can minimize overfitting and adapt to new tasks fast, but they frequently use shallow neural networks with limited feature extraction capacity. We present a simple and effective approach called Meta-Transfer-Adjustment learning (MTA) in this paper, which enables deep neural networks with powerful feature extraction capabilities to be applied to few-shot scenarios while avoiding overfitting and gaining the capacity for quickly adapting to new tasks via training on numerous tasks. Our presented approach is classified into two major parts, the Feature Adjustment (FA) module, and the Task Adjustment (TA) module. The feature adjustment module (FA) helps the model to make better use of the deep network to improve feature extraction, while the task adjustment module (TA) is utilized for further improve the model’s fast response and generalization capabilities. The proposed model delivers good classification results on the benchmark small sample datasets MiniImageNet and Fewshot-CIFAR100, as proved experimentally.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001985",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Computer science",
      "Deep learning",
      "Engineering",
      "Feature (linguistics)",
      "Feature extraction",
      "Feature learning",
      "Generalization",
      "Geodesy",
      "Geography",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Meta learning (computer science)",
      "Overfitting",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Systems engineering",
      "Task (project management)",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Yadang"
      },
      {
        "surname": "Yan",
        "given_name": "Hui"
      },
      {
        "surname": "Yang",
        "given_name": "Zhi-Xin"
      },
      {
        "surname": "Wu",
        "given_name": "Enhua"
      }
    ]
  },
  {
    "title": "Decomposing style, content, and motion for videos",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103686",
    "abstract": "In this paper, we present the first video decomposition framework, named SyCoMo, that factorizes a video into style, content, and motion. Such a fine-grained decomposition enables flexible video editing, and for the first time allows for tripartite video synthesis. SyCoMo is a unified and domain-agnostic learning framework which can process videos of various object categories without domain-specific design or supervision. Different from other motion decomposition work, SyCoMo derives motion from style-free content by isolating style from content in the first place. Content is organized into subchannels, each of which corresponds to an atomic motion. This design naturally forms an information bottleneck which facilitates a clean decomposition. Experiments show that SyCoMo decomposes videos of various categories into interpretable content subchannels and meaningful motion patterns. Ablation studies also show that deriving motion from style-free content makes the keypoints or landmarks of the object more accurate. We demonstrate the photorealistic quality of the novel tripartite video synthesis in addition to three bipartite synthesis tasks named as style, content, and motion transfer.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322002061",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Content (measure theory)",
      "Mathematical analysis",
      "Mathematics",
      "Motion (physics)",
      "Style (visual arts)",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Yaosi"
      },
      {
        "surname": "Yin",
        "given_name": "Dacheng"
      },
      {
        "surname": "Wang",
        "given_name": "Yuwang"
      },
      {
        "surname": "Chen",
        "given_name": "Zhenzhong"
      },
      {
        "surname": "Luo",
        "given_name": "Chong"
      }
    ]
  },
  {
    "title": "Learn decision trees with deep visual primitives",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103682",
    "abstract": "In this paper, we strive to propose a self-interpretable framework, termed PrimitiveTree, that incorporates deep visual primitives condensed from deep features with a conventional decision tree, bridging the gap between deep features extracted from deep neural networks (DNNs) and trees’ transparent decision-making processes. Specifically, we utilize a codebook, which embeds the continuous deep features into a finite discrete space (deep visual primitives) to distill the most common semantic information. The decision tree adopts the spatial location information and the mapped primitives to present the decision-making process of the deep features in a tree hierarchy. Moreover, the trained interpretable PrimitiveTree can inversely explain the constituents of the deep features, highlighting the most critical and semantic-rich image patches attributing to the final predictions of the given DNN. Extensive experiments and visualization results validate the effectiveness and interpretability of our method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322002024",
    "keywords": [
      "Artificial intelligence",
      "Bridging (networking)",
      "Codebook",
      "Computer network",
      "Computer science",
      "Decision tree",
      "Deep learning",
      "Deep neural networks",
      "Interpretability",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Tree (set theory)",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Xue",
        "given_name": "Mengqi"
      },
      {
        "surname": "Zhang",
        "given_name": "Haofei"
      },
      {
        "surname": "Huang",
        "given_name": "Qihan"
      },
      {
        "surname": "Song",
        "given_name": "Jie"
      },
      {
        "surname": "Song",
        "given_name": "Mingli"
      }
    ]
  },
  {
    "title": "Personality modeling from image aesthetic attribute-aware graph representation learning",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103675",
    "abstract": "Recently, inferring users’ personality traits on social media has attracted extensive attention. Existing studies have shown that users’ personality traits can be inferred from their preferences for images. However, since users’ preferences on images are often affected by multiple factors, some liked images cannot effectively reflect their personality traits. To handle this issue, this paper proposes a personality modeling approach based on image aesthetic attribute-aware graph representation learning, which can leverage aesthetic attributes to refine the liked images that are consistent with users’ personality traits. Specifically, we first utilize a Convolutional Neural Network (CNN) to train an aesthetic attribute prediction module. Then, attribute-aware graph representation learning is introduced to refine the images with similar aesthetic attributes from users’ liked images. Finally, the aesthetic attributes of all refined images are combined to predict personality traits through a Multi-Layer Perceptron (MLP). Experimental results and visual analysis have shown that the proposed method is superior to state-of-the-art personality modeling methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032200195X",
    "keywords": [
      "Artificial intelligence",
      "Big Five personality traits",
      "Cognitive psychology",
      "Computer science",
      "Convolutional neural network",
      "Graph",
      "Law",
      "Leverage (statistics)",
      "Machine learning",
      "Personality",
      "Political science",
      "Politics",
      "Psychology",
      "Representation (politics)",
      "Social psychology",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Hancheng"
      },
      {
        "surname": "Zhou",
        "given_name": "Yong"
      },
      {
        "surname": "Li",
        "given_name": "Qiaoyue"
      },
      {
        "surname": "Shao",
        "given_name": "Zhiwen"
      }
    ]
  },
  {
    "title": "DFD-SS: Document Forgery Detection using Spectral – Spatial Features for Hyperspectral Images",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103690",
    "abstract": "In the present era of machines and edge-cutting technologies, still document frauds persist. They are done intuitively by using almost identical inks, that it becomes challenging to detect them—this demands an approach that efficiently investigates the document and leaves it intact. Hyperspectral imaging is one such a type of approach that captures the images from hundreds to thousands of spectral bands and analyzes the images through their spectral and spatial features, which is not possible by conventional imaging. Deep learning is an edge-cutting technology known for solving critical problems in various domains. Utilizing supervised learning imposes constraints on its usage in real scenarios, as the inks used in forgery are not known prior. Therefore, it is beneficial to use unsupervised learning. An unsupervised feature extraction through a Convolutional Autoencoder (CAE) followed by Logistic Regression (LR) for classification is proposed (CAE-LR). Feature extraction is evolved around spectral bands, spatial patches, and spectral-spatial patches. We inspected the impact of spectral, spatial, and spectral-spatial features by mixing inks in equal and unequal proportion using CAE-LR on the UWA writing ink hyperspectral images dataset for blue and black inks. Hyperspectral images are captured at multiple correlated spectral bands, resulting in information redundancy handled by restoring certain principal components. The proposed approach is compared with eight state-of-art approaches used by the researchers. The results depicted that by using the combination of spectral and spatial patches, the classification accuracy enhanced by 4.85% for black inks and 0.13% for blue inks compared to state-of-art results. In the present scenario, the primary area concern is to identify and detect the almost similar inks used in document forgery, are efficiently managed by the proposed approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322002103",
    "keywords": [
      "Artificial intelligence",
      "Autoencoder",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Feature (linguistics)",
      "Feature extraction",
      "Geology",
      "Hyperspectral imaging",
      "Linguistics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Principal component analysis",
      "Redundancy (engineering)",
      "Remote sensing",
      "Spectral bands",
      "Spectral signature"
    ],
    "authors": [
      {
        "surname": "Jaiswal",
        "given_name": "Garima"
      },
      {
        "surname": "Sharma",
        "given_name": "Arun"
      },
      {
        "surname": "Kumar Yadav",
        "given_name": "Sumit"
      }
    ]
  },
  {
    "title": "Isomorphic model-based initialization for convolutional neural networks",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103677",
    "abstract": "Modern deep convolutional neural networks(CNNs) are often designed to be scalable, leading to the model family concept. A model family is a large (possibly infinite) collection of related neural network architectures. The isomorphism of a model family refers to the fact that the models within it share the same high-level structure. Meanwhile, the models within the model family are called isomorphic models for each other. Existing weight initialization methods for CNNs use random initialization or data-driven initialization. Even though these methods can perform satisfactory initialization, the isomorphism of model families is rarely explored. This work proposes an isomorphic model-based initialization method (IM Init) for CNNs. It can initialize any network with another well-trained isomorphic model in the same model family. We first formulate the widely used general network structure of CNNs. Then a structural weight transformation is presented to transform the weight between two isomorphic models. Finally, we apply our IM Init to the model down-sampling and up-sampling scenarios and confirm its effectiveness in improving accuracy and convergence speed through experiments on various image classification datasets. In the model down-sampling scenario, IM Init initializes the smaller target model with a larger well-trained source model. It improves the accuracy of RegNet200MF by 1.59% on the CIFAR-100 dataset and 1.9% on the CUB200 dataset. Inversely, IM Init initializes the larger target model with a smaller well-trained source model in the model up-sampling scenario. It significantly speeds up the convergence of RegNet600MF and improves the accuracy by 30.10% under short training schedules. Code will be available.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001973",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Convergence (economics)",
      "Convolutional neural network",
      "Crystal structure",
      "Crystallography",
      "Economic growth",
      "Economics",
      "Filter (signal processing)",
      "Gene",
      "Initialization",
      "Isomorphism (crystallography)",
      "Network model",
      "Pattern recognition (psychology)",
      "Programming language",
      "Sampling (signal processing)",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Hong"
      },
      {
        "surname": "Li",
        "given_name": "Yang"
      },
      {
        "surname": "Yang",
        "given_name": "Hanqing"
      },
      {
        "surname": "He",
        "given_name": "Bin"
      },
      {
        "surname": "Zhang",
        "given_name": "Yu"
      }
    ]
  },
  {
    "title": "The encoding method of position embeddings in vision transformer",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103664",
    "abstract": "In contrast to Convolutional Neural Networks (CNNs), Vision Transformers (ViT) cannot capture sequence ordering of input tokens and require position embeddings. As a learnable fixed-dimension vector, the position embedding improves accuracy while limiting the migration of the model between different input sizes. Hence, this paper conducts an empirical study on position embeddings of pre-trained models, which mainly focuses on two questions: (1) What do the position embeddings learn from training? (2) How do the position embeddings affect the self-attention modules? This paper analyzes the pattern of position embedding in pre-trained models and finds that the linear combination of Gabor filters and edge markers can fit the learned position embeddings well. The Gabor filters and edge markers can occupy some channels to append the position information, and the edge markers have flowed to values in self-attention modules. The experimental results can guide future work to choose suitable position embeddings.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001845",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Economics",
      "Electrical engineering",
      "Embedding",
      "Encoding (memory)",
      "Engineering",
      "Finance",
      "Pattern recognition (psychology)",
      "Position (finance)",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Jiang",
        "given_name": "Kai"
      },
      {
        "surname": "Peng",
        "given_name": "Peng"
      },
      {
        "surname": "Lian",
        "given_name": "Youzao"
      },
      {
        "surname": "Xu",
        "given_name": "Weisheng"
      }
    ]
  },
  {
    "title": "No-Reference Video Quality Assessment using novel hybrid features and two-stage hybrid regression for score level fusion",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103676",
    "abstract": "This paper presents a novel No-Reference Video Quality Assessment (NR-VQA) model that utilizes proposed 3D steerable wavelet transform-based Natural Video Statistics (NVS) features as well as human perceptual features. Additionally, we proposed a novel two-stage regression scheme that significantly improves the overall performance of quality estimation. In the first stage, transform-based NVS and human perceptual features are separately passed through the proposed hybrid regression scheme: Support Vector Regression (SVR) followed by Polynomial curve fitting. The two visual quality scores predicted from the first stage are then used as features for the similar second stage. This predicts the final quality scores of distorted videos by achieving score level fusion. Extensive experiments were conducted using five authentic and four synthetic distortion databases. Experimental results demonstrate that the proposed method outperforms other published state-of-the-art benchmark methods on synthetic distortion databases and is among the top performers on authentic distortion databases. The source code is available at https://github.com/anishVNIT/two-stage-vqa.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001961",
    "keywords": [
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Benchmark (surveying)",
      "Code (set theory)",
      "Computer network",
      "Computer science",
      "Data mining",
      "Distortion (music)",
      "Economics",
      "Epistemology",
      "Geodesy",
      "Geography",
      "Mathematics",
      "Metric (unit)",
      "Operating system",
      "Operations management",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Quality (philosophy)",
      "Quality Score",
      "Regression",
      "Set (abstract data type)",
      "Source code",
      "Statistics",
      "Support vector machine",
      "Wavelet",
      "Wavelet transform"
    ],
    "authors": [
      {
        "surname": "Vishwakarma",
        "given_name": "Anish Kumar"
      },
      {
        "surname": "Bhurchandi",
        "given_name": "Kishor M."
      }
    ]
  },
  {
    "title": "Multimodal object tracking by exploiting appearance and class information",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103669",
    "abstract": "In this work, we study the method exploiting natural language network to improve tracking performance. We propose a novel architecture which can combine class and visual information presented in tracking. To this end, we introduce a multimodal feature association network, allowing us to correlate the target class with its appearance during training and aid the localization of the target during inference. Specifically, we first utilize an appearance model to extract the target visual features, from which we obtain appearance cues, for instance shape and color. In order to employ target class information, we design a learned lightweight embedding network to embed the target class into a feature representation. The association network of our architecture contains a multimodal fusion module and a predictor module. The fusion module is used to combine features from class and appearance, yielding multimodal features with more expressive representations for the subsequent module. The predictor module is used to determine the target location in the current frame, from which we associate the class to the appearance. The class embedding module thus can learn appearance cues by exploiting the back-propagation functionality. To verify the abilities of our method, we select the official training and test splits of the LaSOT with annotated images and classes to perform experiments. In particular, we analyze the imbalance in the samples and employ a class validator discriminator to alleviate this problem. Extensive experimental results on LaSOT, UAV20L and UAV123@10fps demonstrate our method achieves competitive results while maintaining a considerable real-time speed.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001894",
    "keywords": [
      "Artificial intelligence",
      "Association (psychology)",
      "Class (philosophy)",
      "Computer science",
      "Computer vision",
      "Detector",
      "Discriminator",
      "Embedding",
      "Epistemology",
      "Eye tracking",
      "Feature (linguistics)",
      "Inference",
      "Law",
      "Linguistics",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Pedagogy",
      "Philosophy",
      "Political science",
      "Politics",
      "Psychology",
      "Representation (politics)",
      "Telecommunications",
      "Tracking (education)",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Mao",
        "given_name": "Zhongjie"
      },
      {
        "surname": "Chen",
        "given_name": "Xi"
      },
      {
        "surname": "Yan",
        "given_name": "Jia"
      },
      {
        "surname": "Qu",
        "given_name": "Tao"
      }
    ]
  },
  {
    "title": "SiamMBFAN: Siamese tracker with multi-branch feature aggregation network",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103671",
    "abstract": "Siamese trackers have attracted considerable attention in the field of object tracking because of their high precision and speed. However, one of the main disadvantages of Siamese trackers is that their feature extraction network is relatively single. They often use AlexNet or ResNet50 as the backbone network. AlexNet is shallow and thus cannot easily extract abundant semantic information, whereas ResNet50 has many convolutional layers, reducing the real-time performance of Siamese trackers. We propose a multi-branch feature aggregation network with different designs in the shallow and deep convolutional layers. We use the residual module to build the shallow convolutional layers to extract textural and edge features. The deep convolution layers, designed with two independent branches, are built with residual and parallel modules to extract different semantic features. The proposed network has a depth of only nine modules, and thus it is a simple and effective network. We then apply the network to a Siamese tracker to form SiamMBFAN. We design multi-layer classification and regression subnetworks in the Siamese tracker by aggregating the last three modules of the two branches, improving the localization ability of the tracker. Our tracker achieves a better balance between performance and speed. Finally, SiamMBFAN is tested on four challenging benchmarks, including OTB100, VOT2016, VOT2018, and UAV123. Compared with other trackers, our tracker improves by 7% (OTB100).",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001912",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Hao"
      },
      {
        "surname": "Piao",
        "given_name": "Yan"
      },
      {
        "surname": "Huang",
        "given_name": "Bailiang"
      },
      {
        "surname": "Tan",
        "given_name": "Baolin"
      }
    ]
  },
  {
    "title": "HLTD-CSA: Cover selection algorithm based on hybrid local texture descriptor for color image steganography",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103646",
    "abstract": "Cover selection is one of the important techniques to improve the security of image steganography. However, existing methods mostly focus on cover selection of grayscale image. In this paper, we propose a novel Cover Selection Algorithm of color images based on Hybrid Local Texture Descriptor, named HLTD-CSA. A green-channel related Local Binary Pattern (LBP) is designed which utilizes local statistics of intra-channel and cross-channel correlations efficiently. Besides, Local Phase Quantization is introduced to serve as a complementary component of our improved LBP. To further enhance the performance of cover selection for color image, a hybrid local texture descriptor (HLTD) is obtained by combining these two types of local texture descriptors with a proper combination strategy. Finally, the proposed algorithm selects images which have larger values of HLTD to construct the cover image set. Extensive experiments are conducted to verify the effectiveness of our method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001663",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Cover (algebra)",
      "Engineering",
      "Feature selection",
      "Grayscale",
      "Histogram",
      "Image (mathematics)",
      "Image processing",
      "Image texture",
      "Local binary patterns",
      "Mathematics",
      "Mechanical engineering",
      "Pattern recognition (psychology)",
      "Selection (genetic algorithm)"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Menghua"
      },
      {
        "surname": "He",
        "given_name": "Peisong"
      },
      {
        "surname": "Liu",
        "given_name": "Jiayong"
      }
    ]
  },
  {
    "title": "DFD-SS: Document Forgery Detection using Spectral – Spatial Features for Hyperspectral Images",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103690",
    "abstract": "In the present era of machines and edge-cutting technologies, still document frauds persist. They are done intuitively by using almost identical inks, that it becomes challenging to detect them—this demands an approach that efficiently investigates the document and leaves it intact. Hyperspectral imaging is one such a type of approach that captures the images from hundreds to thousands of spectral bands and analyzes the images through their spectral and spatial features, which is not possible by conventional imaging. Deep learning is an edge-cutting technology known for solving critical problems in various domains. Utilizing supervised learning imposes constraints on its usage in real scenarios, as the inks used in forgery are not known prior. Therefore, it is beneficial to use unsupervised learning. An unsupervised feature extraction through a Convolutional Autoencoder (CAE) followed by Logistic Regression (LR) for classification is proposed (CAE-LR). Feature extraction is evolved around spectral bands, spatial patches, and spectral-spatial patches. We inspected the impact of spectral, spatial, and spectral-spatial features by mixing inks in equal and unequal proportion using CAE-LR on the UWA writing ink hyperspectral images dataset for blue and black inks. Hyperspectral images are captured at multiple correlated spectral bands, resulting in information redundancy handled by restoring certain principal components. The proposed approach is compared with eight state-of-art approaches used by the researchers. The results depicted that by using the combination of spectral and spatial patches, the classification accuracy enhanced by 4.85% for black inks and 0.13% for blue inks compared to state-of-art results. In the present scenario, the primary area concern is to identify and detect the almost similar inks used in document forgery, are efficiently managed by the proposed approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322002103",
    "keywords": [
      "Artificial intelligence",
      "Autoencoder",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Feature (linguistics)",
      "Feature extraction",
      "Geology",
      "Hyperspectral imaging",
      "Linguistics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Principal component analysis",
      "Redundancy (engineering)",
      "Remote sensing",
      "Spectral bands",
      "Spectral signature"
    ],
    "authors": [
      {
        "surname": "Jaiswal",
        "given_name": "Garima"
      },
      {
        "surname": "Sharma",
        "given_name": "Arun"
      },
      {
        "surname": "Kumar Yadav",
        "given_name": "Sumit"
      }
    ]
  },
  {
    "title": "Adaptive multi-histogram reversible data hiding with contrast enhancement",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103637",
    "abstract": "Unlike existing reversible data hiding with contrast enhancement (RDHCE) methods, which excessively improve the image contrast for achieving the required capacity, the proposed method improves the image contrast appropriately while providing satisfactory embedding capacity. To this end, an adaptive multi-histogram RDHCE method is proposed in this study to improve the local and global contrast by considering the local properties of the histograms. On the one hand, fuzzy C-means clustering combining multiple features that are deliberately designed for contrast enhancement is employed to generate seven sharply-distributed prediction error histograms (PEHs). Subsequently, the genetic algorithm is utilized to adaptively select the optimal pairs achieving the best embedding performance for each PEH according to the local characteristics of PEH distribution, resulting in improving the local contrast adaptively and embedding significant amount of data. Additionally, two-sided histogram shifting (HS) is utilized to improve the global contrast appropriately while embedding reasonable amount of data. The experimental results demonstrate that the proposed method achieves better local and global contrast while providing a high embedding capacity compared with other existing RDHCE methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001572",
    "keywords": [
      "Adaptive histogram equalization",
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Contrast (vision)",
      "Embedding",
      "Fuzzy logic",
      "Histogram",
      "Histogram equalization",
      "Image (mathematics)",
      "Information hiding",
      "Mathematics",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Tiancong"
      },
      {
        "surname": "Yang",
        "given_name": "Caijie"
      },
      {
        "surname": "Weng",
        "given_name": "Shaowei"
      },
      {
        "surname": "Hou",
        "given_name": "Tanshuai"
      }
    ]
  },
  {
    "title": "Language-guided graph parsing attention network for human-object interaction recognition",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103640",
    "abstract": "This paper focuses on the task of human-object interaction (HOI) recognition, which aims to classify the interaction between human and objects. It is a challenging task partially due to the extremely imbalanced data among classes. To solve this problem, we propose a language-guided graph parsing attention network (LG-GPAN) that makes use of the word distribution in language to guide the classification in vision. We first associate each HOI class name with a word embedding vector in language and then all the vectors can construct a language space specified for HOI recognition. Simultaneously, the visual feature is extracted from the inputs via the proposed graph parsing attention network (GPAN) for better visual representation. The visual feature is then transformed into the linguistic one in language space. Finally, the output score is obtained via measuring the distance between the linguistic feature and the word embedding of classes in language space. Experimental results on the popular CAD-120 and V-COCO datasets validate our design choice and demonstrate its superior performance in comparison to the state-of-the-art.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001602",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Embedding",
      "Feature (linguistics)",
      "Feature vector",
      "Geometry",
      "Graph",
      "Graph embedding",
      "Linguistics",
      "Mathematics",
      "Natural language processing",
      "Object (grammar)",
      "Parsing",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Theoretical computer science",
      "Word (group theory)",
      "Word embedding"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Qiyue"
      },
      {
        "surname": "Xie",
        "given_name": "Xuemei"
      },
      {
        "surname": "Zhang",
        "given_name": "Jin"
      },
      {
        "surname": "Shi",
        "given_name": "Guangming"
      }
    ]
  },
  {
    "title": "Context-dependent emotion recognition",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103679",
    "abstract": "Most previous methods for emotion recognition focus on facial emotion and ignore the rich context information that implies important emotion states. To make full use of the contextual information to make up for the facial information, we propose the Context-Dependent Net (CD-Net) for robust context-aware human emotion recognition. Inspired by the long-range dependency of the transformer, we introduce the tubal transformer which forms the shared feature representation space to facilitate the interactions among the face, body, and context features. Besides, we introduce the hierarchical feature fusion to recombine the enhanced multi-scale face, body, and context features for emotion classification. Experimentally, we verify the effectiveness of the proposed CD-Net on the two large emotion datasets, CAER-S and EMOTIC. On the one hand, the quantitative evaluation results demonstrate the superiority of the proposed CD-Net over other state-of-the-art methods. On the other hand, the visualization results show CD-Net can capture the dependencies among the face, body, and context components and focus on the important features related to the emotion.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001997",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Context model",
      "Dependency (UML)",
      "Emotion recognition",
      "Facial expression",
      "Feature (linguistics)",
      "Focus (optics)",
      "Linguistics",
      "Object (grammar)",
      "Optics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Transformer",
      "Visualization",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Zili"
      },
      {
        "surname": "Lao",
        "given_name": "Lingjie"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiaoya"
      },
      {
        "surname": "Li",
        "given_name": "Yong"
      },
      {
        "surname": "Zhang",
        "given_name": "Tong"
      },
      {
        "surname": "Cui",
        "given_name": "Zhen"
      }
    ]
  },
  {
    "title": "Fine-grained neural architecture search for image super-resolution",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103654",
    "abstract": "Designing efficient deep neural networks has achieved great interest in image super-resolution (SR). However, exploring diverse network structures is computationally expensive. More importantly, each layer in a network has a distinct role that leads to the design of a specialized structure. In this work, we present a novel neural architecture search (NAS) algorithm that efficiently explores layer-wise structures. Specifically, we construct a supernet allowing flexibility in choosing the number of channels and per-channel activation functions according to the role of each layer. The search process runs efficiently via channel pruning since gradient descent jointly optimizes the Mult-Adds and the accuracy of the searched models. We facilitate estimating the model Mult-Adds in a differentiable manner using relaxations in the backward pass. The searched model, named FGNAS, outperforms the state-of-the-art NAS-based SR methods by a large margin.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001742",
    "keywords": [
      "Archaeology",
      "Architecture",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Geography",
      "Image (mathematics)",
      "Pattern recognition (psychology)",
      "Resolution (logic)"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Heewon"
      },
      {
        "surname": "Hong",
        "given_name": "Seokil"
      },
      {
        "surname": "Han",
        "given_name": "Bohyung"
      },
      {
        "surname": "Myeong",
        "given_name": "Heesoo"
      },
      {
        "surname": "Lee",
        "given_name": "Kyoung Mu"
      }
    ]
  },
  {
    "title": "Bounding box regression with balance for harmonious object detection",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103665",
    "abstract": "Localization is an essential part of object detection, which is usually accomplished by bounding box regression guided by ℓ n -norm-based or IoU-based loss functions, where IoU is known for its scale-invariant characteristics. However, introducing the scale-invariance into regression loss in traditional IoU-based methods may result in a bias in favor of smaller boxes and cause redundancy and unstable oscillations. To make up for these shortages of IoU-based losses, we propose a Scale-Balanced Factor (SF) that stabilizes the regression process via a simple adaptive factor. Furthermore, to compensate for the imbalance of different types of losses caused by SF and other IoU-based loss functions, regression losses are always multiplied by a hyperparameter, which is purely empirical and is hard to find an optimum. To address this issue, a Multi-Task Reinforced Equilibrium (MRE) is proposed to dynamically tweak the learning rate of each task based on reinforcement learning. The MRE can guarantee more balanced parameters and maximize the benefit of SF or other improvement methods for IoU. By incorporating the proposed SF and MRE into the classic detectors (RetinaNet, YOLO, and Faster R-CNN, etc.), we have achieved significant performance gains on MS COCO (0.8 AP ∼ 1.9 AP) and PASCAL VOC (0.6 AP ∼ 2.2 AP).",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001857",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Bounding overwatch",
      "Computer science",
      "Economic shortage",
      "Government (linguistics)",
      "Linguistics",
      "Mathematics",
      "Operating system",
      "Pascal (unit)",
      "Philosophy",
      "Programming language",
      "Redundancy (engineering)",
      "Regression",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Chenzhong"
      },
      {
        "surname": "Gong",
        "given_name": "Xun"
      }
    ]
  },
  {
    "title": "Accelerating BPC-PaCo through Visually Lossless Techniques",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103672",
    "abstract": "Fast image codecs are a current need in applications that deal with large amounts of images. Graphics Processing Units (GPUs) are suitable processors to speed up most kinds of algorithms, especially when they allow fine-grain parallelism. Bitplane Coding with Parallel Coefficient processing (BPC-PaCo) is a recently proposed algorithm for the core stage of wavelet-based image codecs tailored for the highly parallel architectures of GPUs. This algorithm provides complexity scalability to allow faster execution at the expense of coding efficiency. Its main drawback is that the speedup and loss in image quality is controlled only roughly, resulting in visible distortion at low and medium rates. This paper addresses this issue by integrating techniques of visually lossless coding into BPC-PaCo. The resulting method minimizes the visual distortion introduced in the compressed file, obtaining higher-quality images to a human observer. Experimental results also indicate 12% speedups with respect to BPC-PaCo.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001924",
    "keywords": [
      "Algorithm",
      "Codec",
      "Coding (social sciences)",
      "Computer graphics (images)",
      "Computer hardware",
      "Computer science",
      "Data compression",
      "Database",
      "Graphics",
      "Lossless compression",
      "Mathematics",
      "Parallel computing",
      "Scalability",
      "Speedup",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Aulí-Llinàs",
        "given_name": "Francesc"
      },
      {
        "surname": "de Cea-Dominguez",
        "given_name": "Carlos"
      },
      {
        "surname": "Hernández-Cabronero",
        "given_name": "Miguel"
      }
    ]
  },
  {
    "title": "AS-Net: An attention-aware downsampling network for point clouds oriented to classification tasks",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103639",
    "abstract": "3D point cloud has tremendous potential in many application tasks. However, the huge amount of data limits this potential. To simplify point clouds and improve their downstream application efficiency, this paper proposes AS-Net, an attention-aware downsampling network oriented to classification tasks. AS-Net realizes downsampling through an Attention-aware Sampling Module, which including an Input Embedding Module and an Attention Module. The former is designed to extract the global and local features of the point cloud, the latter is to generate the Sampling-Map to simulate the differentiable downsampling. Thanks to the attention mechanism, AS-Net may select the critical points of the original point cloud for classification tasks. In addition, AS-Net designs a Constraint Matching Module to match the sampled points to be a subset of the original point cloud at the inference phase. For end-to-end training, AS-Net construct a joint loss function that includes a task loss, a sampling loss, and a constraint loss. Extensive experiments on the ModelNet10/40 and ShapeNet datasets demonstrate that AS-Net achieves a good performance on the point cloud classification task. Especially when the downsampling size is small, the result is better than the referenced methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001596",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cloud computing",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Deep learning",
      "Economics",
      "Embedding",
      "Filter (signal processing)",
      "Geometry",
      "Image (mathematics)",
      "Inference",
      "Management",
      "Mathematics",
      "Net (polyhedron)",
      "Operating system",
      "Point (geometry)",
      "Point cloud",
      "Sampling (signal processing)",
      "Softmax function",
      "Task (project management)",
      "Upsampling"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Yakun"
      },
      {
        "surname": "Wang",
        "given_name": "Anhong"
      },
      {
        "surname": "Bu",
        "given_name": "Donghan"
      },
      {
        "surname": "Feng",
        "given_name": "Zewen"
      },
      {
        "surname": "Liang",
        "given_name": "Jie"
      }
    ]
  },
  {
    "title": "Joint strong edge and multi-stream adaptive fusion network for non-uniform image deblurring",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103663",
    "abstract": "Non-uniform motion deblurring has been a challenging problem in the field of computer vision. Currently, deep learning-based deblurring methods have made promising achievements. In this paper, we propose a new joint strong edge and multi-stream adaptive fusion network to achieve non-uniform motion deblurring. The edge map and the blurred map are jointly used as network inputs and Edge Extraction Network (EEN) guides the Deblurring Network (DN) for image recovery and to complement the important edge information. The Multi-stream Adaptive Fusion Module (MAFM) adaptively fuses the edge information and features from the encoder and decoder to reduce feature redundancy to avoid image artifacts. Furthermore, the Dense Attention Feature Extraction Module (DAFEM) is designed to focus on the severely blurred regions of blurry images to obtain important recovery information. In addition, an edge loss function is added to measure the difference of edge features between the generated and clear images to further recover the edges of the deblurred images. Experiments show that our method outperforms currently public methods in terms of PSNR, SSIM and VIF, and generates images with less blur and sharper edges.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001833",
    "keywords": [
      "Architectural engineering",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Deblurring",
      "Engineering",
      "Enhanced Data Rates for GSM Evolution",
      "Fusion",
      "Image (mathematics)",
      "Image fusion",
      "Image processing",
      "Image restoration",
      "Joint (building)",
      "Linguistics",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Zihan"
      },
      {
        "surname": "Cui",
        "given_name": "Guangmang"
      },
      {
        "surname": "Zhao",
        "given_name": "Jufeng"
      },
      {
        "surname": "Xiang",
        "given_name": "Qinlei"
      },
      {
        "surname": "He",
        "given_name": "Bintao"
      }
    ]
  },
  {
    "title": "Hardware implementation of HEVC CABAC binarization/de-binarization",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103673",
    "abstract": "High efficiency video coding (HEVC) video codec applies different techniques in order to achieve high compression ratios and video quality that supports real-time applications. One of the critical techniques in HEVC is the Context adaptive Binary Arithmetic Coding (CABAC) which is type of entropy coding. CABAC comes at the cost of increased computational complexity, especially for parallelization and pipeline of these blocks: binarization, context modeling and binary arithmetic encoding. The Binarization (BZ) and de-Binarization (DBZ) methods are considered as important techniques in HEVC CABAC encoder and decoder respectively. Indeed, an important goal is to get high throughput in hardware architectures of CABAC BZ and DBZ in order to achieve high resolution applications. This work is the only one found on recent literature which focuses on design and implementation of full BZ and full DBZ compatible with H.265 and H.264. Consequently, a hardware architectures of BZ and DBZ are designed and implemented by using VHDL language, targeted an FPGA virtex4 xc4vsx25-12ff668 board and emulated with ModelSim. As a result, the implementation of BZ and DBZ can process 2 bins/cycle for each syntax element when operated at 697.83 MHz and 789.26 MHz, respectively. The proposed designs exhibits an improved high-throughput of 1395.66 Mbins/s for BZ and 1578.52 Mbins/s for the DBZ. The obtained Area Efficiencies in our proposed BZ and DBZ are about 0.544 Mbins/s/slices and 0.606 Mbins/s/slices, respectively, and it is better than many recent works.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001936",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Binary number",
      "Coding (social sciences)",
      "Computer hardware",
      "Computer science",
      "Context-adaptive binary arithmetic coding",
      "Context-adaptive variable-length coding",
      "Data compression",
      "Decoding methods",
      "Encoder",
      "Entropy encoding",
      "Field-programmable gate array",
      "Macroblock",
      "Mathematics",
      "ModelSim",
      "Operating system",
      "Parallel computing",
      "Statistics",
      "Stratix",
      "VHDL"
    ],
    "authors": [
      {
        "surname": "Menasri",
        "given_name": "Wahiba"
      },
      {
        "surname": "Djabri",
        "given_name": "Manel"
      },
      {
        "surname": "Chennoufi",
        "given_name": "Sarah"
      },
      {
        "surname": "Skoudarli",
        "given_name": "Abdellah"
      },
      {
        "surname": "Bouhedda",
        "given_name": "Mounir"
      },
      {
        "surname": "Benzineb",
        "given_name": "Omar"
      }
    ]
  },
  {
    "title": "Multi-scale gradient attention guidance and adaptive style fusion for image inpainting",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103681",
    "abstract": "Image inpainting aims to fill in the missing regions of damaged images with plausible content. Existing inpainting methods tend to produce ambiguous artifacts and implausible structures. To address the above issues, our method aims to fully utilize the information of known regions to provide style and structural guidance for missing regions. Specifically, the Adaptive Style Fusion (ASF) module reduces artifacts by transferring visual style features from known regions to missing regions. The Gradient Attention Guidance (GAG) module generates accurate structures by aggregating semantic information along gradient boundary regions. In addition, the Multi-scale Attentional Feature Extraction (MAFE) module extracts global contextual information and enhances the representation of image features. The sufficient experimental results on the three datasets demonstrate that our proposed method has superior performance in terms of visual plausibility and structural consistency compared to state-of-the-art inpainting methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322002012",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Boundary (topology)",
      "Cartography",
      "Computer science",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Feature (linguistics)",
      "Geography",
      "Image (mathematics)",
      "Inpainting",
      "Law",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Scale (ratio)",
      "Style (visual arts)"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Ye"
      },
      {
        "surname": "Wang",
        "given_name": "Chao"
      },
      {
        "surname": "Geng",
        "given_name": "Shuze"
      },
      {
        "surname": "Yu",
        "given_name": "Yang"
      },
      {
        "surname": "Hao",
        "given_name": "Xiaoke"
      }
    ]
  },
  {
    "title": "Contour enhanced image super-resolution",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103659",
    "abstract": "Recently, very deep convolution neural network (CNN) has shown strong ability in single image super-resolution (SISR) and has obtained remarkable performance. However, most of the existing CNN-based SISR methods rarely explicitly use the high-frequency information of the image to assist the image reconstruction, thus making the reconstructed image looks blurred. To address this problem, a novel contour enhanced Image Super-Resolution by High and Low Frequency Fusion Network (HLFN) is proposed in this paper. Specifically, a contour learning subnetwork is designed to learn the high-frequency information, which can better learn the texture of the image. In order to reduce the redundancy of the contour information learned by the contour learning subnetwork during fusion, the spatial channel attention block (SCAB) is introduced, which can select the required high-frequency information adaptively. Moreover, a contour loss is designed and it is used with the ℓ 1 loss to optimize the network jointly. Comprehensive experiments demonstrate the superiority of our HLFN over state-of-the-art SISR methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001791",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Block (permutation group theory)",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Geometry",
      "Image (mathematics)",
      "Image fusion",
      "Image resolution",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Redundancy (engineering)",
      "Subnetwork",
      "Superresolution"
    ],
    "authors": [
      {
        "surname": "Kong",
        "given_name": "Linhua"
      },
      {
        "surname": "Wang",
        "given_name": "Yiming"
      },
      {
        "surname": "Chang",
        "given_name": "Dongxia"
      },
      {
        "surname": "Zhao",
        "given_name": "Yao"
      }
    ]
  },
  {
    "title": "Multi-level mutual supervision for cross-domain Person Re-identification",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103674",
    "abstract": "The challenges of cross-domain person re-identification mainly derive from two aspects: (1) The missing of target data labels. (2) The bias between source domain and target domain. Most of existing works focus on only one problem in the above two or deal with them separately. In this paper, we propose a new approach referred as to multi-level mutual supervision to achieve full utilization of labeled source data and unlabeled target data. Along this approach, we construct a dual-branch framework of which the upper branch is trained with original source data and target data while the lower branch is trained with augmented source data and target data. By applying common-pseudo-label and Maximum Mean Discrepancy (MMD) loss in our framework, the mutual supervision in multi levels is achieved. The results show that our model achieves SOTA performance on multiple popular benchmark datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001948",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Botany",
      "Computer science",
      "Construct (python library)",
      "Data mining",
      "Data source",
      "Domain (mathematical analysis)",
      "Dual (grammatical number)",
      "Focus (optics)",
      "Geodesy",
      "Geography",
      "Identification (biology)",
      "Labeled data",
      "Literature",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Mutual information",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Programming language"
    ],
    "authors": [
      {
        "surname": "Tang",
        "given_name": "Chunren"
      },
      {
        "surname": "Xue",
        "given_name": "Dingyu"
      },
      {
        "surname": "Chen",
        "given_name": "Dongyue"
      }
    ]
  },
  {
    "title": "Infrared dim and small target detection based on U-Transformer",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103684",
    "abstract": "Infrared dim and small target detection is a key technology for space-based infrared search and tracking systems. Traditional detection methods have a high false alarm rate and fail to handle complex background and high-noise scenarios. Also, the methods cannot effectively detect targets on a small scale. In this paper, a U-Transformer method is proposed, and a transformer is introduced into the infrared dim and small target detection. First, a U-shaped network is constructed. In the encoder part, the self-attention mechanism is used for infrared dim and small target feature extraction, which helps to solve the problems of losing dim and small target features of deep networks. Meanwhile, by using the encoding and decoding structure, infrared dim and small target features are filtered from the complex background while the shallow features and semantic information of the target are retained. Experiments show that anchor-free and transformer have great potential for infrared dim and small target detection. On the datasets with a complex background, our method outperforms the state-of-the-art detectors and meets the real-time requirement. The code is publicly available at https://github.com/Linaom1214/U-Transformer.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322002048",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Decoding methods",
      "Detector",
      "Electrical engineering",
      "Encoder",
      "Engineering",
      "False alarm",
      "Infrared",
      "Operating system",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Telecommunications",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Lin",
        "given_name": "Jian"
      },
      {
        "surname": "Zhang",
        "given_name": "Kai"
      },
      {
        "surname": "Yang",
        "given_name": "Xi"
      },
      {
        "surname": "Cheng",
        "given_name": "Xiangzheng"
      },
      {
        "surname": "Li",
        "given_name": "Chenhui"
      }
    ]
  },
  {
    "title": "VI-NET: A hybrid deep convolutional neural network using VGG and inception V3 model for copy-move forgery classification",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103644",
    "abstract": "Nowadays, various image editing tools are available that can be utilized for manipulating the original images; here copy-move forgery is most common forgery. In copy-move forgery, some part of the original image is copied and pasted into the same image at some other location. However, Artificial Intelligence (AI) based approaches can extract manipulated features easily. In this study, a deep learning-based method is proposed to classify the copy-move forged images. For classifying the forged images, a deep learning (DL) based hybrid model is presented named as VI-NET using fusion of two DL architectures, i.e., VGG16 and Inception V3. Further, output of two models is concatenated and connected with two additional convolutional layers. Cross-validation protocols, K10 (90 % training, 10 % testing), K5 (80 % training, 20 % testing), and K2 (50 % training, 50 % testing) are applied on the COMOFOD dataset. Moreover, the performance of VI-NET is compared with transfer learning and machine learning models using evaluation metrics such as accuracy, precision, recall, F1 score, etc. Proposed hybrid model performed better than other approaches with classification accuracy of 99 ± 0.2 % in comparison to accuracy of 95 ± 4 % (Inception V3), 93 ± 5 % (MobileNet), 59 ± 8 % (VGG16), 60 ± 1 % (Decision tree), 87 ± 1 % (KNN), 54 ± 1 % (Naïve Bayes) and 65 ± 1 % (random forest) under K10 protocol. Similarly, results are evaluated based on K2 and K5 validation protocols. It is experimentally observed that the proposed model performance is better than existing standard and customized deep learning architectures.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032200164X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convolutional neural network",
      "Decision tree",
      "Deep learning",
      "F1 score",
      "Image (mathematics)",
      "Machine learning",
      "Naive Bayes classifier",
      "Pattern recognition (psychology)",
      "Random forest",
      "Support vector machine",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Kumar",
        "given_name": "Sanjeev"
      },
      {
        "surname": "Gupta",
        "given_name": "Suneet K."
      },
      {
        "surname": "Kaur",
        "given_name": "Manjit"
      },
      {
        "surname": "Gupta",
        "given_name": "Umesh"
      }
    ]
  },
  {
    "title": "Siamese visual tracking with multilayer feature fusion and corner distance IoU loss",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103687",
    "abstract": "The tracker based on the Siamese network regards tracking tasks as solving a similarity problem between the target template and search area. Using shallow networks and offline training, these trackers perform well in simple scenarios. However, due to the lack of semantic information, they have difficulty meeting the accuracy requirements of the task when faced with complex backgrounds and other challenging scenarios. In response to this problem, we propose a new model, which uses the improved ResNet-22 network to extract deep features with more semantic information. Multilayer feature fusion is used to obtain a high-quality score map to reduce the influence of interference factors in the complex background on the tracker. In addition, we propose a more powerful Corner Distance IoU (intersection over union) loss function so that the algorithm can better regression to the bounding box. In the experiments, the tracker was extensively evaluated on the object tracking benchmark data sets, OTB2013 and OTB2015, and the visual object tracking data sets, VOT2016 and VOT2017, and achieved competitive performance, proving the effectiveness of this method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322002073",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Fusion",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Pedagogy",
      "Philosophy",
      "Psychology",
      "Tracking (education)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Weisheng"
      },
      {
        "surname": "Zhu",
        "given_name": "Junye"
      }
    ]
  },
  {
    "title": "Weakly supervised learning based on hypergraph manifold ranking",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103666",
    "abstract": "Significant challenges still remain despite the impressive recent advances in machine learning techniques, particularly in multimedia data understanding. One of the main challenges in real-world scenarios is the nature and relation between training and test datasets. Very often, only small sets of coarse-grained labeled data are available to train models, which are expected to be applied on large datasets and fine-grained tasks. Weakly supervised learning approaches handle such constraints by maximizing useful training information in labeled and unlabeled data. In this research direction, we propose a weakly supervised approach that analyzes the dataset manifold to expand the available labeled set. A hypergraph manifold ranking algorithm is exploited to represent the contextual similarity information encoded in the unlabeled data and identify strong similarity relations, which are taken as a path to label expansion. The expanded labeled set is subsequently exploited for a more comprehensive and accurate training process. The proposed model was evaluated jointly with supervised and semi-supervised classifiers, including Graph Convolutional Networks. The experimental results on image and video datasets demonstrate significant gains and accurate results for different classifiers in diverse scenarios.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001869",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Dimensionality reduction",
      "Hypergraph",
      "Mathematics",
      "Nonlinear dimensionality reduction",
      "Pattern recognition (psychology)",
      "Ranking (information retrieval)"
    ],
    "authors": [
      {
        "surname": "Presotto",
        "given_name": "João Gabriel Camacho"
      },
      {
        "surname": "dos Santos",
        "given_name": "Samuel Felipe"
      },
      {
        "surname": "Valem",
        "given_name": "Lucas Pascotti"
      },
      {
        "surname": "Faria",
        "given_name": "Fabio Augusto"
      },
      {
        "surname": "Papa",
        "given_name": "João Paulo"
      },
      {
        "surname": "Almeida",
        "given_name": "Jurandy"
      },
      {
        "surname": "Pedronette",
        "given_name": "Daniel Carlos Guimarães"
      }
    ]
  },
  {
    "title": "Image copy-move forgery detection based on dynamic threshold with dense points",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103658",
    "abstract": "Copy-move tampering is one of the most popular tampering techniques at present. The tampered region of the image has good fusion with the original image, which increases the difficulty of detection. After years of research, the current detection method based on key points still has the following problems: 1) Failure to achieve forgery detection of small areas/self-similar areas/smooth areas, 2) Lack of reasonable feature point extraction methods, 3) The various stages of copy-move forgery detection (CMFD) work are relatively independent and lack close connections, 4) A fixed threshold is used as the region of interest similarity metric in the matching and localization stages. The failure to consider tampered images and the diversity of tampered regions leads to the limited detection capability of the algorithm. Considering the actual situation of tampering with the picture, to solve the above problems, we propose a copy-move forgery detection method based on the dynamic threshold. First, we determine the point extraction strategy in each super pixel block according to the size of the simple line interface calculation (SLIC) super pixel block and the Weber local descriptor (WLD) descriptor to ensure the reasonable allocation of feature points and reduce unnecessary points. These key points are then characterized by the scaling, flip and rotation invariants of the fractional general Jacobi-Fourier moments (FJFMs). Then, the matching and mismatch filtering thresholds of each feature point are determined through the WLD and SLIC features, and the SLIC feature is used to replace the distance threshold to improve the detection accuracy of small manufacturing areas. Finally, based on the matching results and SLIC features, an effective positioning method is proposed to improve the speed and accuracy of positioning. Experimental results show that the proposed algorithm is superior to the classic methods in recent years in terms of time and accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032200178X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Geometry",
      "Image (mathematics)",
      "Linguistics",
      "Matching (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Point (geometry)",
      "Rotation (mathematics)",
      "Similarity (geometry)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Xiangyang"
      },
      {
        "surname": "Chen",
        "given_name": "Wencong"
      },
      {
        "surname": "Niu",
        "given_name": "Panpan"
      },
      {
        "surname": "Yang",
        "given_name": "Hongying"
      }
    ]
  },
  {
    "title": "VirtualActionNet: A strong two-stream point cloud sequence network for human action recognition",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103641",
    "abstract": "In this paper, we propose a strong two-stream point cloud sequence network VirtualActionNet for 3D human action recognition. In the data preprocessing stage, we transform the depth sequence into a point cloud sequence as the input of our VirtualActionNet. In order to encode intra-frame appearance structures, static point cloud technologies are first employed as a virtual action generation sequence module to abstract the point cloud sequence into a virtual action sequence. Then, a two-stream network framework is presented to model the virtual action sequence. Specifically, we design an appearance stream module for aggregating all the appearance information preserved in each virtual action frame. Moreover, a motion stream module is introduced to capture dynamic changes along the time dimension. Finally, a joint loss strategy is adopted during data training to improve the action prediction accuracy of the two-stream network. Extensive experiments on three publicly available datasets demonstrate the effectiveness of the proposed VirtualActionNet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001614",
    "keywords": [
      "Action (physics)",
      "Artificial intelligence",
      "Biology",
      "Cloud computing",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Data pre-processing",
      "Dimension (graph theory)",
      "Frame (networking)",
      "Genetics",
      "Geometry",
      "Mathematics",
      "Operating system",
      "Physics",
      "Point (geometry)",
      "Point cloud",
      "Preprocessor",
      "Pure mathematics",
      "Quantum mechanics",
      "Sequence (biology)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Xing"
      },
      {
        "surname": "Huang",
        "given_name": "Qian"
      },
      {
        "surname": "Wang",
        "given_name": "Zhijian"
      },
      {
        "surname": "Yang",
        "given_name": "Tianjin"
      }
    ]
  },
  {
    "title": "JFLN: Joint Feature Learning Network for 2D sketch based 3D shape retrieval",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103668",
    "abstract": "Cross-modal retrieval attracts much research attention due to its wide applications in numerous search systems. Sketch based 3D shape retrieval is a typical challenging cross-modal retrieval task for the huge divergence between sketch modality and 3D shape view modality. Existing approaches project the sketches and shapes into a common space for feature update and data alignment. However, these methods contain several disadvantages: Firstly, the majority approaches ignore the modality-shared information for divergence compensation in descriptor generation process. Secondly, traditional fusion method of multi-view features introduces much redundancy, which decreases the discrimination of shape descriptors. Finally, most approaches only focus on the cross-modal alignment, which omits the modality-specific data relevance. To address these limitations, we propose a Joint Feature Learning Network (JFLN). Firstly, we design a novel modality-shared feature extraction network to exploit both modality-specific characteristics and modality-shared information for descriptor generation. Subsequently, we introduce a hierarchical view attention module to gradually focus on the effective information for multiview feature updating and aggregation. Finally, we propose a novel cross-modal feature learning network, which can simultaneously contribute to modality-specific data distribution and cross-modal data alignment. We conduct exhaustive experiments on three public databases. The experimental results validate the superiority of the proposed method. Full Codes are available at https://github.com/dlmuyy/JFLN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001882",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Data mining",
      "Exploit",
      "Feature (linguistics)",
      "Feature extraction",
      "Feature learning",
      "Focus (optics)",
      "Law",
      "Linguistics",
      "Machine learning",
      "Modal",
      "Modality (human–computer interaction)",
      "Operating system",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Political science",
      "Polymer chemistry",
      "Redundancy (engineering)",
      "Relevance (law)",
      "Sketch"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Yue"
      },
      {
        "surname": "Liang",
        "given_name": "Qi"
      },
      {
        "surname": "Ma",
        "given_name": "Ruixin"
      },
      {
        "surname": "Nie",
        "given_name": "Weizhi"
      },
      {
        "surname": "Su",
        "given_name": "Yuting"
      }
    ]
  },
  {
    "title": "ETS-3D: An Efficient Two-Stage Framework for Stereo 3D Object Detection",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103634",
    "abstract": "We propose an efficient two-stage framework for stereo 3D object detection, called ETS-3D. Contrary to many recent approaches that rely on depth maps predicted using time-consuming stereo matching models, our approach utilizes the well-designed features to generate high-quality 3D proposals in stage-1, without explicitly exploiting predicted depth map. Specifically, we leverage pixel-wise correlation to produce normalized cost volumes to weight the left image features, and fuse multi-scale weighted features to obtain the weighted and fused features for 3D proposal generation. To maintain fast computation, only the filtered positive 3D proposals are fed into the stage-2 sub-network for further proposal refinement and quality prediction. Furthermore, we reconstruct the 3D proposal features in stage-2 to make use of different feature representations, achieving more accurate detection results. The experimental results on the KITTI 3D object detection benchmark demonstrate that our method achieves state-of-the-art performance, and can run at more than 10 fps.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001547",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Geology",
      "Object (grammar)",
      "Paleontology",
      "Stage (stratigraphy)"
    ],
    "authors": [
      {
        "surname": "Ji",
        "given_name": "Chaofeng"
      },
      {
        "surname": "Liu",
        "given_name": "Guizhong"
      },
      {
        "surname": "Zhao",
        "given_name": "Dan"
      }
    ]
  },
  {
    "title": "Motion detection in moving camera videos using background modeling and FlowNet",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103616",
    "abstract": "Real-time moving object detection is challenging for moving cameras due to the moving background. Many studies use homography matrix to compensate for global motion by warping the background model to the current frame. Then, the pixel difference between the current frame and the background model is used for background subtraction. Moving pixels are extracted by applying adaptive threshold and some post-processing techniques. On the other hand, deep learning-based dense optical flow can be efficient enough to extract the moving pixels, but it increases computational cost. This study proposes a method to enhance a classical background modeling method with deep learning-based dense optical flow. The main contribution of this paper is to propose a fusing algorithm for dense optical flow and background modeling approach. The background modeling methods are error-prone, especially with continuous camera movement, while the optical flow method alone may not always be efficient. Our hybrid method fuses both techniques to improve the detection accuracy. We propose a software architecture to run background modeling and dense optical flow methods in parallel processes. The proposed implementation approach significantly increases the method’s working speed, while the proposed fusion and combining strategy improve detection results. The experimental results show that the proposed method can run at high speed and has satisfying performance against the methods in the literature.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001407",
    "keywords": [
      "Artificial intelligence",
      "Background subtraction",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Foreground detection",
      "Frame (networking)",
      "Homography",
      "Image (mathematics)",
      "Image warping",
      "Mathematics",
      "Motion (physics)",
      "Motion detection",
      "Motion estimation",
      "Object detection",
      "Optical flow",
      "Pattern recognition (psychology)",
      "Pixel",
      "Projective space",
      "Projective test",
      "Statistics",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Delibasoglu",
        "given_name": "Ibrahim"
      },
      {
        "surname": "Kosesoy",
        "given_name": "Irfan"
      },
      {
        "surname": "Kotan",
        "given_name": "Muhammed"
      },
      {
        "surname": "Selamet",
        "given_name": "Feyza"
      }
    ]
  },
  {
    "title": "Dual attention interactive fine-grained classification network based on data augmentation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103632",
    "abstract": "The key to fine-grained image classification is to find discriminative regions. Most existing methods only use simple baseline networks or low-recognition attention modules to discover object differences, which will limit the model to finding discriminative regions hidden in images. This article proposes an effective method to solve this problem. The first is a novel layered training method, which uses a new training method to enhance the feature extraction ability of the baseline model. The second step focuses on key regions of the image based on improved long short-term memory (LSTM) and multi-head attention. In the third step, based on the feature map obtained by the dual attention network, spatial mapping is performed by a multi-layer perceptron (MLP). Then the element-by-element mutual multiplication calculation of the channel is performed to obtain a feature map with finer granularity. Finally, the CUB-200-2011, FGVC Aircraft, Stanford Cars, and MedMNIST v2 datasets achieved good performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001523",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Computer science",
      "Dual (grammatical number)",
      "Literature"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Qiangxi"
      },
      {
        "surname": "Kuang",
        "given_name": "Wenlan"
      },
      {
        "surname": "Li",
        "given_name": "Zhixin"
      }
    ]
  },
  {
    "title": "PTR-CNN for in-loop filtering in video coding",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103615",
    "abstract": "A deep learning method called PTR-CNN (Predicted frame with Transform unit partition and prediction Residual aided CNN) is proposed for in-loop filtering in video compression. To reduce the computational complexity of an end-to-end CNN in-loop filter, a non-learning method of reference frame selection is designed to select the highest quality frame based on the frame’s blurriness and smoothiness scores. The transform unit (TU) partition and the prediction residual (PR) of the current frame are used as extra inputs to the neural network as the filtering guidance. The selected similar and high quality reference frame (RF) and the current unfiltered frame (CUF) are input to a CNN based motion compensation module to generate a predicted frame (PF). Finally input the PF, the CUF, the CUF’s TU partition and the CUF’s PR into the main CNN to reconstruct the filtered frame. The model is implemented in Tensorflow and tested in HEVC and AV1. Experimental results show that the complexity of proposed PTR-CNN is less than SOTA CNN-based reference aided in-loop filtering methods and slightly outperforms their RD performance. The scheme introduces a complexity overhead of 7% on the encoder. In particular, for random access, the proposed model achieves 11.78% coding gain over HEVC with DBF/SAO off, while has a gain of 4.76% over HEVC with DBF/SAO on. Ablation study demonstrates that the RF contributes about 10% of the total gain, and the TU and PR contribute over 4% of the total one, proving the effectiveness of each module. Moreover, it is observed that the proposed method can restore detailed structures and textures and hence improve the subjective quality.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001390",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Coding (social sciences)",
      "Compression artifact",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Encoder",
      "Frame (networking)",
      "Image (mathematics)",
      "Image compression",
      "Image processing",
      "Mathematics",
      "Operating system",
      "Reference frame",
      "Residual",
      "Statistics",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Shao",
        "given_name": "Tong"
      },
      {
        "surname": "Liu",
        "given_name": "Tianqi"
      },
      {
        "surname": "Wu",
        "given_name": "Dapeng"
      },
      {
        "surname": "Tsai",
        "given_name": "Chia-Yang"
      },
      {
        "surname": "Lei",
        "given_name": "Zhijun"
      },
      {
        "surname": "Katsavounidis",
        "given_name": "Ioannis"
      }
    ]
  },
  {
    "title": "Shadow detection via multi-scale feature fusion and unsupervised domain adaptation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103596",
    "abstract": "Shadow detection is significant for scene understanding. As a common scenario, soft shadows have more ambiguous boundaries than hard shadows. However, they are rarely present in the available benchmarks since annotating for them is time-consuming and needs expert help. This paper discusses how to transfer the shadow detection capability from available shadow data to soft shadow data and proposes a novel shadow detection framework (MUSD) based on multi-scale feature fusion and unsupervised domain adaptation. Firstly, we set the existing labeled shadow dataset (i.e., SBU) as the source domain and collect an unlabeled soft shadow dataset (SSD) as the target domain to formulate an unsupervised domain adaptation problem. Next, we design an efficient shadow detection network based on the double attention module and multi-scale feature fusion. Then, we use the global–local feature alignment strategy to align the task-related feature distributions between the source and target domains. This allows us to obtain a robust model and achieve domain adaptation effectively. Extensive experimental results show that our method can detect soft shadows more accurately than existing state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001225",
    "keywords": [
      "Adaptation (eye)",
      "Artificial intelligence",
      "Cartography",
      "Classifier (UML)",
      "Computer science",
      "Computer vision",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Feature (linguistics)",
      "Fusion",
      "Geography",
      "Image (mathematics)",
      "Image fusion",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Psychology",
      "Psychotherapist",
      "Scale (ratio)",
      "Shadow (psychology)"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Kai"
      },
      {
        "surname": "Wu",
        "given_name": "Wen"
      },
      {
        "surname": "Shao",
        "given_name": "Yan-Li"
      },
      {
        "surname": "Fang",
        "given_name": "Jing-Long"
      },
      {
        "surname": "Wang",
        "given_name": "Xing-Qi"
      },
      {
        "surname": "Wei",
        "given_name": "Dan"
      }
    ]
  },
  {
    "title": "Multi-scale attention guided network for end-to-end face alignment and recognition",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103628",
    "abstract": "Attention modules embedded in deep networks mediate the selection of informative regions for object recognition. In addition, the combination of features learned from different branches of a network can enhance the discriminative power of these features. However, fusing features with inconsistent scales is a less-studied problem. In this paper, we first propose a multi-scale channel attention network with an adaptive feature fusion strategy (MSCAN-AFF) for face recognition (FR), which fuses the relevant feature channels and improves the network’s representational power. In FR, face alignment is performed independently prior to recognition, which requires the efficient localization of facial landmarks, which might be unavailable in uncontrolled scenarios such as low-resolution and occlusion. Therefore, we propose utilizing our MSCAN-AFF to guide the Spatial Transformer Network (MSCAN-STN) to align feature maps learned from an unaligned training set in an end-to-end manner. Experiments on benchmark datasets demonstrate the effectiveness of our proposed MSCAN-AFF and MSCAN-STN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001481",
    "keywords": [
      "Artificial intelligence",
      "Backbone network",
      "Computer network",
      "Computer science",
      "Discriminative model",
      "End-to-end principle",
      "Face (sociological concept)",
      "Facial recognition system",
      "Feature (linguistics)",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Shakeel",
        "given_name": "M. Saad"
      },
      {
        "surname": "Zhang",
        "given_name": "Yuxuan"
      },
      {
        "surname": "Wang",
        "given_name": "Xin"
      },
      {
        "surname": "Kang",
        "given_name": "Wenxiong"
      },
      {
        "surname": "Mahmood",
        "given_name": "Arif"
      }
    ]
  },
  {
    "title": "PointCaps: Raw point cloud processing using capsule networks with Euclidean distance routing",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103612",
    "abstract": "Raw point cloud processing using capsule networks is widely adopted in classification, reconstruction, and segmentation due to its ability to preserve spatial agreement of the input data. However, most of the existing capsule based network approaches are computationally heavy and fail at representing the entire point cloud as a single capsule. We address these limitations in existing capsule network based approaches by proposing PointCaps, a novel convolutional capsule architecture with parameter sharing. Along with PointCaps, we propose a novel Euclidean distance routing algorithm and a class-independent latent representation. The latent representation captures physically interpretable geometric parameters of the point cloud, with dynamic Euclidean routing, PointCaps well-represents the spatial (point-to-part) relationships of points. PointCaps has a significantly lower number of parameters and requires a significantly lower number of FLOPs while achieving better reconstruction with comparable classification and segmentation accuracy for raw point clouds compared to state-of-the-art capsule networks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001365",
    "keywords": [
      "Artificial intelligence",
      "Capsule",
      "Cloud computing",
      "Computer science",
      "Euclidean distance",
      "Euclidean geometry",
      "Geology",
      "Geometry",
      "Mathematics",
      "Operating system",
      "Paleontology",
      "Point (geometry)",
      "Point cloud"
    ],
    "authors": [
      {
        "surname": "Denipitiyage",
        "given_name": "Dishanika"
      },
      {
        "surname": "Jayasundara",
        "given_name": "Vinoj"
      },
      {
        "surname": "Rodrigo",
        "given_name": "Ranga"
      },
      {
        "surname": "Edussooriya",
        "given_name": "Chamira U.S."
      }
    ]
  },
  {
    "title": "Confidence based class weight and embedding discrepancy constraint network for partial domain adaptation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103630",
    "abstract": "Partial domain adaptation (PDA) is a special domain adaptation task where the label space of the target domain is a subset of the source domain. In this work, we present a novel adversarial PDA method named Confidence Based Class Weight and Embedding Discrepancy Constraint Network (CEN). Specifically, we design a robust weighting scheme that takes sample confidence and class information into account. It can automatically distinguish outlier samples in the source domain and reduce their importance. Besides, we consider the relationship between feature norm and domain shift. We limit the expectation of the feature norms of both domains to an adaptive value. By this means, we can align the feature distributions and help the deep model learn domain-invariant representations. Comprehensive experiments on three domain adaptation datasets Office-31, Office-home, and Visda2017 show that our approach surpasses state-of-the-art methods on various PDA tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032200150X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Class (philosophy)",
      "Classifier (UML)",
      "Computer science",
      "Constraint (computer-aided design)",
      "Data mining",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Embedding",
      "Feature (linguistics)",
      "Geometry",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Medicine",
      "Outlier",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Radiology",
      "Theoretical computer science",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Luo",
        "given_name": "Mingkai"
      },
      {
        "surname": "Yang",
        "given_name": "Zhao"
      },
      {
        "surname": "Ai",
        "given_name": "Weiwei"
      },
      {
        "surname": "Liu",
        "given_name": "Jiehao"
      }
    ]
  },
  {
    "title": "Semantic guided knowledge graph for large-scale zero-shot learning",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103629",
    "abstract": "Zero-shot learning has received growing attention, which aims to improve generalization to unseen concepts. The key challenge in zero-shot tasks is to precisely model the relationship between seen and unseen classes. Most existing zero-shot learning methods capture inter-class relationships via a shared embedding space, leading to inadequate use of relationships and poor performance. Recently, knowledge graph-based methods have emerged as a new trend of zero-shot learning. These methods use a knowledge graph to accurately model the inter-class relationships. However, the currently dominant method for zero-shot learning directly extracts the fixed connection from off-the-shelf WordNet, which will inherit the inherent noise in WordNet. In this paper, we propose a novel method that adopts class-level semantic information as a guidance to construct a new semantic guided knowledge graph (SG-KG), which can correct the errors in the existing knowledge graph and accurately model the inter-class relationships. Specifically, our method includes two main steps: noise suppression and semantic enhancement. Noise suppression is used to eliminate noise edges in the knowledge graph, and semantic enhancement is used to connect two classes with strong relations. To promote high efficient information propagation among classes, we develop a novel multi-granularity fusion network (MGFN) that integrates discriminative information from multiple GCN branches. Extensive experiments on the large-scale ImageNet-21K dataset and AWA2 dataset demonstrate that our method consistently surpasses existing methods and achieves a new state-of-the-art result.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001493",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Discriminative model",
      "Embedding",
      "Generalization",
      "Granularity",
      "Graph",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Natural language processing",
      "Operating system",
      "Pattern recognition (psychology)",
      "Theoretical computer science",
      "WordNet"
    ],
    "authors": [
      {
        "surname": "Wei",
        "given_name": "Jiwei"
      },
      {
        "surname": "Sun",
        "given_name": "Haotian"
      },
      {
        "surname": "Yang",
        "given_name": "Yang"
      },
      {
        "surname": "Xu",
        "given_name": "Xing"
      },
      {
        "surname": "Li",
        "given_name": "Jingjing"
      },
      {
        "surname": "Shen",
        "given_name": "Heng Tao"
      }
    ]
  },
  {
    "title": "EPLL image restoration with a bounded asymmetrical Student’s-t mixture model",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103611",
    "abstract": "The expected patch log-likelihood (EPLL) model is a patch prior-based image restoration method which received extensive attention in image processing in recent years for its outstanding ability to preserve the detail and structure. However, due to using the Gaussian mixture model (GMM) with the noise sensitivity as the local prior, the EPLL model suffers from undesired artifact and poor robustness frequently. In this paper, to restrain the generation of artifact of EPLL model, we replace the GMM with a bounded asymmetrical Student’s-t mixture model (BASMM), which is sufficiently flexible to fit different shapes of image data, such as non-Gaussian, non-symmetric, and bounded support data. Then, the anisotropic nonlocal self-similarity (ANSS) based regularization parameters are designed to improve the robustness of the proposed model. Experimental results demonstrate the competitiveness of our proposed model compared with that of state-of-the-art methods in performance both visually and quantitatively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001353",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Bounded function",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Gaussian",
      "Gaussian network model",
      "Gene",
      "Image (mathematics)",
      "Mathematical analysis",
      "Mathematics",
      "Mixture model",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Regularization (linguistics)",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Qiqiong"
      },
      {
        "surname": "Cao",
        "given_name": "Guo"
      },
      {
        "surname": "Shi",
        "given_name": "Hao"
      },
      {
        "surname": "Zhang",
        "given_name": "Youqiang"
      },
      {
        "surname": "Fu",
        "given_name": "Peng"
      }
    ]
  },
  {
    "title": "An unsupervised fusion network for boosting denoising performance",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103626",
    "abstract": "While many efforts have been devoted to addressing image denoising and achieve continuously improving results during the past few decades, it is fair to say that no a stand-alone method is consistently better than others. Nonetheless, many existing denoising methods, each having a different denoising capability, can yield various but complementary denoised images with respect to specific local areas. To effectively exploit the complementarity and diversity among the denoised images obtained with different denoisers, in this work we fuse them to produce an overall better result, which is fundamental to achieve robust and competitive denoising performance especially for complex scenes. A framework called deep fusion network (DFNet) is proposed to generate a consistent estimation about the final denoised image, taking advantage of the complementarity of denoisers and suppressing the bias. Specifically, given a noisy image, we first exploit a set of representative image denoisers to denoise it respectively, and obtain the corresponding initial denoised images. Then these initial denoised images are concatenated and fed into the proposed DFNet, and the proposed DFNet seeks to adjust its network parameters to produce the fused image (as the final denoised image) with an unsupervised training strategy through minimizing the carefully designed loss function. The experimental results show that our approach outperforms the stand-alone methods as well as the ones using combination strategy by large margin both in objective and subjective evaluations. Compared to the those methods that are relatively close to our strategy, the proposed DFNet is extensible and parameter free, which means it can cope with a variable number of different denoisers and avoid the manual intervention during the fusion process. The proposed DFNet has greater flexibility and better practicality.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001468",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Boosting (machine learning)",
      "Complementarity (molecular biology)",
      "Computer science",
      "Computer security",
      "Exploit",
      "Genetics",
      "Image (mathematics)",
      "Image denoising",
      "Image fusion",
      "Machine learning",
      "Margin (machine learning)",
      "Noise reduction",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Shaoping"
      },
      {
        "surname": "Chen",
        "given_name": "Xiaojun"
      },
      {
        "surname": "Luo",
        "given_name": "Jie"
      },
      {
        "surname": "Cheng",
        "given_name": "Xiaohui"
      },
      {
        "surname": "Xiao",
        "given_name": "Nan"
      }
    ]
  },
  {
    "title": "Anti-phishing technique based on dynamic image captcha using multi secret sharing scheme",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103624",
    "abstract": "With the cutting-edge improvement of web, online abuses have been increasing rapidly. Phishing is the most widely recognized abuses performed by digital crooks nowadays. It is an activity to steal private data (for example, client names, passwords and Visa data) in an electronic correspondence. It is a sort of fraud with the end goal of monetary benefit and other fake exercises. It utilizes phony websites that resemble genuine ones. Phishing messages might contain links to sites that are contaminated with malware. In this paper, “an anti-phishing approach using multi secret sharing scheme” is implemented as an answer to this problem. Here, Dynamic Image CAPTCHA based verification using multi secret sharing is performed. Image CAPTCHA is divided into two pieces called shares. Multiple secret pictures are revealed by overlapping the same set of shares at different angles. In the proposed approach, shares are of different modes i.e., user’s share is imprinted on a physical transparency while server’s share is in digital mode. By using the proposed approach, websites and end clients can cross confirm their identity.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001444",
    "keywords": [
      "Authentication (law)",
      "CAPTCHA",
      "Computer science",
      "Computer security",
      "Cryptography",
      "Internet privacy",
      "Mathematical analysis",
      "Mathematics",
      "Password",
      "Phishing",
      "Scheme (mathematics)",
      "Secret sharing",
      "The Internet",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Arora",
        "given_name": "Akanksha"
      },
      {
        "surname": "Garg",
        "given_name": "Hitendra"
      },
      {
        "surname": "Shivani",
        "given_name": "Shivendra"
      }
    ]
  },
  {
    "title": "CCNet: CNN model with channel attention and convolutional pooling mechanism for spatial image steganalysis",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103633",
    "abstract": "Image steganalysis based on convolutional neural networks(CNN) has attracted great attention. However, existing networks lack attention to regional features with complex texture, which makes the ability of discrimination learning miss in network. In this paper, we described a new CNN designed to focus on useful features and improve detection accuracy for spatial-domain steganalysis. The proposed model consists of three modules: noise extraction module, noise analysis module and classification module. A channel attention mechanism is used in the noise extraction module and analysis module, which is realized by embedding the SE(Squeeze-and-Excitation) module into the residual block. Then, we use convolutional pooling instead of average pooling to aggregate features. The experimental results show that detection accuracy of the proposed model is significantly better than those of the existing models such as SRNet, Zhu-Net and GBRAS-Net. Compared with these models, our model has better generalization ability, which is critical for practical application.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001535",
    "keywords": [
      "Artificial intelligence",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Convolutional neural network",
      "Epistemology",
      "Image (mathematics)",
      "Mechanism (biology)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pooling",
      "Steganalysis",
      "Steganography"
    ],
    "authors": [
      {
        "surname": "Fu",
        "given_name": "Tong"
      },
      {
        "surname": "Chen",
        "given_name": "Liquan"
      },
      {
        "surname": "Fu",
        "given_name": "Zhangjie"
      },
      {
        "surname": "Yu",
        "given_name": "Kunliang"
      },
      {
        "surname": "Wang",
        "given_name": "Yu"
      }
    ]
  },
  {
    "title": "Block-based progressive visual cryptography scheme with uniform progressive recovery and consistent background",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103631",
    "abstract": "Block-based progressive visual cryptography scheme (BPVCS) divides a secret image into non-overlapping blocks and encodes each block as sub-shadows. The final shadows for BPVCS are created by combining the associated sub-shadows. When enough shadows are superimposed, some of the secret blocks will be exposed. More information will be revealed as more shadows are used. This is referred to as progressive recovery. Hou et al. introduced a ( 2 , n ) -BPVCS. Yang et al. further extended the ( 2 , n ) scheme to a general ( k , n ) scheme. However, Yang et al. ( k , n ) -BPVCS suffers from the non-uniform progressive recovery and inconsistent background of recovered secret blocks. In this paper, we introduce a ( k , n ) -BPVCS to address the mentioned two defects. Theoretical analysis and experimental results are provided to illustrate the benefits of the proposed approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001511",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Block (permutation group theory)",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Cryptography",
      "Mathematical analysis",
      "Mathematics",
      "Scheme (mathematics)",
      "Secret sharing",
      "Visual cryptography"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Xiaotian"
      },
      {
        "surname": "Luo",
        "given_name": "Zhonglin"
      }
    ]
  },
  {
    "title": "HEVC’s intra mode process expedited using Histogram of Oriented Gradients",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103594",
    "abstract": "The brute-force behavior of High Efficiency Video Coding (HEVC) is the biggest hurdle in the communication of multimedia content. Therefore, two novel methods will be presented here to expedite the intra mode decision process of HEVC. In the first algorithm, the feasibility of Histogram of Oriented Gradients (HOG) for early intra mode decision is presented by using statistical evidence. Then, HOG of the current block and 35 intra predictions are obtained. The intra-prediction that gives the least sum of absolute difference (SAD) with the HOG of the current block is selected as the termination point. In the second algorithm, the difference between the Hardmard-cost of intra modes is modeled to achieve fast intra mode decision. The proposed algorithms accelerated the encoding process of the HEVC by 5% and 35.57%, while their Bjontegaard Delta Bit Rate (BD-BR) is 1.09% and 1.61%, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001213",
    "keywords": [
      "Computer science",
      "Computer vision",
      "Engineering",
      "Histogram",
      "Human–computer interaction",
      "Image (mathematics)",
      "Mode (computer interface)",
      "Process (computing)",
      "Process engineering",
      "Programming language"
    ],
    "authors": [
      {
        "surname": "Tariq",
        "given_name": "Junaid"
      },
      {
        "surname": "Ijaz",
        "given_name": "Amir"
      },
      {
        "surname": "Armghan",
        "given_name": "Ammar"
      },
      {
        "surname": "Rahman",
        "given_name": "Hameedur"
      },
      {
        "surname": "Ali",
        "given_name": "Hashim"
      },
      {
        "surname": "Alenezi",
        "given_name": "Fayadh"
      }
    ]
  },
  {
    "title": "An enhanced image quality assessment by synergizing superpixels and visual saliency",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103610",
    "abstract": "Superpixel and saliency-based evaluation methods play important roles in full reference image quality assessment (FR IQA). However, we find that these methods have one complementary principle and three limitations: (1) the weighted maps of superpixel-based methods conflict with the perception of the human visual system; (2) saliency-based methods are inefficient in terms of the block distortion; (3) the general two-direction gradient extraction factor must be extended to be multidirectional. To address these limitations, we propose an enhanced image quality assessment by synergizing superpixels and visual saliency. Specifically, the calculation of a newly proposed framework involves three similarities and two strategies: the saliency, superpixel and multidirectional gradient similarities of the neighborhoods, and the saliency pooling strategy, the fusion strategy of these similarities. Theoretical analysis and experimental results show that the proposed method can effectively address the limitations noted above and outperform the existing methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001341",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Epistemology",
      "Image (mathematics)",
      "Image quality",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Psychology",
      "Quality (philosophy)"
    ],
    "authors": [
      {
        "surname": "Deng",
        "given_name": "Jiehang"
      },
      {
        "surname": "Chen",
        "given_name": "Haomin"
      },
      {
        "surname": "Yuan",
        "given_name": "Zhongming"
      },
      {
        "surname": "Gu",
        "given_name": "Guosheng"
      },
      {
        "surname": "Xu",
        "given_name": "Shihe"
      },
      {
        "surname": "Weng",
        "given_name": "Shaowei"
      },
      {
        "surname": "Wang",
        "given_name": "Hao"
      }
    ]
  },
  {
    "title": "A GCN-based fast CU partition method of intra-mode VVC",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103621",
    "abstract": "In this paper, a global convolutional network (GCN)-based fast coding unit (CU) partition method of intra-mode VVC is proposed. By using the GCN module with large kernel size convolutions, the proposed method can capture global information in CUs, leading to an accurate partition mode prediction in the quad-tree plus multi-type tree (QTMT) structure. Ranked according to predicted probabilities, the partition modes with lower probabilities are discarded, which reduces the computational complexity of VVC. Additionally, tradeoffs between performance and complexity can be achieved with different strategies. Experimental results demonstrated that the proposed method can reduce encoding time by 51.06% ∼ 61.15% while increasing Bjøntegaard delta bit-rate (BD-BR) by 0.84% ∼ 1.52% when implemented in VTM 10.0, outperforming the state-of-the-art methods, and that the proposed method can be used to accelerate VVenC 1.0 at the preset slower, achieving higher performance and lower complexity compared with the original VVenC 1.0 at the presets slow and medium.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001419",
    "keywords": [
      "Algorithm",
      "Coding (social sciences)",
      "Combinatorics",
      "Computational complexity theory",
      "Computer science",
      "Discrete mathematics",
      "Kernel (algebra)",
      "Mathematics",
      "Partition (number theory)",
      "Statistics",
      "Tree (set theory)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Saiping"
      },
      {
        "surname": "Feng",
        "given_name": "Shixuan"
      },
      {
        "surname": "Chen",
        "given_name": "Jingwu"
      },
      {
        "surname": "Zhou",
        "given_name": "Chunjie"
      },
      {
        "surname": "Yang",
        "given_name": "Fuzheng"
      }
    ]
  },
  {
    "title": "Gender and ethnicity recognition based on visual attention-driven deep architectures",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103627",
    "abstract": "Most of the time, when people observe, interact or speak to each other, they focus the attention on the ocular parts of the face. This daily life experience has a strong impact on the analysis of periocular facial regions. These facial regions may be exploited in order to identify individuals for several applications, including access control and services such as telebanking and electronic transactions. In this paper we suggest studying the efficiency of the periocular regions on gender and race prediction. Most researchers propose a local texture description based on LBP (Local Binary Pattern) and HoG (Histogram of Oriented Gradients) for the purpose of predicting gender. On the other hand, Deep learning techniques were proposed to predict the gender. However, this requires a huge labeled periocular data for gender which is not available. Also, the expressivity of gender and race can be decreased on the final representation of the Deep architectures comparing to the earlier stages. To overcome these points and for the aim of predicting gender and race, considering also the high impact of DCNNs (Deep Convolutional Neural Networks) techniques to solve several aspects in biometrics, we suggest a Deep architecture based on visual attention on the periocular part. The visual saliency extraction is based on primary layers’ activation by analyzing the feature-maps. We study how the visual attention-based features coupled to Deep Neural Networks can be used to discriminate between gender and race, hence extract a significant feature from periocular regions. Different pretrained architectures such as Alexnet and ResNet-50 were considered to extract visual saliency points or interest points. Several experiments were performed on periocular regions and a comparative study was conducted. The present results not only demonstrate the feasibility but also the robustness of the extracted interest points.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032200147X",
    "keywords": [
      "Artificial intelligence",
      "Cognitive psychology",
      "Computer science",
      "Ethnic group",
      "Law",
      "Political science",
      "Psychology"
    ],
    "authors": [
      {
        "surname": "Khellat-Kihel",
        "given_name": "Souad"
      },
      {
        "surname": "Muhammad",
        "given_name": "Jawad"
      },
      {
        "surname": "Sun",
        "given_name": "Zhenan"
      },
      {
        "surname": "Tistarelli",
        "given_name": "Massimo"
      }
    ]
  },
  {
    "title": "A no-reference perceptual image quality assessment database for learned image codecs",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103617",
    "abstract": "The drastic growth of research in image compression, especially deep learning-based image compression techniques, poses new challenges to objective image quality assessment (IQA). Typical artifacts encountered in the emerging image codecs are significantly different from that produced by traditional block-based codecs, leading to inapplicability of the existing objective IQA algorithms. Towards advancing the development of objective IQA algorithms for recent compression artifacts, we built a learning-based compressed image quality assessment (LCIQA) database involving traditional block-based image codecs, hybrid neural network based image codecs, convolutional neural network based and generative adversarial network (GAN) based end-to-end optimized image coding approaches. Our study confirms the statistical difference and human perception difference between reconstructions of learned compression and traditional block-based compression. We propose a two-step deep learning model for learning-based compressed image quality assessment. Extensive experiments on LCIQA database demonstrate that our proposed model performs better than other counterparts on learning-based compressed images, especially on GAN compressed images, and achieves competitive performance to the state-of-the-art IQA metrics on traditional compressed images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001377",
    "keywords": [
      "Artificial intelligence",
      "Codec",
      "Computer hardware",
      "Computer science",
      "Computer vision",
      "Epistemology",
      "Image (mathematics)",
      "Image quality",
      "Information retrieval",
      "Neuroscience",
      "Perception",
      "Philosophy",
      "Psychology",
      "Quality (philosophy)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Jiaqi"
      },
      {
        "surname": "Fang",
        "given_name": "Zhigao"
      },
      {
        "surname": "Yu",
        "given_name": "Lu"
      }
    ]
  },
  {
    "title": "Imitative Collaboration: A mirror-neuron inspired mixed reality collaboration method with remote hands and local replicas",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103600",
    "abstract": "Mixed reality can overlay and display 3D digital content in the real world, convey abstract concepts to users, and promote the understanding of complex tasks. However, the abstract graphics overlaid on the physical space may cause a certain cognitive load for local users and reduce the efficiency of collaboration. To improve the efficiency of remote collaboration, we conducted an elicitation study on assembly tasks, explored the user needs for collaboration, and defined the design goals of our remote collaboration method. Inspired by the mirror-neuron mechanism, we present an imitative collaboration method that allows local users to imitate the interaction behavior of remote users to complete tasks. We also propose a series of interaction methods for remote users to select, copy, and interact with the local point clouds to facilitate the expression of collaboration intentions. Finally, the results of a user study evaluating our imitative collaboration method on assembly tasks are reported, confirming that our method improves collaboration efficiency while reducing the cognitive load of local users.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001249",
    "keywords": [
      "Cognition",
      "Cognitive load",
      "Computer graphics (images)",
      "Computer science",
      "Epistemology",
      "Geometry",
      "Graphics",
      "Human–computer interaction",
      "Mathematics",
      "Mechanism (biology)",
      "Multimedia",
      "Neuroscience",
      "Overlay",
      "Philosophy",
      "Point (geometry)",
      "Programming language",
      "Psychology"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Zhenning"
      },
      {
        "surname": "Pan",
        "given_name": "Zhigeng"
      },
      {
        "surname": "Li",
        "given_name": "Weiqing"
      },
      {
        "surname": "Su",
        "given_name": "Zhiyong"
      }
    ]
  },
  {
    "title": "Spatial-frequency HEVC multiple description video coding with adaptive perceptual redundancy allocation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103614",
    "abstract": "Multiple description coding (MDC) approaches improve the error-resilient performance of video transmission by introducing redundancy. The existing multiple description video coding (MDVC) schemes are rarely designed for particular coding structure of HEVC, and the characteristics of the human visual system (HVS) are seldom considered. In this paper, a spatial-frequency multiple description video coding with adaptive perceptual redundancy allocation framework, named SF-PMDVC, is proposed for HEVC. For descriptions generation, after polyphase down-sampling in spatial domain, a transformation based on integer discrete cosine transform (DCT) is expended to adapt to the flexible coding unit partitioning process in HEVC, and the frequency coefficients are segmented and mapped to reduce the bitrate of each description. To further improve the performance of MDVC, an adaptive perceptual redundancy allocation strategy based on visual saliency is proposed, which improves the coding efficiency adapting to the visual perception. Experimental results show that the proposed scheme improves the error resiliency of HEVC by achieving superior objective and subjective reconstructed video quality as compared to the state-of-the-art MDVC methods for HEVC.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001389",
    "keywords": [
      "Algorithm",
      "Algorithmic efficiency",
      "Artificial intelligence",
      "Coding (social sciences)",
      "Coding tree unit",
      "Computer science",
      "Computer vision",
      "Context-adaptive binary arithmetic coding",
      "Context-adaptive variable-length coding",
      "Data compression",
      "Decoding methods",
      "Discrete cosine transform",
      "Human visual system model",
      "Image (mathematics)",
      "Mathematics",
      "Operating system",
      "Redundancy (engineering)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Feifeng"
      },
      {
        "surname": "Chen",
        "given_name": "Jing"
      },
      {
        "surname": "Zeng",
        "given_name": "Huanqiang"
      },
      {
        "surname": "Cai",
        "given_name": "Canhui"
      }
    ]
  },
  {
    "title": "SIM-MFR: Spatial interactions mechanisms based multi-feature representation for background modeling",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103622",
    "abstract": "Moving object detection is frequently used as a springboard for advanced computer vision analysis in complex scenes. Nevertheless, due to unstable changes in the background, most existing background model hardly maintain superior performance. To this concern, we propose a novel pixel-level background model that has three innovations. First, we introduce K-means to directly model the spatiotemporal dependencies between pixels. These dependencies are exploited to discover static core information in the high-frequency changing spatial domain, resulting in excellent property in dynamic backgrounds. Besides, the notion of complementarity is taken as a feature selection criterion. In multi-feature model, the ability to supervise each other between features is important in the ambiguity challenges, e.g., shadow. Finally, feature models recommend each other in the update mechanism, and the diffusion rate of effective information in each feature model can be maximized by finding the best candidate feature. By virtue of this mechanism, model can be updated efficiently when large background migration occurs, e.g., PTZ. Experimental results on some standard benchmarks show that SIM-MFR can achieve promising performance compared to some state-of-the-art approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001420",
    "keywords": [
      "Ambiguity",
      "Artificial intelligence",
      "Biology",
      "Complementarity (molecular biology)",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Feature (linguistics)",
      "Genetics",
      "Law",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Political science",
      "Politics",
      "Programming language",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "He",
        "given_name": "Wei"
      },
      {
        "surname": "Li",
        "given_name": "Jiexin"
      },
      {
        "surname": "Qi",
        "given_name": "Qi"
      },
      {
        "surname": "Tu",
        "given_name": "Bing"
      },
      {
        "surname": "Ou",
        "given_name": "Xianfeng"
      },
      {
        "surname": "Guo",
        "given_name": "Longyuan"
      }
    ]
  },
  {
    "title": "A zero-watermark algorithm for multiple images based on visual cryptography and image fusion",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103569",
    "abstract": "At present, it is difficult for the multiple images zero-watermark algorithm to protect all the images in the image set, and repeated operations will reduce the efficiency of the algorithm. To solve these issues, the proposed algorithm can design a reasonable copyright protection scheme according to the number of images in the image set to realize the protection of all images, and reduce the cost of time and storage. The gray-weighted average image fusion method is used to fuse multiple normalized standard images into one image. The LWT(Lifting the Wavelet Transform)-QR decomposition is applied to the effective area of the fusion image to obtain the robust feature image. Non-extended visual cryptography is used to enhance the security of the algorithm. A zero-watermark image is obtained by using the XOR manipulation for the feature image and the public shared image. Experimental results demonstrate that the proposed algorithm has good performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001031",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Bitwise operation",
      "Computer science",
      "Computer vision",
      "Cryptography",
      "Digital watermarking",
      "Feature (linguistics)",
      "Feature detection (computer vision)",
      "Image (mathematics)",
      "Image fusion",
      "Image processing",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Secret sharing",
      "Visual cryptography",
      "Watermark"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Baowei"
      },
      {
        "surname": "Wang",
        "given_name": "Weishen"
      },
      {
        "surname": "Zhao",
        "given_name": "Peng"
      }
    ]
  },
  {
    "title": "Mutual information maximizing GAN inversion for real face with identity preservation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103566",
    "abstract": "Recent generative adversarial networks (GANs) have yielded remarkable performance in face image synthesis. GAN inversion embeds an image into the latent space of a pretrained generator, enabling it to be used for real face manipulation. However, current inversion approaches for real faces suffer the dilemma of initialization collapse and identity loss. In this paper, we propose a hierarchical GAN inversion for real faces with identity preservation based on mutual information maximization. We first use a facial domain guaranteed initialization to avoid the initialization collapse. Furthermore, we prove that maximizing the mutual information between inverted faces and their identities is equivalent to minimizing the distance between identity features from inverted and original faces. Optimization for real face inversion with identity preservation is implemented on this mutual information-maximizing constraint. Extensive experimental results show that our approach outperforms state-of-the-art solutions for inverting and editing real faces, particularly in terms of face identity preservation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001018",
    "keywords": [
      "Acoustics",
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Face (sociological concept)",
      "Identity (music)",
      "Initialization",
      "Inversion (geology)",
      "Mathematical optimization",
      "Mathematics",
      "Maximization",
      "Mutual information",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Physics",
      "Programming language",
      "Social science",
      "Sociology",
      "Structural basin"
    ],
    "authors": [
      {
        "surname": "Lin",
        "given_name": "Chengde"
      },
      {
        "surname": "Xiong",
        "given_name": "Shengwu"
      },
      {
        "surname": "Chen",
        "given_name": "Yaxiong"
      }
    ]
  },
  {
    "title": "PRA-TPE: Perfectly Recoverable Approximate Thumbnail-Preserving Image Encryption",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103589",
    "abstract": "With the popularity of cloud servers, an increasing number of people are willing to store their images in the cloud due to many conveniences such as online browsing and managing images. On the other hand, this inevitably causes users’ concerns about image privacy leakage. Many image encryption schemes are proposed to prevent privacy leakage, while most of them focus only on privacy protection and ignore the usability of encrypted images. For this purpose, Marohn et al. (2017) designed two approximate thumbnail-preserving encryption (TPE) schemes to balance image privacy and usability. However, the decrypted image in these two schemes are only perceptually close to the original one and the original image cannot be perfectly recovered. To this end, we design a perfectly recoverable approximate TPE scheme in this paper, which combines reversibledata hiding (RDH) with encryption schemes. The thumbnails of the original and processed images are similar to balance image privacy and usability well. Meanwhile, the reversibility of RDH and encryption schemes is utilized to ensure the perfect recoverability in the proposed scheme. Experiments show that the proposed approximate TPE scheme is no longer limited to balancing usability and privacy but attains perfect recovery.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001122",
    "keywords": [
      "Cloud computing",
      "Computer network",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Encryption",
      "Human–computer interaction",
      "Image (mathematics)",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Popularity",
      "Psychology",
      "Scheme (mathematics)",
      "Server",
      "Social psychology",
      "Theoretical computer science",
      "Thumbnail",
      "Usability"
    ],
    "authors": [
      {
        "surname": "Ye",
        "given_name": "Xi"
      },
      {
        "surname": "Zhang",
        "given_name": "Yushu"
      },
      {
        "surname": "Zhao",
        "given_name": "Ruoyu"
      },
      {
        "surname": "Lan",
        "given_name": "Rushi"
      },
      {
        "surname": "Xiang",
        "given_name": "Yong"
      }
    ]
  },
  {
    "title": "HEVC video information hiding scheme based on adaptive double-layer embedding strategy",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103549",
    "abstract": "High Efficiency Video Coding (HEVC) is well-known as an internationally popular video coding standard, and HEVC-based steganography has received increasing attention. In this paper, a new adaptive HEVC video information hiding method based on Prediction Unit (PU) partition mode and double-layer embedding strategy is proposed. Double-layer embedding is a method to complete the first-layer embedding using the mapping rules of PU partition mode, and to perform the second-layer embedding after the first-layer embedding. The cost assignment function designed in this paper can accurately evaluate the second-layer data embedding distortion. The frame position, motion properties and block size of PU are taken into consideration for the second-layer data embedding, and the syndrome-trellis codes (STCs) are used to minimize the embedding distortion. Experimental results show that the proposed adaptive double-layer embedding algorithm has better embedding efficiency and less embedding distortion in most cases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322000876",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Coding (social sciences)",
      "Combinatorics",
      "Computer science",
      "Decoding methods",
      "Distortion function",
      "Embedding",
      "Information hiding",
      "Mathematics",
      "Partition (number theory)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "He",
        "given_name": "Songhan"
      },
      {
        "surname": "Xu",
        "given_name": "Dawen"
      },
      {
        "surname": "Yang",
        "given_name": "Lin"
      },
      {
        "surname": "Liu",
        "given_name": "Yong"
      }
    ]
  },
  {
    "title": "Self-Balanced R-CNN for instance segmentation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103595",
    "abstract": "Current state-of-the-art two-stage models on instance segmentation task suffer from several types of imbalances. In this paper, we address the Intersection over the Union (IoU) distribution imbalance of positive input Regions of Interest (RoIs) during the training of the second stage. Our Self-Balanced R-CNN (SBR-CNN), an evolved version of the Hybrid Task Cascade (HTC) model, brings brand new loop mechanisms of bounding box and mask refinements. With an improved Generic RoI Extraction (GRoIE), we also address the feature-level imbalance at the Feature Pyramid Network (FPN) level, originated by a non-uniform integration between low- and high-level features from the backbone layers. In addition, the redesign of the architecture heads toward a fully convolutional approach with FCC further reduces the number of parameters and obtains more clues to the connection between the task to solve and the layers used. Moreover, our SBR-CNN model shows the same or even better improvements if adopted in conjunction with other state-of-the-art models. In fact, with a lightweight ResNet-50 as backbone, evaluated on COCO minival 2017 dataset, our model reaches 45.3% and 41.5% AP for object detection and instance segmentation, with 12 epochs and without extra tricks. The code is available at https://github.com/IMPLabUniPr/mmdetection/tree/sbr_cnn.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001201",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Rossi",
        "given_name": "Leonardo"
      },
      {
        "surname": "Karimi",
        "given_name": "Akbar"
      },
      {
        "surname": "Prati",
        "given_name": "Andrea"
      }
    ]
  },
  {
    "title": "Blind quality assessment of tone-mapped images using multi-exposure sequences",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103553",
    "abstract": "The tone mapping operator (TMO) enables high dynamic range (HDR) images to be presented on low dynamic range (LDR) consumer electronic devices. However, the results obtained by this method are not always ideal due to the reduced number of bits. In comparison, the multi-exposure image fusion (MEF) bypasses the intermediate HDR image composition and directly produces an image presented on standard devices. Inspired by this, this paper proposes a quality assessment method for tone-mapped image (TMI) based on generating multi-exposure sequences. Specifically, the method uses a generative adversarial network (GAN) to generate a set of sequences with different exposure levels based on the TMIs. Then a two-branch convolutional neural network (CNN) is used to extract features from the tone-mapped images and the multi-exposure reference sequences, respectively. Finally, the transformer is used to mine the intrinsic connections between TMIs and multi-exposure sequences and learn the mapping relationships from feature space to quality space. We conducted extensive experiments on the ESPL-LIVE HDR database. The applicability and effectiveness of the proposed method are verified by comparing and analyzing relevant features and model configurations with existing mainstream evaluation algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032200089X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Dynamic range",
      "Feature (linguistics)",
      "High dynamic range",
      "Image (mathematics)",
      "Image quality",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Tone mapping",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Jiachen"
      },
      {
        "surname": "Zhou",
        "given_name": "Yanshuang"
      },
      {
        "surname": "Zhao",
        "given_name": "Yang"
      },
      {
        "surname": "Wen",
        "given_name": "Jiabao"
      }
    ]
  },
  {
    "title": "Self-Balanced R-CNN for instance segmentation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103595",
    "abstract": "Current state-of-the-art two-stage models on instance segmentation task suffer from several types of imbalances. In this paper, we address the Intersection over the Union (IoU) distribution imbalance of positive input Regions of Interest (RoIs) during the training of the second stage. Our Self-Balanced R-CNN (SBR-CNN), an evolved version of the Hybrid Task Cascade (HTC) model, brings brand new loop mechanisms of bounding box and mask refinements. With an improved Generic RoI Extraction (GRoIE), we also address the feature-level imbalance at the Feature Pyramid Network (FPN) level, originated by a non-uniform integration between low- and high-level features from the backbone layers. In addition, the redesign of the architecture heads toward a fully convolutional approach with FCC further reduces the number of parameters and obtains more clues to the connection between the task to solve and the layers used. Moreover, our SBR-CNN model shows the same or even better improvements if adopted in conjunction with other state-of-the-art models. In fact, with a lightweight ResNet-50 as backbone, evaluated on COCO minival 2017 dataset, our model reaches 45.3% and 41.5% AP for object detection and instance segmentation, with 12 epochs and without extra tricks. The code is available at https://github.com/IMPLabUniPr/mmdetection/tree/sbr_cnn.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001201",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Rossi",
        "given_name": "Leonardo"
      },
      {
        "surname": "Karimi",
        "given_name": "Akbar"
      },
      {
        "surname": "Prati",
        "given_name": "Andrea"
      }
    ]
  },
  {
    "title": "HEVC video information hiding scheme based on adaptive double-layer embedding strategy",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103549",
    "abstract": "High Efficiency Video Coding (HEVC) is well-known as an internationally popular video coding standard, and HEVC-based steganography has received increasing attention. In this paper, a new adaptive HEVC video information hiding method based on Prediction Unit (PU) partition mode and double-layer embedding strategy is proposed. Double-layer embedding is a method to complete the first-layer embedding using the mapping rules of PU partition mode, and to perform the second-layer embedding after the first-layer embedding. The cost assignment function designed in this paper can accurately evaluate the second-layer data embedding distortion. The frame position, motion properties and block size of PU are taken into consideration for the second-layer data embedding, and the syndrome-trellis codes (STCs) are used to minimize the embedding distortion. Experimental results show that the proposed adaptive double-layer embedding algorithm has better embedding efficiency and less embedding distortion in most cases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322000876",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Coding (social sciences)",
      "Combinatorics",
      "Computer science",
      "Decoding methods",
      "Distortion function",
      "Embedding",
      "Information hiding",
      "Mathematics",
      "Partition (number theory)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "He",
        "given_name": "Songhan"
      },
      {
        "surname": "Xu",
        "given_name": "Dawen"
      },
      {
        "surname": "Yang",
        "given_name": "Lin"
      },
      {
        "surname": "Liu",
        "given_name": "Yong"
      }
    ]
  },
  {
    "title": "Progressive multi-branch embedding fusion network for underwater image enhancement",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103587",
    "abstract": "The underwater image enhancement techniques are essential for ocean research and engineering applications. In this paper, we propose a progressive multi-branch embedding fusion network (PMEFN) to improve image quality. Specifically, a multi-branch embedding fusion module (MEFM) is designed. The distorted images and its sharpened versions are used as the input, which are fused to learn the contextualized features based on a two-branch hybrid encoder–decoder module ( H E D M 2 ) combined with the triple attention module to focus on the noise region. Afterwards, we use the multi-stage refining framework to decompose the image enhancement or marine snow removal tasks into multiple stages and progressively learn the nonlinear functions from the distorted inputs. Additionally, the outputs generated at each stage are further refined and enhanced based on a three-branch hybrid encoder–decoder module ( H E D M 3 ). We perform experiments using real underwater datasets, including EUVP, UFO-120, UIEB, and synthetic dataset MSRB. The experimental results show that the proposed method has a superior performance as compared to other methods in terms of quantitative performance and visual quality. In addition, the effectiveness of each component is further validated by performing ablation experiments.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001134",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Embedding",
      "Fusion",
      "Geology",
      "Image (mathematics)",
      "Linguistics",
      "Mathematics",
      "Oceanography",
      "Philosophy",
      "Underwater"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Kaichuan"
      },
      {
        "surname": "Meng",
        "given_name": "Fei"
      },
      {
        "surname": "Tian",
        "given_name": "Yubo"
      }
    ]
  },
  {
    "title": "Progressive enhancement network with pseudo labels for weakly supervised temporal action localization",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103590",
    "abstract": "Weakly supervised temporal action localization (WSTAL) is crucial for real world applications, as it relieves the huge burden of frame-level annotations for fully supervised action detection. Most existing WSTAL methods focused on classifying video snippets, or detecting action boundaries. However, the predictions from these well-designed models have not been fully utilized. Accordingly, we propose a weakly-supervised framework called the progressive enhancement network (PEN), which takes full advantages of the predictions generated by the preceding models to enhance the subsequent models. Specifically, snippet-level pseudo labels are generated from the preceding predictions by considering the similarity and temporal distance between action snippets. Then subsequent models are progressively enhanced by using pseudo labels as a supervision, and utilizing their underlying semantics to make the feature representation more qualified for the temporal localization task. Extensive experiments which are carried out on two popular benchmarks, THUMOS’14 and ActivityNet v1.2, demonstrate the effectiveness of our method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001171",
    "keywords": [
      "Action (physics)",
      "Artificial intelligence",
      "Computer science",
      "Economics",
      "Feature (linguistics)",
      "Frame (networking)",
      "Image (mathematics)",
      "Information retrieval",
      "Law",
      "Linguistics",
      "Machine learning",
      "Management",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Political science",
      "Politics",
      "Programming language",
      "Quantum mechanics",
      "Representation (politics)",
      "Semantics (computer science)",
      "Similarity (geometry)",
      "Snippet",
      "Task (project management)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Qingyun"
      },
      {
        "surname": "Song",
        "given_name": "Yan"
      },
      {
        "surname": "Zou",
        "given_name": "Rong"
      },
      {
        "surname": "Shu",
        "given_name": "Xiangbo"
      }
    ]
  },
  {
    "title": "High dynamic range imaging with short- and long-exposures based on artificial remapping using multiscale exposure fusion",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103585",
    "abstract": "High dynamic range imaging (HDRI) is an excellent high-quality image acquisition technique, which can reflect real human visual characteristics from one (or several) captured low dynamic range (LDR) image. However, the input LDR image only provides partial information of the scene. Besides, in traditional HDRI methods that require multiple captured images as input, field of view errors can be induced, which will be difficult to apply it to the emerging image acquisition systems. Here, we propose a novel HDRI method that reconstructs an HDR image from only a pair of short- and long-exposure images based on artificial remapping using multi-scale exposure fusion. Firstly, we introduce a simulated exposure model called artificial remapping to synthesize a multi-exposure image sequence from the input LDR image pairs. Then, weighting maps of the sequence for fusion can be obtained according to the evaluation factors of contrast, saturation, as well as improved exposedness. Finally, we utilize the pyramid based multiscale exposure fusion framework to integrate them into an enhanced HDR image. Comparative experiments, fully implemented on some source images, have been demonstrated that better performance can be realized compared with some competing methods in qualitative and quantitative evaluation. Note that the operation of the proposed method is simple yet effective, which is easy to popularize. The method thus can be potentially applied to the emerging image acquisition systems where two images are captured simultaneously by two image sensors or by one image sensor with a pair of short- and long-exposure setting.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001158",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Dynamic range",
      "Geometry",
      "High dynamic range",
      "High-dynamic-range imaging",
      "Image (mathematics)",
      "Image fusion",
      "Image processing",
      "Image quality",
      "Mathematics",
      "Medicine",
      "Multiple exposure",
      "Pyramid (geometry)",
      "Radiology",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Junbao"
      },
      {
        "surname": "Wu",
        "given_name": "Lingfeng"
      },
      {
        "surname": "Li",
        "given_name": "Na"
      }
    ]
  },
  {
    "title": "Progressive Erasing Network with consistency loss for fine-grained visual classification",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103570",
    "abstract": "Fine-grained Visual Categorization (FGVC) in computer vision aims to recognize images belonging to multiple subordinate categories of a super-category. The difficulty of FGVC lies in the close resemblance among inter-classes and large variations among intra-classes. Most existing networks only focus on a few discriminative regions, while ignoring many subtle complementary features. So we propose a Progressive Erasing Network (PEN). In PEN, a Multi-Grid Erasure mechanism augments data samples and assists in capturing the local discriminative features, where the overall structure of the image is destroyed indirectly through pixel-wise erasure. Cross-layer feature aggregation by extracting salient class features is of great significance in FGVC. However, the capability of cross-layer feature representation based on a simple aggregation strategy is still inefficient. To this end, the proposed Consistency loss explores the cross-layer semantic affinity, which guides the Cross-Layer Incentive (CLI) block to mine more efficient feature representations of different granularity. We also integrate Cross Entropy and Complementary Entropy to take the distribution of negative classes into account for better classification performance. Our method uses end-to-end training with only classification labels. Experimental results show that our model outperforms the state-of-the-art on three fine-grained benchmarks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001043",
    "keywords": [
      "Artificial intelligence",
      "Categorization",
      "Computer science",
      "Consistency (knowledge bases)",
      "Cross entropy",
      "Discriminative model",
      "Erasure",
      "Feature (linguistics)",
      "Feature learning",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language"
    ],
    "authors": [
      {
        "surname": "Peng",
        "given_name": "Jin"
      },
      {
        "surname": "Wang",
        "given_name": "Yongxiong"
      },
      {
        "surname": "Zhou",
        "given_name": "Zeping"
      }
    ]
  },
  {
    "title": "ACGAN: Attribute controllable person image synthesis GAN for pose transfer",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103572",
    "abstract": "At present, pose transfer and attribute control tasks are still the challenges for image synthesis network. At the same time, there are often artifacts in the images generated by the image synthesis network when the above two tasks are completed. The existence of artifacts causes the loss of the generated image details or introduces some wrong image information, which leads to the decline of the overall performance of the existing work. In this paper, a generative adversarial network (GAN) named ACGAN is proposed to accomplish the above two tasks and effectively eliminate artifacts in generated images. The proposed network was compared quantitatively and qualitatively with previous works on the DeepFashion dataset and better results are obtained. Moreover, the overall network has advantages over the previous works in speed and number of parameters.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001067",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Control (management)",
      "Generative adversarial network",
      "Image (mathematics)",
      "Image synthesis",
      "Parallel computing",
      "Rendering (computer graphics)",
      "Transfer (computing)",
      "View synthesis"
    ],
    "authors": [
      {
        "surname": "Lin",
        "given_name": "ShaoYue"
      },
      {
        "surname": "Zhang",
        "given_name": "YanJun"
      }
    ]
  },
  {
    "title": "Deep image compression based on multi-scale deformable convolution",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103573",
    "abstract": "Deep image compression efficiency has been improved in the past years. However, to fully exploit context information for compressing image objects of different scales and shapes, more adaptive geometric structure of inputs should be considered. In this paper, we novelly introduce deformable convolution and its spatial attention extension into deep image compression task to fully exploit the context information. Specifically, a novel deep image compression network with Multi-Scale Deformable Convolution and Spatial Attention, named MS-DCSA, is proposed to better extract compact and efficient latent representation as well as reconstruct higher-quality images. First, multi-scale deformable convolution is presented to provide multi-scale receptive fields for learning spatial sampling offsets in deformable operations. Subsequently, multi-scale deformable spatial attention module is developed to generate attention masks to re-weight extracted features according to their importance. In addition, the multi-scale deformable convolution is applied to design delicate up/down sampling modules. Extensive experiments demonstrate that the proposed MS-DCSA network achieves improved performance on both PSNR and MS-SSIM quality metrics, compared to conventional as well as competing deep image compression methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001079",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Composite material",
      "Compression (physics)",
      "Compression artifact",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Convolution (computer science)",
      "Image (mathematics)",
      "Image compression",
      "Image processing",
      "Materials science",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Daowen"
      },
      {
        "surname": "Li",
        "given_name": "Yingming"
      },
      {
        "surname": "Sun",
        "given_name": "Heming"
      },
      {
        "surname": "Yu",
        "given_name": "Lu"
      }
    ]
  },
  {
    "title": "Two-stream interactive network based on local and global information for No-Reference Stereoscopic Image Quality Assessment",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103586",
    "abstract": "Nowadays, stereoscopic image quality assessment (SIQA) based on convolutional neural network (CNN) has become the mainstream model of image quality assessment (IQA). Compared with the two-dimensional quality evaluation model, stereoscopic image quality evaluation is more challenging due to the effects of depth and parallax information. In this paper, we propose a two-stream interactive network model to perform quality evaluation, which can well simulate the process of human stereo visual perception. Meanwhile, we enhance the extraction of local and global features of images by asymmetric convolution kernel and interactive sub-networks of inter-layers, respectively, which can further optimize our network model. Our proposed algorithm was evaluated on four public databases. The final experimental results show that our proposed algorithm exhibits good performance not only on the whole database but also on each single distortion type.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032200116X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Epistemology",
      "External quality assessment",
      "Image (mathematics)",
      "Image quality",
      "Information retrieval",
      "Medicine",
      "Pathology",
      "Philosophy",
      "Quality (philosophy)",
      "Quality assessment",
      "Stereoscopy"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Yun"
      },
      {
        "surname": "Huang",
        "given_name": "Baoqing"
      },
      {
        "surname": "Yue",
        "given_name": "Guanghui"
      },
      {
        "surname": "Wu",
        "given_name": "Jingkai"
      },
      {
        "surname": "Wang",
        "given_name": "Xiaoxu"
      },
      {
        "surname": "Zheng",
        "given_name": "Zhi"
      }
    ]
  },
  {
    "title": "A3N: Attention-based adversarial autoencoder network for detecting anomalies in video sequence",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103598",
    "abstract": "This paper presents a novel attention-based adversarial autoencoder network (A3N) that consists of a two-stream decoder to detect abnormal events in video sequences. The first stream of the decoder is a reconstructive model responsible for recreating the input frame sequence. However, the second stream is a future predictive model used to predict the future frame sequence through adversarial learning. A global attention mechanism is employed at the decoder side that helps to decode the encoded sequences effectively. The training of A3N is carried out on normal video data. The attention-based reconstructive model is used during the inference stage to compute the anomaly score. A3N delivers a considerable average speed of 0.0227 s ( ∼ 44 fps ) for detecting anomalies in the testing phase on used datasets. Several experiments and ablation analyses have been performed on UCSD Pedestrian, CUHK Avenue and ShanghaiTech datasets to validate the efficiency of the proposed model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001237",
    "keywords": [
      "Adversarial system",
      "Algorithm",
      "Anomaly detection",
      "Artificial intelligence",
      "Artificial neural network",
      "Autoencoder",
      "Biology",
      "Computer science",
      "Computer vision",
      "Decoding methods",
      "Encoder",
      "Frame (networking)",
      "Genetics",
      "Inference",
      "Machine learning",
      "Operating system",
      "Pattern recognition (psychology)",
      "Sequence (biology)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Aslam",
        "given_name": "Nazia"
      },
      {
        "surname": "Rai",
        "given_name": "Prateek Kumar"
      },
      {
        "surname": "Kolekar",
        "given_name": "Maheshkumar H."
      }
    ]
  },
  {
    "title": "Self-supervised multi-scale pyramid fusion networks for realistic bokeh effect rendering",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103580",
    "abstract": "Images with visual pleasing bokeh effect are often unattainable for mobile cameras with compact optics and tiny sensors. To balance the aesthetic requirements on photo quality and expensive high-end SLR cameras, synthetic bokeh effect rendering has emerged as an attractive machine learning topic for engineering applications on imaging systems. However, most of bokeh rendering models either heavily relied on prior knowledge such as scene depth or were topic-irrelevant data-driven networks without task-specific knowledge, which restricted models’ training efficiency and testing accuracy. Since bokeh is closely related to a phenomenon called ”circle of confusion”, therefore, in this paper, following the principle of bokeh generation, a novel self-supervised multi-scale pyramid fusion network has been proposed for bokeh rendering. During the pyramid fusion process, structure consistencies are employed to emphasize the importance of respective bokeh components. Task-specific knowledge which mimics the ”circle of confusion” phenomenon through disk blur convolutions is utilized as self-supervised information for network training. The proposed network has been evaluated and compared with several state-of-the-art methods on a public large-scale bokeh dataset- the ”EBB!” Dataset. The experiment performance demonstrates that the proposed network has much better processing efficiency and can achieve better realistic bokeh effect with much less parameters size and running time. Related source codes and pre-trained models of the proposed model will be available soon on https://github.com/zfw-cv/MPFNet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001110",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Confusion",
      "Machine learning",
      "Optics",
      "Physics",
      "Psychoanalysis",
      "Psychology",
      "Pyramid (geometry)",
      "Rendering (computer graphics)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Zhifeng"
      },
      {
        "surname": "Jiang",
        "given_name": "Aiwen"
      },
      {
        "surname": "Zhang",
        "given_name": "Chunjie"
      },
      {
        "surname": "Li",
        "given_name": "Hanxi"
      },
      {
        "surname": "Liu",
        "given_name": "Bo"
      }
    ]
  },
  {
    "title": "Deep chroma prediction of Wyner–Ziv frames in distributed video coding of wireless capsule endoscopy video",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103578",
    "abstract": "Compression of captured video frames is crucial for saving the power in wireless capsule endoscopy (WCE). A low complexity encoder is desired to limit the power consumption required for compressing the WCE video. Distributed video coding (DVC) technique is best suitable for designing a low complexity encoder. In this technique, frames captured in RGB colour space are converted into YCbCr colour space. Both Y and CbCr representing luma and chroma components of the Wyner–Ziv (WZ) frames are processed and encoded in existing DVC techniques proposed for WCE video compression. In the WCE video, consecutive frames exhibit more similarity in texture and colour properties. The proposed work uses these properties to present a method for processing and encoding only the luma component of a WZ frame. The chroma components of the WZ frame are predicted by an encoder–decoder based deep chroma prediction model at the decoder by matching luma and texture information of the keyframe and WZ frame. The proposed method reduces the computations required for encoding and transmitting of WZ chroma component. The results show that the proposed DVC with a deep chroma prediction model performs better when compared to motion JPEG and existing DVC systems for WCE at the reduced encoder complexity.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001055",
    "keywords": [
      "Artificial intelligence",
      "Coding (social sciences)",
      "Computer science",
      "Computer vision",
      "Encoder",
      "Frame (networking)",
      "Inter frame",
      "Mathematics",
      "Multiview Video Coding",
      "Operating system",
      "RGB color model",
      "Reference frame",
      "Statistics",
      "Telecommunications",
      "Video processing",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "B.",
        "given_name": "Sushma"
      },
      {
        "surname": "P.",
        "given_name": "Aparna"
      }
    ]
  },
  {
    "title": "Image blind restoration based on degradation representation network",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103564",
    "abstract": "Most deep learning (DL)-based image restoration methods have exploited excellent performance by learning a non-linear mapping function from low quality images to high quality images. However, two major problems restrict the development of the image restoration methods. First, most existing methods based on fixed degradation suffer from significant performance drop when facing the unknown degradation, because of the huge gap between the fixed degradation and the unknown degradation. Second, the unknown-degradation estimation may lead to restoration task failure due to uncertain estimation errors. To handle the unknown degradation in the real application, we introduce a degradation representation network for single image blind restoration (DRN). Different from the methods of estimating pixel space, we use an encoder network to learn abstract representations for estimating different degradation kernels in the representation space. Furthermore, a degradation perception module with flexible adaptability to different degradation kernels is used to restore more structural details. In our experiments, we compare our DRN with several state-of-the-art methods for two image restoration tasks, including image super-resolution (SR) and image denoising. Quantitative results show that our degradation representation network is accurate and efficient for single image restoration.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001006",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Degradation (telecommunications)",
      "Image (mathematics)",
      "Image processing",
      "Image quality",
      "Image restoration",
      "Law",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Jin",
        "given_name": "Yan"
      },
      {
        "surname": "Jiang",
        "given_name": "Zhiwei"
      },
      {
        "surname": "Xue",
        "given_name": "Zhizhong"
      },
      {
        "surname": "Hu",
        "given_name": "Yibiao"
      }
    ]
  },
  {
    "title": "On the multi-level embedding of crypto-image reversible data hiding",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103556",
    "abstract": "Crypto-space reversible image steganography has attracted increasing attention, given its ability to embed authentication information without revealing the image content. This paper presents an efficient reversible data hiding scheme for crypto-images: a block predictor is applied to compute prediction errors, then an adaptive block mapping algorithm is utilized to compress them whose amplitudes are within a small threshold, finally, this strategy can be applied in a multi-level manner to achieve a higher embedding capacity. Due to the correlations among adjacent pixels in the block, images can be sufficiently compressed to reserve abundant space for additional data embedding. Different from the prior arts, the compression code of the image is fully encrypted. Experimental results verify that the embedded data and original image can be perfectly recovered, the security is higher compared with the state-of-the-arts, and a significant improvement in the average embedding rate is achieved on two large-scale image datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032200092X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Authentication (law)",
      "Block (permutation group theory)",
      "Code (set theory)",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Embedding",
      "Encryption",
      "Geometry",
      "Image (mathematics)",
      "Information hiding",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pixel",
      "Programming language",
      "Set (abstract data type)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Xu"
      },
      {
        "surname": "Chang",
        "given_name": "Ching-Chun"
      },
      {
        "surname": "Lin",
        "given_name": "Chia-Chen"
      },
      {
        "surname": "Chang",
        "given_name": "Chin-Chen"
      }
    ]
  },
  {
    "title": "A multi-stage spatio-temporal adaptive network for video super-resolution",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103555",
    "abstract": "Video super-resolution aims at restoring the spatial resolution of the reference frame based on consecutive input low-resolution (LR) frames. Existing implicit alignment-based video super-resolution methods commonly utilize convolutional LSTM (ConvLSTM) to handle sequential input frames. However, vanilla ConvLSTM processes input features and hidden states independently in operations and has limited ability to handle the inter-frame temporal redundancy in low-resolution fields. In this paper, we propose a multi-stage spatio-temporal adaptive network (MS-STAN). A spatio-temporal adaptive ConvLSTM (STAC) module is proposed to handle input features in low-resolution fields. The proposed STAC module utilizes the correlation between input features and hidden states in the ConvLSTM unit and modulates the hidden states adaptively conditioned on fused spatio-temporal features. A residual stacked bidirectional (RSB) architecture is further proposed to fully exploit the processing ability of the STAC unit. The proposed STAC and RSB architecture promote the vanilla ConvLSTM’s ability to exploit the inter-frame correlations, thus improving the reconstruction quality. Furthermore, different from existing methods that only aggregate features from the temporal branch once at a specified stage of the network, the proposed network is organized in a multi-stage manner. The corresponding temporal correlation in features at different stages can be fully exploited. Experimental results on Vimeo-90K-T and UMD10 datasets show that the proposed method has comparable performance with current video super-resolution methods. The code is available at https://github.com/yhjoker/MS-STAN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322000918",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Exploit",
      "Frame (networking)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Redundancy (engineering)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Yuhang"
      },
      {
        "surname": "Chen",
        "given_name": "Zhenzhong"
      },
      {
        "surname": "Liu",
        "given_name": "Shan"
      }
    ]
  },
  {
    "title": "Posture and sequence recognition for Bharatanatyam dance performances using machine learning approaches",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103548",
    "abstract": "Understanding the underlying semantics of performing arts like dance is a challenging task. Analysis of dance is useful to preserve cultural heritage, make video recommendation systems, and build tutoring systems. To create such a dance analysis application, three aspects of dance analysis must be addressed: (1) segment the dance video to find representative action elements, (2) recognize the detected action elements, and (3) recognize sequences formed by combining action elements according to specific rules. This paper attempts to address the three fundamental problems of dance analysis raised above, with a focus on Indian Classical Dance, em Bharatanatyam. Since dance is driven by music, we use both musical and motion information to extract action elements. The action elements are then recognized using machine learning and deep learning techniques. Finally, the Hidden Markov Model (HMM) and Long Short-Term Memory (LSTM) are used to recognize the dance sequence.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322000864",
    "keywords": [
      "Action (physics)",
      "Art",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Dance",
      "Engineering",
      "Focus (optics)",
      "Genetics",
      "Hidden Markov model",
      "Motion (physics)",
      "Optics",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Semantics (computer science)",
      "Sequence (biology)",
      "Systems engineering",
      "Task (project management)",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Mallick",
        "given_name": "Tanwi"
      },
      {
        "surname": "Das",
        "given_name": "Partha Pratim"
      },
      {
        "surname": "Majumdar",
        "given_name": "Arun Kumar"
      }
    ]
  },
  {
    "title": "A two stream convolutional neural network with bi-directional GRU model to classify dynamic hand gesture",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103554",
    "abstract": "Dynamic hand gesture recognition is still an interesting topic for the computer vision community. A set of feature vectors can represent any hand gesture. A Recurrent Neural Network (RNN) can recognize these feature vectors as a hand gesture that analyzes the temporal and contextual information of the gesture sequence. Thus, we proposed a hybrid deep learning framework to recognize dynamic hand gestures. In the Hybrid model GoogleNet is pipelined with a Bidirectional GRU unit to recognize the dynamic hand gesture. Dynamic hand gestures consist of many frames, and features of each frame need to be extracted to get the temporal and dynamic information of the performed gesture. As RNN takes input as a sequence of feature vectors, we extract features from videos using pretrained GoogleNet. As Gated Recurrent Unit is one of the variants of RNN to classify the sequential data, we created a feature vector that corresponds to each video and passed it to the bidirectional GRU (BGRU) network to classify the gestures. We evaluate our model on four publicly available hand gesture datasets. The proposed method performs well and is comparable with the existing methods. For instance, we achieved 98.6% accuracy on Northwestern University Hand Gesture(NWUHG), 99.6% on SKIG, 99.4% on Cambridge Hand Gesture (CHG) datasets respectively. We performed our experiments on DHG14/28 dataset and achieved an accuracy of 97.8% with 14-gesture classes and 92.1% on 28-gesture classes. DHG14/28 dataset contains skeleton and depth data, and our proposed model used depth data and achieved comparable accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322000906",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Feature (linguistics)",
      "Feature vector",
      "Frame (networking)",
      "Gesture",
      "Gesture recognition",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Recurrent neural network",
      "Speech recognition",
      "Support vector machine",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Verma",
        "given_name": "Bindu"
      }
    ]
  },
  {
    "title": "A penalty function semi-continuous thresholding methods for constraints of hashing problems",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103552",
    "abstract": "The Hashing process is an effective tool for handling large-scale data (for example, images, videos, or multi-model data) retrieval problems. To get better retrieval accuracy, hashing models usually are imposed with three rigorous constraints, i.e., discrete binary constraint, uncorrelated condition, and the balanced constraint, which will lead to being ‘NP-hard’. In this study, we divide the whole constraints set into the uncorrelated (orthogonality) constraint and the binary discrete balance constraint and propose a fast and accurate penalty function semi-continuous thresholding (PFSCT) hash coding algorithm based on forward–backward algorithms. In addition, we theoretically analyze the equivalence between the relaxed model and the original problems. Extensive numerical experiments on diverse large-scale benchmark datasets demonstrate comparable performance and effectiveness of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322000888",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer security",
      "Double hashing",
      "Dynamic perfect hashing",
      "Evolutionary biology",
      "Function (biology)",
      "Hash function",
      "Hash table",
      "Image (mathematics)",
      "K-independent hashing",
      "Mathematical optimization",
      "Mathematics",
      "Penalty method",
      "Perfect hash function",
      "Thresholding"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Qian"
      },
      {
        "surname": "Shen",
        "given_name": "Zhengwei"
      },
      {
        "surname": "Chen",
        "given_name": "Zhe"
      }
    ]
  },
  {
    "title": "Fast coding unit partitioning algorithms for versatile video coding intra coding",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103542",
    "abstract": "Versatile video coding (VVC) is the newest video compression standard. It adopts quadtree with nested multi-type tree (QT-MTT) to encode square or rectangular coding units (CUs). The QT-MTT coding structure is more flexible for encoding video texture, but it is also accompanied by many time-consuming algorithms. So, this work proposes fast algorithms to determine horizontal or vertical split for binary or ternary partition of a 32 × 32 CU in the VVC intra coding to replace the rate-distortion optimization (RDO) process, which is time-consuming. The proposed fast algorithms are actually a two-step algorithm, including feature analysis method and deep learning method. The feature analysis method is based on variances of pixels, and the deep learning method applies the convolution neural networks (CNNs) for classification. Experimental results show that the proposed method can reduce encoding time by 28.94% on average but increase Bjontegaard delta bit rate (BDBR) by about 0.83%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322000827",
    "keywords": [
      "Algorithm",
      "Coding (social sciences)",
      "Coding tree unit",
      "Computer science",
      "Context-adaptive binary arithmetic coding",
      "Context-adaptive variable-length coding",
      "Data compression",
      "Decoding methods",
      "Mathematics",
      "Statistics",
      "Tunstall coding",
      "Variable-length code"
    ],
    "authors": [
      {
        "surname": "Fang",
        "given_name": "Jiunn-Tsair"
      },
      {
        "surname": "Liu",
        "given_name": "Bang-Hao"
      },
      {
        "surname": "Chang",
        "given_name": "Pao-Chi"
      }
    ]
  },
  {
    "title": "Unsupervised blind image quality assessment based on joint structure and natural scene statistics features",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103579",
    "abstract": "Compared with the widely used supervised blind image quality assessment (BIQA) models, unsupervised BIQA models require little prior knowledge for calculating the objective quality scores of distorted images. In this paper, we propose an unsupervised BIQA method that aims to achieve both good performance and generalization capability with low computational complexity. Carefully selected and extensive structure and natural scene statistics (NSS) features can better represent image quality. First, we employ phase congruency (PC) and finely selected gradient magnitude map and Laplacian of Gaussian response (GM-LOG) features to represent image structure information. Second, we calculate the local mean-subtracted and contrast-normalized (MSCN) coefficients and the Karhunen–Loéve transform (KLT) coefficients to represent the naturalness of the distorted images. Last, multivariate Gaussian (MVG) model with joint features extracted from both the pristine images and the distorted images is adopted to calculate the objective image quality. Extensive experiments conducted on nine IQA databases demonstrate that the proposed method achieves better performance than the state-of-the-art BIQA methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001092",
    "keywords": [
      "Architectural engineering",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Contrast (vision)",
      "Economics",
      "Engineering",
      "Gaussian",
      "Generalization",
      "Image (mathematics)",
      "Image quality",
      "Joint (building)",
      "Mathematical analysis",
      "Mathematics",
      "Metric (unit)",
      "Naturalness",
      "Neuroscience",
      "Operations management",
      "Pattern recognition (psychology)",
      "Perception",
      "Physics",
      "Quality Score",
      "Quantum mechanics",
      "Scene statistics",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "He",
        "given_name": "Qinglin"
      },
      {
        "surname": "Yang",
        "given_name": "Chao"
      },
      {
        "surname": "Yang",
        "given_name": "Fanxi"
      },
      {
        "surname": "An",
        "given_name": "Ping"
      }
    ]
  },
  {
    "title": "Multi-task learning for video anomaly detection",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103547",
    "abstract": "We propose a multi-task learning framework for video anomaly detection based on a novel pipeline. Our model contains two crossing streams, one stream employs the backbone of Attention-R2U-net for future frame prediction, while the other is designed based on an encoder–decoder network to reconstruct the input optical flow maps. In addition, the latent layers of the two streams are merged together and assigned with a Deep SVDD-based loss at each location individually. Through the combination of these three tasks, the two-stream-crossing pipeline can be trained end-to-end to provide a comprehensive evaluation for the anomaly targets. Experimental results on several popular benchmark datasets show that our model outperforms the state-of-the-art competing models, which can be applied to different types of anomalous targets and meanwhile achieves remarkable precision.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322000852",
    "keywords": [
      "Anomaly (physics)",
      "Anomaly detection",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Condensed matter physics",
      "Deep learning",
      "Economics",
      "Encoder",
      "Frame (networking)",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Management",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics",
      "Pipeline (software)",
      "Programming language",
      "Task (project management)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Chang",
        "given_name": "Xingya"
      },
      {
        "surname": "Zhang",
        "given_name": "Yuxin"
      },
      {
        "surname": "Xue",
        "given_name": "Dingyu"
      },
      {
        "surname": "Chen",
        "given_name": "Dongyue"
      }
    ]
  },
  {
    "title": "Fine-grained-based multi-feature fusion for occluded person re-identification",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103581",
    "abstract": "Many previous occluded person re-identification(re-ID) methods try to use additional clues (pose estimation or semantic parsing models) to focus on non-occluded regions. However, these methods extremely rely on the performance of additional clues and often capture pedestrian features by designing complex modules. In this work, we propose a simple Fine-Grained Multi-Feature Fusion Network (FGMFN) to extract discriminative features, which is a dual-branch structure consisting of global feature branch and partial feature branch. Firstly, we utilize a chunking strategy to extract multi-granularity features to make the pedestrian information contained in it more comprehensive. Secondly, a spatial transformer network is introduced to localize the pedestrian’s upper body, and then introduce a relation-aware attention module to explore the fine-grained information. Finally, we fuse the features obtained from the two branches to obtain a more robust pedestrian representation. Extensive experiments verify the effectiveness of our method under the occlusion scenario.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001109",
    "keywords": [
      "Artificial intelligence",
      "Backbone network",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Discriminative model",
      "Electrical engineering",
      "Engineering",
      "Feature (linguistics)",
      "Feature extraction",
      "Focus (optics)",
      "Fuse (electrical)",
      "Granularity",
      "Linguistics",
      "Operating system",
      "Optics",
      "Parsing",
      "Pattern recognition (psychology)",
      "Pedestrian",
      "Pedestrian detection",
      "Philosophy",
      "Physics",
      "Transport engineering"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Guoqing"
      },
      {
        "surname": "Chen",
        "given_name": "Chao"
      },
      {
        "surname": "Chen",
        "given_name": "Yuhao"
      },
      {
        "surname": "Zhang",
        "given_name": "Hongwei"
      },
      {
        "surname": "Zheng",
        "given_name": "Yuhui"
      }
    ]
  },
  {
    "title": "ADPNet: Attention based dual path network for lane detection",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103574",
    "abstract": "Recently, the task of lane detection has been greatly improved with the rapid development of deep learning and autonomous driving. However, there exist limitations like the challenging complex scenarios and real-time efficiency. In this paper, we present a novel Attention Based Dual Path Network (ADPNet) to handle the task of lane detection. The ADPNet treat the process of lane detection as a task of binary semantic segmentation, where the Detail Path is designed to capture detailed low-level information and the Semantic Path with dual attention module is designed to capture contextual high-level information. We use the Feature Aggregation Module to fuse the information of the two paths, followed by the process of lane fitting to get a parametric description of lanes. The proposed ADPNet achieves good trade-off between the accuracy and real-time efficiency on TuSimple and CULane, which are two popular lane detection benchmark datasets. The results demonstrate that our architecture outperforms the current state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001080",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Attention network",
      "Benchmark (surveying)",
      "Binary classification",
      "Computer science",
      "Data mining",
      "Dual (grammatical number)",
      "Economics",
      "Electrical engineering",
      "Engineering",
      "Feature (linguistics)",
      "Fuse (electrical)",
      "Geodesy",
      "Geography",
      "Linguistics",
      "Literature",
      "Management",
      "Mathematics",
      "Object detection",
      "Operating system",
      "Parametric statistics",
      "Path (computing)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Process (computing)",
      "Programming language",
      "Segmentation",
      "Statistics",
      "Support vector machine",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Ren",
        "given_name": "Fenglei"
      },
      {
        "surname": "Zhou",
        "given_name": "Haibo"
      },
      {
        "surname": "Yang",
        "given_name": "Lu"
      },
      {
        "surname": "Liu",
        "given_name": "Fulong"
      },
      {
        "surname": "He",
        "given_name": "Xin"
      }
    ]
  },
  {
    "title": "Discriminative feature mining hashing for fine-grained image retrieval",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103592",
    "abstract": "With the development of multimedia technology, fine-grained image retrieval has gradually become a new hot topic in computer vision, while its accuracy and speed are limited due to the low discriminative high-dimensional real-valued embedding. To solve this problem, we propose an end-to-end framework named DFMH (Discriminative Feature Mining Hashing), which consists of the DFEM (Discriminative Feature Extracting Module) and SHCM (Semantic Hash Coding Module). Specifically, DFEM explores more discriminative local regions by attention drop and obtains finer local feature expression by attention re-sample. SHCM generates high-quality hash codes by combining the quantization loss and bit balance loss. Validated by extensive experiments and ablation studies, our method consistently outperforms both the state-of-the-art generic retrieval methods as well as fine-grained retrieval methods on three datasets, including CUB Birds, Stanford Dogs and Stanford Cars.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322001195",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Discriminative model",
      "Double hashing",
      "Feature (linguistics)",
      "Feature hashing",
      "Hash function",
      "Hash table",
      "Image (mathematics)",
      "Image retrieval",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Lang",
        "given_name": "Wenxi"
      },
      {
        "surname": "Sun",
        "given_name": "Han"
      },
      {
        "surname": "Xu",
        "given_name": "Can"
      },
      {
        "surname": "Liu",
        "given_name": "Ningzhong"
      },
      {
        "surname": "Zhou",
        "given_name": "Huiyu"
      }
    ]
  },
  {
    "title": "Distributed three-level QR codes based on visual cryptography scheme",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103567",
    "abstract": "Owing to the large storage and fast machine recognition, QR codes have been widely utilized in many fields such as mobile payment, website navigation and user identity authentication. However, any QR code reader can access to the message contained in the QR code, the security becomes a major challenge to QR codes for privacy usage scenarios. Moreover, the management of QR codes for users are also inconvenient, since the human vision is hard to distinguish a QR code from the others. To solve the security and management problems, we propose the three-level QR codes for a group of participants. The first-level management information and the second-level public information are recognizable for the human vision and QR code reader device, respectively. The third-level privacy information is protected using visual cryptography scheme, and can be decoded using simple and non-cryptography computations. Furthermore, the shares can be stored or transferred in not only e-format but also print-format and photo-format, leading to the broad applicability. Experimental results and analysis demonstrate that the proposed scheme can encode three-level information into several distributed QR codes, and has more advantages compared with the previous schemes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032200102X",
    "keywords": [
      "Authentication (law)",
      "Biochemistry",
      "Chemistry",
      "Code (set theory)",
      "Computer science",
      "Computer security",
      "Cryptography",
      "ENCODE",
      "Gene",
      "Mathematical analysis",
      "Mathematics",
      "Programming language",
      "Scheme (mathematics)",
      "Secret sharing",
      "Set (abstract data type)",
      "Theoretical computer science",
      "Visual cryptography"
    ],
    "authors": [
      {
        "surname": "Fu",
        "given_name": "Zhengxin"
      },
      {
        "surname": "Fang",
        "given_name": "Liguo"
      },
      {
        "surname": "Huang",
        "given_name": "Hangying"
      },
      {
        "surname": "Yu",
        "given_name": "Bin"
      }
    ]
  },
  {
    "title": "Unified Chinese License Plate detection and recognition with high efficiency",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103541",
    "abstract": "Recently, deep learning-based methods have reached an excellent performance on License Plate (LP) detection and recognition tasks. However, it is still challenging to build a robust model for Chinese LPs since there are not enough large and representative datasets. In this work, we propose a new dataset named Chinese Road Plate Dataset (CRPD) that contains multi-objective Chinese LP images as a supplement to the existing public benchmarks. The images are mainly captured with electronic monitoring systems with detailed annotations. To our knowledge, CRPD is the largest public multi-objective Chinese LP dataset with annotations of vertices. With CRPD, a unified detection and recognition network with high efficiency is presented as the baseline. The network is end-to-end trainable with totally real-time inference efficiency (30 fps with 640 p). The experiments on several public benchmarks demonstrate that our method has reached competitive performance. The code and dataset will be publicly available at https://github.com/yxgong0/CRPD.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322000815",
    "keywords": [
      "Artificial intelligence",
      "Baseline (sea)",
      "Code (set theory)",
      "Computer science",
      "Data mining",
      "Deep learning",
      "Geology",
      "Inference",
      "License",
      "MIT License",
      "Machine learning",
      "Oceanography",
      "Operating system",
      "Pattern recognition (psychology)",
      "Programming language",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Gong",
        "given_name": "Yanxiang"
      },
      {
        "surname": "Deng",
        "given_name": "Linjie"
      },
      {
        "surname": "Tao",
        "given_name": "Shuai"
      },
      {
        "surname": "Lu",
        "given_name": "Xinchen"
      },
      {
        "surname": "Wu",
        "given_name": "Peicheng"
      },
      {
        "surname": "Xie",
        "given_name": "Zhiwei"
      },
      {
        "surname": "Ma",
        "given_name": "Zheng"
      },
      {
        "surname": "Xie",
        "given_name": "Mei"
      }
    ]
  },
  {
    "title": "Information hiding in the sharing domain",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103520",
    "abstract": "Secret image sharing (SIS) can divide a secret image into several shadow images for protection. Information hiding in the sharing domain (IHSD) fuses SIS and information hiding (IH) to simultaneously share any secret image and hide any information, and this technique can be applied in cloud computing, law enforcement and medical diagnoses. IHSD not only marks shadow images with information to prevent malicious tampering and for convenient management, search and identification but also enhances the robustness of IH. In this paper, we first introduce a formal definition of IHSD. Then, we describe a general IHSD model and algorithms with a concrete example in detail. In IHSD, we design the random element utilization model to control the random pixels generated from SIS. Then, we obtain shadow images with hidden information to realize SIS and IH simultaneously. The inputs of SIS with secret images, steganography and extra information in algorithms are without any limitations. Theoretical analyses, experiments and comparisons are presented to prove the effectiveness and feasibility of IHSD.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322000645",
    "keywords": [
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Cryptography",
      "Digital watermarking",
      "Domain (mathematical analysis)",
      "Gene",
      "Image (mathematics)",
      "Image sharing",
      "Information hiding",
      "Information sharing",
      "Mathematical analysis",
      "Mathematics",
      "Psychology",
      "Psychotherapist",
      "Randomness",
      "Robustness (evolution)",
      "Secret sharing",
      "Shadow (psychology)",
      "Statistics",
      "Steganography",
      "Theoretical computer science",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Xing",
        "given_name": "Fengyue"
      },
      {
        "surname": "Yan",
        "given_name": "Xuehu"
      },
      {
        "surname": "Yu",
        "given_name": "Long"
      },
      {
        "surname": "Sun",
        "given_name": "Yuyuan"
      }
    ]
  },
  {
    "title": "Improved inter-view correlations for low complexity MV-HEVC",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103525",
    "abstract": "The more advanced multi-view extension, MV-HEVC, effectively exploits visual similarities between multi-view videos and enables high compression efficiency. Each view in the multi-view sequence depends on the captured scene, the distance between cameras and recording angles. Increasing the distance between dependent viewpoints generates an inter-view disparity. This impacts the inter-view similarities, affects the disparity estimation and further increases the computational complexity of the MV-HEVC encoder. In this paper, an efficient earlier disparity estimation is proposed for low complexity MV-HEVC. This algorithm is based on reducing the complexity of disparity estimation by eliminating the inter-view offset. Moreover, the inter-view similarities are controlled by considering the reliability of each coding unit size in the search range. This reliability is estimated by reducing the number of searching points within a new limited window. For reliable motion estimation, we further proposed an earlier decision of coding units splitting in the dependent views according to those in the reference views. Experimental results show that the proposed algorithm can achieve an average encoding time saving of 20.37%–40,61% with marginal performance degradation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322000682",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Coding (social sciences)",
      "Computational complexity theory",
      "Computer science",
      "Computer vision",
      "Encoder",
      "Mathematics",
      "Offset (computer science)",
      "Operating system",
      "Programming language",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Belbel",
        "given_name": "Amel"
      },
      {
        "surname": "Bekhouch",
        "given_name": "Amara"
      },
      {
        "surname": "Doghmane",
        "given_name": "Noureddine"
      },
      {
        "surname": "Harize",
        "given_name": "Saliha"
      },
      {
        "surname": "Kouadria",
        "given_name": "Nasreddine"
      }
    ]
  },
  {
    "title": "Underwater image super-resolution and enhancement via progressive frequency-interleaved network",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103545",
    "abstract": "Underwater images usually contain severely blurred details, color distortion, and low contrast, warranting efficient methods to obtain clean images. However, most convolutional neural network-based approaches involve high computational cost, numerous model parameters, and even poor performance. Besides, the mapping from input to output is learned using a single path, ignoring the frequency domain information. To solve these challenges, we propose a novel progressive frequency-interleaved network (PFIN) for underwater imagery super-resolution and enhancement. Specifically, progressive frequency-domain module (PFDM) and convolution-guided module (CGM) constitute PFIN for effective color deviation correction and detail enhancement. PFDM that possesses global spatial attention, multi-scale residual, and frequency information modulation blocks gradually learn frequency features and explicitly compensate for detail loss. Furthermore, CGM comprising a series of convolution blocks generates discriminative characteristics to modulate in PFDM for better accommodating degraded representations. Extensive experiments demonstrate the superiority of our PFIN regarding quantitative evaluations and visual quality.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322000839",
    "keywords": [
      "Algorithm",
      "Amplifier",
      "Artificial intelligence",
      "Artificial neural network",
      "Bandwidth (computing)",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Discriminative model",
      "Distortion (music)",
      "Frequency domain",
      "Geology",
      "Oceanography",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Residual",
      "Spatial frequency",
      "Telecommunications",
      "Underwater"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Li"
      },
      {
        "surname": "Xu",
        "given_name": "Lizhong"
      },
      {
        "surname": "Tian",
        "given_name": "Wei"
      },
      {
        "surname": "Zhang",
        "given_name": "Yunfei"
      },
      {
        "surname": "Feng",
        "given_name": "Hui"
      },
      {
        "surname": "Chen",
        "given_name": "Zhe"
      }
    ]
  },
  {
    "title": "SecureDL: A privacy preserving deep learning model for image recognition over cloud",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103503",
    "abstract": "The key benefits of cloud services such as low cost, access flexibility, and mobility have attracted worldwide users to utilize deep learning algorithms for computer vision. These cloud servers are maintained by third parties, where users are always concerned about sharing their confidential data with them. In this paper, we addressed these concerns for by developing SecureDL, a privacy-preserving image recognition model for encrypted data over cloud. The proposed block-based image encryption scheme is well designed to protect image’s visual information. The scheme constitutes an order-preserving permutation ordered binary number system and pseudo-random matrices. The proposed method is proved to be secure in a probabilistic viewpoint, and using various cryptographic attacks. Experiments are conducted over several image recognition datasets, and the trade-off analytics between the achieved recognition accuracy and data encryption is well described. SecureDL overcomes the storage and computational overheads that occur with fully-homomorphic and multi-party computation based secure recognition schemes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322000529",
    "keywords": [
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Cloud computing",
      "Computer network",
      "Computer science",
      "Computer security",
      "Cryptography",
      "Data mining",
      "Encryption",
      "Flexibility (engineering)",
      "Geometry",
      "Homomorphic encryption",
      "Information privacy",
      "Mathematics",
      "Operating system",
      "Secure multi-party computation",
      "Server",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Tanwar",
        "given_name": "Vishesh Kumar"
      },
      {
        "surname": "Raman",
        "given_name": "Balasubramanian"
      },
      {
        "surname": "Rajput",
        "given_name": "Amitesh Singh"
      },
      {
        "surname": "Bhargava",
        "given_name": "Rama"
      }
    ]
  },
  {
    "title": "A brief survey on adaptive video streaming quality assessment",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103526",
    "abstract": "Quality of experience (QoE) assessment for adaptive video streaming plays a significant role in advanced network management systems. It is especially challenging in case of dynamic adaptive streaming schemes over HTTP (DASH) which has increasingly complex characteristics including additional playback issues. In this paper, we provide a brief overview of adaptive video streaming quality assessment. Upon our review of related works, we analyze and compare different variations of objective QoE assessment models with or without using machine learning techniques for adaptive video streaming. Through the performance analysis, we observe that hybrid models perform better than both quality-of-service (QoS) driven QoE approaches and signal fidelity measurement. Moreover, the machine learning-based model slightly outperforms the model without using machine learning for the same setting. In addition, we find that existing video streaming QoE assessment models still have limited performance, which makes it difficult to be applied in practical communication systems. Therefore, based on the success of deep learned feature representations for traditional video quality prediction, we also apply the off-the-shelf deep convolutional neural network (DCNN) to evaluate the perceptual quality of streaming videos, where the spatio-temporal properties of streaming videos are taken into consideration. Experiments demonstrate its superiority, which sheds light on the future development of specifically designed deep learning frameworks for adaptive video streaming quality assessment. We believe this survey can serve as a guideline for QoE assessment of adaptive video streaming.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322000694",
    "keywords": [
      "Artificial intelligence",
      "Computer network",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Dynamic Adaptive Streaming over HTTP",
      "Economics",
      "Epistemology",
      "Fidelity",
      "Image (mathematics)",
      "Image quality",
      "Machine learning",
      "Metric (unit)",
      "Multimedia",
      "Operations management",
      "Philosophy",
      "Quality (philosophy)",
      "Quality of experience",
      "Quality of service",
      "Real-time computing",
      "Subjective video quality",
      "Telecommunications",
      "Video quality"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Wei"
      },
      {
        "surname": "Min",
        "given_name": "Xiongkuo"
      },
      {
        "surname": "Li",
        "given_name": "Hong"
      },
      {
        "surname": "Jiang",
        "given_name": "Qiuping"
      }
    ]
  },
  {
    "title": "Counting Objects by Diffused Index: Geometry-free and training-free approach",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103527",
    "abstract": "Counting objects is a fundamental but challenging problem. In this paper, we propose diffusion-based, geometry-free, and learning-free methodologies to count the number of objects in images. The main idea is to represent each object by a unique index value regardless of its intensity or size, and to simply count the number of index values. First, we place different vectors, refer to as seed vectors, uniformly throughout the mask image. The mask image has boundary information of the objects to be counted. Secondly, the seeds are diffused using an edge-weighted harmonic variational optimization model within each object. We propose an efficient algorithm based on an operator splitting approach and alternating direction minimization method, and theoretical analysis of this algorithm is given. An optimal solution of the model is obtained when the distributed seeds are completely diffused such that there is a unique intensity within each object, which we refer to as an index. For computational efficiency, we stop the diffusion process before a full convergence, and propose to cluster these diffused index values. We refer to this approach as Counting Objects by Diffused Index (CODI). We explore scalar and multi-dimensional seed vectors. For Scalar seeds, we use Gaussian fitting in histogram to count, while for vector seeds, we exploit a high-dimensional clustering method for the final step of counting via clustering. The proposed method is flexible even if the boundary of the object is not clear nor fully enclosed. We present counting results in various applications such as biological cells, agriculture, concert crowd, and transportation. Some comparisons with existing methods are presented.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322000700",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Boundary (topology)",
      "Cluster analysis",
      "Computer science",
      "Computer vision",
      "Geometry",
      "Histogram",
      "Image (mathematics)",
      "Mathematical analysis",
      "Mathematics",
      "Object (grammar)",
      "Scalar (mathematics)"
    ],
    "authors": [
      {
        "surname": "Tang",
        "given_name": "Mengyi"
      },
      {
        "surname": "Yashtini",
        "given_name": "Maryam"
      },
      {
        "surname": "Kang",
        "given_name": "Sung Ha"
      }
    ]
  },
  {
    "title": "Multi-stream feature refinement network for human object interaction detection",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103529",
    "abstract": "Human–Object Interaction (HOI) detection is a crucial problem for comprehensive visual understanding, which aims to detect < h u m a n , a c t i o n , o b j e c t > triplets within an image. Many existing methods often exploit to integrate the human and object visual features , the spatial layout of human–object pairs, human poses, contextual information, and even object semantic information into a framework to infer the interactions, proving that all these components can contribute to improve the HOI detection. However, most methods simply concatenate these components that are not explicitly embedded in the feature learning for HOI detection. In this paper, we are trying to fuse these components explicitly using a multi-stream feature refinement network. The network extracts the visual features of humans, contexts, and objects, which receives the attentions from human poses, spatial configurations, and semantic prior knowledge of objects to refine these visual features, respectively. In addition, an additional graph neural network is employed here to learn the structural features of human–object pairs. We verify our method on V-COCO and HICO-DET datasets with extensive experiments. The experimental results demonstrate that our method is a simple yet effective for HOI detection, achieving superior performance to those state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322000712",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Electrical engineering",
      "Engineering",
      "Exploit",
      "Feature (linguistics)",
      "Fuse (electrical)",
      "Graph",
      "Linguistics",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Theoretical computer science",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Shao",
        "given_name": "Zhanpeng"
      },
      {
        "surname": "Hu",
        "given_name": "Zhongyan"
      },
      {
        "surname": "Yang",
        "given_name": "Jianyu"
      },
      {
        "surname": "Li",
        "given_name": "Youfu"
      }
    ]
  },
  {
    "title": "Restoration of speckle noise corrupted SAR images using regularization by denoising",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103546",
    "abstract": "Speckle noise removal is a well-established problem in synthetic aperture radar (SAR) image processing. Among different methods focused on the reconstruction of SAR images, variational models have achieved state-of-the-art performance. In this paper, a Rayleigh based speckle reduction algorithm is developed using the variational framework. The forward model is combined with recently proposed regularization by denoising (RED) prior. However, RED has been proposed in literature for the additive noise model. Multiplicative noise in SAR images prevents the direct application of RED to variational models. Hence, logarithm transformation is applied to change the multiplicative noise model to additive model, and the forward model from Rayleigh to Fisher–Tippett distribution. The resulting optimization problem is solved using the alternating direction method of multipliers. Further, the proof of the convergence analysis is carried out for the above framework. Simulations convey that the proposed method has better despeckling performance compared to that of state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322000840",
    "keywords": [
      "Algorithm",
      "Analog signal",
      "Artificial intelligence",
      "Computer hardware",
      "Computer science",
      "Digital signal processing",
      "Image (mathematics)",
      "Image processing",
      "Image restoration",
      "Mathematics",
      "Multiplicative noise",
      "Noise (video)",
      "Noise reduction",
      "Regularization (linguistics)",
      "Signal transfer function",
      "Speckle noise",
      "Speckle pattern",
      "Synthetic aperture radar"
    ],
    "authors": [
      {
        "surname": "Baraha",
        "given_name": "Satyakam"
      },
      {
        "surname": "Sahoo",
        "given_name": "Ajit Kumar"
      }
    ]
  },
  {
    "title": "Deep learning and RGB-D based human action, human–human and human–object interaction recognition: A survey",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103531",
    "abstract": "Human activity recognition is one of the most studied topics in the field of computer vision. In recent years, with the availability of RGB-D sensors and powerful deep learning techniques, research on human activity recognition has gained momentum. From simple human atomic actions, the research has advanced towards recognizing more complex human activities using RGB-D data. This paper presents a comprehensive survey of the advanced deep learning based recognition methods and categorizes them in human atomic action, human–human interaction, human–object interaction. The reviewed methods are further classified based on the individual modality used for recognition i.e. RGB based, depth based, skeleton based, and hybrid. We also review and categorize recent challenging RGB-D datasets for the same. In addition, the paper also briefly reviews RGB-D datasets and methods for online activity recognition. The paper concludes with a discussion on limitations, challenges, and recent trends for promising future directions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322000724",
    "keywords": [
      "Action (physics)",
      "Action recognition",
      "Artificial intelligence",
      "Categorization",
      "Class (philosophy)",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Field (mathematics)",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Pure mathematics",
      "Quantum mechanics",
      "RGB color model"
    ],
    "authors": [
      {
        "surname": "Khaire",
        "given_name": "Pushpajit"
      },
      {
        "surname": "Kumar",
        "given_name": "Praveen"
      }
    ]
  },
  {
    "title": "Fragile watermarking scheme for tamper localization in images using logistic map and singular value decomposition",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103500",
    "abstract": "This paper presents a fragile watermarking scheme for tamper localization using Singular Value Decomposition (SVD) and logistic map. The proposed scheme divides the image into blocks of size 2 × 2 pixels and generates an 8-bit watermark from each block. The watermark is computed by permuting the six Most Significant Bits (MSBs) of each pixel in the block using the logistic map, followed by SVD. To secure, the watermark thus generated is further encrypted using the logistic map. This encrypted watermark is embedded into 2 Least Significant Bits (LSBs) of each pixel to enable tamper detection and localization. The experimental results demonstrate that the proposed scheme can precisely locate tampered regions under copy-paste, content removal, text addition, noise addition, vector quantization, collage, content only, and constant feature attacks. Tamper localization accuracy is better or comparable to the state-of-the-art tamper localization algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322000517",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Chaotic",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Digital watermarking",
      "Encryption",
      "Geometry",
      "Image (mathematics)",
      "Logistic map",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pixel",
      "Scheme (mathematics)",
      "Singular value decomposition",
      "Watermark"
    ],
    "authors": [
      {
        "surname": "N.R.",
        "given_name": "Neena Raj"
      },
      {
        "surname": "R.",
        "given_name": "Shreelekshmi"
      }
    ]
  },
  {
    "title": "Neural Style Transfer for image within images and conditional GANs for destylization",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103483",
    "abstract": "In this paper, the feature representation of an image by CNN is used to hide the secret image into the cover image. The style of the cover image hides the content of the secret image and produce a stego image using Neural Style Transfer (NST) algorithm, which resembles the cover image and also contains the semantic content of secret image. The main technical contributions are to hide the content of the secret image in the in-between hidden layered style features of the cover image, which is the first of its kind in the present state-of-art-technique. Also, to recover the secret image from the stego image, destylization is done with the help of conditional generative adversarial networks (GANs) using Residual in Residual Dense Blocks (RRDBs). Further, stego images from different layer combinations of content and style features are obtained and evaluated. Evaluation is based on the visual similarity and quality loss between the cover-stego pair and the secret-reconstructed secret pair of images. From the experiments, it has been observed that the proposed algorithm has 43.95 dB Peak Signal-to-Noise Ratio (PSNR)), .995 Structural Similarity Index (SSIM), and .993 Visual Information Fidelity (VIF) for the ImageNet dataset. The proposed algorithm is found to be more robust against StegExpose than the traditional methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322000360",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Mathematics",
      "Parallel computing",
      "Pattern recognition (psychology)",
      "Style (visual arts)",
      "Transfer (computing)",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Mallika",
        "given_name": ""
      },
      {
        "surname": "Ubhi",
        "given_name": "Jagpal Singh"
      },
      {
        "surname": "Aggarwal",
        "given_name": "Ashwani Kumar"
      }
    ]
  },
  {
    "title": "Adaptive reversible data hiding for JPEG images with multiple two-dimensional histograms",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103487",
    "abstract": "Joint photographic experts group (JPEG) can provide good quality with small file size but also eliminate extensively the redundancies of images. Therefore, hiding data into JPEG images in terms of maintaining high visual quality at small file sizes has been a great challenge for researchers. In this paper, an adaptive reversible data hiding method for JPEG images containing multiple two-dimensional (2D) histograms is proposed. Adaptability is mainly reflected in three aspects. The first one is to preferentially select sharper histograms for data embedding after K histograms are established by constructing the k th ( k ∈ { 1 , 2 , … , K } ) histogram using the k th non-zero alternating current (AC) coefficient of all the quantized discrete cosine transform blocks. On the other hand, to fully exploit the strong correlation between coefficients of one histogram, the smoothness of each coefficient is estimated by a block smoothness estimator so that a sharply-distributed 2D-histogram is constructed by combining two coefficients with similar smoothness into a pair. The pair corresponding to low complexity is selected priorly for data embedding, leading to high embedding performance while maintaining low file size. Besides, we design multiple embedding strategies to adaptively select the embedding strategy for each 2D histogram. Experimental results demonstrate that the proposed method can achieve higher rate–distortion performance which maintaining lower file storage space, compared with previous studies.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322000414",
    "keywords": [
      "Adaptive histogram equalization",
      "Algorithm",
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Computer science",
      "Data compression",
      "Discrete cosine transform",
      "Embedding",
      "Estimator",
      "File size",
      "Geometry",
      "Histogram",
      "Histogram equalization",
      "Image (mathematics)",
      "Information hiding",
      "JPEG",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Weng",
        "given_name": "Shaowei"
      },
      {
        "surname": "Zhou",
        "given_name": "Ye"
      },
      {
        "surname": "Zhang",
        "given_name": "Tiancong"
      }
    ]
  },
  {
    "title": "Facial makeup transfer with GAN for different aging faces",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103464",
    "abstract": "Facial aging is widely used in criminal tracking and the search for lost children. If the aging face is made up, it will greatly affect the discrimination of the tracking system. Therefore, the research on the makeup of different aging faces is extremely important. Existing studies have achieved a good transition from the non-makeup domain to the makeup domain in facial makeup transfer. But few studies involve the transfer of facial makeup at different ages. In addition, existing datasets rarely contain both age and makeup attributes, which make the transfer of facial makeup for different ages full of challenges. To solve the above problems, we propose a learning framework, called AM-Net, which can realize facial makeup transfer for different ages while protecting identity information. AM-Net is composed of two sub-network modules: Aging-Net and Makeup-Net. AM-Net first learns the aging mechanism of faces through Aging-Net, and then, it feeds the learned aging mode to Makeup-Net. After that, AM-Net trains Makeup-Net to realize the mapping relationship between the non-makeup domain to the makeup domain and transfer the makeup style to the face of the non-makeup. Throughout the network, multiple losses are applied to ensure AM-Net preserve information about the identity, background, etc. Extensive experiments are conducted on different datasets with different state-of-the-art methods, which prove the effectiveness of AM-Net.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322000244",
    "keywords": [
      "Aesthetics",
      "Art",
      "Artificial intelligence",
      "Computer science",
      "Domain (mathematical analysis)",
      "Face (sociological concept)",
      "Geometry",
      "Identity (music)",
      "Mathematical analysis",
      "Mathematics",
      "Net (polyhedron)",
      "Parallel computing",
      "Pedagogy",
      "Psychology",
      "Social science",
      "Sociology",
      "Tracking (education)",
      "Transfer (computing)"
    ],
    "authors": [
      {
        "surname": "Fang",
        "given_name": "Sen"
      },
      {
        "surname": "Duan",
        "given_name": "Mingxing"
      },
      {
        "surname": "Li",
        "given_name": "Kenli"
      },
      {
        "surname": "Li",
        "given_name": "Keqin"
      }
    ]
  },
  {
    "title": "Dual-path image pair joint discrimination for visible–infrared person re-identification",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103512",
    "abstract": "Because the imaging spectra of infrared images and visible light images are different, there is a huge modal difference between visible light images and infrared ones. Existing methods use image conversion to solve the problem of modal difference between two images, but these methods usually fail to focus on the complete information of images, which lead to the results of cross modal person re-identification are unstable. To solve this problem, we propose a new visible–infrared person re-identification method, called dual-path image pair joint discriminant model (DPJD), which simultaneously optimizes the distance within and between classes, and supervises the network learning to identify feature representations. We generate images with different modalities for the samples, and separately compose the same modality image pair and different modality image pair so as to overcome the inconsistent alignment issues. In addition, we also propose a discriminant module based on dual-path (DMDP) to improve the generation quality and discrimination accuracy of image pairs. Experiments on two benchmark datasets SYSU-MM01 and RegDB demonstrate its effectiveness.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322000591",
    "keywords": [
      "Architectural engineering",
      "Art",
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Computer vision",
      "Dual (grammatical number)",
      "Engineering",
      "Identification (biology)",
      "Image (mathematics)",
      "Infrared",
      "Joint (building)",
      "Literature",
      "Optics",
      "Path (computing)",
      "Pattern recognition (psychology)",
      "Physics",
      "Programming language"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Zhongjie"
      },
      {
        "surname": "Liu",
        "given_name": "Li"
      },
      {
        "surname": "Zhang",
        "given_name": "Huaxiang"
      }
    ]
  },
  {
    "title": "LRHW-AP: Using ranking-based metric as loss for Person Re-Identification",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103517",
    "abstract": "Optimizing a ranking-based metric as the loss function, such as Average Precision (AP), has been found very effective in image retrieval tasks, but it has received less attention in Person Re-Identification (Re-ID). In this paper, Low Rank High Weight (LRHW) AP is proposed to apply the AP-optimizing method on the Re-ID task. LRHW-AP employs high weight on the low rank positive instances, which provides more information for model optimization than high rank positive instances and distribute in high gradient area. We propose a new pooling method called Power Activation Weighted Mean (PAWM) pooling which can unify a set of pooling methods because of a changeable activation function and a trainable parameter. Thus one can adjust and train PAWM to adapt to the target task to improve the model performance. Besides, we incorporate Warmup and Exponentially Decay Scheduler with a delay period, called Warmup Delay Exponentially Decay Scheduler, which brings further improvement. Through an extensive set of ablation studies, we verify that all methods mentioned above contribute to the performance boosts on Re-ID and the model achieves 95.3% rank-1 and 88.4% mAP on Market1501 with ResNet50.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322000633",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Economics",
      "Evolutionary biology",
      "Function (biology)",
      "Identification (biology)",
      "Machine learning",
      "Management",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Pooling",
      "Programming language",
      "Rank (graph theory)",
      "Ranking (information retrieval)",
      "Set (abstract data type)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Yifei"
      },
      {
        "surname": "Liang",
        "given_name": "Yaling"
      },
      {
        "surname": "Chen",
        "given_name": "Ziheng"
      }
    ]
  },
  {
    "title": "Deep multi-query video retrieval",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103501",
    "abstract": "Video retrieval methods have been developed for a single query. Multi-query video retrieval problem has not been investigated yet. In this study, an efficient and fast multi-query video retrieval framework is developed. Query videos are assumed to be related to more than one semantic. The framework supports an arbitrary number of video queries. The method is built upon using binary video hash codes. As a result, it is fast and requires a lower storage space. Database and query hash codes are generated by a deep hashing method that not only generates hash codes but also predicts query labels when they are chosen outside the database. The retrieval is based on the Pareto front multi-objective optimization method. Re-ranking performed on the retrieved videos by using non-binary deep features increases the retrieval accuracy considerably. Simulations carried out on two multi-label video databases show that the proposed method is efficient and fast in terms of retrieval accuracy and time.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322000530",
    "keywords": [
      "Algorithm",
      "Block code",
      "Computer science",
      "Computer security",
      "Data mining",
      "Decoding methods",
      "Hamming code",
      "Hamming space",
      "Hash function",
      "Information retrieval",
      "Query expansion",
      "Query optimization",
      "Ranking (information retrieval)"
    ],
    "authors": [
      {
        "surname": "Akbacak",
        "given_name": "Enver"
      },
      {
        "surname": "Vural",
        "given_name": "Cabir"
      }
    ]
  },
  {
    "title": "A dynamic constraint representation approach based on cross-domain dictionary learning for expression recognition",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103458",
    "abstract": "Facial expression recognition (FER) is an active research area that has attracted much attention from both academics and practitioners of different fields. In this paper, we investigate an interesting and challenging issue in FER, where the training and testing samples are from a cross-domain dictionary. In this context, the data and feature distribution are inconsistent, and thus most of the existing recognition methods may not perform well. Given this, we propose an effective dynamic constraint representation approach based on cross-domain dictionary learning for expression recognition. The proposed approach aims to dynamically represent testing samples from source and target domains, thereby fully considering the feature elasticity in a cross-domain dictionary. We are therefore able to use the proposed approach to predict class information of unlabeled testing samples. Comprehensive experiments carried out using several public datasets confirm that the proposed approach is superior compared to some state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322000189",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Constraint (computer-aided design)",
      "Context (archaeology)",
      "Domain (mathematical analysis)",
      "Expression (computer science)",
      "Facial expression recognition",
      "Facial recognition system",
      "Feature (linguistics)",
      "Feature learning",
      "Geometry",
      "Labeled data",
      "Law",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Programming language",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Zhe"
      },
      {
        "surname": "Chiong",
        "given_name": "Raymond"
      },
      {
        "surname": "Hu",
        "given_name": "Zheng-ping"
      },
      {
        "surname": "Dhakal",
        "given_name": "Sandeep"
      }
    ]
  },
  {
    "title": "XOR-based visual cryptography scheme with essential shadows",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103513",
    "abstract": "Visual cryptography scheme with essential shadows (EVCS) is of great significance since it provides different levels of the importance to shadows. In this paper, we propose a general construction method for (t, s, k, n)-VCS with essential shadows based on XOR operation ((t, s, k, n)-EXVCS), which originates from the partition of access structure. The secret image is encrypted into s essential shadows and n-s non-essential shadows. Any k shadows including at least t essentials can cooperate to decode the secret image and the decoding process is implemented by XOR operation on the involved shadows. Our scheme achieves perfectly reconstruction of secret image in the revealed image and the less size of shadows and revealed images. The experiments are conducted to testify the feasibility and practicability of the proposed scheme.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322000608",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Bitwise operation",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Cryptography",
      "Decoding methods",
      "Encryption",
      "Exclusive or",
      "Image (mathematics)",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Process (computing)",
      "Programming language",
      "Scheme (mathematics)",
      "Secret sharing",
      "Theoretical computer science",
      "Visual cryptography"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Peng"
      },
      {
        "surname": "Yin",
        "given_name": "Liping"
      },
      {
        "surname": "Ma",
        "given_name": "Jianfeng"
      },
      {
        "surname": "Wang",
        "given_name": "Hongtao"
      }
    ]
  },
  {
    "title": "Cheating in (halftone-secret) visual cryptography: Analysis of blind authentication schemes",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103489",
    "abstract": "In visual cryptography (VC), cheating is an important security concern where dishonest participants will fool honest ones and make them accept a fake secret by providing fake shares. Share and blind authentications are two categories of cheating prevention, and the last one relies on the inherent robust of shares against cheating attacks. In the previous studies, cheating in VC only focuses on operating a ‘pixel block’ instead of a region of adjacent pixels. However, the well-known advantage of VC is to decode the secret image by using the human vision system (HVS), so it leads to a natural issue to reconsider cheating a region. In this paper, we formally address the binocular cheating attack (BCA) for a region to augment effectiveness of original cheating for a block. Finally, we demonstrate how to realize BCA by presenting non-trivial techniques against some blind authentication schemes, and further obtain implausible results. The BCA can also be applied to halftone secret.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322000426",
    "keywords": [
      "Artificial intelligence",
      "Authentication (law)",
      "Cheating",
      "Computer science",
      "Computer security",
      "Cryptography",
      "Digital watermarking",
      "Image (mathematics)",
      "Internet privacy",
      "Psychology",
      "Secret sharing",
      "Social psychology",
      "Visual cryptography"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Yu-Chi"
      },
      {
        "surname": "Horng",
        "given_name": "Gwoboa"
      }
    ]
  },
  {
    "title": "Cattle behavior recognition based on feature fusion under a dual attention mechanism",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103524",
    "abstract": "In recent years, artificial intelligence has been widely used in such fields as agricultural informatization, precision agriculture and precision animal husbandry. Due to limited research on deep learning in real-time agricultural and pastoral situations, deep learning and computer vision have become very important topics in the agricultural field. Recent studies have shown that the fusion of features under different attention mechanisms will help advance the utilization of such features, and will thus influence the accuracy and generalization ability of the models used. In this paper, we propose a lightweight network structure based on feature fusion under a dual attention mechanism with the same activation and joint loss functions. More specifically, we propose an innovative method to improve the network structure of two different attention mechanisms, and achieve feature fusion by combining the two. At the same time, we keep the activation functions consistent with those of the original network structure, and we develop a joint loss function to expand the use of various features. We also take the novel approach of applying the trajectory behavior analysis method to walking and standing. Experiments using both a publicly available data set and a data set obtained from a farm show that our algorithm achieves state-of-the-art performance in terms of accuracy and generalization ability, as compared to other methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322000670",
    "keywords": [
      "Architectural engineering",
      "Art",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Dual (grammatical number)",
      "Engineering",
      "Epistemology",
      "Evolutionary biology",
      "Feature (linguistics)",
      "Field (mathematics)",
      "Function (biology)",
      "Fusion",
      "Fusion mechanism",
      "Generalization",
      "Joint (building)",
      "Linguistics",
      "Lipid bilayer fusion",
      "Literature",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Mechanism (biology)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Pure mathematics",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Shang",
        "given_name": "Cheng"
      },
      {
        "surname": "Wu",
        "given_name": "Feng"
      },
      {
        "surname": "Wang",
        "given_name": "MeiLi"
      },
      {
        "surname": "Gao",
        "given_name": "Qiang"
      }
    ]
  },
  {
    "title": "Colorful 3D reconstruction at high resolution using multi-view representation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103486",
    "abstract": "High-quality 3D models should contain accurate shapes, as well as other correct attributes, such as realistic surface color. However, current researches were mostly focused on the reconstruction of shapes. We present a method to reconstruct high-resolution colorful 3D models from single images. Shapes and colors are learned separately, using a coarse-to-fine strategy in which the 3D color is expressed as 3-channel volumes. Colorful volumes share the same spatial dimension with generated shape volumes. We propose orthographic colorful maps to retain and recover projected coordinates and corresponding color for 3D surface points. To achieve a fine granularity increase in the quality of maps from low-resolution to high-resolution, we introduce 2D super resolution during reconstructing 3D shapes and color volumes. Models are carved by utilizing predicted high-resolution silhouette, depth and color details. Experimental results in a subset of the ShapeNet dataset and the Colorful Human dataset show the effectiveness of our method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322000402",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Dimension (graph theory)",
      "Geometry",
      "Granularity",
      "Law",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Pure mathematics",
      "Representation (politics)",
      "Resolution (logic)",
      "Silhouette",
      "Surface (topology)"
    ],
    "authors": [
      {
        "surname": "Zheng",
        "given_name": "Yanping"
      },
      {
        "surname": "Zeng",
        "given_name": "Guang"
      },
      {
        "surname": "Li",
        "given_name": "Haisheng"
      },
      {
        "surname": "Cai",
        "given_name": "Qiang"
      },
      {
        "surname": "Du",
        "given_name": "Junping"
      }
    ]
  },
  {
    "title": "Improving lung region segmentation accuracy in chest X-ray images using a two-model deep learning ensemble approach",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103521",
    "abstract": "We propose a deep learning framework to improve segmentation accuracy of the lung region in Chest X-Ray (CXR) images. The proposed methodology implements a “divide and conquer” strategy where the original CXRs are subdivided into smaller image patches, segmented them individually, and then reassembled to achieve the complete segmentation. This approach ensembles two models, the first of which is a traditional Convolutional Neural Network (CNN) used to classify the image patches and subsequently merge them to obtain a pre-segmentation. The second model is a modified U-Net architecture to segment the patches and subsequently combines them to obtain another pre-segmented image. These two pre-segmented images are combined using a binary disjunction operation to get the initial segmentation, which is later post-processed to obtain the final segmentation. The post-processing steps consist of traditional image processing techniques such as erosion, dilation, connected component labeling, and region-filling algorithms. The robustness of the proposed methodology is demonstrated using two public (MC, JPCL) and one proprietary (The University of Texas Medical Branch - UTMB) datasets of CXR images. The proposed framework outperformed many state-of-the-arts competitions presented in the literature.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322000657",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Connected-component labeling",
      "Convolutional neural network",
      "Deep learning",
      "Dilation (metric space)",
      "Gene",
      "Image (mathematics)",
      "Image processing",
      "Image segmentation",
      "Information retrieval",
      "Mathematics",
      "Merge (version control)",
      "Pattern recognition (psychology)",
      "Robustness (evolution)",
      "Scale-space segmentation",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Rahman",
        "given_name": "Md Fashiar"
      },
      {
        "surname": "Zhuang",
        "given_name": "Yan"
      },
      {
        "surname": "Tseng",
        "given_name": "Tzu-Liang (Bill)"
      },
      {
        "surname": "Pokojovy",
        "given_name": "Michael"
      },
      {
        "surname": "McCaffrey",
        "given_name": "Peter"
      },
      {
        "surname": "Walser",
        "given_name": "Eric"
      },
      {
        "surname": "Moen",
        "given_name": "Scott"
      },
      {
        "surname": "Vo",
        "given_name": "Alex"
      }
    ]
  },
  {
    "title": "Visible–infrared person re-identification based on key-point feature extraction and optimization",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103511",
    "abstract": "Feature extraction for visible–infrared person re-identification (VI-ReID) is challenging because of the cross-modality discrepancy in the images taken by different spectral cameras. Most of the existing VI-ReID methods often ignore the potential relationship between features. In this paper, we intend to transform low-order person features into high-order graph features, and make full use of the hidden information between person features. Therefore, we propose a multi-hop attention graph convolution network (MAGC) to extract robust person joint feature information using residual attention mechanism while reducing the impact of environmental noise. The transfer of higher order graph features within MAGC enables the network to learn the hidden relationship between features. We also introduce the self-attention semantic perception layer (SSPL) which can adaptively select more discriminant features to further promote the transmission of useful information. The experiments on VI-ReID datasets demonstrate its effectiveness.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032200058X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convolution (computer science)",
      "Feature extraction",
      "Graph",
      "Pattern recognition (psychology)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Gao",
        "given_name": "Wenbo"
      },
      {
        "surname": "Liu",
        "given_name": "Li"
      },
      {
        "surname": "Zhu",
        "given_name": "Lei"
      },
      {
        "surname": "Zhang",
        "given_name": "Huaxiang"
      }
    ]
  },
  {
    "title": "Improving lung region segmentation accuracy in chest X-ray images using a two-model deep learning ensemble approach",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103521",
    "abstract": "We propose a deep learning framework to improve segmentation accuracy of the lung region in Chest X-Ray (CXR) images. The proposed methodology implements a “divide and conquer” strategy where the original CXRs are subdivided into smaller image patches, segmented them individually, and then reassembled to achieve the complete segmentation. This approach ensembles two models, the first of which is a traditional Convolutional Neural Network (CNN) used to classify the image patches and subsequently merge them to obtain a pre-segmentation. The second model is a modified U-Net architecture to segment the patches and subsequently combines them to obtain another pre-segmented image. These two pre-segmented images are combined using a binary disjunction operation to get the initial segmentation, which is later post-processed to obtain the final segmentation. The post-processing steps consist of traditional image processing techniques such as erosion, dilation, connected component labeling, and region-filling algorithms. The robustness of the proposed methodology is demonstrated using two public (MC, JPCL) and one proprietary (The University of Texas Medical Branch - UTMB) datasets of CXR images. The proposed framework outperformed many state-of-the-arts competitions presented in the literature.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322000657",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Connected-component labeling",
      "Convolutional neural network",
      "Deep learning",
      "Dilation (metric space)",
      "Gene",
      "Image (mathematics)",
      "Image processing",
      "Image segmentation",
      "Information retrieval",
      "Mathematics",
      "Merge (version control)",
      "Pattern recognition (psychology)",
      "Robustness (evolution)",
      "Scale-space segmentation",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Rahman",
        "given_name": "Md Fashiar"
      },
      {
        "surname": "Zhuang",
        "given_name": "Yan"
      },
      {
        "surname": "Tseng",
        "given_name": "Tzu-Liang (Bill)"
      },
      {
        "surname": "Pokojovy",
        "given_name": "Michael"
      },
      {
        "surname": "McCaffrey",
        "given_name": "Peter"
      },
      {
        "surname": "Walser",
        "given_name": "Eric"
      },
      {
        "surname": "Moen",
        "given_name": "Scott"
      },
      {
        "surname": "Vo",
        "given_name": "Alex"
      }
    ]
  },
  {
    "title": "Classifying between computer generated and natural images: An empirical study from RAW to JPEG format",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103506",
    "abstract": "Computer generated (CG) images have been gradually overspread on the Internet, resulting in difficult discrimination from natural images (NIs) captured by an authentic imaging device. Although some discriminators can deal with NIs in JPEG format, the classification between uncompressed NIs (that are possibly generated in any imaging procedure before compression) and CG ones still remains unknown. Thus, this paper aims to establish multiple discriminators classifying between NIs and CG images. We first describe the main imaging procedure and its intrinsic property, which characterizes the discriminative features for classification. Then, the residual noise (representing intrinsic characteristic) is extracted. Its statistical distribution indeed helps us establish multiple discriminators, consisting of the generalized likelihood ratio test (GLRT) under the framework of hypothesis testing theory. Extensive experiments empirically verify our proposed multiple discriminators outperform many prior arts. Furthermore, the robustness of discriminators is validated with considering some post-processing attacks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322000578",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Data compression",
      "Discriminative model",
      "Gene",
      "JPEG",
      "Pattern recognition (psychology)",
      "Residual",
      "Robustness (evolution)",
      "Uncompressed video",
      "Video processing",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Qiao",
        "given_name": "Tong"
      },
      {
        "surname": "Luo",
        "given_name": "Xiangyang"
      },
      {
        "surname": "Yao",
        "given_name": "Hongwei"
      },
      {
        "surname": "Shi",
        "given_name": "Ran"
      }
    ]
  },
  {
    "title": "A general framework for reversible data hiding in encrypted images by reserving room before encryption",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103478",
    "abstract": "In this paper a general framework to adopt different predictors for reversible data hiding in the encrypted image is presented. Employing linear regression, we propose innovative predictors that contribute more significantly to accomplish more payload than conventional ones. Reserving room before encryption (RRBE) is designated in the proposed scheme making possible to attain high embedding capacity. In RRBE procedure, pre-processing is allowed before image encryption. In our scheme, pre-processing comprises of three main steps: computing prediction-errors, blocking and labeling of the errors. By blocking, we obviate the need for lossless compression to when a content owner is not enthusiastic. Lossless compression is employed in recent state of the art schemes to improve payload. We surpass the prior arts exploiting proper predictors, more efficient labeling procedure and blocking of the prediction-errors.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322000347",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Blocking (statistics)",
      "Computer engineering",
      "Computer network",
      "Computer science",
      "Computer security",
      "Data compression",
      "Data mining",
      "Embedding",
      "Encryption",
      "Image (mathematics)",
      "Information hiding",
      "Lossless compression",
      "Mathematical analysis",
      "Mathematics",
      "Network packet",
      "Payload (computing)",
      "Scheme (mathematics)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Mohammadi",
        "given_name": "Ammar"
      }
    ]
  },
  {
    "title": "An efficient six-parameter perspective motion model for VVC",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103514",
    "abstract": "Tilt and pan camera movements are common in computer games or social media videos. These types of videos contain numerous perspective transforms while today’s video codecs rely on translational and affine motion models for motion compensation. The general perspective motion model with 8 parameters (8PMM) has unreasonably high processing time. In this paper, the eight-parameter perspective transform is simplified into a six-parameter transform to keep the time complexity within an acceptable range while modeling the most relevant transforms. Also, two motion prediction modes, Advanced Perspective Motion Vector Prediction (APMVP) and Perspective Model Merge (PMM), are proposed. The implementation results show an average of 7.0% BD-rate reduction over H.266/VVC Test Model with a maximum of 20% encoding time overhead. The results also show a 71% processing time reduction in comparison to 8PMM while experiencing an average of 5.6% increase in BD-Rate. Much better visual quality is measured through VMAF quality meter.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032200061X",
    "keywords": [
      "Affine transformation",
      "Algorithm",
      "Artificial intelligence",
      "Codec",
      "Computer hardware",
      "Computer science",
      "Computer vision",
      "Correctness",
      "Image (mathematics)",
      "Information retrieval",
      "Mathematics",
      "Merge (version control)",
      "Motion compensation",
      "Motion estimation",
      "Motion vector",
      "Perspective (graphical)",
      "Pure mathematics",
      "Quarter-pixel motion"
    ],
    "authors": [
      {
        "surname": "Soltani Mohammadi",
        "given_name": "Iman"
      },
      {
        "surname": "Ghanbari",
        "given_name": "Mohammad"
      },
      {
        "surname": "Hashemi",
        "given_name": "Mahmoud Reza"
      }
    ]
  },
  {
    "title": "An efficient real-time target tracking algorithm using adaptive feature fusion",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103505",
    "abstract": "Visual-based target tracking is easily influenced by multiple factors, such as background clutter, targets’ fast-moving, illumination variation, object shape change, occlusion, etc. These factors influence the tracking accuracy of a target tracking task. To address this issue, an efficient real-time target tracking method based on a low-dimension adaptive feature fusion is proposed to allow us the simultaneous implementation of the high-accuracy and real-time target tracking. First, the adaptive fusion of a histogram of oriented gradient (HOG) feature and color feature is utilized to improve the tracking accuracy. Second, a convolution dimension reduction method applies to the fusion between the HOG feature and color feature to reduce the over-fitting caused by their high-dimension fusions. Third, an average correlation energy estimation method is used to extract the relative confidence adaptive coefficients to ensure tracking accuracy. We experimentally confirm the proposed method on an OTB100 data set. Compared with nine popular target tracking algorithms, the proposed algorithm gains the highest tracking accuracy and success tracking rate. Compared with the traditional Sum of Template and Pixel-wise LEarners (STAPLE) algorithm, the proposed algorithm can obtain a higher success rate and accuracy, improving by 2.3% and 1.9%, respectively. The experimental results also demonstrate that the proposed algorithm can reach the real-time target tracking with 50+fps. The proposed method paves a more promising way for real-time target tracking tasks under a complex environment, such as appearance deformation, illumination change, motion blur, background, similarity, scale change, and occlusion.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322000566",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Clutter",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Histogram",
      "Image (mathematics)",
      "Linguistics",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Pedagogy",
      "Philosophy",
      "Psychology",
      "Radar",
      "Telecommunications",
      "Tracking (education)",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Yanyan"
      },
      {
        "surname": "Pan",
        "given_name": "Changcheng"
      },
      {
        "surname": "Bie",
        "given_name": "Minglin"
      },
      {
        "surname": "Li",
        "given_name": "Jin"
      }
    ]
  },
  {
    "title": "Decomposition and replacement: Spatial knowledge distillation for monocular depth estimation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103523",
    "abstract": "Knowledge distillation has become a key technique for making smart and light-weight networks through model compression and transfer learning. Unlike previous methods that applied knowledge distillation to the classification task, we propose to exploit the decomposition-and-replacement based distillation scheme for depth estimation from a single RGB color image. To do this, Laplacian pyramid-based knowledge distillation is firstly presented in this paper. The key idea of the proposed method is to transfer the rich knowledge of the scene depth, which is well encoded through the teacher network, to the student network in a structured way by decomposing it into the global context and local details. This is fairly desirable for the student network to restore the depth layout more accurately with limited resources. Moreover, we also propose a new guidance concept for knowledge distillation, so-called ReplaceBlock, which replaces blocks randomly selected in the decoded feature of the student network with those of the teacher network. Our ReplaceBlock gives a smoothing effect in learning the feature distribution of the teacher network by considering the spatial contiguity in the feature space. This process is also helpful to clearly restore the depth layout without the significant computational cost. Based on various experimental results on benchmark datasets, the effectiveness of our distillation scheme for monocular depth estimation is demonstrated in details. The code and model are publicly available at : https://github.com/tjqansthd/Lap_Rep_KD_Depth.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322000669",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Context (archaeology)",
      "Data mining",
      "Distillation",
      "Feature (linguistics)",
      "Geodesy",
      "Geography",
      "Geometry",
      "Key (lock)",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Monocular",
      "Operating system",
      "Organic chemistry",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Process (computing)",
      "Pyramid (geometry)"
    ],
    "authors": [
      {
        "surname": "Song",
        "given_name": "Minsoo"
      },
      {
        "surname": "Kim",
        "given_name": "Wonjun"
      }
    ]
  },
  {
    "title": "Video anomaly detection using CycleGan based on skeleton features",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103508",
    "abstract": "Anomaly detection is a challenging task in the field of intelligent video surveillance. It aims to identify anomalous events by monitoring the video captured by visual sensors. The main difficulty of this task is that the definition of anomalies is ambiguous. In recent years, most anomaly detection methods use a two-stage learning strategy, i.e., feature extraction and model building. In this paper, with the idea of refactoring, we propose an end-to-end anomaly detection framework using cyclic consistent adversarial networks (CycleGAN). Dynamic skeleton features are used as network constraints to alleviate the inaccuracy of feature extraction algorithms of a single generative adversarial network. In the training phase, only normal video frames and the corresponding skeleton features are used to train the generator and discriminator. In the testing phase, anomalous behaviors with high reconstruction errors can be filtered out by manually set thresholds. To the best of our knowledge, this is the first time CycleGAN has been used for video anomaly detection. Experimental results on challenging datasets show that our method can accurately detect anomalous behaviors in videos collected by video surveillance systems and is comparable to the current state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322000542",
    "keywords": [
      "Adversarial system",
      "Anomaly (physics)",
      "Anomaly detection",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Condensed matter physics",
      "Detector",
      "Discriminator",
      "Economics",
      "Feature (linguistics)",
      "Feature extraction",
      "Generator (circuit theory)",
      "Linguistics",
      "Management",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Power (physics)",
      "Programming language",
      "Quantum mechanics",
      "Set (abstract data type)",
      "Skeleton (computer programming)",
      "Task (project management)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Fan",
        "given_name": "Zheyi"
      },
      {
        "surname": "Yi",
        "given_name": "Shuhan"
      },
      {
        "surname": "Wu",
        "given_name": "Di"
      },
      {
        "surname": "Song",
        "given_name": "Yu"
      },
      {
        "surname": "Cui",
        "given_name": "Mengjie"
      },
      {
        "surname": "Liu",
        "given_name": "Zhiwen"
      }
    ]
  },
  {
    "title": "A reliable and unobtrusive approach to display area detection for imperceptible display camera communication",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103510",
    "abstract": "Object framework detection has been extensively studied in computer vision for applications such as document digitization and whiteboard scanning. Similarly, it is essential for display-camera communication systems, particularly when imperceptible data modulation is employed to enable simultaneous video playback and data transmission. Reliable and accurate localization of the encoded display area is critical for data demodulation and decoding. However, existing systems typically adapt established methods developed for other applications that do not meet the system requirements for high-rate data transmission. In this article, we propose a novel method for display area detection in the camera images by embedding a new localization marker into the display corners. While the localization marker is less obtrusive than conventional fiducial markers, our detection algorithm demonstrated excellent reliability regardless of the display content and background, according to simulation and experimental results. In addition, the detector achieved subpixel accuracy and real-time performance with modern smartphones.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322000554",
    "keywords": [
      "Artificial intelligence",
      "Channel (broadcasting)",
      "Computer graphics (images)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Decoding methods",
      "Demodulation",
      "Detector",
      "Digitization",
      "Physics",
      "Pixel",
      "Power (physics)",
      "Quantum mechanics",
      "Reliability (semiconductor)",
      "Subpixel rendering",
      "Telecommunications",
      "Transmission (telecommunications)"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Jianshuang"
      },
      {
        "surname": "Klein",
        "given_name": "Johannes"
      },
      {
        "surname": "Jochims",
        "given_name": "Jörn"
      },
      {
        "surname": "Weissner",
        "given_name": "Niklas"
      },
      {
        "surname": "Kays",
        "given_name": "Rüdiger"
      }
    ]
  },
  {
    "title": "AMBTC-based secret image sharing by simple modular arithmetic",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103482",
    "abstract": "A distortionless secret image sharing scheme using finite field G F ( P G ) ( P G is the largest prime less than a given number) is investigated for sharing the absolute moment block truncation coding (AMBTC) images. Two adjusting operations are devised to modify the AMBTC trios (i.e., quantization pixels and bit maps) suitable for G F ( P G ) sharing. Polynomials under G F ( P G ) are constructed for encrypting the quantization pixels and bit maps. When sharing the trios under G F ( P G ) , the AMBTC image is perfectly recovered. Moreover, the bit map sharing under G F ( P G ) can be extended to G F ( P S ) ( P S is the smallest prime larger than a given number). Another scheme using G F ( P S ) is constituted by combining quantization pixel sharing under G F ( P G ) and bit map sharing under G F ( P S ) . But the scheme using G F ( P S ) is not always lossless. Experimental results show that the proposed schemes are effective. Since G F ( P G ) and G F ( P S ) are simple modular arithmetic, improved computational efficiency is obtained.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322000359",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Computer science",
      "Cryptography",
      "Discrete mathematics",
      "Finite field",
      "GF(2)",
      "Homomorphic secret sharing",
      "Image (mathematics)",
      "Image sharing",
      "Mathematics",
      "Pixel",
      "Quantization (signal processing)",
      "Secret sharing"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Ching-Nung"
      },
      {
        "surname": "Wu",
        "given_name": "Xiaotian"
      },
      {
        "surname": "Chung",
        "given_name": "Min-Jung"
      },
      {
        "surname": "Zhang",
        "given_name": "Xuliang"
      }
    ]
  },
  {
    "title": "Multi-receptive Field Aggregation Network for single image deraining",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103469",
    "abstract": "Image deraining is a significant problem that ensures the visual quality of images to prompt computer vision systems. However, due to the insufficiency of captured rain streaks features and global information, current image deraining methods often face the issues of rain streaks remaining and image blurring. In this paper, we propose a Multi-receptive Field Aggregation Network (MRFAN) to restore a cleaner rain-free image. Specifically, we construct a Multi-receptive Field Feature Extraction Block (MFEB) to capture rain features with different receptive fields. In MFEB, we design a Self-supervised Block (SSB) and an Aggregation Block (AGB). SSB can make the network adaptively focus on the critical rain features and rain-covered areas. AGB effectively aggregates and redistributes the multi-scale features to help the network simulate rain streaks better. Experiments show that our method achieves better results on both synthetic datasets and real-world rainy images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322000256",
    "keywords": [
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Computer science",
      "Computer vision",
      "Construct (python library)",
      "Face (sociological concept)",
      "Feature (linguistics)",
      "Feature extraction",
      "Field (mathematics)",
      "Focus (optics)",
      "Geometry",
      "Image (mathematics)",
      "Linguistics",
      "Mathematics",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Programming language",
      "Pure mathematics",
      "Receptive field",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Liang",
        "given_name": "Songliang"
      },
      {
        "surname": "Meng",
        "given_name": "Xiaozhe"
      },
      {
        "surname": "Su",
        "given_name": "Zhuo"
      },
      {
        "surname": "Zhou",
        "given_name": "Fan"
      }
    ]
  },
  {
    "title": "Tracking by dynamic template: Dual update mechanism",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103456",
    "abstract": "Recently, Siamese trackers have received widespread attention for visual object tracking owing to their good balance between speed and performance. Those Siamese trackers heavily depend on target template while conventional practice fixes the template to initial frame. This strategy makes it unable to cope with variation of target appearance, which often leads to tracking failures and causes the gap in performance from other tracking methods. Despite the performance gain achieved by few template update methods with target templates generated by the tracked results, these tracked templates are easy to accumulate errors and cause tracking drift. In this paper, we propose two template update mechanisms to effectively adapt the target template during the tracking process which is dubbed as DTDU (Dynamic Template with Dual Update). Unlike predecessors that directly use the tracked template, we use initial template to perform similar transformation to the tracked template. Then the similar transformed template and the tracked template are combined linearly to capture the variation of target appearance. These updated templates are stored in a memory bank and retrieved to generate the final target template. In order to enhance quick update of memory bank to accommodate the target appearance, we use the retrieved template to further update the templates in memory bank for subsequent tracking. Extensive experiments on OTB-2015, VOT2016, VOT2018 and GOT-10k datasets have proved the effectiveness of these two update mechanisms and the proposed tracker achieves a real-time speed of 44 fps.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322000165",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Biochemistry",
      "BitTorrent tracker",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Dual (grammatical number)",
      "Eye tracking",
      "Frame (networking)",
      "Gene",
      "Image (mathematics)",
      "Literature",
      "Operating system",
      "Pattern recognition (psychology)",
      "Pedagogy",
      "Process (computing)",
      "Programming language",
      "Psychology",
      "Telecommunications",
      "Template",
      "Template matching",
      "Tracking (education)",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Jing"
      },
      {
        "surname": "Wang",
        "given_name": "Yating"
      },
      {
        "surname": "Huang",
        "given_name": "Xiangdong"
      },
      {
        "surname": "Su",
        "given_name": "Yuting"
      }
    ]
  },
  {
    "title": "ACSiam: Asymmetric convolution structures for visual tracking with Siamese network",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103465",
    "abstract": "Object trackers based on Siamese network usually transform the tracking task into a matching problem between the candidate samples and the target template. However, with the increasing depth and width of backbone networks, researches on Siamese trackers using backbone networks are not very advanced. Therefore, it is necessary for us to further investigate the characteristics of backbone network. As a fact, the ability of backbone network to extract features can directly determine the performance of object tracker. Given this, in this paper, we first propose an asymmetric convolutional network to improve the representational capability of backbone network. And then, the strip convolution is employed to enhance the operational capability of square kernel convolution in the backbone network. Besides, we also construct a novel module named Feature Dropblock (i.e., FD) to simulate the occlusion of hidden space, which goal is to improve the performance of backbone network in the target tracking under occlusion. To demonstrate the effectiveness of the proposed tracker, extensive ablation studies are conducted. Better results are obtained on the tracking benchmarks OTB100 and VOT2018, compared to other state-of-the-art trackers.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322000207",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Backbone network",
      "BitTorrent tracker",
      "Combinatorics",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Eye tracking",
      "Feature (linguistics)",
      "Kernel (algebra)",
      "Linguistics",
      "Mathematics",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Pedagogy",
      "Philosophy",
      "Psychology",
      "Tracking (education)",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Zhen"
      },
      {
        "surname": "Wen",
        "given_name": "Chaohe"
      },
      {
        "surname": "Luo",
        "given_name": "Lingkun"
      },
      {
        "surname": "Gan",
        "given_name": "Hongping"
      },
      {
        "surname": "Zhang",
        "given_name": "Tao"
      }
    ]
  },
  {
    "title": "A fuzzy convolutional neural network for enhancing multi-focus image fusion",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103485",
    "abstract": "The images captured by the cameras contain distortions, misclassified pixels, uncertainties and poor contrast. Therefore, the multi-focus image fusion (MFIF) integrates various input image features to produce a single fused image using all its objects in focus. However, it is computationally complex, which leads to inconsistency. Hence, the MFIF method is employed to generate the fused image by integrating the fuzzy sets (FS) and convolutional neural network (CNN) to detect focused and unfocused parts in both source images. It is also compared with other competing six MFIF methods like Neutrosophic set based stationary wavelet transform (NSWT), guided filters, CNN, ensemble CNN, image fusion-based CNN and deep regression pair learning (DRPL). Benchmark datasets validate the superiority of the proposed FCNN method in terms of four non-reference assessment measures having mutual information (1.1678), edge information (0.7281), structural similarity (0.9850) and human perception (0.8020) and two reference metrics such as Peak signal-to-noise ratio (57.23) and root mean square error (1.814).",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322000396",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Focus (optics)",
      "Fuzzy logic",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Image fusion",
      "Mathematics",
      "Mean squared error",
      "Optics",
      "Pattern recognition (psychology)",
      "Peak signal-to-noise ratio",
      "Physics",
      "Pixel",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Bhalla",
        "given_name": "Kanika"
      },
      {
        "surname": "Koundal",
        "given_name": "Deepika"
      },
      {
        "surname": "Sharma",
        "given_name": "Bhisham"
      },
      {
        "surname": "Hu",
        "given_name": "Yu-Chen"
      },
      {
        "surname": "Zaguia",
        "given_name": "Atef"
      }
    ]
  },
  {
    "title": "Single-image depth estimation using relative depths",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103459",
    "abstract": "Depth estimation from a single RGB image is a challenging task. It is ill-posed since a single 2D image may correspond to various 3D scenes at different scales. On the other hand, estimating the relative depth relationship between two objects in a scene is easier and may yield more reliable results. Thus, in this paper, we propose a novel algorithm for monocular depth estimation using relative depths. First, using a convolutional neural network, we estimate two types of depths at multiple spatial resolutions: ordinary depth maps and relative depth tensors. Second, we restore a relative depth map from each relative depth tensor. A relative depth map is equivalent to an ordinary depth map with global scale information removed. For the restoration, sparse pairwise comparison matrices are constructed from available relative depths, and missing entries are filled in using the alternative least square (ALS) algorithm. Third, we decompose the ordinary and relative depth maps into components and recombine them to yield a final depth map. To reduce the computational complexity, relative depths at fine spatial resolutions are directly used to refine the final depth map. Extensive experimental results on the NYUv2 dataset demonstrate that the proposed algorithm provides state-of-the-art performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322000190",
    "keywords": [
      "Algorithm",
      "Approximation error",
      "Artificial intelligence",
      "Cartography",
      "Computer science",
      "Depth map",
      "Geography",
      "Image (mathematics)",
      "Mathematics",
      "Pairwise comparison",
      "Pattern recognition (psychology)",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "Lee",
        "given_name": "Jae-Han"
      },
      {
        "surname": "Kim",
        "given_name": "Chang-Su"
      }
    ]
  },
  {
    "title": "A novel unsupervised multiple feature hashing for image retrieval and indexing (MFHIRI)",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103467",
    "abstract": "Recently, techniques that can automatically figure out the incisive information from gigantic visual databases are urging popularity. The existing multi-feature hashing method has achieved good results by fusing multiple features, but in processing these multi-features, fusing multi-features into one feature will cause the feature dimension to be very high, increasing the amount of calculation. On the one hand, it is not easy to discover the internal ties between different features. This paper proposes a novel unsupervised multiple feature hashing for image retrieval and indexing (MFHIRI) method to learn multiple views in a composite manner. The proposed scheme learns the binary codes of various information sources in a composite manner, and our scheme relies on weighted multiple information sources and improved KNN concept. In particular, here we adopt an adaptive weighing scheme to preserve the similarity and consistency among binary codes. Precisely, we follow the graph modeling theory to construct improved KNN concept, which further helps preserve different statistical properties of individual sources. The important aspect of improved KNN scheme is that we can find the neighbors of a data point by searching its neighbors’ neighbors. During optimization, the sub-problems are solved in parallel which efficiently lowers down the computation cost. The proposed approach shows consistent performance over state-of-the-art (three single-view and eight multi-view approaches) on three broadly followed datasets viz. CIFAR-10, NUS-WIDE and Caltech-256.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322000220",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Binary code",
      "Binary number",
      "Computer science",
      "Computer security",
      "Consistency (knowledge bases)",
      "Data mining",
      "Feature (linguistics)",
      "Hash function",
      "Image (mathematics)",
      "Image retrieval",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Search engine indexing"
    ],
    "authors": [
      {
        "surname": "Sharma",
        "given_name": "Saurabh"
      },
      {
        "surname": "Gupta",
        "given_name": "Vishal"
      },
      {
        "surname": "Juneja",
        "given_name": "Mamta"
      }
    ]
  },
  {
    "title": "FuseKin: Weighted image fusion based kinship verification under unconstrained age group",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103470",
    "abstract": "Kinship verification using facial images is mainly performed with a single face sample per person. To perform with a single sample, it is very difficult to specify an age group where kin pairs may have higher similarities. To address the above problem, we propose a novel weighted multi sample fusion (WMSF) method. The proposed WMSF method combines kin signals present in multiple samples per person (MSPP) to form a FuseKin image. To select the most discriminant features from the extracted feature vector, we propose a patch based discriminative analysis (PDA) method. Weights are calculated using the PDA method so as to reduce the discrimination between positive FuseKin pairs. Experiments were conducted on two different datasets which contain multiple face image samples per person, namely Family101 and Family in the Wild (FIW) to validate the performance of the proposed methods. Our method achieves competitive results as compared to other state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322000268",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Discriminative model",
      "Face (sociological concept)",
      "Feature (linguistics)",
      "Feature vector",
      "Group (periodic table)",
      "Image (mathematics)",
      "Kinship",
      "Law",
      "Linear discriminant analysis",
      "Linguistics",
      "Mathematics",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Sample (material)",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Mukherjee",
        "given_name": "Moumita"
      },
      {
        "surname": "Meenpal",
        "given_name": "Toshanlal"
      },
      {
        "surname": "Goyal",
        "given_name": "Aarti"
      }
    ]
  },
  {
    "title": "High-capacity reversible data hiding in encrypted images based on adaptive block encoding",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103481",
    "abstract": "This paper proposes a new high-capacity reversible data hiding scheme in encrypted images. The content owner first divides the cover image into blocks. Then, the block permutation and the bitwise stream cipher processes are applied to encrypt the image. Upon receiving the encrypted image, the data hider analyzes the image blocks and adaptively decides an optimal block-type labeling strategy. Based on the adaptive block encoding, the image is compressed to vacate the spare room, and the secret data are encrypted and embedded into the spare space. According to the granted authority, the receiver can restore the cover image, extract the secret data, or do both. Experimental results show that the embedding capacity of the proposed scheme outperforms state-of-the-art schemes. In addition, security level and robustness of the proposed scheme are also investigated.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322000372",
    "keywords": [
      "Acoustics",
      "Artificial intelligence",
      "Biochemistry",
      "Bitwise operation",
      "Block (permutation group theory)",
      "Block cipher",
      "Business",
      "Chemistry",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Cover (algebra)",
      "Embedding",
      "Encoding (memory)",
      "Encryption",
      "Engineering",
      "Gene",
      "Geometry",
      "Image (mathematics)",
      "Information hiding",
      "Marketing",
      "Mathematical analysis",
      "Mathematics",
      "Mechanical engineering",
      "Permutation (music)",
      "Physics",
      "Programming language",
      "Robustness (evolution)",
      "Scheme (mathematics)",
      "Spare part",
      "Stream cipher",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Gao",
        "given_name": "Kai"
      },
      {
        "surname": "Horng",
        "given_name": "Ji-Hwei"
      },
      {
        "surname": "Chang",
        "given_name": "Chin-Chen"
      }
    ]
  },
  {
    "title": "An adaptive kernelized correlation filters with multiple features in the tracking application",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103484",
    "abstract": "Automatic target detection and tracking systems are used extensively in complex scenes. In long-term tracking, some visual attributes of objects are changing, such as illumination, size, profile, and so on. To address the issue, it is particularly important to describe the essential properties of the objects in tracking. An enhanced kernelized correlation filter tracking strategy fused multiple features with location prediction is proposed. To make the object appearance models more accuracy and robustness, based on the original histogram of oriented gradient features, we integrate the hue, saturation, value, and grayscale information to construct a new descriptor to represent the target appearance. Moreover, location prediction and bi-linear interpolation are employed to obtain the more accurate target position. Experiments show that the proposed strategy can obtain superior or competitive performance in challenging benchmark data sets. In practice, the algorithm is applied to track shuttle bus targets in the airport apron.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322000384",
    "keywords": [
      "Active appearance model",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Eye tracking",
      "Gene",
      "Grayscale",
      "Histogram",
      "Image (mathematics)",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Robustness (evolution)",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Guo",
        "given_name": "Dequan"
      },
      {
        "surname": "Zhang",
        "given_name": "Gexiang"
      },
      {
        "surname": "Neri",
        "given_name": "Ferrante"
      },
      {
        "surname": "Peng",
        "given_name": "Sheng"
      },
      {
        "surname": "Yang",
        "given_name": "Qiang"
      },
      {
        "surname": "Liu",
        "given_name": "Paul"
      }
    ]
  },
  {
    "title": "Shareability-Exclusivity Representation on Product Grassmann Manifolds for Multi-camera video clustering",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103457",
    "abstract": "With the rapid popularity of multi-camera networks, one human action is usually captured by multiple cameras located at different angles simultaneously. Multi-camera videos contain the distinct perspectives of one action, therefore multiple views can overcome the impacts of illumination and occlusion. In this paper, we propose a novel multi-camera video clustering model, named Shareability-Exclusivity Representation on Product Grassmann Manifolds (PGM-SER), to address two key issues in traditional multi-view clustering methods (MVC): (1) Most MVC methods directly construct a shared similarity matrix by fusing multi-view data or their corresponding similarity matrices, which ignores the exclusive information in each view; (2) Most MVC methods are designed for multi-view vectorial data, which cannot handle the nonlinear manifold structure hidden in multi-camera videos. The proposed PGM-SER firstly adopts Product Grassmann Manifolds to represent multi-camera videos, then simultaneously learn their shared and exclusive information in global structures to achieve multi-camera video clustering. We provide an effective optimization algorithm to solve PGM-SER and present the corresponding convergence analysis. Finally, PGM-SER is tested on three multi-camera human action video datasets and obtain satisfied experimental results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322000177",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Computer vision",
      "Geometry",
      "Image (mathematics)",
      "Law",
      "Mathematics",
      "Political science",
      "Politics",
      "Product (mathematics)",
      "Representation (politics)",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Yongli"
      },
      {
        "surname": "Luo",
        "given_name": "Cuicui"
      },
      {
        "surname": "Gao",
        "given_name": "Junbin"
      },
      {
        "surname": "Wang",
        "given_name": "Boyue"
      },
      {
        "surname": "Sun",
        "given_name": "Yanfeng"
      },
      {
        "surname": "Yin",
        "given_name": "Baocai"
      }
    ]
  },
  {
    "title": "Verifiable varying sized ( m , n , n ) multi-image secret sharing with combiner verification and cheater identification",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103466",
    "abstract": "In conventional multi-image secret sharing schemes (MISSS) images are shared by the trusted dealer and the shares are sent to the set of participants through a secure channel. During reconstruction, the participants submit shares to a trusted combiner. But the method will collapse if any of the actors perform cheating. This brings verifiable image secret sharing in the research arena. Verifying the trustworthiness of the dealer by the shareholders before accepting shares (dealer verification), examining the genuineness of the shares submission request received from a combiner (combiner verification), checking the authenticity of the shares received from participants by the combiner (cheating detection/ cheater identification) are techniques related to verifiable secret sharing. Medical images, images used at the military or diplomatic level; contain sensible information. Thus the authenticity of the reconstructed images should be checked beforehand. In the case of multi-image secret sharing, the researchers use bit padding if the plaintext images are of different sizes. This adds an extra level of burden during sharing and retrieval. A verifiable varying size ( m , n , n ) multi-image secret sharing is addressed in this article. Here m ( w h e r e m ≤ n ) varying sized images are shared among n participants and during reconstruction all the shares are required. The major contribution of the addressed technique is that it has the capability of dealer authentication, combiner verification, and cheater identification. Another advancement is that most of the communication can be made through a public channel. The test results generate noise like images and statistical analysis, security analysis say in favor of the claims. Comparison with some state-of-the-art techniques gives it a stable platform in verifiable multi-image secret sharing.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322000232",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Authentication (law)",
      "Biology",
      "Biometrics",
      "Botany",
      "Cheating",
      "Computer science",
      "Computer security",
      "Cryptography",
      "Identification (biology)",
      "Image (mathematics)",
      "Image sharing",
      "Programming language",
      "Psychology",
      "Secret sharing",
      "Set (abstract data type)",
      "Social psychology",
      "Theoretical computer science",
      "Verifiable secret sharing"
    ],
    "authors": [
      {
        "surname": "Soreng",
        "given_name": "Aswini Vinay"
      },
      {
        "surname": "Kandar",
        "given_name": "Shyamalendu"
      }
    ]
  },
  {
    "title": "Learning discriminative representations for multi-label image recognition",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103448",
    "abstract": "Multi-label recognition is a fundamental, and yet is a challenging task in computer vision. Recently, deep learning models have achieved great progress towards learning discriminative features from input images. However, conventional approaches are unable to model the inter-class discrepancies among features in multi-label images, since they are designed to work for image-level feature discrimination. In this paper, we propose a unified deep network to learn discriminative features for the multi-label task. Given a multi-label image, the proposed method first disentangles features corresponding to different classes. Then, it discriminates between these classes via increasing the inter-class distance while decreasing the intra-class differences in the output space. By regularizing the whole network with the proposed loss, the performance of applying the well-known ResNet-101 is improved significantly. Extensive experiments have been performed on COCO-2014, VOC2007 and VOC2012 datasets, which demonstrate that the proposed method outperforms state-of-the-art approaches by a significant margin of 3.5% on large-scale COCO dataset. Moreover, analysis of the discriminative feature learning approach shows that it can be plugged into various types of multi-label methods as a general module.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322000116",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Deep learning",
      "Discriminative model",
      "Economics",
      "Feature (linguistics)",
      "Feature learning",
      "Image (mathematics)",
      "Linguistics",
      "Machine learning",
      "Management",
      "Margin (machine learning)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Hassanin",
        "given_name": "Mohammed"
      },
      {
        "surname": "Radwan",
        "given_name": "Ibrahim"
      },
      {
        "surname": "Khan",
        "given_name": "Salman"
      },
      {
        "surname": "Tahtali",
        "given_name": "Murat"
      }
    ]
  },
  {
    "title": "TGP-PCQA: Texture and geometry projection based quality assessment for colored point clouds",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103449",
    "abstract": "Colored point cloud (PC) will inevitably encounter distortion during its acquisition, processing, coding and transmission, which may affect the visual quality of the colored PC. Therefore, it is necessary to design an effective tool for colored PC quality assessment (PCQA). In this paper, considering the mapping relationship of perception between the colored PC and its corresponding projection images, we propose a novel PCQA method based on texture and geometry projection (denoted as TGP-PCQA). The main idea of the proposed TGP-PCQA method is to obtain texture and geometry projection maps from different perspectives for evaluating the colored PC. Specifically, 4D tensor decomposition is used to obtain the combination and difference information between the reference and distorted texture projection maps for mainly characterizing texture distortion of colored PC. Meanwhile, the edge features of the geometry projection map are calculated to measure the global or local geometry distortion. All of the extracted features are combined to predict an overall quality of colored PC. In addition, this paper establishes a multi-distorted colored PC database named CPCD2.0 with compression distortions and Gaussian noise, which orients to the influence of both geometry and texture components in distortion. Experimental results on two open subjective evaluation databases (IRPC and SJTU-PCQA) and the self-built CPCD2.0 database show that the proposed TGP-PCQA method outperforms the state-of-the-art PCQA methods. We are also providing the self-built CPCD2.0 database free of charge at https://github.com/cherry0415/CPCD2.0.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322000128",
    "keywords": [
      "Algorithm",
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Colored",
      "Colors of noise",
      "Composite material",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Distortion (music)",
      "Geometry",
      "Image (mathematics)",
      "Materials science",
      "Mathematics",
      "Noise reduction",
      "Pattern recognition (psychology)",
      "Point cloud",
      "Projection (relational algebra)",
      "Projection plane"
    ],
    "authors": [
      {
        "surname": "He",
        "given_name": "Zhouyan"
      },
      {
        "surname": "Jiang",
        "given_name": "Gangyi"
      },
      {
        "surname": "Yu",
        "given_name": "Mei"
      },
      {
        "surname": "Jiang",
        "given_name": "Zhidi"
      },
      {
        "surname": "Peng",
        "given_name": "Zongju"
      },
      {
        "surname": "Chen",
        "given_name": "Fen"
      }
    ]
  },
  {
    "title": "Robust object tracking via deformation samples generator",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103446",
    "abstract": "In object tracking applications, it is common for trackers to experience drift problems when the object of interest becomes deformed, which compromises the ability of the tracker to track the object. It is therefore desirable to develop a learning tracker classifier that is robust to deformations. The performance of existing trackers that employ deep classification networks degrades when the amount of training data is limited and does not cover all possible scenarios. While these limitations can be mitigated in part by using larger training datasets, these datasets may still not cover all situations and the positive samples are still monotonous. To overcome this problem, we propose a novel deformation samples generator that generates samples that would normally be difficult for the tracker to classify. In the proposed framework, both the classifier and deformation samples generator learn in a joint manner. Our experiments show that the proposed approach outperforms state-of-the-art methods in both quantitative and qualitative evaluations for the visual object tracking task.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322000104",
    "keywords": [
      "Artificial intelligence",
      "BitTorrent tracker",
      "Classifier (UML)",
      "Computer science",
      "Computer vision",
      "Eye tracking",
      "Generator (circuit theory)",
      "Machine learning",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Pedagogy",
      "Physics",
      "Power (physics)",
      "Psychology",
      "Quantum mechanics",
      "Tracking (education)",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Gao",
        "given_name": "Xuesong"
      },
      {
        "surname": "Zhou",
        "given_name": "Yuan"
      },
      {
        "surname": "Huo",
        "given_name": "Shuwei"
      },
      {
        "surname": "Li",
        "given_name": "Zizi"
      },
      {
        "surname": "Li",
        "given_name": "Keqiu"
      }
    ]
  },
  {
    "title": "Multi-domain residual encoder–decoder networks for generalized compression artifact reduction",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2021.103425",
    "abstract": "A fundamental requirement for designing compression artifact reduction techniques is to restore the artifact free image from its compressed version regardless of the compression level. Most existing algorithms require the prior knowledge of JPEG encoding parameters to operate effectively. Although there are works that attempt to train universal models to deal with different compression levels, some JPEG quality factors (QF) are still missing. To overcome these potential limitations, in this paper, we present a generalized JPEG-compression artifact reduction framework that relies on improved QF estimator and rectified networks to take into account all possible QF values. Our method, called a generalized compression artifact reducer (G-CAR), first predicts QF by analyzing luminance patches with high activity. Then, based on the estimated QF, images are adaptively restored by the cascaded residual encoder–decoder networks learned in multiple domains. Results tested on six benchmark datasets demonstrate the effectiveness of our proposed model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032100287X",
    "keywords": [
      "Algorithm",
      "Artifact (error)",
      "Artificial intelligence",
      "Automotive engineering",
      "Benchmark (surveying)",
      "Composite material",
      "Compression (physics)",
      "Compression artifact",
      "Compression ratio",
      "Computer science",
      "Computer vision",
      "Data compression",
      "Discrete cosine transform",
      "Encoder",
      "Encoding (memory)",
      "Engineering",
      "Geodesy",
      "Geography",
      "Geometry",
      "Image (mathematics)",
      "Image compression",
      "Image processing",
      "Internal combustion engine",
      "JPEG",
      "JPEG 2000",
      "Materials science",
      "Mathematics",
      "Operating system",
      "Reduction (mathematics)",
      "Residual"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Yi"
      },
      {
        "surname": "Chandler",
        "given_name": "Damon M."
      },
      {
        "surname": "Mou",
        "given_name": "Xuanqin"
      }
    ]
  },
  {
    "title": "Optimized convolutional pose machine for 2D hand pose estimation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103461",
    "abstract": "Hand pose estimation is a challenging task owing to the high flexibility and serious self-occlusion of the hand. Therefore, an optimized convolutional pose machine (OCPM) was proposed in this study to estimate the hand pose accurately. Traditional CPMs have two components, a feature extraction module and an information processing module. First, the backbone network of the feature extraction module was replaced by Resnet-18 to reduce the number of network parameters. Furthermore, an attention module called the convolutional block attention module (CBAM) is embedded into the feature extraction module to enhance the information extraction. Then, the structure of the information processing module was adjusted through a residual connection in each stage that consist of a series of continuous convolutional operations, and requires a dense fusion between the output from all previous stages and the feature extraction module. The experimental results on two public datasets showed that the OCPM network achieved excellent performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322000219",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Engineering",
      "Feature (linguistics)",
      "Feature extraction",
      "Geometry",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pose",
      "Residual",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Pan",
        "given_name": "Tianhong"
      },
      {
        "surname": "Wang",
        "given_name": "Zheng"
      },
      {
        "surname": "Fan",
        "given_name": "Yuan"
      }
    ]
  },
  {
    "title": "Video frame deletion detection based on time–frequency analysis",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103436",
    "abstract": "With the emergence of diverse multimedia editing software, a great number of edited or tampered video resources appear on the Internet, some of which can mix with the genuine ones. Digital video authenticity is an important step to make the best use of these video resources. As a common video forgery operation, frame tampering can change the video content and confuse viewers by removing or inserting some specific frames. In this paper, we explore the traces created by compression process and propose a new method to detect frame tampering based on the high-frequency features of reconstructed DCT coefficients in the tampered sequences. Experimental results demonstrate that our proposed method can effectively detect frame tampering operation, and accurately locate the breakpoint of frame tampering in the streams.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322000013",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Filter (signal processing)",
      "Frame (networking)",
      "Speech recognition",
      "Telecommunications",
      "Time–frequency analysis"
    ],
    "authors": [
      {
        "surname": "Jin",
        "given_name": "Xiao"
      },
      {
        "surname": "Su",
        "given_name": "Yuting"
      },
      {
        "surname": "Jing",
        "given_name": "Peiguang"
      }
    ]
  },
  {
    "title": "Attention mechanism enhancement algorithm based on cycle consistent generative adversarial networks for single image dehazing",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2021.103434",
    "abstract": "This paper proposes AMEA-GAN, an attention mechanism enhancement algorithm. It is cycle consistency-based generative adversarial networks for single image dehazing, which follows the mechanism of the human retina and to a great extent guarantees the color authenticity of enhanced images. To address the color distortion and fog artifacts in real-world images caused by most image dehazing methods, we refer to the human visual neurons and use the attention mechanism of similar Horizontal cell and Amazon cell in the retina to improve the structure of the generator adversarial networks. By introducing our proposed attention mechanism, the effect of haze removal becomes more natural without leaving any artifacts, especially in the dense fog area. We also use an improved symmetrical structure of FUNIE-GAN to improve the visual color perception or the color authenticity of the enhanced image and to produce a better visual effect. Experimental results show that our proposed model generates satisfactory results, that is, the output image of AMEA-GAN bears a strong sense of reality. Compared with state-of-the-art methods, AMEA-GAN not only dehazes images taken in daytime scenes but also can enhance images taken in nighttime scenes and even optical remote sensing imagery.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002935",
    "keywords": [
      "Adversarial system",
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Distortion (music)",
      "Epistemology",
      "Generator (circuit theory)",
      "Image (mathematics)",
      "Mechanism (biology)",
      "Philosophy",
      "Physics",
      "Power (physics)",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Yan"
      },
      {
        "surname": "Al-Shehari",
        "given_name": "Hassan"
      },
      {
        "surname": "Zhang",
        "given_name": "Hongying"
      }
    ]
  },
  {
    "title": "Local perspective based synthesis for vehicle re-identification: A transformation state adversarial method",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2021.103432",
    "abstract": "Vehicle re-identification (V-ReID) aims at discovering an image of a specific vehicle from a set of images typically captured by different cameras. Vehicles are one of the most important objects in cross-camera target recognition systems, and recognizing them is one of the most difficult tasks due to the subtle differences in the visible characteristics of vehicle rigid objects. Compared to various methods that can improve re-identification accuracy, data augmentation is a more straightforward and effective technique. In this paper, we propose a novel data synthesis method for V-ReID based on local-region perspective transformation, transformation state adversarial learning and a candidate pool. Specifically, we first propose a parameter generator network, which is a lightweight convolutional neural network, to generate the transformation states. Secondly, an adversarial module is designed in our work, it ensures that noise information is added as much as possible while keeping the labeling and structure of the dataset intact. With this adversarial module, we are able to promote the performance of the network and generate more proper and harder training samples. Furthermore, we use a candidate pool to store harder samples for further selection to improve the performance of the model. Our system pays more balanced attention to the features of vehicles. Extensive experiments show that our method significantly boosts the performance of V-ReID on the VeRi-776, VehicleID and VERI-Wild datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002911",
    "keywords": [
      "Adversarial system",
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Botany",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Data mining",
      "Gene",
      "Generator (circuit theory)",
      "Identification (biology)",
      "Image (mathematics)",
      "Machine learning",
      "Noise (video)",
      "Pattern recognition (psychology)",
      "Perspective (graphical)",
      "Physics",
      "Power (physics)",
      "Programming language",
      "Quantum mechanics",
      "Set (abstract data type)",
      "State (computer science)",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Yanbing"
      },
      {
        "surname": "Ke",
        "given_name": "Wei"
      },
      {
        "surname": "Lin",
        "given_name": "Hong"
      },
      {
        "surname": "Lam",
        "given_name": "Chan-Tong"
      },
      {
        "surname": "Lv",
        "given_name": "Kai"
      },
      {
        "surname": "Sheng",
        "given_name": "Hao"
      },
      {
        "surname": "Xiong",
        "given_name": "Zhang"
      }
    ]
  },
  {
    "title": "Graph-based relational reasoning in a latent space for skeleton-based action recognition",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2021.103410",
    "abstract": "Motivated by the powerful capability of deep neural networks in feature learning, a new graph-based neural network is proposed to learn local and global relational information on skeleton sequences represented as spatio-temporal graphs (STGs). The pipeline of our network architecture consists of three main stages. As the first stage, spatial–temporal sub-graphs (sub-STGs) are projected into a latent space in which every point is represented as a linear subspace. The second stage is based on message passing to acquire the localized correlated features of the nodes in the latent space. The third stage relies on graph convolutional networks (GCNs) to reason the long-range spatio-temporal dependencies through a graph representation of the latent space. Finally, the average pooling layer and the softmax classifier are then employed to predict the action categories based on the extracted local and global correlations. We validate our model in terms of action recognition using three challenging datasets: the NTU RGB+D, Kinetics Motion, and SBU Kinect Interaction datasets. The experimental results demonstrate the effectiveness of our approach and show that our proposed model outperforms the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002741",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Convolutional neural network",
      "Feature vector",
      "Graph",
      "Pattern recognition (psychology)",
      "Pooling",
      "Softmax function",
      "Subspace topology",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Ding",
        "given_name": "Wenwen"
      },
      {
        "surname": "Zhou",
        "given_name": "GuangHui"
      },
      {
        "surname": "Ding",
        "given_name": "ChongYang"
      },
      {
        "surname": "Li",
        "given_name": "Guang"
      },
      {
        "surname": "Liu",
        "given_name": "Kai"
      }
    ]
  },
  {
    "title": "Multimodal face aging framework via learning disentangled representation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103452",
    "abstract": "Existing face aging (FA) approaches usually concentrate on a universal aging pattern, and produce restricted aging faces from one-to-one mapping. However, the diversity of living environments impact individuals differently in their oldness. To simulate various aging effects, we propose a multimodal FA framework based on face disentanglement technique of age-specific and age-irrelevant information. A Variational Autoencoder (VAE)-based encoder is designed to represent the distribution of the age-specific attributes. To capture the age-irrelevant features, a cycle-consistency loss of unpaired faces is utilized among various age spans. The extensive experimental results demonstrate that the sampled age-specific codes along with an age-irrelevant feature make the multimodal FA diverse and realistic.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322000141",
    "keywords": [
      "Artificial intelligence",
      "Autoencoder",
      "Computer science",
      "Consistency (knowledge bases)",
      "Deep learning",
      "Encoder",
      "Face (sociological concept)",
      "Feature (linguistics)",
      "Feature learning",
      "Gerontology",
      "Healthy aging",
      "Law",
      "Linguistics",
      "Medicine",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Lu"
      },
      {
        "surname": "Wang",
        "given_name": "Shenghui"
      },
      {
        "surname": "Wan",
        "given_name": "Lili"
      },
      {
        "surname": "Yu",
        "given_name": "Haibo"
      }
    ]
  },
  {
    "title": "A self-embedding secure fragile watermarking scheme with high quality recovery",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103437",
    "abstract": "In recent years, with the development of cloud storage, more and more people upload images to the cloud for storage. However, confidentiality and integrity issues may arise during transmission and storage to the cloud. Aiming at these security problems, a fragile watermarking scheme based on the encrypted domain is proposed. A watermark is divided into two types, one is for detection, the other is for recovery. After embedding the two types of watermarks into the host image, the watermarked image will be transferred to the cloud for storage. A three-level tamper detection mechanism is used in the detection process, and the first-level tamper detection can be processed in the cloud. While in recovery process, a mechanism of “block-level detection, pixel-level recovery” is proposed to recover the tampered area. The experimental results show that the watermarked image has greatly changed the original image and guarantees the confidentiality. The three-level tamper detection mechanism can accurately detect the tampered area, the image can be effectively restored in different situations, when the tampering rate is as high as 80%, the average PSNR reaches 34.62 dB, and the average SSIM is higher than 0.93.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322000025",
    "keywords": [
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Cloud computing",
      "Cloud storage",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Digital watermarking",
      "Embedding",
      "Encryption",
      "Geometry",
      "Image (mathematics)",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Pixel",
      "Process (computing)",
      "Scheme (mathematics)",
      "Upload",
      "Watermark"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Li"
      },
      {
        "surname": "Kuang",
        "given_name": "Da"
      },
      {
        "surname": "Li",
        "given_name": "Cheng-long"
      },
      {
        "surname": "Zhuang",
        "given_name": "Yu-jian"
      },
      {
        "surname": "Duan",
        "given_name": "Shao-hua"
      },
      {
        "surname": "Zhou",
        "given_name": "Xiao-yi"
      }
    ]
  },
  {
    "title": "Rotation-aware correlation filters for robust visual tracking",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2021.103422",
    "abstract": "Recent years have witnessed several modified discriminative correlation filter (DCF) models exhibiting excellent performance in visual tracking. A fundamental drawback to these methods is that rotation of the target is not well addressed which leads to model deterioration. In this paper, we propose a novel rotation-aware correlation filter to address the issue. Specifically, samples used for training of the modified DCF model are rectified when rotation occurs, rotation angle is effectively calculated using phase correlation after transforming the search patch from Cartesian coordinates to the Log-polar coordinates, and an adaptive selection mechanism is further adopted to choose between a rectified target patch and a rectangular patch. Moreover, we extend the proposed approach for robust tracking by introducing a simple yet effective Kalman filter prediction strategy. Extensive experiments on five standard benchmarks show that the proposed method achieves superior performance against state-of-the-art methods while running in real-time on single CPU.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002844",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cartesian coordinate system",
      "Computer science",
      "Computer vision",
      "Correlation",
      "Discriminative model",
      "Eye tracking",
      "Filter (signal processing)",
      "Geometry",
      "Kalman filter",
      "Mathematics",
      "Pedagogy",
      "Polar coordinate system",
      "Psychology",
      "Rotation (mathematics)",
      "Tracking (education)"
    ],
    "authors": [
      {
        "surname": "Liao",
        "given_name": "Jiawen"
      },
      {
        "surname": "Qi",
        "given_name": "Chun"
      },
      {
        "surname": "Cao",
        "given_name": "Jianzhong"
      },
      {
        "surname": "Wang",
        "given_name": "Xiaofang"
      },
      {
        "surname": "Ren",
        "given_name": "Long"
      },
      {
        "surname": "Zhang",
        "given_name": "Chaoning"
      }
    ]
  },
  {
    "title": "Exemplar-based image inpainting using adaptive two-stage structure-tensor based priority function and nonlocal filtering",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2021.103430",
    "abstract": "For the exemplar-based image inpainting problem, the filling order and local intensity smoothness are two crucial factors that should be considered carefully. This work gives a new exemplar-based image inpainting method, preventing geometric structures from being destroyed and reconstructing textures well to obtain elegant-looking outputs. For a better filling order, we define a new adaptive two-stage structure-tensor based priority function. To promote the local intensity smoothness, we adopt a non-local way, and at the same time, propose a weighted filter based on a Gaussian-like function to generate the ideal filling patch by combining non-local patches. We compare the proposed method with some recent state-of-the-art image inpainting approaches on different tasks, such as texture and structure synthesis, object removal, and remote sensing images inpainting. Experimental results demonstrate the superiority of the proposed method, both visually and quantitatively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002893",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Evolutionary biology",
      "Filter (signal processing)",
      "Function (biology)",
      "Gaussian",
      "Geometry",
      "Image (mathematics)",
      "Image processing",
      "Image texture",
      "Inpainting",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Smoothness",
      "Structure tensor",
      "Tensor (intrinsic definition)",
      "Texture synthesis"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Ting"
      },
      {
        "surname": "Huang",
        "given_name": "Ting-Zhu"
      },
      {
        "surname": "Deng",
        "given_name": "Liang-Jian"
      },
      {
        "surname": "Zhao",
        "given_name": "Xi-Le"
      },
      {
        "surname": "Hu",
        "given_name": "Jin-Fan"
      }
    ]
  },
  {
    "title": "Detecting moving object from dynamic background video sequences via simulating heat conduction",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103439",
    "abstract": "Moving object detection is one of the essential tasks for surveillance video analysis. The dynamic background often composed by waving trees, rippling water or fountains, etc. in nature scene greatly interferes with the detection of moving objects in the form of noise. In this paper, a method simulating heat conduction is proposed to extract moving objects from dynamic background video sequences. Based on the visual background extractor (ViBe) with an adaptable distance threshold, we design a temperature field relying on the generated mask image to distinguish between the moving objects and the noise caused by dynamic background. In temperature field, a brighter pixel is associated with more energy. It will transfer a certain amount of energy to its neighboring darker pixels. Through multiple steps of energy transfer the noise regions loss more energy so that they become darker than the detected moving objects. After heat conduction, K-Means algorithm with the customized initial clustering centers is utilized to separate the moving objects from background. We test our method on many videos with dynamic background from public datasets. The results show that the proposed method is feasible and effective for moving object detection from dynamic background sequences.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322000037",
    "keywords": [
      "Artificial intelligence",
      "Background image",
      "Background subtraction",
      "Cluster analysis",
      "Computer science",
      "Computer vision",
      "Energy (signal processing)",
      "Image (mathematics)",
      "Noise (video)",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Physics",
      "Pixel",
      "Programming language",
      "Quantum mechanics",
      "Rippling"
    ],
    "authors": [
      {
        "surname": "Dai",
        "given_name": "Yuan"
      },
      {
        "surname": "Yang",
        "given_name": "Long"
      }
    ]
  },
  {
    "title": "BGGMM-HMT based locally optimum image watermark detector in high-order NSST difference domain",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103450",
    "abstract": "Imperceptibility, robustness and data payload are three main requirements of any image watermarking systems to guarantee desired functionalities, but there is a tradeoff among them from the information-theoretic perspective. How to achieve this balance is a major challenge. In this paper, we propose a new statistical image watermarking scheme, which is based on the high-order difference coefficients in nonsubsampled Shearlet transform (NSST) domain and the bounded generalized Gaussian mixture model-based hidden Markov tree (BGGMM-HMT). In the watermark embedding process, we use a nonlinear embedding approach to hide the digital watermark into the robust high-order difference coefficients, which can achieve better imperceptibility. In the watermark detection process, high-order difference coefficients are accurately modeled by using BGGMM-HMT, where the distribution characteristics of high-order difference coefficients can be captured through BGGMM, and the scale dependencies of high-order difference coefficients can be captured through HMT. Statistical model parameters are then estimated by combining the approach of minimizing the higher bound on data negative log-likelihood function and upward–downward algorithm. Finally, an image watermark detector based on BGGMM-HMT is developed using the locally optimum (LO) decision rule. For the proposed detector, the receiver operating characteristic (ROC) expression is derived in detail. We evaluate the proposed scheme from different aspects and compare it with the state-of-the-art schemes. After a large number of experimental tests, the encouraging results obtained prove the effectiveness of our watermarking scheme.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032200013X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Bounded function",
      "Chemistry",
      "Computer science",
      "Digital watermarking",
      "Embedding",
      "Gene",
      "Generalized normal distribution",
      "Image (mathematics)",
      "Mathematical analysis",
      "Mathematics",
      "Normal distribution",
      "Pattern recognition (psychology)",
      "Robustness (evolution)",
      "Statistics",
      "Watermark"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Xiang-yang"
      },
      {
        "surname": "Shen",
        "given_name": "Xin"
      },
      {
        "surname": "Niu",
        "given_name": "Pan-pan"
      },
      {
        "surname": "Yang",
        "given_name": "Hong-ying"
      }
    ]
  },
  {
    "title": "Security measurement of a medical communication scheme based on chaos and DNA coding",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2021.103424",
    "abstract": "To encrypt sensitive information existing in a color DICOM images, a medical privacy protection scheme (called as MPPS) based on chaos and DNA coding was proposed by using two coupled chaotic systems to produce cryptographic primitives. Relying on some empirical analyzes and experimental results, the designers of MPPS claimed that it can withstand a chosen-plaintext attack and some other classic attacking models. However, this statement is groundless. In this paper, we investigate the essential properties of MPPS and DNA coding, and we then propose an efficient chosen-plaintext attack to disclose its equivalent secret-key. The attack only needs ⌈ log 256 ( 3 ⋅ M ⋅ N ) ⌉ + 4 pair of chosen plain-images and the corresponding cipher-images, where M × N and “3” are the size of the RGB color image and the number of color channels, respectively. In addition, the other claimed superiorities are questioned from the perspective of modern cryptography. Both theoretical and experimental results are presented to support the efficiency of the proposed attack and the other reported security faults. The proposed cryptanalysis results will promote the proper application of DNA encoding to protect multimedia privacy data, especially that in a DICOM image.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002868",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cipher",
      "Ciphertext",
      "Coding (social sciences)",
      "Computer science",
      "Computer security",
      "Cryptanalysis",
      "Cryptography",
      "Cryptosystem",
      "DICOM",
      "Encryption",
      "Mathematics",
      "Plaintext",
      "Statistics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Lei"
      },
      {
        "surname": "Li",
        "given_name": "Chengqing"
      },
      {
        "surname": "Li",
        "given_name": "Chao"
      }
    ]
  },
  {
    "title": "Collaborative Distribution Alignment for 2D image-based 3D shape retrieval",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2021.103426",
    "abstract": "Retrieving 3D shapes with 2D images has become a popular research area nowadays, and a great deal of work has been devoted to reducing the discrepancy between 3D shapes and 2D images to improve retrieval performance. However, most approaches ignore the semantic information and decision boundaries of the two domains, and cannot achieve both domain alignment and category alignment in one module. In this paper, a novel Collaborative Distribution Alignment (CDA) model is developed to address the above existing challenges. Specifically, we first adopt a dual-stream CNN, following a similarity guided constraint module, to generate discriminative embeddings for input 2D images and 3D shapes (described as multiple views). Subsequently, we explicitly introduce a joint domain-class alignment module to dynamically learn a class-discriminative and domain-agnostic feature space, which can narrow the distance between 2D image and 3D shape instances of the same underlying category, while pushing apart the instances from different categories. Furthermore, we apply a decision boundary refinement module to avoid generating class-ambiguity embeddings by dynamically adjusting inconsistencies between two discriminators. Extensive experiments and evaluations on two challenging benchmarks, MI3DOR and MI3DOR-2, demonstrate the superiority of the proposed CDA method for 2D image-based 3D shape retrieval task.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002881",
    "keywords": [
      "Ambiguity",
      "Artificial intelligence",
      "Boundary (topology)",
      "Class (philosophy)",
      "Computer science",
      "Computer vision",
      "Constraint (computer-aided design)",
      "Decision boundary",
      "Discriminative model",
      "Domain (mathematical analysis)",
      "Economics",
      "Feature (linguistics)",
      "Feature vector",
      "Geometry",
      "Image (mathematics)",
      "Image retrieval",
      "Linguistics",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Similarity (geometry)",
      "Support vector machine",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Nian"
      },
      {
        "surname": "Zhou",
        "given_name": "Heyu"
      },
      {
        "surname": "Liu",
        "given_name": "An-An"
      },
      {
        "surname": "Huang",
        "given_name": "Xiangdong"
      },
      {
        "surname": "Zhang",
        "given_name": "Shenyuan"
      },
      {
        "surname": "Jin",
        "given_name": "Guoqing"
      },
      {
        "surname": "Guo",
        "given_name": "Junbo"
      },
      {
        "surname": "Li",
        "given_name": "Xuanya"
      }
    ]
  },
  {
    "title": "A CBIR system based on saliency driven local image features and multi orientation texture features",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2021.103396",
    "abstract": "In Content-based Image Retrieval (CBIR), the user provides the query image in which only a selective portion of the image carries the foremost vital information known as the object region of the image. However, the human visual system also focuses on a particular salient region of an image to instinctively understand its semantic meaning. Therefore, the human visual attention technique can be well imposed in the CBIR scheme. Inspired by these facts, we initially utilized the signature saliency map-based approach to decompose the image into its respective main object region (ObR) and non-object region (NObR). ObR possesses most of the vital image information, so block-level normalized singular value decomposition (SVD) has been used to extract salient features of the ObR. In most natural images, NObR plays a significant role in understanding the actual semantic meaning of the image. Accordingly, multi-directional texture features have been extracted from NObR using Gabor filter on different wavelengths. Since the importance of ObR and NObR features are not equal, a new homogeneity-based similarity matching approach has been devised to enhance retrieval accuracy. Finally, we have demonstrated retrieval performances using both the combined and distinct ObR and NObR features on seven standard coral, texture, object, and heterogeneous datasets. The experimental outcomes show that the proposed CBIR system has a promising retrieval efficiency and outperforms various existing systems substantially.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002649",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Content-based image retrieval",
      "Human visual system model",
      "Image (mathematics)",
      "Image processing",
      "Image retrieval",
      "Image texture",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Salient",
      "Singular value decomposition",
      "Visual Word"
    ],
    "authors": [
      {
        "surname": "Pradhan",
        "given_name": "Jitesh"
      },
      {
        "surname": "Pal",
        "given_name": "Arup Kumar"
      },
      {
        "surname": "Banka",
        "given_name": "Haider"
      }
    ]
  },
  {
    "title": "Accurate bounding-box regression with distance-IoU loss for visual tracking",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2021.103428",
    "abstract": "Most existing trackers are based on using a classifier and multi-scale estimation to estimate the target state. Consequently, and as expected, trackers have become more stable while tracking accuracy has stagnated. While trackers adopt a maximum overlap method based on an intersection-over-union (IoU) loss to mitigate this problem, there are defects in the IoU loss itself, that make it impossible to continue to optimize the objective function when a given bounding box is completely contained within/without another bounding box; this makes it very challenging to accurately estimate the target state. Accordingly, in this paper, we address the above-mentioned problem by proposing a novel tracking method based on a distance-IoU (DIoU) loss, such that the proposed tracker consists of target estimation and target classification. The target estimation part is trained to predict the DIoU score between the target ground-truth bounding-box and the estimated bounding-box. The DIoU loss can maintain the advantage provided by the IoU loss while minimizing the distance between the center points of two bounding boxes, thereby making the target estimation more accurate. Moreover, we introduce a classification part that is trained online and optimized with a Conjugate-Gradient-based strategy to guarantee real-time tracking speed. Comprehensive experimental results demonstrate that the proposed method achieves competitive tracking accuracy when compared to state-of-the-art trackers while with a real-time tracking speed.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032100290X",
    "keywords": [
      "Aerospace engineering",
      "Artificial intelligence",
      "BitTorrent tracker",
      "Bounding overwatch",
      "Classifier (UML)",
      "Computer science",
      "Computer vision",
      "Engineering",
      "Eye tracking",
      "Ground truth",
      "Image (mathematics)",
      "Intersection (aeronautics)",
      "Minimum bounding box",
      "Pattern recognition (psychology)",
      "Pedagogy",
      "Psychology",
      "Tracking (education)"
    ],
    "authors": [
      {
        "surname": "Yuan",
        "given_name": "Di"
      },
      {
        "surname": "Shu",
        "given_name": "Xiu"
      },
      {
        "surname": "Fan",
        "given_name": "Nana"
      },
      {
        "surname": "Chang",
        "given_name": "Xiaojun"
      },
      {
        "surname": "Liu",
        "given_name": "Qiao"
      },
      {
        "surname": "He",
        "given_name": "Zhenyu"
      }
    ]
  },
  {
    "title": "A-contrario framework for detection of alterations in varnished surfaces",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2021.103357",
    "abstract": "Preventive conservation is the constant monitoring of the state of conservation of an artwork to reduce the risk of damages and so to minimize the necessity of restorations. Many methods have been proposed during time, generally including a mix of different analytical techniques. In this work, we present a probabilistic approach based on the a-contrario framework for the detection of alterations on varnished surfaces, in particular those of historical musical instruments. Our method is a one step Number of False Alarms (NFA) clustering solution which considers simultaneously gray-level and spatial density information in a single background model. The proposed approach is robust to noise and avoids parameter tuning as well as any assumption about the shape and size of the worn-out areas. Tests have been conducted on UV induced fluorescence (UVIFL) image sequences included in the “Violins UVIFL imagery” dataset. UVIFL photography is a well known diagnostic technique used to see details of a surface not perceivable with visible light. The obtained results prove the capability of the algorithm to properly detect the altered regions. Comparisons with other the state-of-the-art clustering methods show improvement in both precision and recall.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002352",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Computer vision",
      "Economics",
      "Image (mathematics)",
      "Management",
      "Pattern recognition (psychology)",
      "Precision and recall",
      "Probabilistic logic",
      "Violin"
    ],
    "authors": [
      {
        "surname": "Rezaei",
        "given_name": "Alireza"
      },
      {
        "surname": "Le Hégarat-Mascle",
        "given_name": "Sylvie"
      },
      {
        "surname": "Aldea",
        "given_name": "Emanuel"
      },
      {
        "surname": "Dondi",
        "given_name": "Piercarlo"
      },
      {
        "surname": "Malagodi",
        "given_name": "Marco"
      }
    ]
  },
  {
    "title": "A novel dynamic gesture understanding algorithm fusing convolutional neural networks with hand-crafted features",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2022.103454",
    "abstract": "Dynamic gestures have attracted much attention in recent years due to their user-friendly interactive characteristics. However, accurate and efficient dynamic gesture understanding remains a challenge due to complex scenarios and motion information. Conventional handcrafted features are computationally cheap but can only extract low-level image features. This leads to performance degradation when dealing with complex scenes. In contrast, deep learning-based methods have a stronger feature expression ability and hence can capture more abstract and high-level image features. However, they critically rely on a large amount of training data. To address the above issues, a novel dynamic gesture understanding algorithm based on feature fusion is proposed for accurate dynamic gesture prediction. It leverages the advantages of handcrafted features and transfer learning. Aimed at small-scale dynamic gesture data, transfer learning is introduced for capturing effective feature expression. To precisely model the critical temporal information associated with dynamic gestures, a novel feature descriptor, namely, A l e x N e t 2 , is proposed for effective feature expression of dynamic gestures from the spatial and temporal domain. On this basis, a decision-level feature fusion framework based on support vector machine (SVM) and Dempster–Shafer (DS) evidence theory is constructed to utilize handcrafted features and A l e x N e t 2 to realize high-precision dynamic gesture understanding. To verify the effectiveness and robustness of the proposed recognition algorithm, analysis and comparison experiments are performed on the public Cambridge gesture dataset and Northwestern University hand gesture dataset. The proposed gesture recognition algorithm achieves prediction accuracies of 99.50% and 96.97% on these two datasets. Experimental results show that the proposed recognition framework exhibits a better recognition performance in comparison with related prediction algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322000153",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Gene",
      "Gesture",
      "Gesture recognition",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Robustness (evolution)",
      "Support vector machine",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Yanhong"
      },
      {
        "surname": "Song",
        "given_name": "Shouan"
      },
      {
        "surname": "Yang",
        "given_name": "Lei"
      },
      {
        "surname": "Bian",
        "given_name": "Guibin"
      },
      {
        "surname": "Yu",
        "given_name": "Hongnian"
      }
    ]
  },
  {
    "title": "Cross-dataset emotion recognition from facial expressions through convolutional neural networks",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2021.103395",
    "abstract": "The face is the window to the soul. This is what the 19th-century French doctor Duchenne de Boulogne thought. Using electric shocks to stimulate muscular contractions and induce bizarre-looking expressions, he wanted to understand how muscles produce facial expressions and reveal the most hidden human emotions. Two centuries later, this research field remains very active. We see automatic systems for recognizing emotion and facial expression being applied in medicine, security and surveillance systems, advertising and marketing, among others. However, there are still fundamental questions that scientists are trying to answer when analyzing a person’s emotional state from their facial expressions. Is it possible to reliably infer someone’s internal state based only on their facial muscles’ movements? Is there a universal facial setting to express basic emotions such as anger, disgust, fear, happiness, sadness, and surprise? In this research, we seek to address some of these questions through convolutional neural networks. Unlike most studies in the prior art, we are particularly interested in examining whether characteristics learned from one group of people can be generalized to predict another’s emotions successfully. In this sense, we adopt a cross-dataset evaluation protocol to assess the performance of the proposed methods. Our baseline is a custom-tailored model initially used in face recognition to categorize emotion. By applying data visualization techniques, we improve our baseline model, deriving two other methods. The first method aims to direct the network’s attention to regions of the face considered important in the literature but ignored by the baseline model, using patches to hide random parts of the facial image so that the network can learn discriminative characteristics in different regions. The second method explores a loss function that generates data representations in high-dimensional spaces so that examples of the same emotion class are close and examples of different classes are distant. Finally, we investigate the complementarity between these two methods, proposing a late-fusion technique that combines their outputs through the multiplication of probabilities. We compare our results to an extensive list of works evaluated in the same adopted datasets. In all of them, when compared to works that followed an intra-dataset protocol, our methods present competitive numbers. Under a cross-dataset protocol, we achieve state-of-the-art results, outperforming even commercial off-the-shelf solutions from well-known tech companies.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002637",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Emotion recognition",
      "Facial expression",
      "Pattern recognition (psychology)",
      "Psychology",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Dias",
        "given_name": "William"
      },
      {
        "surname": "Andaló",
        "given_name": "Fernanda"
      },
      {
        "surname": "Padilha",
        "given_name": "Rafael"
      },
      {
        "surname": "Bertocco",
        "given_name": "Gabriel"
      },
      {
        "surname": "Almeida",
        "given_name": "Waldir"
      },
      {
        "surname": "Costa",
        "given_name": "Paula"
      },
      {
        "surname": "Rocha",
        "given_name": "Anderson"
      }
    ]
  },
  {
    "title": "Visual cryptography scheme for secret color images with color QR codes",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2021.103405",
    "abstract": "Visual cryptography scheme can divide the secret image into several shares. It can be used to enhance the secure transmission of the secret image on the Internet. Most schemes cannot fully restore the secret color image and generate meaningful shares. This paper proposes two new schemes by using color XOR to solve these problems. The first proposed scheme generates meaningless shares. The second proposed scheme can generate n − 1 meaningful shares and a meaningless share. Based on the first proposed scheme, n − 1 shares are modified to color QR codes in the second proposed scheme. These color QR codes can be decoded by the general decoder instead of the standard decoder. All shares are performed the color XOR to restore the secret color image completely. This paper uses some experiments to test two proposed schemes. Experimental results show that the two proposed schemes are feasible.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002704",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Bitwise operation",
      "Color image",
      "Computer science",
      "Computer vision",
      "Cryptography",
      "Decoding methods",
      "Exclusive or",
      "Homomorphic secret sharing",
      "Image (mathematics)",
      "Image processing",
      "Mathematical analysis",
      "Mathematics",
      "Programming language",
      "Scheme (mathematics)",
      "Secret sharing",
      "Theoretical computer science",
      "Visual cryptography"
    ],
    "authors": [
      {
        "surname": "Pan",
        "given_name": "Jeng-Shyang"
      },
      {
        "surname": "Liu",
        "given_name": "Tao"
      },
      {
        "surname": "Yang",
        "given_name": "Hong-Mei"
      },
      {
        "surname": "Yan",
        "given_name": "Bin"
      },
      {
        "surname": "Chu",
        "given_name": "Shu-Chuan"
      },
      {
        "surname": "Zhu",
        "given_name": "Tongtong"
      }
    ]
  },
  {
    "title": "A novel active shape model-based DeepNeural network for age invariance face recognition",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2021.103393",
    "abstract": "Scientific efforts have expanded in age-invariant face recognition (AIFR). Matching faces of large age difference is, therefore, a problem, mostly because of a substantial disparity in the appearance of both young and old age. Owing to age, both the appearance and shape of the face are impaired, making recognition of the face the most challenging task. In recent years, AIFR has become a very common and demanding task. The set of feature extraction and classification algorithm is of prime importance in this field. As the numbers of features obtained from the datasets are large, there is a need to introduce a dimensionality reduction method to map high dimensionality feature space to low variance filter to form the final integrated face age model to be used in the classification process. In this paper, we introduced a novel concept of an improved Active Shape Model (ASM) in conjunction with a specially designed 7-layered Convolutional Neural Network (CNN) in order to accomplish a combination of feature extraction and classification in a single unit. The study approach involves conducting extensive experiments to evaluate the proposed system's performance using three standard datasets: FG-NET, LAG, and CACD. The results reveal that the proposed method outperforms state-of-the-art approaches and achieves excellent accuracy in face recognition across age. The maximum accuracies achieved by demonstrated ASM-CNN methodology for FG-NET, LAG, and CACD databases are 95.02%, 91.76 % and 99.4 % respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002625",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Curse of dimensionality",
      "Dimensionality reduction",
      "Face (sociological concept)",
      "Facial recognition system",
      "Feature extraction",
      "Invariant (physics)",
      "Mathematical physics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Dhamija",
        "given_name": "Ashutosh"
      },
      {
        "surname": "Dubey",
        "given_name": "R.B."
      }
    ]
  },
  {
    "title": "Person re-identification based on deep learning — An overview",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2021.103418",
    "abstract": "Person re-identification(ReID) is an intelligent video surveillance technology that retrieves the same person from different cameras. This task is extremely challenging due to changes in person poses, different camera views, and occlusion. In recent years, person ReID based on deep learning technology has received widespread attention due to the rapid development and excellent performance of deep learning. In this paper, we first divide person ReID based on deep learning approaches into seven types, i.e., fused hand-crafted features deep model, representation learning model, metric learning model, part-based deep model, video-based model, GAN-based model, unsupervised model. Furthermore, we launched a brief overview of the seven types. Then, we introduce some examples of commonly used datasets, compare the performance of some algorithms on image and video datasets in recent years, and analyze the advantages and disadvantages of various methods. Finally, we summarize the possible future research directions of person ReID technology.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002765",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Deep learning",
      "Economics",
      "Identification (biology)",
      "Law",
      "Machine learning",
      "Management",
      "Metric (unit)",
      "Operations management",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Wei",
        "given_name": "Wenyu"
      },
      {
        "surname": "Yang",
        "given_name": "Wenzhong"
      },
      {
        "surname": "Zuo",
        "given_name": "Enguang"
      },
      {
        "surname": "Qian",
        "given_name": "Yunyun"
      },
      {
        "surname": "Wang",
        "given_name": "Lihua"
      }
    ]
  },
  {
    "title": "Cross-layer progressive attention bilinear fusion method for fine-grained visual classification",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2021.103414",
    "abstract": "Fine-grained visual classification (FGVC) is a critical task in the field of computer vision. However, FGVC is full of challenges due to the large intra-class variation and small inter-class variation of the classes to be classified on an image. The key in dealing with the problem is to capture subtle visual differences from the image and effectively represent the discriminative features. Existing methods are often limited by insufficient localization accuracy and insufficient feature representation capabilities. In this paper, we propose a cross-layer progressive attention bilinear fusion (CPABF in short) method, which can efficiently express the characteristics of discriminative regions. The CPABF method involves three components: 1) Cross-Layer Attention (CLA) locates and reinforces the discriminative region with low computational costs; 2) The Cross-Layer Bilinear Fusion Module (CBFM) effectively integrates the semantic information from the low-level to the high-level 3) Progressive Training optimizes the parameters in the network to the best state in a delicate way. The CPABF shows excellent performance on the four FGVC datasets and outperforms some state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002789",
    "keywords": [
      "Artificial intelligence",
      "Astrophysics",
      "Bilinear interpolation",
      "Chemistry",
      "Class (philosophy)",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Discriminative model",
      "Economics",
      "Feature (linguistics)",
      "Feature extraction",
      "Fusion",
      "Key (lock)",
      "Law",
      "Layer (electronics)",
      "Linguistics",
      "Machine learning",
      "Management",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Task (project management)",
      "Variation (astronomy)",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Chaoqing"
      },
      {
        "surname": "Qian",
        "given_name": "Yurong"
      },
      {
        "surname": "Gong",
        "given_name": "Weijun"
      },
      {
        "surname": "Cheng",
        "given_name": "Junjong"
      },
      {
        "surname": "Wang",
        "given_name": "Yongqiang"
      },
      {
        "surname": "Wang",
        "given_name": "Yuefei"
      }
    ]
  },
  {
    "title": "Attention integrated hierarchical networks for no-reference image quality assessment",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2021.103399",
    "abstract": "Quality assessment of natural images is influenced by perceptual mechanisms, e.g., attention and contrast sensitivity, and quality perception can be generated in a hierarchical process. This paper proposes an architecture of Attention Integrated Hierarchical Image Quality networks (AIHIQnet) for no-reference quality assessment. AIHIQnet consists of three components: general backbone network, perceptually guided neck network, and head network. Multi-scale features extracted from the backbone network are fused to simulate image quality perception in a hierarchical manner. The attention and contrast sensitivity mechanisms modelled by an attention module capture essential information for quality perception. Considering that image rescaling potentially affects perceived quality, appropriate pooling methods in the non-convolution layers in AIHIQnet are employed to accept images with arbitrary resolutions. Comprehensive experiments on publicly available databases demonstrate outstanding performance of AIHIQnet compared to state-of-the-art models. Ablation experiments were performed to investigate the variants of the proposed architecture and reveal importance of individual components.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002674",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Computer vision",
      "Contrast (vision)",
      "Convolution (computer science)",
      "Data mining",
      "Electronic engineering",
      "Engineering",
      "Epistemology",
      "Image (mathematics)",
      "Image quality",
      "Neuroscience",
      "Operating system",
      "Pattern recognition (psychology)",
      "Perception",
      "Philosophy",
      "Pooling",
      "Process (computing)",
      "Quality (philosophy)",
      "Sensitivity (control systems)"
    ],
    "authors": [
      {
        "surname": "You",
        "given_name": "Junyong"
      },
      {
        "surname": "Korhonen",
        "given_name": "Jari"
      }
    ]
  },
  {
    "title": "CDGAN: Cyclic Discriminative Generative Adversarial Networks for image-to-image transformation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2021.103382",
    "abstract": "Generative Adversarial Networks (GANs) have facilitated a new direction to tackle the image-to-image transformation problem. Different GANs use generator and discriminator networks with different losses in the objective function. Still there is a gap to fill in terms of both the quality of the generated images and close to the ground truth images. In this work, we introduce a new Image-to-Image Transformation network named Cyclic Discriminative Generative Adversarial Networks (CDGAN) that fills the above mentioned gaps. The proposed CDGAN generates high quality and more realistic images by incorporating the additional discriminator networks for cycled images in addition to the original architecture of the CycleGAN. The proposed CDGAN is tested over three image-to-image transformation datasets. The quantitative and qualitative results are analyzed and compared with the state-of-the-art methods. The proposed CDGAN method outperforms the state-of-the-art methods when compared over the three baseline Image-to-Image transformation datasets. The code is available at https://github.com/KishanKancharagunta/CDGAN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002522",
    "keywords": [],
    "authors": [
      {
        "surname": "Kishan Babu",
        "given_name": "Kancharagunta"
      },
      {
        "surname": "Dubey",
        "given_name": "Shiv Ram"
      }
    ]
  },
  {
    "title": "Proximal-Gen for fast compressed sensing recovery",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2021.103358",
    "abstract": "Compressed sensing (CS) can recover an image from a few random measurements by exploiting the sparsity assumption on the structure of images. Some recent generative model-based CS recovery methods have removed the sparsity constraint, but their recovery process is slow and the recovered signal is constrained to be in the generator range. Here, we propose a new framework, called Proximal-Gen, for CS recovery. Specifically, we first formulate a general domain of the recovered signals, this allows the subsequent recovery algorithms to recover the signals that deviate from the generator range. Then based on the general domain, we develop a fast recovery algorithm, which mainly consists of two sub-algorithms, namely network-based projected gradient descent (NPGD) and denoiser-based proximal gradient descent (DPGD). The NPGD is used to obtain an intermediate signal lying in the generator range, while the DPGD is proposed to recover a deviation signal. Compared with multiple recent generative model-based recovery methods, our method can achieve better reconstruction performance and higher efficiency under most measurements.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002364",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Composite material",
      "Compressed sensing",
      "Computer science",
      "Constraint (computer-aided design)",
      "Domain (mathematical analysis)",
      "Generator (circuit theory)",
      "Geometry",
      "Gradient descent",
      "Materials science",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Physics",
      "Power (physics)",
      "Process (computing)",
      "Programming language",
      "Quantum mechanics",
      "Range (aeronautics)",
      "SIGNAL (programming language)",
      "Signal recovery"
    ],
    "authors": [
      {
        "surname": "Cai",
        "given_name": "Lei"
      },
      {
        "surname": "Fu",
        "given_name": "Yuli"
      },
      {
        "surname": "Zhu",
        "given_name": "Tao"
      },
      {
        "surname": "Xiang",
        "given_name": "Youjun"
      },
      {
        "surname": "Zeng",
        "given_name": "Huanqiang"
      }
    ]
  },
  {
    "title": "Reversal of pixel rotation: A reversible data hiding system towards cybersecurity in encrypted images",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2021.103421",
    "abstract": "Due to privacy and security concerns, the researches of reversible data hiding in encrypted images (RDHEI) have become increasingly important. Conventional schemes vacate the spare room after image encryption (VRAE) suffer from the low embedding rate, high error rate of data extraction, and imperfect image recovery. To address these issues, we propose a separable reversible data hiding scheme for encrypted images that utilizes a novel pixel rotation technique to embed data into fully encrypted images. The block complexities of four decrypted rotation states are considered when recovering image. To realize perfect image recovery, we further devise a lossless version (LPR-RDHEI). Experimental results demonstrate that the proposed PR-RDHEI scheme achieves an embedding rate of 0.4994 bpp on average and ensures lossless data extraction. Meanwhile, the proposed LPR-RDHEI scheme still has a 0.4494 bpp embedding rate on average. The embedding rates of our two schemes are significantly improved compared with state-of-the-arts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002832",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Data compression",
      "Data extraction",
      "Embedding",
      "Encryption",
      "Geometry",
      "Image (mathematics)",
      "Information hiding",
      "Law",
      "Lossless compression",
      "MEDLINE",
      "Mathematical analysis",
      "Mathematics",
      "Pixel",
      "Political science",
      "Rotation (mathematics)",
      "Scheme (mathematics)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Xu"
      },
      {
        "surname": "Chang",
        "given_name": "Ching-Chun"
      },
      {
        "surname": "Lin",
        "given_name": "Chia-Chen"
      },
      {
        "surname": "Chang",
        "given_name": "Chin-Chen"
      }
    ]
  },
  {
    "title": "DVL2021: An ultra high definition video dataset for perceptual quality study",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2021.103374",
    "abstract": "This paper describes an ultra high definition (UHD) video dataset named DVL2021 for the perceptual study of video quality assessment (VQA). To our knowledge, DVL2021 is the first authentically distorted 4K (3840 × 2160) UHD video quality dataset. The dataset contains 206 versatile 4K UHD video sequences, which are all collected in in-the-wild scenarios. Each sequence is captured at 50 frames per second (fps), stored in raw 10-bit 4:2:0 YUV format, and has a duration of 10 s. Following the subjective evaluation method of TV image quality granted by ITU-R BT.500-13, 32 unique participants take part in the manual annotation process, whose ages are from teenage to sixties (32.7 years old on average). DVL2021 has the following merits: (1) enormous variety of video contents, (2) captured by different types of cameras, (3) complex types and multiple levels of authentic distortion, (4) broadly distributed temporal/spatial information, and (5) a wide spectrum of mean opinion scores (MOS) distribution. Furthermore, we conduct a benchmark experiment by evaluating several mainstream VQA methods on DVL2021. The baseline results are higher than 0.75 in Spearman’s rank order correlation coefficient (SROCC) metric. Our study provides a basis for the UHD VQA problem. DVL2021 is publicly available at https://github.com/GZHU-DVL/DVL2021.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002479",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Neuroscience",
      "Perception",
      "Physics",
      "Psychology",
      "Quality (philosophy)",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Xing",
        "given_name": "Fengchuang"
      },
      {
        "surname": "Wang",
        "given_name": "Yuan-Gen"
      },
      {
        "surname": "Wang",
        "given_name": "Hanpin"
      },
      {
        "surname": "He",
        "given_name": "Jiefeng"
      },
      {
        "surname": "Yuan",
        "given_name": "Jinchun"
      }
    ]
  },
  {
    "title": "M2OVQA: Multi-space signal characterization and multi-channel information aggregation for quality assessment of compressed omnidirectional videos",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2021.103419",
    "abstract": "Considering the high requirements for omnidirectional video compression, we propose an objective quality evaluation method to assess quality loss in encoding omnidirectional videos. According to characteristics of 360° videos, we consider multi-space signal characterization (MSSC) to fully characterize the distortions of video signals from spatial/image domains to frequency domains and from image content to motion information, and further consider multi-channel information aggregation (MCIA) to fuse scores from multiple projection planes and temporal divided groups. The main innovation of our method is to establish a universal framework in bridging the connection between typical quality assessment and 360° quality assessment to measure 360° video quality effectively and efficiently. Experimental results show that our method outperforms state-of-the-art 2D quality metrics and quality metrics for omnidirectional images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002819",
    "keywords": [
      "Antenna (radio)",
      "Artificial intelligence",
      "Bridging (networking)",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Electrical engineering",
      "Engineering",
      "Epistemology",
      "Fuse (electrical)",
      "Image (mathematics)",
      "Image quality",
      "Metric (unit)",
      "Omnidirectional antenna",
      "Omnidirectional camera",
      "Operations management",
      "Philosophy",
      "Programming language",
      "Quality (philosophy)",
      "SIGNAL (programming language)",
      "Telecommunications",
      "Video quality"
    ],
    "authors": [
      {
        "surname": "Chai",
        "given_name": "Xiongli"
      },
      {
        "surname": "Shao",
        "given_name": "Feng"
      }
    ]
  },
  {
    "title": "DCA-CycleGAN: Unsupervised single image dehazing using Dark Channel Attention optimized CycleGAN",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2021.103431",
    "abstract": "Single image dehazing has great significance in computer vision. In this paper, we propose a novel unsupervised Dark Channel Attention optimized CycleGAN (DCA-CycleGAN) to deal with the challenging scene with uneven and dense haze concentration. Firstly, the DCA-CycleGAN adopts the dark channel as input and then generate attention through a DCA subnetwork to handle the nonhomogeneous haze. Secondly, in addition to the conventional global discriminator, we also leverage two local discriminators to enhance the dehazing performance on the local dense haze, and a new local adversarial loss calculated strategy is been proposed. Specifically, the dehazing generator consists of two subnetworks: an auto-encoder and a dark channel attention subnetwork. The auto-encoder consists of an encoder, a feature transformation module, and a decoder. The dark channel attention subnetwork has the same structure as the encoder and the feature transformation module to ensure the same receptive field, which utilizes the dark channel to generate attention map and fine-tune the auto-encoder. Experimental results against several state-of-the-art methods demonstrate that our method can generate better visual effects, and is effective.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002923",
    "keywords": [
      "Artificial intelligence",
      "Channel (broadcasting)",
      "Computer science",
      "Computer vision",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Mo",
        "given_name": "Yaozong"
      },
      {
        "surname": "Li",
        "given_name": "Chaofeng"
      },
      {
        "surname": "Zheng",
        "given_name": "Yuhui"
      },
      {
        "surname": "Wu",
        "given_name": "Xiaojun"
      }
    ]
  },
  {
    "title": "A novel partial-to-partial registration method based on sampling network",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2021.103411",
    "abstract": "Point cloud registration is mainly to estimate a rigid transformation between point clouds. The traditional optimization-based registration method requires a good initial position, and it is easy to fall into a local optimal solution. Some learning-based methods are introduced to reduce the dependence on the initial transformation, but they cannot handle partial-to-partial registration tasks. This paper proposes a learning-based registration method for partial-to-partial scenario. The local geometry is encoded into the feature representation of each point. A transformer network is used to enhance attention features. A designed sampling network down-sample key matching points and their corresponding features. The rigid transformation is calculated according to virtual correspondence by a singular value decomposition layer. The ModelNet40 dataset and Stanford 3D Scanning models are used to test the registration performance. Experimental results show that the proposed method achieves better registration accuracy than traditional methods, and it is robust to any initial transformation and noise.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002753",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Gene",
      "Geometric transformation",
      "Geometry",
      "Image (mathematics)",
      "Image registration",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Point (geometry)",
      "Point cloud",
      "Point set registration",
      "Rigid transformation",
      "Robustness (evolution)",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Song",
        "given_name": "Yanan"
      },
      {
        "surname": "Shen",
        "given_name": "Weiming"
      },
      {
        "surname": "Lu",
        "given_name": "Peng"
      }
    ]
  },
  {
    "title": "Tamper video detection and localization using an adaptive segmentation and deep network technique",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2021.103401",
    "abstract": "In this work we have explored the hybrid deep learning architecture for recognizing the tampering from the videos. This hybrid architecture explores the features from the authentic videos to categorize the tampered portions from the forged videos. Initially, the process begins by compressing the input video using the Discrete cosine transform (DCT) based double compression approach. Then, the filtering process is carried out to improve the quality of compressed frame using the bilateral filtering. Then, the modified segmentation approach is applied to segment the frames into different regions. The features from these segmented portions are extracted and fed into hybrid DNN-AGSO (deep neural network- Adaptivf RELATED WORKSe Galactic Swarm Optimization) using Gabor wavelet transform (GWT) technique. Three different datasets are used to evaluate the overall performance they are, VTD, MFC-18, and VIRAT by MATLAB platform. The recognition rate achieved by VTD, MFC-18, and VIRAT datasets are 96%, 95.2%, and 93.47% respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002686",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Discrete cosine transform",
      "Frame (networking)",
      "Image (mathematics)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Process (computing)",
      "Segmentation",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Raveendra",
        "given_name": "Malle"
      },
      {
        "surname": "Nagireddy",
        "given_name": "K."
      }
    ]
  },
  {
    "title": "Human identification system using 3D skeleton-based gait features and LSTM model",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2021.103416",
    "abstract": "Vision-based gait emerged as the preferred biometric in smart surveillance systems due to its unobtrusive nature. Recent advancements in low-cost depth sensors resulted in numerous 3D skeleton-based gait analysis techniques. For spatial–temporal analysis, existing state-of-the-art algorithms use frame-level information as the timestamp. This paper proposes gait event-level spatial–temporal features and LSTM-based deep learning model that treats each gait event as a timestamp to identify individuals from walking patterns observed in single and multi-view scenarios. On four publicly available datasets, the proposed system stands superior to state-of-the-art approaches utilizing a variety of conventional benchmark protocols. The proposed system achieved a recognition rate of greater than 99% in low-level ranks during the CMC test, making it suitable for practical applications. The statistical study of gait event-level features demonstrated retrieved features’ discriminating capacity in classification. Additionally, the ANOVA test performed on findings from K folds demonstrated the proposed system’s significance in human identification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002807",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Biometrics",
      "Botany",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Event (particle physics)",
      "Feature (linguistics)",
      "Gait",
      "Gait analysis",
      "Geodesy",
      "Geography",
      "Identification (biology)",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Physiology",
      "Quantum mechanics",
      "Real-time computing",
      "Timestamp"
    ],
    "authors": [
      {
        "surname": "M.",
        "given_name": "Rashmi"
      },
      {
        "surname": "Guddeti",
        "given_name": "Ram Mohana Reddy"
      }
    ]
  },
  {
    "title": "Multi-oriented run length based static and dynamic features fused with Choquet fuzzy integral for human fall detection in videos",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2021.103375",
    "abstract": "Since a huge part of elderly people are living alone, assisted-living tools have become an essential in-home telemonitoring device. Hence, this paper proposes an automatic human fall detection in videos. In order to improve the system reliability, a new shape descriptor called multi-oriented run length (MORL) is proposed. This descriptor is exploited in a proposed scheme to generate static and dynamic features to represent human falls with complementary information. The generated static and dynamic features are fused through the Choquet fuzzy integral. Experimental results conducted on three well-known datasets containing almost 1300 video segments show an interesting adaptation of the proposed approaches. More precisely, the proposed MORL descriptor shows its superiority against known descriptors such as LBP and HOG. Moreover, Choquet fuzzy integral significantly improves the results versus standard combiners. In general, the obtained results highlight the reliability of the proposed system versus recent studies for human fall detection.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002480",
    "keywords": [
      "Adaptation (eye)",
      "Artificial intelligence",
      "Choquet integral",
      "Computer science",
      "Data mining",
      "Fuzzy logic",
      "Mathematical analysis",
      "Mathematics",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Reliability (semiconductor)",
      "Scheme (mathematics)"
    ],
    "authors": [
      {
        "surname": "Hadjadji",
        "given_name": "Bilal"
      },
      {
        "surname": "Saumard",
        "given_name": "Matthieu"
      },
      {
        "surname": "Aron",
        "given_name": "Michael"
      }
    ]
  },
  {
    "title": "Optimization and regularization of complex task decomposition for blind removal of multi-factor degradation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2021.103384",
    "abstract": "Most existing image restoration methods based on deep neural networks are developed for images which only degraded by a single degradation mode and imaging under an ideal condition. They cannot be directly used to restore the images degraded by multi-factor coupling. A complex task decomposition regularization optimization strategy (TDROS) is proposed to solve the problem. The restoration of images degraded by multi-factor coupling is a complex task that can be solved by separating these multiple factors, that is, breaking the complex task into numbers of simpler tasks to make the entire complex problem be overcome more easily. Motivated by this idea, the TDROS decomposes the complex task of image restoration into two sub-task: the potential task constrained by regularization and the main task for reconstructing high-definition images. In TDROS, the front of the neural network is focused on the restoration of images degraded by additive noise, while the other part of the network is focused mainly on the restoration of images degraded by blur. We applied the TDROS to an 11-layer convolutional neural network (CNN) and compared it with initial CNNs from the aspects of restoration accuracy and generalization ability. Based on these results, we used TDROS to design a novel network model for the restoration of atmospheric turbulence-degraded images. The experimental results demonstrate that the proposed TDROS can improve the generalization ability of the existing network more effectively than current popular methods, offering a better solution for the problem of severely degraded image restoration. Moreover, the TDROS concept provides a flexible framework for low-level visual complex tasks and can be easily incorporated into existing CNNs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002534",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convolutional neural network",
      "Engineering",
      "Generalization",
      "Image (mathematics)",
      "Image processing",
      "Image restoration",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Regularization (linguistics)",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Gongping"
      },
      {
        "surname": "Gao",
        "given_name": "Zhisheng"
      },
      {
        "surname": "Zhou",
        "given_name": "Bin"
      },
      {
        "surname": "Zuo",
        "given_name": "Chenglin"
      }
    ]
  },
  {
    "title": "Deep network based stereoscopic image quality assessment via binocular summing and differencing",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2021.103420",
    "abstract": "With the development of deep networks in dealing with various visual tasks, the deep network based on binocular vision is expected to tackle the issue of stereoscopic image quality assessment. Here, we present a stereoscopic image quality assessment method using the deep network with four channels together, which takes the left view, right view, binocular summing view, and binocular differencing view as the inputs of the network. The visual features are enhanced through the concatenation in a weighted way, so that the binocular vision can be adequately included in the binocular addition and subtraction information. Compared with the state-of-the-art metrics, the proposed method exhibits relatively high performances on four benchmark databases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002820",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Binocular disparity",
      "Binocular rivalry",
      "Binocular vision",
      "Cartography",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Concatenation (mathematics)",
      "Geography",
      "Image (mathematics)",
      "Image quality",
      "Mathematics",
      "Neuroscience",
      "Perception",
      "Psychology",
      "Stereoscopy",
      "Subtraction",
      "Visual perception"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Jinbin"
      },
      {
        "surname": "Wang",
        "given_name": "Xuejin"
      },
      {
        "surname": "Chai",
        "given_name": "Xiongli"
      },
      {
        "surname": "Shao",
        "given_name": "Feng"
      },
      {
        "surname": "Jiang",
        "given_name": "Qiuping"
      }
    ]
  },
  {
    "title": "SpyGAN sketch: Heterogeneous Face Matching in video for crime investigation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2021.103400",
    "abstract": "Automatic retrieval of faces from videos based on query images effectively helps during the investigation. When the suspect’s image is unavailable, a face sketch, drawn based on eyewitness’s memory recollection, is used to search against photos. Present research works primarily focus on Heterogeneous Face Matching (HFM) sketches to mugshot images in databases. This paper proposes a sketch-face matching in a video that includes profile faces, different illumination, and poses, using a new Generative Adversarial Network called SpyGAN. Faces in the video detected using YOLOv3 are converted into realistic sketches by the proposed SpyGAN focusing on key facial regions. The generated sketches are represented using PCA-SIFT descriptors and are matched based on the cosine distance metric. Experimental results show that the proposed methodology has achieved an accuracy of 88.9% on the Chokepoint dataset and 78% on the OWN Short face-video linked dataset and has demonstrated effectiveness over the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002662",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Cosine similarity",
      "Economics",
      "Face (sociological concept)",
      "Focus (optics)",
      "Image (mathematics)",
      "Law",
      "Matching (statistics)",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Scale-invariant feature transform",
      "Sketch",
      "Social science",
      "Sociology",
      "Statistics",
      "Suspect"
    ],
    "authors": [
      {
        "surname": "B.",
        "given_name": "Yogameena"
      },
      {
        "surname": "Jakkamsetti",
        "given_name": "Geeta"
      },
      {
        "surname": "S.",
        "given_name": "Aishwarya"
      }
    ]
  },
  {
    "title": "Two-stream encoder–decoder network for localizing image forgeries",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2021.103417",
    "abstract": "This paper proposes a novel two-stream encoder–decoder network that utilizes both the high-level and the low-level image features for precisely localizing forged regions in a manipulated image. This is motivated by the fact that the forgery creation process generally introduces both the high-level artefacts (e.g., unnatural contrast) and the low-level artefacts (e.g., noise inconsistency) to the forged images. In the proposed two-stream network, one stream learns the low-level manipulation-related features in the encoder side by extracting noise residuals through a set of high-pass filters in the first layer. In the second stream, the encoder learns the high-level image manipulation features from the input image RGB values. The coarse feature maps each encoder are upsampled by the corresponding decoder network to produce the dense feature maps. The dense feature maps of the two streams are concatenated and fed to a final convolutional layer with sigmoidal activation to produce the pixel-wise prediction. We have carried out experimental analyses on multiple standard forensics datasets to evaluate the performance of the proposed method. The experimental results show the efficacy of the proposed method with respect to the state-of-the-art.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002777",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Encoder",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Linguistics",
      "Noise (video)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Process (computing)",
      "Programming language",
      "RGB color model",
      "Set (abstract data type)",
      "Upsampling"
    ],
    "authors": [
      {
        "surname": "Mazumdar",
        "given_name": "Aniruddha"
      },
      {
        "surname": "Bora",
        "given_name": "Prabin Kumar"
      }
    ]
  },
  {
    "title": "Human skeleton representation for 3D action recognition based on complex network coding and LSTM",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2021.103386",
    "abstract": "3D skeleton sequences contain more effective and discriminative information than RGB video and are more suitable for human action recognition. Accurate extraction of human skeleton information is the key to the high accuracy of action recognition. Considering the correlation between joint points, in this work, we first propose a skeleton feature extraction method based on complex network. The relationship between human skeleton points in each frame is coded as a network. The changes of action over time are described by a time series network composed of skeleton points. Network topology attributes are used as feature vectors, complex network coding and LSTM are combined to recognize human actions. The method was verified on the NTU RGB + D60, MSR Action3D and UTKinect-Action3D dataset, and have achieved good performance, respectively. It shows that the method of extracting skeleton features based on complex network can properlyidentify different actions. This method that considers the temporal information and the relationship between skeletons at the same time plays an important role in the accurate recognition of human actions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002558",
    "keywords": [
      "Artificial intelligence",
      "Coding (social sciences)",
      "Computer science",
      "Computer vision",
      "Discriminative model",
      "Feature (linguistics)",
      "Feature extraction",
      "Human skeleton",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "RGB color model",
      "Skeleton (computer programming)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Shen",
        "given_name": "Xiangpei"
      },
      {
        "surname": "Ding",
        "given_name": "Yanrui"
      }
    ]
  },
  {
    "title": "A new head pose tracking method based on stereo visual SLAM",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2021.103402",
    "abstract": "Real-time and reliable head pose tracking is the basis of human–computer interaction and face analysis applications. Aiming at the problems of accuracy and real time performance in current tracking method, a new head pose tracking method based on stereo visual SLAM is proposed in this paper. The sparse head map is constructed based on ORB feature points extraction and stereo matching, then the 3D-2D matching relations between 3D mappoints and 2D feature points are obtained by projection matching. Finally, the camera pose solved by the Bundle Adjustment is converted to head pose, which realizes the tracking of head pose. The experimental results show that this method can obtain high precise head pose. The mean errors of three Euler angles are all less than 1°. Therefore, the proposed head pose tracking method can track and estimate precise head pose in real time under smooth background.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002698",
    "keywords": [
      "3D pose estimation",
      "Algorithm",
      "Artificial intelligence",
      "Bundle adjustment",
      "Computer science",
      "Computer vision",
      "Euler angles",
      "Feature (linguistics)",
      "Geology",
      "Geometry",
      "Geomorphology",
      "Head (geology)",
      "Image (mathematics)",
      "Linguistics",
      "Matching (statistics)",
      "Mathematics",
      "Mobile robot",
      "Orb (optics)",
      "Pedagogy",
      "Philosophy",
      "Pose",
      "Projection (relational algebra)",
      "Psychology",
      "Robot",
      "Simultaneous localization and mapping",
      "Statistics",
      "Tracking (education)"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Suibin"
      },
      {
        "surname": "Yang",
        "given_name": "Kun"
      },
      {
        "surname": "Xiao",
        "given_name": "Hua"
      },
      {
        "surname": "Han",
        "given_name": "Peng"
      },
      {
        "surname": "Qiu",
        "given_name": "Jian"
      },
      {
        "surname": "Peng",
        "given_name": "Li"
      },
      {
        "surname": "Liu",
        "given_name": "Dongmei"
      },
      {
        "surname": "Luo",
        "given_name": "Kaiqing"
      }
    ]
  },
  {
    "title": "A contrast improved OR and XOR based ( k , n ) visual cryptography scheme without pixel expansion",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2021.103408",
    "abstract": "Visual cryptography scheme (VCS) is a branch of secret sharing, in which a secret image is encrypted into n noise-like shares. In the recovering phase, the secret image can be reconstructed by stacking sufficient shares. It’s obvious that VCS with multiple decryptions can further widen the range of application. However, the investigations on the existing scheme with multiple decryptions are not sufficient. In this paper, we develop a novel contrast improved OR and XOR based ( k , n )-VCS without pixel expansion. Significantly, we give a general simplified calculation formula to compute the theoretical contrast of the proposed scheme. In addition, if there are no computing devices, then we can reconstruct the secret image by stacking the shares directly. Meanwhile, we recover the secret image perfectly by performing XOR operation when computing devices are available. Since the proposed scheme is based on the parity basis matrices, our scheme has no pixel expansion. Finally, Theoretical analysis and experimental results are conducted to evaluate the efficiency and security of the proposed scheme.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002716",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Basis (linear algebra)",
      "Bitwise operation",
      "Computer science",
      "Computer security",
      "Contrast (vision)",
      "Cryptography",
      "Decoding methods",
      "Encryption",
      "Exclusive or",
      "Geometry",
      "Image (mathematics)",
      "Mathematical analysis",
      "Mathematics",
      "Nuclear magnetic resonance",
      "Physics",
      "Pixel",
      "Programming language",
      "Scheme (mathematics)",
      "Secret sharing",
      "Stacking",
      "Theoretical computer science",
      "Visual cryptography"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Yongkang"
      },
      {
        "surname": "Fu",
        "given_name": "Fang-Wei"
      }
    ]
  },
  {
    "title": "A comprehensive framework of multiple semantics preservation in neural style transfer",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2021.103378",
    "abstract": "Recently neural style transfer has achieved great development, but there is still a big gap compared with manual creation. Most of the existing methods ignore the comprehensive consideration of preserving various semantic information of original content images, resulting in distortion or loss of original content features of the generated works, which are dull and difficult to convey the original themes and emotions. In this paper, we analyze the ability of the existing methods to maintain single semantic information and propose a fast style transfer framework with multi-semantic preservation. The experiments indicate that our method can effectively retain the original semantic information including salience and depth features, so that the final artwork has better visual effect by highlighting its regional focus and depth information. Compared with existing methods, our method has better ability in semantic preservation and can generate more artworks with distinct regions, controllable semantics, diverse contents and rich emotions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002509",
    "keywords": [
      "Amplifier",
      "Archaeology",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Computer network",
      "Computer science",
      "Distortion (music)",
      "Focus (optics)",
      "History",
      "Natural language processing",
      "Optics",
      "Physics",
      "Programming language",
      "Salience (neuroscience)",
      "Semantics (computer science)",
      "Style (visual arts)",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Ye",
        "given_name": "Wujian"
      },
      {
        "surname": "Zhu",
        "given_name": "Xueke"
      },
      {
        "surname": "Xu",
        "given_name": "Zuoteng"
      },
      {
        "surname": "Liu",
        "given_name": "Yijun"
      },
      {
        "surname": "Chang",
        "given_name": "Chin-Chen"
      }
    ]
  },
  {
    "title": "QoE optimization for HTTP adaptive streaming: Performance evaluation of MEC-assisted and client-based methods",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2021.103415",
    "abstract": "Seamless streaming of high quality video under unstable network condition is a big challenge. HTTP adaptive streaming (HAS) provides a solution that adapts the video quality according to the network conditions. Traditionally, HAS algorithm runs at the client side while the clients are unaware of bottlenecks in the radio channel and competing clients. The traditional adaptation strategies do not explicitly coordinate between the clients, servers, and cellular networks. The lack of coordination has been shown to lead to suboptimal user experience. As a response, multi-access edge computing (MEC)-assisted adaptation techniques emerged to take advantage of computing and content storage capabilities in mobile networks. In this study, we investigate the performance of both MEC-assisted and client-side adaptation methods in a multi-client cellular environment. Evaluation and comparison are performed in terms of not only the video rate and dynamics of the playback buffer but also the fairness and bandwidth utilization. We conduct extensive experiments to evaluate the algorithms under varying client, server, dataset, and network settings. Results demonstrate that the MEC-assisted algorithms improve fairness and bandwidth utilization compared to the client-based algorithms for most settings. They also reveal that the buffer-based algorithms achieve significant quality of experience; however, these algorithms perform poorly compared with throughput-based algorithms in protecting the playback buffer under rapidly varying bandwidth fluctuations. In addition, we observe that the preparation of the representation sets affects the performance of the algorithms, as does the playback buffer size and segment duration. Finally, we provide suggestions based on the behaviors of the algorithms in a multi-client environment.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002790",
    "keywords": [
      "Adaptation (eye)",
      "Algorithm",
      "Bandwidth (computing)",
      "Client-side",
      "Computer network",
      "Computer science",
      "Distributed computing",
      "Operating system",
      "Optics",
      "Physics",
      "Quality of experience",
      "Quality of service",
      "Real-time computing",
      "Server",
      "Throughput",
      "Wireless"
    ],
    "authors": [
      {
        "surname": "ur Rahman",
        "given_name": "Waqas"
      },
      {
        "surname": "Amin",
        "given_name": "Muhammad Bilal"
      },
      {
        "surname": "Hossain",
        "given_name": "Md Delowar"
      },
      {
        "surname": "Seon Hong",
        "given_name": "Choong"
      },
      {
        "surname": "Huh",
        "given_name": "Eui-Nam"
      }
    ]
  },
  {
    "title": "Deep low-rank feature learning and encoding for cross-age face recognition",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2021.103423",
    "abstract": "Cross-age face recognition (CAFR) is a challenging task, due to significant intra-personal variations. Furthermore, the training and testing data may contain random noise components. To address these issues, this paper proposes a deep low-rank feature learning and encoding method. Firstly, our method employs manifold learning in the low-rank optimization, which preserves the global and local structure of the data samples, while learning the clean low-rank features. Secondly, we encode the low-rank features using our locality-constrained feature encoding method, which learns an age-insensitive codebook from training data, and enables the intra-class samples to share the same local bases in a codebook. In the testing stage, the gallery and probe features are encoded by the learned codebook, which represents the images of the same identity by similar codewords for recognition. Furthermore, the periocular region of human faces is investigated for CAFR. Extensive experiments on five datasets demonstrate the effectiveness of our method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002856",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Codebook",
      "Combinatorics",
      "Computer science",
      "Deep learning",
      "ENCODE",
      "Encoding (memory)",
      "Face (sociological concept)",
      "Facial recognition system",
      "Feature (linguistics)",
      "Feature learning",
      "Gene",
      "Law",
      "Linguistics",
      "Locality",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Rank (graph theory)",
      "Representation (politics)",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Saad Shakeel",
        "given_name": "M."
      },
      {
        "surname": "Lam",
        "given_name": "Kin-Man"
      }
    ]
  },
  {
    "title": "Single image shadow detection via uncertainty analysis and GCN-based refinement strategy",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2021.103397",
    "abstract": "Learning-based shadow detection methods have achieved an impressive performance, while these works still struggle on complex scenes, especially ambiguous soft shadows. To tackle this issue, this work proposes an efficient shadow detection network (ESDNet) and then applies uncertainty analysis and graph convolutional networks for detection refinement. Specifically, we first aggregate global information from high-level features and harvest shadow details in low-level features for obtaining an initial prediction. Secondly, we analyze the uncertainty of our ESDNet for an input shadow image and then take its intensity, expectation, and entropy into account to formulate a semi-supervised graph learning problem. Finally, we solve this problem by training a graph convolution network to obtain the refined detection result for every training image. To evaluate our method, we conduct extensive experiments on several benchmark datasets, i.e., SBU, UCF, ISTD, and even on soft shadow scenes. Experimental results demonstrate that our strategy can improve shadow detection performance by suppressing the uncertainties of false positive and false negative regions, achieving state-of-the-art results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002650",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Entropy (arrow of time)",
      "Geodesy",
      "Geography",
      "Graph",
      "Image (mathematics)",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Physics",
      "Psychology",
      "Psychotherapist",
      "Quantum mechanics",
      "Shadow (psychology)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Wen"
      },
      {
        "surname": "Zhou",
        "given_name": "Kai"
      },
      {
        "surname": "Chen",
        "given_name": "Xiao-Diao"
      }
    ]
  },
  {
    "title": "Complex Network-based features extraction in RGB-D human action recognition",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2021.103371",
    "abstract": "Analysis of human behavior through visual information has been one of the active research areas in computer vision community during the last decade. Vision-based human action recognition (HAR) is a crucial part of human behavior analysis, which is also of great demand in a wide range of applications. HAR was initially performed via images from a conventional camera; however, depth sensors have recently embedded as an additional informative resource to cameras. In this paper, we have proposed a novel approach to largely improve the performance of human action recognition using Complex Network-based feature extraction from RGB-D information. Accordingly, the constructed complex network is employed for single-person action recognition from skeletal data consisting of 3D positions of body joints. The indirect features help the model cope with the majority of challenges in action recognition. In this paper, the meta-path concept in the complex network has been presented to lessen the unusual actions structure challenges. Further, it boosts recognition performance. The extensive experimental results on two widely adopted benchmark datasets, the MSR-Action Pairs, and MSR Daily Activity3D indicate the efficiency and validity of the method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002455",
    "keywords": [
      "Action (physics)",
      "Action recognition",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Class (philosophy)",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Feature extraction",
      "Geodesy",
      "Geography",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "RGB color model"
    ],
    "authors": [
      {
        "surname": "Barkoky",
        "given_name": "Alaa"
      },
      {
        "surname": "Charkari",
        "given_name": "Nasrollah Moghaddam"
      }
    ]
  },
  {
    "title": "Fall detection using body geometry and human pose estimation in video sequences",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2021.103407",
    "abstract": "According to the World Health Organization, falling is a significant health problem that causes thousands of deaths every year. Fall detection and fall prediction tasks enable accurate medical assistance to vulnerable populations whenever required, allowing local authorities to predict daily health care resources and to reduce fall damages accordingly. We present in this paper, a fall detection approach that explores human body geometry available at different frames of the video sequence. Especially, pose estimation, the angle and the distance between the vector formed by the head-centroid of the identified facial image and the center hip of the body, and the vector aligned with the horizontal axis of the center hip, are employed to construct new distinctive image features. A two-class Support Vector Machine (SVM) classifier and a Temporal Convolution Network (TCN) are trained on the newly constructed feature images. At the same time, a Long-Short-Term Memory (LSTM) network is trained on the calculated angle and distance sequences to classify fall and non-fall activities. We perform experiments on the Le2i FD dataset and the UR FD dataset, where we also propose a cross-dataset evaluation. The results demonstrate the effectiveness and efficiency of the developed approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002728",
    "keywords": [
      "Artificial intelligence",
      "Centroid",
      "Classifier (UML)",
      "Computer science",
      "Computer vision",
      "Pattern recognition (psychology)",
      "Pose",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Beddiar",
        "given_name": "Djamila Romaissa"
      },
      {
        "surname": "Oussalah",
        "given_name": "Mourad"
      },
      {
        "surname": "Nini",
        "given_name": "Brahim"
      }
    ]
  },
  {
    "title": "Visual secret sharing scheme with (n,n) threshold based on WeChat Mini Program codes",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2021.103409",
    "abstract": "With the widespread use of WeChat Mini Programs, their security has attracted more and more attention. The WeChat Mini Program can be accessed by scanning a WeChat Mini Program code. We should protect the code thus to protect the Mini Program. In this paper, based on studying the function in each pattern of WeChat Mini Program codes, a visual secret sharing (VSS) scheme for WeChat Mini Program codes (MPCVSS) with ( n , n ) ( n ≥ 4 ) threshold is proposed to control and identify the users of Mini Program. MPCVSS combines the error-correcting characteristic of WeChat Mini Program codes with the theory of VSS. A secret WeChat Mini Program code is shared into n shared codes slightly modified from n cover codes. Each shared code is a valid code that can be scanned and decoded correctly. The secret code can be recovered by XORing n shares. The recovered code can be decoded as the same secret Mini Program. Theoretical analysis and experiments show that the proposed VSS scheme is practical and feasible.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032100273X",
    "keywords": [
      "Algorithm",
      "Code (set theory)",
      "Computer science",
      "Cover (algebra)",
      "Cryptography",
      "Engineering",
      "Mathematical analysis",
      "Mathematics",
      "Mechanical engineering",
      "Programming language",
      "Scheme (mathematics)",
      "Secret sharing",
      "Set (abstract data type)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Jia"
      },
      {
        "surname": "Wang",
        "given_name": "Yongjie"
      },
      {
        "surname": "Yan",
        "given_name": "Xuehu"
      },
      {
        "surname": "Wang",
        "given_name": "Jiayu"
      },
      {
        "surname": "Li",
        "given_name": "Longlong"
      }
    ]
  },
  {
    "title": "3DFP-FCGAN: Face completion generative adversarial network with 3D facial prior",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2022",
    "doi": "10.1016/j.jvcir.2021.103380",
    "abstract": "Face completion is a domain-specific image inpainting problem. Most existing face completion methods fail to synthesize fine-grained facial structures due to the undifferentiated treatment of face images and other scene images. To handle this problem, we propose an end-to-end deep generative model based approach which makes full use of the facial prior knowledge, including 2D facial geometry priors from facial parsing maps and landmarks, as well as the 3D depth prior. We adopt a coarse-to-fine inpainting framework where the 2D facial geometry priors based on coarse faces are extracted to guide the refinement network for better planar facial textures and structures. Moreover, a novel 3D regularized reconstruction loss is proposed for the enhancement of the stereo perception of generated faces. Experimental results on two large-scale benchmarks CelebA and CelebA-HQ show that our method significantly outperforms the state-of-the-arts in generating more visually realistic and pleasing faces. Code is available at .",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002510",
    "keywords": [
      "Artificial intelligence",
      "Bayesian probability",
      "Computer science",
      "Computer vision",
      "Domain (mathematical analysis)",
      "Face (sociological concept)",
      "Face detection",
      "Face hallucination",
      "Facial recognition system",
      "Generative adversarial network",
      "Generative grammar",
      "Image (mathematics)",
      "Inpainting",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Prior probability",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Jing"
      },
      {
        "surname": "Wang",
        "given_name": "Weikang"
      },
      {
        "surname": "Yu",
        "given_name": "Jiexiao"
      },
      {
        "surname": "Zhang",
        "given_name": "Chunping"
      },
      {
        "surname": "Su",
        "given_name": "Yuting"
      }
    ]
  }
]