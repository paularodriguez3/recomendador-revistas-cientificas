[
  {
    "title": "Reversible adversarial steganography for security enhancement",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103935",
    "abstract": "In adaptive steganography, existing universal stego post-processing methods can enhance security, but suffer from incorrect extraction of messages and undesired visual defects, especially in flat regions. To address this issue, a reversible adversarial steganography method is proposed by modifying the LSB or 2ndLSB of stego-images, which has promising visual quality and security. To that end, the content-adaptive adversarial perturbations are first generated, which consider image texture with noise residual features to counter deep learning-based steganalyzers. Then, a data compression strategy of adversarial perturbations is designed by applying lossless run-length encoding based on the sparse nature of non-zero elements in the perturbations to reduce the perturbation’s quantity. Finally, reversible data hiding based on ternary coding is applied to embed and extract stego images with compressed adversarial perturbations. Extensive experimental results demonstrate that the proposed method can effectively enhance security and visual quality compared with state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001852",
    "keywords": [
      "Adversarial system",
      "Algorithm",
      "Artificial intelligence",
      "Coding (social sciences)",
      "Computer science",
      "Computer vision",
      "Data compression",
      "Image (mathematics)",
      "Information hiding",
      "Lossless compression",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Residual",
      "Statistics",
      "Steganography",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Luo",
        "given_name": "Jie"
      },
      {
        "surname": "He",
        "given_name": "Peisong"
      },
      {
        "surname": "Liu",
        "given_name": "Jiayong"
      },
      {
        "surname": "Wang",
        "given_name": "Hongxia"
      },
      {
        "surname": "Wu",
        "given_name": "Chunwang"
      },
      {
        "surname": "Zhou",
        "given_name": "Shenglie"
      }
    ]
  },
  {
    "title": "AMCFNet: Asymmetric multiscale and crossmodal fusion network for RGB-D semantic segmentation in indoor service robots",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103951",
    "abstract": "Red-green-blue and depth (RGB-D) semantic segmentation is essential for indoor service robots to achieve accurate artificial intelligence. Various RGB-D indoor semantic segmentation methods have been proposed since the widespread adoption of depth maps. These methods have focused mainly on integrating the multiscale and crossmodal features extracted from RGB images and depth maps in the encoder and used unified strategies to recover the local details at the decoder progressively. However, these methods emphasized crossmodal fusion at the encoder, neglecting the distinguishability between RGB and depth features during decoding, thereby undermining the segmentation performance. To adequately exploit the features, we propose an efficient encoder-decoder architecture called asymmetric multiscale and crossmodal fusion network (AMCFNet) endowed with a differential feature integration strategy. Unlike existing methods, we use simple crossmodal fusion at the encoder and design an elaborate decoder to improve the semantic segmentation performance. Specifically, considering high- and low-level features, we propose a semantic aggregation module (SAM) to process the multiscale and crossmodal features in the last three network layers to aggregate high-level semantic information through a cascaded pyramid structure. Moreover, we design a spatial detail supplement module using low-level spatial details from depth maps to adaptively fuse these details and the information obtained from the SAM. Extensive experiments are conducted to demonstrate that the proposed AMCFNet outperforms state-of-the-art approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002018",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Crossmodal",
      "Encoder",
      "Feature (linguistics)",
      "Linguistics",
      "Neuroscience",
      "Operating system",
      "Pattern recognition (psychology)",
      "Perception",
      "Philosophy",
      "RGB color model",
      "Robot",
      "Segmentation",
      "Visual perception"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Wujie"
      },
      {
        "surname": "Yue",
        "given_name": "Yuchun"
      },
      {
        "surname": "Fang",
        "given_name": "Meixin"
      },
      {
        "surname": "Mao",
        "given_name": "Shanshan"
      },
      {
        "surname": "Yang",
        "given_name": "Rongwang"
      },
      {
        "surname": "Yu",
        "given_name": "Lu"
      }
    ]
  },
  {
    "title": "Towards object tracking for quadruped robots",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103958",
    "abstract": "With the development of quadruped robot technology, object tracking for quadruped robots has become an important research topic where violent camera shaking caused by the robot’s movement makes this task very challenging. In this letter, a quadruped robot object tracking dataset (QROD-111), including 111 video sequences, is first established, which was collected through our quadruped robot platform. A tracking algorithm based on Siamese network is then proposed where an alignment module is introduced to alleviate tracking difficulties caused by the quadruped robot movement. Moreover, a scale adaptation subnetwork is designed to alleviate the impact of the object scale variation during the whole tracking process. Experimental results demonstrate that our algorithm can achieve advanced performance for quadruped robot object tracking.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002080",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Engineering",
      "Object (grammar)",
      "Operating system",
      "Pedagogy",
      "Process (computing)",
      "Psychology",
      "Robot",
      "Subnetwork",
      "Systems engineering",
      "Task (project management)",
      "Tracking (education)",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Yang"
      },
      {
        "surname": "Zhang",
        "given_name": "Kao"
      },
      {
        "surname": "Chen",
        "given_name": "Zhao"
      },
      {
        "surname": "Ouyang",
        "given_name": "Wanping"
      },
      {
        "surname": "Cui",
        "given_name": "Mingpeng"
      },
      {
        "surname": "Jiang",
        "given_name": "Chenxi"
      },
      {
        "surname": "Yang",
        "given_name": "Daiqin"
      },
      {
        "surname": "Chen",
        "given_name": "Zhenzhong"
      }
    ]
  },
  {
    "title": "Alibaba and forty thieves algorithm and novel Prioritized Prewitt Pattern(PPP)-based convolutional neural network (CNN) using hyperspherically compressed weights for facial emotion recognition",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103948",
    "abstract": "The visual representations created using the self-distillation paradigm of Bootstrap Your Emotion Latent (BYEL) are empirically found to be less evenly distributed than those created using proposed technique. This proposed work promotes the compression of weights on a hypersphere by minimizing the hyperspherical energy of network weights using a novel method of optimizing manifolds through Riemannian metrics and the Conjugate gradient technique. The proposed work demonstrates how regularising the networks of the BYEL architecture reduces the hyperspherical energy of neurons by directly optimising a measure of uniformity alongside the standard loss. This leads to more uniformly distributed representation and better performance for downstream tasks. The Alibaba and Forty Thieves Algorithm-based Optimization (AFTAO) methodology is used to select the most precise collection of hyperparameters for a novel Prioritized Prewitt Pattern (PPP)-based Convolutional Neural Network (CNN) that results in a higher accuracy for all the six datasets used for facial emotion recognition.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001980",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Boltzmann machine",
      "Computer science",
      "Convolutional neural network",
      "Hyperparameter",
      "Hypersphere",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Sherly Alphonse",
        "given_name": "A."
      },
      {
        "surname": "Abinaya",
        "given_name": "S."
      },
      {
        "surname": "Abirami",
        "given_name": "S."
      }
    ]
  },
  {
    "title": "DDFusion: An efficient multi-exposure fusion network with dense pyramidal convolution and de-correlation fusion",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103947",
    "abstract": "In this work, we propose DDFusion, a novel multi-exposure image fusion network. DDFusion addresses the limitations of existing methods by effectively recovering details near extremely bright regions and learning associations between non-contiguous regions. To achieve this, our network incorporates a dense pyramidal (DensePy) convolution block in the encoder for multi-scale feature extraction, and a de-correlation fusion (DF) block for enabling structurally coherent and edge-preserving multi-scale feature fusion. It facilitates a smoother transition from highlighted areas to adjacent regions in the fused image. Experimental results demonstrate the superiority of DDFusion over state-of-the-art deep methods in terms of both visual quality and quantitative evaluation. Moreover, DDFusion achieves stronger multi-scale feature extraction capability with smaller computational complexity.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001979",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Block (permutation group theory)",
      "Computer science",
      "Convolution (computer science)",
      "Correlation",
      "Encoder",
      "Enhanced Data Rates for GSM Evolution",
      "Feature (linguistics)",
      "Feature extraction",
      "Fusion",
      "Geometry",
      "Image (mathematics)",
      "Image fusion",
      "Linguistics",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "Peng",
        "given_name": "Pai"
      },
      {
        "surname": "Liu",
        "given_name": "Yang"
      },
      {
        "surname": "Jing",
        "given_name": "Zhongliang"
      },
      {
        "surname": "Pan",
        "given_name": "Han"
      },
      {
        "surname": "Zhang",
        "given_name": "Hao"
      }
    ]
  },
  {
    "title": "Pose2Trajectory: Using transformers on body pose to predict tennis player’s trajectory",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103954",
    "abstract": "Tracking the trajectory of tennis players can help camera operators in production. Predicting future movement enables cameras to automatically track and predict a player’s future trajectory without human intervention. It is also intellectually satisfying to predict future human movement in the context of complex physical tasks. Swift advancements in sports analytics and the wide availability of videos for tennis have inspired us to propose a novel method called Pose2Trajectory, which predicts a tennis player’s future trajectory as a sequence derived from their body joints’ data and ball position. Demonstrating impressive accuracy, our approach capitalizes on body joint information to provide a comprehensive understanding of the human body’s geometry and motion, thereby enhancing the prediction of the player’s trajectory. We use encoder–decoder Transformer architecture trained on the joints and trajectory information of the players with ball positions. The predicted sequence can provide information to help close-up cameras to keep tracking the tennis player, following centroid coordinates. We generate a high-quality dataset from multiple videos to assist tennis player movement prediction using object detection and human pose estimation methods. It contains bounding boxes and joint information for tennis players and ball positions in singles tennis games. Our method shows promising results in predicting the tennis player’s movement trajectory with different sequence prediction lengths using the joints and trajectory information with the ball position.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002043",
    "keywords": [
      "Artificial intelligence",
      "Astronomy",
      "Body shape",
      "Computer science",
      "Computer vision",
      "Electrical engineering",
      "Engineering",
      "Physics",
      "Trajectory",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "AlShami",
        "given_name": "Ali"
      },
      {
        "surname": "Boult",
        "given_name": "Terrance"
      },
      {
        "surname": "Kalita",
        "given_name": "Jugal"
      }
    ]
  },
  {
    "title": "A hierarchical multi-modal cross-attention model for face anti-spoofing",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103969",
    "abstract": "Facial recognition has become popular in interactive systems as a means to authenticate identity. However, Facial recognition can be easily attacked illegally through face spoofing. In this paper, we propose a hierarchical multi-modal cross-attention model for face anti-spoofing, which can be flexibly applied in both single-modal and multi-modal scenarios. In order to map features among modalities thoroughly, we also design a novel attention mechanism, namely W-MSA-CA (Window-based Multihead Self-Attention and Cross Attention), which leverages both Multi-modal Multihead Self-Attention (MMSA) and Multi-modal Patch Cross attention (MPCA) to fuse multi-modal features. We test the proposed model on the public datasets and the results show that our model’s capability to detect various types of spoofing is effective.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002195",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Electrical engineering",
      "Engineering",
      "Face (sociological concept)",
      "Facial recognition system",
      "Fuse (electrical)",
      "Modal",
      "Modalities",
      "Pattern recognition (psychology)",
      "Polymer chemistry",
      "Social science",
      "Sociology",
      "Speech recognition",
      "Spoofing attack"
    ],
    "authors": [
      {
        "surname": "Xue",
        "given_name": "Hao"
      },
      {
        "surname": "Ma",
        "given_name": "Jing"
      },
      {
        "surname": "Guo",
        "given_name": "Xiaoyu"
      }
    ]
  },
  {
    "title": "ClothSeg: semantic segmentation network with feature projection for clothing parsing",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103980",
    "abstract": "Semantic segmentation of clothing presents a formidable challenge owing to the non-rigid geometric deformation properties inherent in garments. In this paper, we use the Transformer as the encoder to better learn global information for clothing semantic segmentation. In addition, we propose a Feature Projection Fusion (FPF) module to better utilize local information. This module facilitates the integration of deep feature maps with shallow local details, thereby enabling the network to capture both high-level abstractions and fine-grained details of features. We also design a pixel distance loss in training to emphasize the impact of edge features. This loss calculates the mean of the shortest distances between all predicted clothing edges and the true clothing edges during the training process. We perform extensive experiments and our method achieves 56.30% and 74.97% mIoU on the public dataset CFPD and our self-made dataset LIC, respectively, demonstrating a competitive performance when compared to the state-of-the-art.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002304",
    "keywords": [
      "Algorithm",
      "Archaeology",
      "Artificial intelligence",
      "Clothing",
      "Computer science",
      "Computer vision",
      "Encoder",
      "Enhanced Data Rates for GSM Evolution",
      "Feature (linguistics)",
      "History",
      "Linguistics",
      "Operating system",
      "Parsing",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Process (computing)",
      "Projection (relational algebra)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Tang",
        "given_name": "Guangyu"
      },
      {
        "surname": "Yu",
        "given_name": "Feng"
      },
      {
        "surname": "Li",
        "given_name": "Huiyin"
      },
      {
        "surname": "Shi",
        "given_name": "Yankang"
      },
      {
        "surname": "Liu",
        "given_name": "Li"
      },
      {
        "surname": "Peng",
        "given_name": "Tao"
      },
      {
        "surname": "Hu",
        "given_name": "Xinrong"
      },
      {
        "surname": "Jiang",
        "given_name": "Minghua"
      }
    ]
  },
  {
    "title": "CPA-YOLOv7: Contextual and pyramid attention-based improvement of YOLOv7 for drones scene target detection",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103965",
    "abstract": "Target detection in unmanned aerial vehicle application scenarios has other problems, such as dense targets. The existing unmanned aerial vehicle target detection model with high computational complexity makes it difficult to meet real-time unmanned aerial vehicle target detection, and the detection accuracy of small targets is low. To address these problems, we propose an improved YOLOv7 small target detection model based on context and pyramidal attention that can cope with dense unmanned aerial vehicle scenarios - CPA-YOLOv7. This model embeds our proposed lightweight multi-scale attentional feature spatial pyramid pooling module, which can better distinguish between small and large target features, reducing the computational effort while improving the detection accuracy of the model. Secondly, we design a contextual dynamic fusion attention module in the network to fuse global and local contextual information and dynamically assign features to multiple groups of channels; in the multi-scale fusion process, it effectively increases the characterization ability of small target features and enables the network to better focus on small target information. Finally, we improve Wise-Intersection-over-Union loss as the regression loss function, add a moderation factor to retain some of the high and low-quality sample weights to improve the regression accuracy of high-quality anchor frames, and use the dynamic non-monotonic focusing mechanism to increase the model's focus on ordinary quality anchor frames to improve the model's localization performance and robustness to low-quality samples. Numerous experimental results show that on the unmanned aerial vehicle datasets VisDrone2021-DET and AI-TOD, the mAP values of our model are 2.3% and 1.1% higher than those of the YOLOv7 model with fewer parameters introduced, and the computational speed reaches 146 frames per second (FPS), which can meet the real-time requirements of unmanned aerial vehicle detection.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002158",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Feature extraction",
      "Focus (optics)",
      "Fusion",
      "Fusion mechanism",
      "Gene",
      "Linguistics",
      "Lipid bilayer fusion",
      "Object detection",
      "Optics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Pooling",
      "Pyramid (geometry)",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Shi",
        "given_name": "Houwang"
      },
      {
        "surname": "Yang",
        "given_name": "Wenzhong"
      },
      {
        "surname": "Chen",
        "given_name": "Danni"
      },
      {
        "surname": "Wang",
        "given_name": "Min"
      }
    ]
  },
  {
    "title": "A bidirectional fusion branch network with penalty term-based trihard loss for person re-identification",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103972",
    "abstract": "Person re-identification (Re-ID) is the recognition of the same person in different camera views. Because of the existence of highly similar persons and great differences of the same person in different scenes, and the fact that the features extracted by current mainstream models lose some fine-grained information, it is likely for the models to misidentify the query person. To tackle these challenges, we introduce a bidirectional fusion branch network with penalty term-based trihard loss (BFB-PTT). The BFB-PTT constructs a bidirectional fusion branch (BFB) network based on feature pyramid, where low-level features are transferred to a high-level feature space through fewer convolutional layers than most of the traditional CNN-based models have, thus retaining more local features to discriminate different pedestrians more accurately and effectively. Meanwhile, we propose using the penalty term-based trihard loss (PTT) to optimize the spatial structure of pedestrian’s samples, so that the similar samples are drawn closer together in order to reduce the variabilities of the same person in different scenes. We have conducted comprehensive experiments and analyses on the proposed method’s effectiveness on three challenging benchmarks, and the results show that our approach achieves competitive performance with the state-of-art models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002225",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Fusion",
      "Geometry",
      "Identification (biology)",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Pyramid (geometry)",
      "Quantum mechanics",
      "Term (time)"
    ],
    "authors": [
      {
        "surname": "Deng",
        "given_name": "Zelin"
      },
      {
        "surname": "Liu",
        "given_name": "Shaobao"
      },
      {
        "surname": "He",
        "given_name": "Pei"
      },
      {
        "surname": "Song",
        "given_name": "Yun"
      },
      {
        "surname": "Tang",
        "given_name": "Qiang"
      },
      {
        "surname": "Li",
        "given_name": "WenBo"
      }
    ]
  },
  {
    "title": "DenSplitnet: Classifier-invariant neural network method to detect COVID-19 in chest CT data",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103949",
    "abstract": "Objective: COVID-19 has made an unprecedented impact on humanity. The Healthcare sector, in an effort to curb COVID-19, could leverage Artificial Intelligence (AI) to its aid, especially in diagnosing it through the classification of Chest-CT scans. However, data scarcity plagues the medical domain. Therefore, any AI solution must be capable of learning from limited data. Also the resulting AI could relieve radiologists from exhaustion and aid medical practitioners as a valuable diagnostic tool. Methods: Our proposed model DenSplitnet uses Dense blocks to learn image features, Self-Supervised Learning for pre-training to learn the context, and a novel two-way split branch at the final classification layer for classifier-invariant generalization ability. Results: DenSplitnet achieves state-of-the-art performance on four benchmark chest-CT scan datasets for COVID-19. The model performs well on the Sars-cov-2 ct-scan dataset. The Test accuracy is 91.92%, F1-Score is 91.30%, Precision is 96.55%, and Recall is 87.04%. The model achieves test accuracy of 86.17%, Precision of 82.94%, Recall of 83.72%, and F1-Score of 83.23% on the Sars-cov-2 ct-scan multiclass dataset. The model obtains a test accuracy of 86.21%, an F1-Score of 83.91%, and an AUC of 0.95 in the Covid-ct-dataset. The model obtains a test accuracy that is 73.11% and an F1-Score of 70.97% in the TransferLearn-ct dataset. Conclusion: The findings support the practical usability of the DenSplitnet as a technological AI assistance to radiologists and other medical professionals, alongside Grad-CAM plots for explainable AI and the theoretical examination of the classifier-invariant generalization capacity of the network. The healthcare sector can benefit from this technology in a number of ways.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001992",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Classifier (UML)",
      "Computer science",
      "Coronavirus disease 2019 (COVID-19)",
      "Deep learning",
      "Disease",
      "F1 score",
      "Infectious disease (medical specialty)",
      "Leverage (statistics)",
      "Machine learning",
      "Medicine",
      "Pathology",
      "Pattern recognition (psychology)",
      "Precision and recall",
      "Programming language",
      "Test data"
    ],
    "authors": [
      {
        "surname": "Perumal",
        "given_name": "Murukessan"
      },
      {
        "surname": "Srinivas",
        "given_name": "M"
      }
    ]
  },
  {
    "title": "Semantic segmentation with cross convolution and multi-layer feature refinement",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103971",
    "abstract": "Multi-level features and contextual information have been proven effective in semantic segmentation. As a common solution, dilated convolution with multiple dilation rates is widely adopted by various computer vision tasks. However, since a large number of pixels are not involved in convolutional calculation, dilated convolution suffers from the information loss problem. Another important aspect for image segmentation is that most feature fusion methods combine different level features directly, which ignores the semantic gap between shallow layer features and deep layer features. In this work, we propose the multi-scale cross convolution to alleviate the information loss problem. Cross convolution conducts convolutional operations in horizontal and vertical direction with different kernels. By combining cross convolution with dilated convolutions using different convolution kernels, more pixels are engaged in convolutional operation to capture multi-scale features. To address the issue of semantic gap between multi-layer features, a feature fusion scheme is developed in which a dual attention mechanism is applied to conduct feature refinement in both spatial and channel dimensions. Comprehensive experiments are conducted to evaluate the proposed method on Cityscapes and ADE20K datasets. Experimental results demonstrate that the cross convolution and feature fusion method improve segmentation performance significantly and achieve competitive performance over state-of-the-art approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002213",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Ma",
        "given_name": "Yingdong"
      },
      {
        "surname": "Jing",
        "given_name": "Nan"
      }
    ]
  },
  {
    "title": "Random hand gesture authentication via efficient Temporal Segment Set Network",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103985",
    "abstract": "Biometric authentication technologies are rapidly gaining popularity, and hand gestures are emerging as a promising biometric trait due to their rich physiological and behavioral characteristics. Hand gesture authentication can be categorized as defined hand gesture authentication and random hand gesture authentication. Unlike defined hand gesture authentication, random hand gesture authentication is not constrained to specific hand gesture types, allowing users to perform hand gestures randomly during enrollment and verification, thus more flexible and friendly. However, in random hand gesture authentication, the model needs to extract more generalized physiological and behavioral features from different viewpoints and positions without gesture templates, which is more challenging. In this paper, we present a novel efficient Temporal-Segment-Set-Network (TS2N) that directly extracts both behavioral and physiological features from a single RGB video to further enhance the performance of random hand gesture authentication. Our method adopts a new motion pseudo-modality and leverages a set-based representation to capture behavioral characteristics online. Additionally, we propose a channel-spatial attention mechanism, Contextual Squeeze-and-Excitation Network (CoSEN), to better abstract and understand physiological characteristics by explicitly modeling the channel-spatial interdependence, thereby adaptively recalibrating channel-specific and spatial-specific responses. Extensive experiments on the largest public hand gesture authentication dataset SCUT-DHGA demonstrate TS2N’s superiority against 21 state-of-the-art models in terms of EER (5.707% for full version and 6.664% for lite version) and computational cost (98.9022G for full version and 46.3741G for lite version). The code is available at https://github.com/SCUT-BIP-Lab/TS2N.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002353",
    "keywords": [
      "Artificial intelligence",
      "Authentication (law)",
      "Biometrics",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Gesture",
      "Gesture recognition",
      "Human–computer interaction",
      "Programming language",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Lin",
        "given_name": "Yihong"
      },
      {
        "surname": "Song",
        "given_name": "Wenwei"
      },
      {
        "surname": "Kang",
        "given_name": "Wenxiong"
      }
    ]
  },
  {
    "title": "Toward high imperceptibility deep JPEG steganography based on sparse adversarial attack",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103977",
    "abstract": "JPEG steganography is an important branch of information hiding. However, with the development of deep learning-based steganalysis models, steganography is facing great challenges. To better resist these steganalysis models, this paper proposes a deep JPEG steganography framework based on sparse adversarial attacks. According to the vulnerability of deep steganalysis models to adversarial attacks, sparse adversarial attacks are introduced into the deep JPEG steganographic structure to improve the security of stego images. In addition, to resist steganalysis from JPEG and spatial domains, both JPEG and spatial domain steganalysis models are involved in adversarial training. Finally, to further enhance the imperceptibility of adversarial stego images, the visual perception loss is designed from the perspective of human eyes. Experimental results indicate that compared with the existing methods, the proposed method has higher imperceptibility and security and can resist modern deep steganalysis models in both JPEG and spatial domains to a certain extent. The source code is available at https://github.com/imagecbj/SAE-JS.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002274",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Deep learning",
      "Image (mathematics)",
      "Image compression",
      "Image processing",
      "JPEG",
      "Lossless JPEG",
      "Pattern recognition (psychology)",
      "Steganalysis",
      "Steganography",
      "Steganography tools"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Beijing"
      },
      {
        "surname": "Nie",
        "given_name": "Yuxin"
      },
      {
        "surname": "Yang",
        "given_name": "Jianhua"
      }
    ]
  },
  {
    "title": "Vehicle re-identification based on grouping aggregation attention and cross-part interaction",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103937",
    "abstract": "Vehicle re-identification (Re-ID) plays an important role in intelligent transportation systems. To solve the key problem of small inter-class difference, we propose a vehicle Re-ID network based on grouping aggregation attention and cross-part interaction (GACP), which uses a channel context local interaction attention block (CCLI) and a cross-part interaction module (CPIM) to extract global and local discriminative features, respectively. The CCLI constructs the context information of each channel and performs local interaction to realize the communication of local and long-range information, thus effectively infer the weights of channels. In addition, the CCLI further reduces the number of parameters and improves the attention effect through a grouping aggregation module and an attention enhancement constraint. The CPIM enhances the correlation between vehicle parts obtained by rigid segmentation on feature maps to mine robust local information. Extensive experiments on two popular datasets VeRi776 and VehicleID demonstrate the effectiveness of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001876",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Block (permutation group theory)",
      "Botany",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Computer security",
      "Constraint (computer-aided design)",
      "Context (archaeology)",
      "Discriminative model",
      "Feature (linguistics)",
      "Geometry",
      "Identification (biology)",
      "Key (lock)",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Pang",
        "given_name": "Xiyu"
      },
      {
        "surname": "Tian",
        "given_name": "Xin"
      },
      {
        "surname": "Nie",
        "given_name": "Xiushan"
      },
      {
        "surname": "Yin",
        "given_name": "Yilong"
      },
      {
        "surname": "Jiang",
        "given_name": "Gangwu"
      }
    ]
  },
  {
    "title": "V-shaped neural network structure based on multi-scale features for image denoising",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103952",
    "abstract": "Due to the good performance of deep learning, more and more image denoising methods incorporating deep learning are implemented, including fixed-scale-based methods and multi-scale-based methods. It is easy for fixed-scale-based neural networks to achieve a better denoising performance when their depth is increased, whereas feature extraction of multiple scales from images can be obtained using multi-scale-based neural networks. In this work, a multi-scale image denoising method has been proposed which is mainly based on fixed-scale but combines the method of obtaining feature information from shallow image sampling. We propose a diamond-shaped module and a V-shaped subnetwork for extracting the features of images obtained from shallow sampling and improve the up-sampling and down-sampling units to obtain a better sampling effect. Densely connecting images of same size in MSDN can solve the problem of gradient vanishing since increasing the depth of the network and shallow image sampling prevent the loss of image details due to excessive sampling. Experiments show that our method can efficiently retain the image details during image denoising, and this method is especially helpful for preserving minute image details as compared to the other state-of-the-art denoising algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032300202X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Feature extraction",
      "Filter (signal processing)",
      "Image (mathematics)",
      "Image denoising",
      "Linguistics",
      "Noise (video)",
      "Noise reduction",
      "Non-local means",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Sampling (signal processing)",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Jing"
      },
      {
        "surname": "Sang",
        "given_name": "Liu"
      },
      {
        "surname": "Zhang",
        "given_name": "Zhicheng"
      },
      {
        "surname": "Shao",
        "given_name": "Minhao"
      },
      {
        "surname": "Li",
        "given_name": "Yunsong"
      }
    ]
  },
  {
    "title": "An approximate tensor singular value decomposition approach for the fast grouping of whole-body human poses",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103960",
    "abstract": "We propose a fast method to group similar human poses from single-view depth maps using an approximate singular value decomposition approach in the tensor domain. To this end, the input tensor is decomposed, and a representation tensor is learned using the tensor-QR decomposition and ℓ 2 , 1 norm minimization. The spatial structure and, thereby, the spatial information in each depth map, vital in performing accurate grouping of human poses, is preserved by adopting the 3D tensor approach in treating the data. For the seamless integration of discriminatory information of each pose, a new dissimilarity measure based on sub-tensors is devised and integrated into the optimization problem. Experimental analysis showcases the ability of the proposed method to achieve 10%–15% improvement in the grouping results compared with the state-of-the-art counterparts. The proposed algorithm also converges in less than ten iterations on average, thereby improving CPU time.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002109",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Decomposition",
      "Domain (mathematical analysis)",
      "Ecology",
      "Geometry",
      "Law",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Norm (philosophy)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Singular value decomposition",
      "Tensor (intrinsic definition)"
    ],
    "authors": [
      {
        "surname": "Raj",
        "given_name": "M.S. Subodh"
      },
      {
        "surname": "George",
        "given_name": "Sudhish N."
      }
    ]
  },
  {
    "title": "Object drift determination network based on dual-template joint decision-making in long-term visual tracking",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103974",
    "abstract": "Object drift determination is a crucial issue in long-term tracking. Most existing object drift determination criteria require manually selecting different thresholds on different datasets to determine whether the object is lost. In this case, choosing the appropriate threshold is a complex problem. An object drift determination network based on dual-template joint decision-making is proposed to address this issue. The proposed object drift determination network not only does not require selection of thresholds on different datasets, and can be used as a plug-and-play module of short-term visual tracking algorithm to achieve long-term visual tracking tasks with good generalization ability. The proposed object drift determination network is applied to four short-term baseline trackers and constructs four long-term visual tracking algorithms. Experimental results verify that all four improved algorithms significantly improve long-term visual tracking performance compared to the original algorithms. In addition, the determined speed of the object drift determination network proposed in this paper reaches 111 FPS, which has little effect on the long-term visual tracking speed.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002249",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "BitTorrent tracker",
      "Computer science",
      "Computer vision",
      "Dual (grammatical number)",
      "Eye tracking",
      "Generalization",
      "Literature",
      "Mathematical analysis",
      "Mathematics",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Pedagogy",
      "Physics",
      "Psychology",
      "Quantum mechanics",
      "Term (time)",
      "Tracking (education)",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Hou",
        "given_name": "Zhiqiang"
      },
      {
        "surname": "Zhao",
        "given_name": "Jiaxin"
      },
      {
        "surname": "Wang",
        "given_name": "Zhuo"
      },
      {
        "surname": "Ma",
        "given_name": "Sugang"
      },
      {
        "surname": "Yu",
        "given_name": "Wangsheng"
      },
      {
        "surname": "Fan",
        "given_name": "Jiulun"
      }
    ]
  },
  {
    "title": "Multi-scale dynamic fusion for correcting uneven illumination images",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103978",
    "abstract": "Images taken under non-ideal lighting conditions often suffer from uneven illumination, resulting in image distortion and unclear details. To address these issues, researchers have developed various methods for image enhancement. However, most of these methods are only applicable to specific types of images, especially those with underexposed exposures. Therefore, to achieve this goal, in this article, we propose a multi-scale dynamic fusion method for correcting uneven illumination images. This method can balance the overall illumination of unevenly illuminated images captured under non-ideal lighting conditions, while maintaining the original brightness contrast and enhancing the image details. We constructed a multi-branch multi-scale fully convolutional neural network, which uses attention mechanisms to focus on the bright and dark areas that need to be processed in the main branch, while the side branch is used to maintain the image’s color and naturalness. Extracting features at different scales is beneficial for detail recovery, while a larger receptive field results in better brightness contrast and a more realistic visual experience. Finally, the attention fusion method is used to fuse the features of different branches to obtain the corrected results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002286",
    "keywords": [
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Brightness",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Contrast (vision)",
      "Convolutional neural network",
      "Distortion (music)",
      "Electrical engineering",
      "Engineering",
      "Focus (optics)",
      "Fuse (electrical)",
      "Image (mathematics)",
      "Image fusion",
      "Naturalness",
      "Optics",
      "Physics",
      "Quantum mechanics",
      "Scale (ratio)",
      "Visibility"
    ],
    "authors": [
      {
        "surname": "Fan",
        "given_name": "Junyu"
      },
      {
        "surname": "Li",
        "given_name": "Jinjiang"
      },
      {
        "surname": "Ren",
        "given_name": "Lu"
      },
      {
        "surname": "Chen",
        "given_name": "Zheng"
      }
    ]
  },
  {
    "title": "IFGLT: Information fusion guided lightweight Transformer for image denoising",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103994",
    "abstract": "Image denoising is a low-level computer vision task that aims to reconstruct high-quality images from noisy ones. However, large networks with a high computational burden have been employed in existing works in pursuit of high-quality images. This paper introduces an Information Fusion Guided Lightweight Transformer (IFGLT) that can lessen the computational burden and achieve superior restoration results. The Feature Enhancement Module (FEM) optimizes the computing cost of the Transformer layer by layer using various techniques such as group mapping, channel generation, fusion convolution, and window rearrangement. The Information Compensation Module (ICM) gradually compensates for missing information by leveraging the original image. The Lightweight Sample Module (LSM) performs up-sampling and down-sampling with the minimal computing cost by altering the order of feature transformation. The experimental results demonstrate that our proposed IFGLT attains higher objective indices and achieves better visual effects with reduced computing cost in comparison to conventional methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002444",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Electrical engineering",
      "Engineering",
      "Image (mathematics)",
      "Image quality",
      "Information fusion",
      "Noise reduction",
      "Pattern recognition (psychology)",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Fengyin"
      },
      {
        "surname": "Zhou",
        "given_name": "Ziqun"
      },
      {
        "surname": "Men",
        "given_name": "Changyou"
      },
      {
        "surname": "Sun",
        "given_name": "Quan"
      },
      {
        "surname": "Huang",
        "given_name": "Kejie"
      }
    ]
  },
  {
    "title": "Development and performance evaluation of enhanced image dehazing method using deep learning networks",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103976",
    "abstract": "In this work, a deep learning-based high-performance image dehazing technique is proposed for image processing applications. The end-to-end network model is constructed and implemented using a dehazing network, discriminator network, and fine-tuning network models. These three methods are well-trained individually using appropriate datasets. The individual network models are integrated as an end-to-end network model to enhance the dehazing process. The applied input hazy image is processed by the dehazing network model using the estimation of transmission map and atmospheric light along with parallel convolution layers. A discriminated dehazing image was extracted from the discrimination network. Finally, fine-tuning is carried out based on the results of the discriminator network model. Various hazy images from different datasets are collected and applied to the proposed model, and performance metrics such as PSNR, SSIM, and MSE are evaluated. Qualitative, and quantitative comparison and analysis are carried out between the proposed learning-based image dehazing and existing dehazing methods. The average PSNR value of the proposed dehazing model is obtained by a maximum of 40.7 %, and a minimum of 1.34 %, when compared to the existing works. The average SSIM of the proposed work is increased by a maximum of 22.12 %, and a minimum of 3.38 % with respect to the existing works. The maximum average value of MSE for the proposed model is decreased by 72.6 % and the minimum decrease of MSE is 4.08 % when compared to the state-of-art-works.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002262",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Detector",
      "Discriminator",
      "Image (mathematics)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Process (computing)",
      "Telecommunications",
      "Transmission (telecommunications)"
    ],
    "authors": [
      {
        "surname": "Babu",
        "given_name": "G. Harish"
      },
      {
        "surname": "Odugu",
        "given_name": "Venkata Krishna"
      },
      {
        "surname": "Venkatram",
        "given_name": "N."
      },
      {
        "surname": "Satish",
        "given_name": "B."
      },
      {
        "surname": "Revathi",
        "given_name": "K."
      },
      {
        "surname": "Rao",
        "given_name": "B. Janardhana"
      }
    ]
  },
  {
    "title": "Action recognition method based on lightweight network and rough-fine keyframe extraction",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103959",
    "abstract": "To address the issues of large number of parameters and low recognition accuracy of action recognition networks, we propose an effective action recognition method based on lightweight network and rough-fine keyframe extraction. The method consists of three modules. The first module proposes a keyframe extraction network based on grayscale and feature descriptors, and employs a rough-fine idea to extract video keyframe. It reduces the redundancy of keyframe and enhances their ability to express action semantics. The second module introduces an attention-based feature extraction network, which combines decoupling ideas with attention mechanisms to enhance the accuracy of the action recognition network, while significantly reducing the network parameters. The third module is an improved attention module which optimizes the representation of local information. Finally, addition of the residual module fuses feature information between different convolutional layers. Experiments on two different datasets show that the number of parameters in the proposed method is only 6.4M. On publicly available datasets of HMDB51 and UCF101, the method achieves recognition accuracy of 75.69% and 93.18% without pre-training, respectively. The proposed method is valid and feasible on multiple public datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002092",
    "keywords": [
      "Action recognition",
      "Algorithm",
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Convolutional neural network",
      "Data mining",
      "Feature extraction",
      "Grayscale",
      "Image (mathematics)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Redundancy (engineering)",
      "Residual"
    ],
    "authors": [
      {
        "surname": "Pan",
        "given_name": "Hao"
      },
      {
        "surname": "Tian",
        "given_name": "Qiuhong"
      },
      {
        "surname": "Li",
        "given_name": "Saiwei"
      },
      {
        "surname": "Miao",
        "given_name": "Weilun"
      }
    ]
  },
  {
    "title": "MFS enhanced SAM: Achieving superior performance in bimodal few-shot segmentation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103946",
    "abstract": "Recently, Segment Anything Model (SAM) has become popular in computer vision field because of its powerful image segmentation ability and high interactivity of various prompts, which opens a new era of large vision foundation models. But is SAM really omnipotent? In this letter, we establish a comprehensive bimodal few-shot segmentation indoor dataset VT-840-5i, and compare SAM with eight state-of-the-art few-shot segmentation (FSS) methods on two benchmark datasets. Qualitative and quantitative experiment results show that although SAM is very effective in general object segmentation, it still has room for improvement in some challenging scenarios. Therefore, we introduce thermal infrared auxiliary information into the segmentation task and provide multiple fusion strategies (MFS) for readers to choose the most suitable approach for the specific task. Finally, we discuss several potential research trends about SAM in the future. Our test results are available at: https://github.com/VDT-2048/Bi-SAM.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001967",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Cartography",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Economics",
      "Field (mathematics)",
      "Geography",
      "Interactivity",
      "Management",
      "Mathematics",
      "Multimedia",
      "Object (grammar)",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Pure mathematics",
      "Segmentation",
      "Shot (pellet)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Ying"
      },
      {
        "surname": "Song",
        "given_name": "Kechen"
      },
      {
        "surname": "Cui",
        "given_name": "Wenqi"
      },
      {
        "surname": "Ren",
        "given_name": "Hang"
      },
      {
        "surname": "Yan",
        "given_name": "Yunhui"
      }
    ]
  },
  {
    "title": "Infer unseen from seen: Relation regularized zero-shot visual dialog",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103961",
    "abstract": "The Visual Dialog task requires retrieving the correct answer based on detected objects, a current question, and history dialogs. However, in real-world scenarios, most existing models face the hard-positive problem and are unable to reason about unseen features, which limits their generalization ability. To address this issue, we propose two Relation Regularized Modules (RRM) in this article. The first is the Visual Relation Regularized Module (VRRM), which seeks known visual features that have semantic relations with unknown visual features and leverages these known features to assist in understanding the unknown features. The second is the Text Relation Regularized Module (TRRM), which enhances the keywords in the answers to strengthen the understanding of unknown text features. To evaluate the effectiveness of these modules, we propose two zero-shot Visual Dialog splits for verification: Visual Zero-shot VisDial with unseen visual features and Text Zero-shot VisDial with unseen answers. Experimental results demonstrate that our proposed modules achieve state-of-the-art performance in zero-shot Visual Dialog with unseen visual features and unseen answers, while also producing comparable results on the benchmark VisDial v1.0 test dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002110",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Dialog box",
      "Economics",
      "Generalization",
      "Geodesy",
      "Geography",
      "Linguistics",
      "Machine learning",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Natural language processing",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Relation (database)",
      "Shot (pellet)",
      "Task (project management)",
      "Visualization",
      "World Wide Web",
      "Zero (linguistics)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Zefan"
      },
      {
        "surname": "Li",
        "given_name": "Shun"
      },
      {
        "surname": "Ji",
        "given_name": "Yi"
      },
      {
        "surname": "Liu",
        "given_name": "Chunping"
      }
    ]
  },
  {
    "title": "Undirected graph representing strategy for general room layout estimation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103963",
    "abstract": "Room layout estimation aims to predict the spatial structure of a room from a single image. Most existing methods relying on 2D cues are suitable for cuboid rooms, but non-cuboid rooms. 3D layout estimation methods can reconstruct 3D models of general rooms, but they are trained with depth information on high collection costs, consume large computation resources, and run slowly. This paper considers an undirected graph representation method of the general room layouts, which includes cuboid and non-cuboid rooms consisting of a single ceiling, a single floor, and multiple walls. To this end, we first predict the positions of the layout vertices and then use the network automatically learn the connection relationship between the vertices. The final layout is obtained through a simple layout inference post-processing algorithm. The experimental results both on cuboid and non-cuboid datasets validate the effectiveness and efficiency of our method. The code is available at https://github.com/Hui-Yao/2D-graph-layout-estimation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002134",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computation",
      "Computer science",
      "Cuboid",
      "Geometry",
      "Graph",
      "Inference",
      "Law",
      "Mathematics",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Yao",
        "given_name": "Hui"
      },
      {
        "surname": "Miao",
        "given_name": "Jun"
      },
      {
        "surname": "Zheng",
        "given_name": "Yilin"
      },
      {
        "surname": "Zhang",
        "given_name": "Guoxiang"
      },
      {
        "surname": "Chu",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "Undirected graph representing strategy for general room layout estimation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103963",
    "abstract": "Room layout estimation aims to predict the spatial structure of a room from a single image. Most existing methods relying on 2D cues are suitable for cuboid rooms, but non-cuboid rooms. 3D layout estimation methods can reconstruct 3D models of general rooms, but they are trained with depth information on high collection costs, consume large computation resources, and run slowly. This paper considers an undirected graph representation method of the general room layouts, which includes cuboid and non-cuboid rooms consisting of a single ceiling, a single floor, and multiple walls. To this end, we first predict the positions of the layout vertices and then use the network automatically learn the connection relationship between the vertices. The final layout is obtained through a simple layout inference post-processing algorithm. The experimental results both on cuboid and non-cuboid datasets validate the effectiveness and efficiency of our method. The code is available at https://github.com/Hui-Yao/2D-graph-layout-estimation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002134",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computation",
      "Computer science",
      "Cuboid",
      "Geometry",
      "Graph",
      "Inference",
      "Law",
      "Mathematics",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Yao",
        "given_name": "Hui"
      },
      {
        "surname": "Miao",
        "given_name": "Jun"
      },
      {
        "surname": "Zheng",
        "given_name": "Yilin"
      },
      {
        "surname": "Zhang",
        "given_name": "Guoxiang"
      },
      {
        "surname": "Chu",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "Multi-view graph convolution network for the recognition of human action with spatial and temporal occlusion problems",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103957",
    "abstract": "Human action recognition holds great significance in the field of social security. However, the challenges posed by occlusion and changes in viewpoint create specific obstacles in achieving accurate recognition. In this study, we propose a multi-view graph convolution fusion method to effectively address this issue. Specifically, considering the limited availability of public human action datasets that include occlusion, we introduce an adaptive multi-view spatial–temporal occlusion generation method. It allows us to generate occlusion skeleton data from multiple viewpoints, ensuring a close resemblance to real-life scenarios with minimal modifications to existing public datasets. Additionally, we present a plug-and-play multi-view information fusion module, briefed as MGL, which aims to solve the occlusion problem. The MGL combines the capabilities of Graph Convolutional network (GCN) and Long Short-Term Memory network (LSTM), in which GCN is employed to reconstruct human skeleton information using multi-view spatial occlusion data, and LSTM captures the long-term dependencies within the temporal sequences of the multi-view data. Moreover, an attention mask mechanism is introduced to highlight key joint features. Experiment results illustrate the excellent performance of our method on the NTU RGB+D 60 and NTU RGB+D 120 datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002079",
    "keywords": [
      "Action recognition",
      "Artificial intelligence",
      "Artificial neural network",
      "Cardiology",
      "Class (philosophy)",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Graph",
      "Machine learning",
      "Medicine",
      "Occlusion",
      "Pattern recognition (psychology)",
      "RGB color model",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Yang"
      },
      {
        "surname": "Wang",
        "given_name": "Ling"
      },
      {
        "surname": "Hu",
        "given_name": "Dekun"
      },
      {
        "surname": "Cheng",
        "given_name": "Hong"
      }
    ]
  },
  {
    "title": "Infer unseen from seen: Relation regularized zero-shot visual dialog",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103961",
    "abstract": "The Visual Dialog task requires retrieving the correct answer based on detected objects, a current question, and history dialogs. However, in real-world scenarios, most existing models face the hard-positive problem and are unable to reason about unseen features, which limits their generalization ability. To address this issue, we propose two Relation Regularized Modules (RRM) in this article. The first is the Visual Relation Regularized Module (VRRM), which seeks known visual features that have semantic relations with unknown visual features and leverages these known features to assist in understanding the unknown features. The second is the Text Relation Regularized Module (TRRM), which enhances the keywords in the answers to strengthen the understanding of unknown text features. To evaluate the effectiveness of these modules, we propose two zero-shot Visual Dialog splits for verification: Visual Zero-shot VisDial with unseen visual features and Text Zero-shot VisDial with unseen answers. Experimental results demonstrate that our proposed modules achieve state-of-the-art performance in zero-shot Visual Dialog with unseen visual features and unseen answers, while also producing comparable results on the benchmark VisDial v1.0 test dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002110",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Dialog box",
      "Economics",
      "Generalization",
      "Geodesy",
      "Geography",
      "Linguistics",
      "Machine learning",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Natural language processing",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Relation (database)",
      "Shot (pellet)",
      "Task (project management)",
      "Visualization",
      "World Wide Web",
      "Zero (linguistics)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Zefan"
      },
      {
        "surname": "Li",
        "given_name": "Shun"
      },
      {
        "surname": "Ji",
        "given_name": "Yi"
      },
      {
        "surname": "Liu",
        "given_name": "Chunping"
      }
    ]
  },
  {
    "title": "Learning full context feature for human motion prediction",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103955",
    "abstract": "Human motion prediction aims to predict the target poses given the previous poses. Most existing methods are devoted to extracting richer motion features from only the given previous poses to predict the target poses. However, we consider that the post poses after the target poses are helpful in acquiring the context feature and constraint between neighbor motions, which is also important for motion prediction. In this paper, we explore to make use of the post motion information for a powerful human motion prediction method. Specifically, we propose a human motion prediction model which learns the motion constraint from both the previous and post poses, in order to fully utilize the context features of the target poses. During training, the proposed memory dictionary module is used to learn the mapping from previous features to post features. In testing, the proposed memory dictionary module fully exploits the learned mapping to calculate the future motion feature according to the input previous feature. Thus, the context feature of human motion is enriched in our method. We evaluate the proposed method on two large-scale datasets, Human3.6M and CMU-Mocap. The experimental results demonstrate that our method improves the motion prediction performance, especially for long-term human motion.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002055",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Constraint (computer-aided design)",
      "Context (archaeology)",
      "Exploit",
      "Feature (linguistics)",
      "Geometry",
      "Human motion",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Motion (physics)",
      "Motion capture",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Structure from motion"
    ],
    "authors": [
      {
        "surname": "Xing",
        "given_name": "Huiqin"
      },
      {
        "surname": "Zhou",
        "given_name": "Yicong"
      },
      {
        "surname": "Yang",
        "given_name": "Jianyu"
      },
      {
        "surname": "Xiao",
        "given_name": "Yang"
      }
    ]
  },
  {
    "title": "Multi-branch Segmentation-guided Attention Network for crowd counting",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103964",
    "abstract": "Crowd counting has gained increasing attention recently owing to its importance in public safety. However, it remains a challenging task due to background complexities and high annotation costs. To address these challenges, we propose the Multi-branch Segmentation-guided Attention Network (MSGANet). To deal with the complex background, we incorporate segmentation-guided attention branches into both shallow and deep layers of the baseline model, allowing simultaneous consideration of spatial and semantic information. Multi-level attention maps enable the network to minimize the influence of complex backgrounds while focusing on the regions containing crowds. To tackle the issue of costly annotations, MSGANet utilizes only point annotations to generate ground truth density and segmentation maps, eliminating additional expenses. Our results demonstrate that our approach achieves highly competitive performance compared to state-of-the-art crowd counting methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002146",
    "keywords": [
      "Annotation",
      "Artificial intelligence",
      "Baseline (sea)",
      "Computer science",
      "Computer security",
      "Crowds",
      "Economics",
      "Geology",
      "Geometry",
      "Ground truth",
      "Machine learning",
      "Management",
      "Mathematics",
      "Oceanography",
      "Point (geometry)",
      "Segmentation",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Fan",
        "given_name": "Zheyi"
      },
      {
        "surname": "Song",
        "given_name": "Zihao"
      },
      {
        "surname": "Wu",
        "given_name": "Di"
      },
      {
        "surname": "Zhu",
        "given_name": "Yixuan"
      }
    ]
  },
  {
    "title": "Night vision self-supervised Reflectance-Aware Depth Estimation based on reflectance",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103962",
    "abstract": "The depth estimation of nighttime images is a challenging problem due to the lack of accurate ground-truth depth labels. Although various self-supervised methods leveraging texture information have been proposed to solve the problem, the performance is still not satisfactory due to the imaging limitations of visible cameras. To this end, we propose a self-supervised Reflectance-Aware Depth Estimation approach based on reflectance for nighttime images. Two major factors strengthen the proposed approach: a Reflectance Extraction Network and a feature consistency loss. We introduce the Reflectance Extraction Network to extract texture information based on the finding that the texture is beneficial for depth estimation. Then, we utilize the feature consistency loss to help the baseline network to learn the intrinsic feature rather than the images’ light. Experiment results on the challenging Oxford RobotCar dataset confirm the robustness and effectiveness of our approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002122",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Feature (linguistics)",
      "Feature extraction",
      "Gene",
      "Ground truth",
      "Image (mathematics)",
      "Linguistics",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Reflectivity",
      "Robustness (evolution)",
      "Texture (cosmology)"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Yao"
      },
      {
        "surname": "Pu",
        "given_name": "Fangling"
      },
      {
        "surname": "Chen",
        "given_name": "Hongjia"
      },
      {
        "surname": "Tang",
        "given_name": "Rui"
      },
      {
        "surname": "Li",
        "given_name": "Jinwen"
      },
      {
        "surname": "Xu",
        "given_name": "Xin"
      }
    ]
  },
  {
    "title": "End-to-end wavelet block feature purification network for efficient and effective UAV object tracking",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103950",
    "abstract": "Recently, unmanned aerial vehicle (UAV) object tracking tasks have significantly improved with the emergence of deep learning. However, owing to the object feature pollution caused by motion blur, illumination variation, and occlusion, most of the existing trackers often fail to precisely localize the target in the complex real-world circumstances. To overcome this challenge, we present a novel wavelet block feature purification network (WFPN) for efficient and effective UAV tracking. WFPN is mainly composed of downsampling network through wavelet transforms and upsampling network through inverse wavelet transforms. To be specific, the downsampling network performs discrete wavelet transform (DWT) to reduce interference information and preserve original feature details, while the upsampling network applies inverse DWT (IDWT) to reconstruct decontaminated feature information. Additionally, a novel sequential encoder is introduced to achieve a better purification effect. Finally, a pooling distance loss is devised to improve the purification effect of DWT downsampling network. Extensive experiments show that our WFPN achieves promising tracking performance on three well-known UAV benchmarks, especially on sequences with feature pollution. Moreover, our method runs at 33.2 frames per second on the edge platform of Nvidia Jetson AGX Orin, which is suitable for UAVs with limited onboard payload and computing capability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002006",
    "keywords": [
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Computer science",
      "Computer vision",
      "Discrete wavelet transform",
      "Feature (linguistics)",
      "Geometry",
      "Image (mathematics)",
      "Linguistics",
      "Mathematics",
      "Motion blur",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Upsampling",
      "Wavelet",
      "Wavelet transform"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Haijun"
      },
      {
        "surname": "Qi",
        "given_name": "Lihua"
      },
      {
        "surname": "Qu",
        "given_name": "Haoyu"
      },
      {
        "surname": "Ma",
        "given_name": "Wenlai"
      },
      {
        "surname": "Yuan",
        "given_name": "Wei"
      },
      {
        "surname": "Hao",
        "given_name": "Wei"
      }
    ]
  },
  {
    "title": "Random hand gesture authentication via efficient Temporal Segment Set Network",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103985",
    "abstract": "Biometric authentication technologies are rapidly gaining popularity, and hand gestures are emerging as a promising biometric trait due to their rich physiological and behavioral characteristics. Hand gesture authentication can be categorized as defined hand gesture authentication and random hand gesture authentication. Unlike defined hand gesture authentication, random hand gesture authentication is not constrained to specific hand gesture types, allowing users to perform hand gestures randomly during enrollment and verification, thus more flexible and friendly. However, in random hand gesture authentication, the model needs to extract more generalized physiological and behavioral features from different viewpoints and positions without gesture templates, which is more challenging. In this paper, we present a novel efficient Temporal-Segment-Set-Network (TS2N) that directly extracts both behavioral and physiological features from a single RGB video to further enhance the performance of random hand gesture authentication. Our method adopts a new motion pseudo-modality and leverages a set-based representation to capture behavioral characteristics online. Additionally, we propose a channel-spatial attention mechanism, Contextual Squeeze-and-Excitation Network (CoSEN), to better abstract and understand physiological characteristics by explicitly modeling the channel-spatial interdependence, thereby adaptively recalibrating channel-specific and spatial-specific responses. Extensive experiments on the largest public hand gesture authentication dataset SCUT-DHGA demonstrate TS2N’s superiority against 21 state-of-the-art models in terms of EER (5.707% for full version and 6.664% for lite version) and computational cost (98.9022G for full version and 46.3741G for lite version). The code is available at https://github.com/SCUT-BIP-Lab/TS2N.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002353",
    "keywords": [
      "Artificial intelligence",
      "Authentication (law)",
      "Biometrics",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Gesture",
      "Gesture recognition",
      "Human–computer interaction",
      "Programming language",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Lin",
        "given_name": "Yihong"
      },
      {
        "surname": "Song",
        "given_name": "Wenwei"
      },
      {
        "surname": "Kang",
        "given_name": "Wenxiong"
      }
    ]
  },
  {
    "title": "Toward high imperceptibility deep JPEG steganography based on sparse adversarial attack",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103977",
    "abstract": "JPEG steganography is an important branch of information hiding. However, with the development of deep learning-based steganalysis models, steganography is facing great challenges. To better resist these steganalysis models, this paper proposes a deep JPEG steganography framework based on sparse adversarial attacks. According to the vulnerability of deep steganalysis models to adversarial attacks, sparse adversarial attacks are introduced into the deep JPEG steganographic structure to improve the security of stego images. In addition, to resist steganalysis from JPEG and spatial domains, both JPEG and spatial domain steganalysis models are involved in adversarial training. Finally, to further enhance the imperceptibility of adversarial stego images, the visual perception loss is designed from the perspective of human eyes. Experimental results indicate that compared with the existing methods, the proposed method has higher imperceptibility and security and can resist modern deep steganalysis models in both JPEG and spatial domains to a certain extent. The source code is available at https://github.com/imagecbj/SAE-JS.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002274",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Deep learning",
      "Image (mathematics)",
      "Image compression",
      "Image processing",
      "JPEG",
      "Lossless JPEG",
      "Pattern recognition (psychology)",
      "Steganalysis",
      "Steganography",
      "Steganography tools"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Beijing"
      },
      {
        "surname": "Nie",
        "given_name": "Yuxin"
      },
      {
        "surname": "Yang",
        "given_name": "Jianhua"
      }
    ]
  },
  {
    "title": "EERCA-ViT: Enhanced Effective Region and Context-Aware Vision Transformers for image sentiment analysis",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103968",
    "abstract": "Different parts of an image have a strong or weak guiding effect on emotions. The key to emotion recognition of images is to fully exploit the regions associated with emotions. Therefore, this paper proposes a visual sentiment classification model with two branches based on visual transformer, termed as Enhanced Effective Region and Context-Aware Vision Transformers (EERCA-ViT). This model includes a primary branch and an auxiliary branch. The primary branch simulates interdependencies between patches by squeezing and stimulating patches (P-SE), thereby highlighting effective region features in the global feature. The auxiliary branch removes feature patches that have been tagged by the primary branch through the context-aware module (CAM), forcing the network to discover different sentiment discriminative regions. At the same time, a two-part loss function is constructed to improve the robustness of the model. Finally, extensive experiments on six benchmark datasets show that the proposed method outperforms the state-of-the-art image sentiment analysis methods. Furthermore, the effectiveness of the different modules of the framework (P-SE and CAM) has been well demonstrated through extensive comparative experiments.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002183",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Discriminative model",
      "Electrical engineering",
      "Engineering",
      "Exploit",
      "Feature (linguistics)",
      "Gene",
      "Geodesy",
      "Geography",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Robustness (evolution)",
      "Sentiment analysis",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Xiaohua"
      },
      {
        "surname": "Yang",
        "given_name": "Jie"
      },
      {
        "surname": "Hu",
        "given_name": "Min"
      },
      {
        "surname": "Ren",
        "given_name": "Fuji"
      }
    ]
  },
  {
    "title": "A no-reference underwater image quality evaluator via quality-aware features",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103979",
    "abstract": "In this paper, we propose a novel no-reference evaluator based on quality-aware features, called QA-UIQE, for underwater image quality assessment. QA-UIQE extracts and fuses a set of quality-aware features including naturalness, color, contrast, sharpness, and structure. Technically, we first present a new color-cast weighted colorfulness measurement as well as color consistency measurement to characterize color, and design a saliency-weighted contrast measurement to improve the distinguishing ability of measuring contrast. Also, the locally mean subtracted and contrast normalized, maximum local variation, and local entropy are incorporated to measure naturalness, sharpness and structure, respectively. Afterward, we integrate the feature vectors extracted from the training set into Gaussian process regression to predict the image quality. Moreover, we collect a real-world underwater image dataset for testing the generalization ability of our method. The experimental results illustrate that our QA-UIQE has a superior prediction accuracy and is highly consistent with human visual perception.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002298",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Contrast (vision)",
      "Geology",
      "Image (mathematics)",
      "Image quality",
      "Naturalness",
      "Oceanography",
      "Pattern recognition (psychology)",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Set (abstract data type)",
      "Underwater"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Siqi"
      },
      {
        "surname": "Li",
        "given_name": "Yuxuan"
      },
      {
        "surname": "Tan",
        "given_name": "Lu"
      },
      {
        "surname": "Yang",
        "given_name": "Huan"
      },
      {
        "surname": "Hou",
        "given_name": "Guojia"
      }
    ]
  },
  {
    "title": "Constructing comprehensive and discriminative representations with diverse attention for occluded person re-identification",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103993",
    "abstract": "Occluded person re-identification (Re-ID) is a challenging task that aims to match occluded person images to holistic ones across different camera views. Feature diversity is crucial for achieving high-performance Re-ID. Previous methods rely on additional annotations or hand-crafted rules to achieve feature diversity, which are inefficient or infeasible for occluded Re-ID. To address this, we propose the Diverse Attention Net (DANet) which utilizes attention mechanism to achieve diverse feature mining. Specifically, DANet incorporates a pair of complementary Diverse Parallel Attention Modules (DPAM), which, under the attention decorrelation constraint (ADC), help the model automatically capture diverse discriminative features in a global scope. Additionally, we propose an Efficient Transformer layer that can seamlessly integrate with the proposed DPAM and synergistically enhance the capability to handle occlusions. The resulting DANet construct a set of comprehensive representations that encode diverse discriminative features. Extensive experiments demonstrate DANet achieves state-of-the-art performance on both occluded and holistic ReID benchmarks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002432",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Botany",
      "Chemistry",
      "Computer science",
      "Discriminative model",
      "ENCODE",
      "Economics",
      "Feature (linguistics)",
      "Gene",
      "Identification (biology)",
      "Linguistics",
      "Machine learning",
      "Management",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Set (abstract data type)",
      "Task (project management)",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Ren",
        "given_name": "Tengfei"
      },
      {
        "surname": "Lian",
        "given_name": "Qiusheng"
      },
      {
        "surname": "Zhang",
        "given_name": "Dan"
      }
    ]
  },
  {
    "title": "Comprehensive receptive field adaptive graph convolutional networks for action recognition",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103953",
    "abstract": "Action recognition has shown a broad prospect for application in human–computer interaction and autonomous vehicles. According to skeleton data being lightweight and robust to environmental disturbance, skeleton-based methods are increasingly used in action recognition. Existing Graph Convolutional Networks (GCNs) have limitations such as Global Average Pooling (GAP) causing information oversmoothing, attention mechanism increasing computational complexity, and fixed spatial configuration partition strategy limiting flexibility. Our model overcomes these limitations by incorporating Temporal Covariance Pooling (TCP) for better feature utilization, Sample-Shared attention for weight sharing, and a Comprehensive Receptive Field strategy for optimal joint connections. Experiments show that the recognition accuracy of our model outperforms baseline 2s-AGCN by 1.1 % and 0.7 % under X-Sub and X-View for NTU RGB + D dataset, 1.8 % and 1.9 % in X-Sub and X-Set for NTU RGB + D 120 dataset, and 1.6 % on Kinetics-Skeleton dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002031",
    "keywords": [
      "Action recognition",
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Covariance",
      "Dimensionality reduction",
      "Graph",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pooling",
      "RGB color model",
      "Statistics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Qi",
        "given_name": "Hantao"
      },
      {
        "surname": "Guo",
        "given_name": "Xin"
      },
      {
        "surname": "Xin",
        "given_name": "Hualei"
      },
      {
        "surname": "Li",
        "given_name": "Songyang"
      },
      {
        "surname": "Chen",
        "given_name": "Enqing"
      }
    ]
  },
  {
    "title": "HY-LSTM: A new time series deep learning architecture for estimation of pedestrian time to cross in advanced driver assistance system",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103982",
    "abstract": "Advanced driver assistance systems (ADASs), particularly pedestrian protection systems (PPSs), have emerged as a hot research topic with the goal of enhancing traffic safety. The development of reliable on-board pedestrian detection systems is a critical problem for PPSs. It is extremely difficult to provide the required resilience of this type of system due to the fluctuating look of pedestrians (e.g., varied attire, changing size, aspect ratio, and dynamic shape) and the unstructured surroundings. The detection of pedestrians has gained huge focus among researchers because of its huge applications in the domain of automated vehicles. In the previous decades, the majority of examinations are done to obtain better solutions for detecting pedestrians, but fewer of them concentrated on determining the pedestrian. It is one of crucial interest to study regarding transport safety as it implies minimizing the count of traffic collisions and protecting pedestrians who are more susceptible to accidents. Predicting pedestrian conduct is critical for road safety, traffic management systems, ADAS, and autonomous vehicles in general. The fundamental problem in the field of self-driving and smart automobiles is identifying impediments, particularly people, and taking action to avoid collisions with them. Various studies have been conducted in this sector by many researchers, yet there are still many mistakes in the proper identification of pedestrians. Hence, this paper devises an approach to estimate pedestrian time for crossing in an advanced driving assistance system (ADAS). The inputted videos undergo keyframe extraction wherein crucial keyframes are extracted. The deep joint segmentation is further applied for identifying the pedestrian followed by intention classification. Then, the estimation of pedestrian time to cross is predicted using Hybrid-Long short term memory (HY-LSTM), and it is a new time series model obtained by unifying LSTM and Object-based convolution neural network (OCNN), where the layer and hyperparameters of OCNN are optimally derived using Gradient Chef Based Optimization (GCBO). The proposed GCBO-HY-LSTM outperformed showing the least Mean absolute error (MAE) of 0.038, Mean square error (MSE) of 0.029, and Root Mean square error (RMSE) of 0.170.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002328",
    "keywords": [
      "Advanced driver assistance systems",
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Computer security",
      "Deep learning",
      "Engineering",
      "Field (mathematics)",
      "Focus (optics)",
      "Identification (biology)",
      "Mathematics",
      "Optics",
      "Pedestrian",
      "Pedestrian detection",
      "Physics",
      "Pure mathematics",
      "Real-time computing",
      "Segmentation",
      "Transport engineering"
    ],
    "authors": [
      {
        "surname": "S",
        "given_name": "Veluchamy"
      },
      {
        "surname": "K",
        "given_name": "Michael Mahesh"
      },
      {
        "surname": "R",
        "given_name": "Muthukrishnan"
      },
      {
        "surname": "S",
        "given_name": "Karthi"
      }
    ]
  },
  {
    "title": "A recent survey on perceived group sentiment analysis",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103988",
    "abstract": "Group sentiment analysis in today’s era is influencing many applications. Sentiment recognition is applicable for individuals, and group as well as crowd. This paper aims to highlight the ongoing present research scenario on perceived group sentiment analysis. The paper provides a brief overview of the strengths and weaknesses of well-known group sentiment analysis approaches. These approaches are broadly split into two basic categories: top-down and bottom-up approaches. A combination of top-down and bottom-up approaches is found superior in performance. This comprehensive review also discusses the available dataset with taxonomic representation and the importance of fusion techniques in group emotion recognition. This paper also narrates the novel attempt to demarcate the group emotion recognition approaches into the categories evolved by the influence of machine learning and deep learning at the feature extraction level and at the classification level. The multi-modality based group sentiment analysis has a great scope to overcome the challenges and improve the performance on real time applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002389",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Data science",
      "Group (periodic table)",
      "Law",
      "Machine learning",
      "Natural language processing",
      "Organic chemistry",
      "Political science",
      "Politics",
      "Programming language",
      "Psychology",
      "Representation (politics)",
      "Scope (computer science)",
      "Sentiment analysis",
      "Social psychology",
      "Software engineering",
      "Strengths and weaknesses",
      "Top-down and bottom-up design"
    ],
    "authors": [
      {
        "surname": "Rathod",
        "given_name": "Bhoomika"
      },
      {
        "surname": "Vanzara",
        "given_name": "Rakeshkumar"
      },
      {
        "surname": "Pandya",
        "given_name": "Devang"
      }
    ]
  },
  {
    "title": "IPVNet: Learning implicit point-voxel features for open-surface 3D reconstruction",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103970",
    "abstract": "Reconstruction of 3D open surfaces (e.g., non-watertight meshes) is an underexplored area of computer vision. Recent learning-based implicit techniques have removed previous barriers by enabling reconstruction in arbitrary resolutions. Yet, such approaches often rely on distinguishing between the inside and outside of a surface in order to extract a zero level set when reconstructing the target. In the case of open surfaces, this distinction often leads to artifacts such as the artificial closing of surface gaps. However, real-world data may contain intricate details defined by salient surface gaps. Implicit functions that regress an unsigned distance field have shown promise in reconstructing such open surfaces. Nonetheless, current unsigned implicit methods rely on a discretized representation of the raw data. This not only bounds the learning process to the representation’s resolution, but it also introduces outliers in the reconstruction. To enable accurate reconstruction of open surfaces without introducing outliers, we propose a learning-based implicit point-voxel model (IPVNet). IPVNet predicts the unsigned distance between a surface and a query point in 3D space by leveraging both raw point cloud data and its discretized voxel counterpart. Experiments on synthetic and real-world public datasets demonstrates that IPVNet outperforms the state of the art while producing far fewer outliers in the resulting reconstruction.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002201",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Discretization",
      "Geometry",
      "Law",
      "Mathematical analysis",
      "Mathematics",
      "Outlier",
      "Pattern recognition (psychology)",
      "Point cloud",
      "Political science",
      "Politics",
      "Polygon mesh",
      "Representation (politics)",
      "Surface (topology)",
      "Surface reconstruction",
      "Voxel"
    ],
    "authors": [
      {
        "surname": "Arshad",
        "given_name": "Mohammad Samiul"
      },
      {
        "surname": "Beksi",
        "given_name": "William J."
      }
    ]
  },
  {
    "title": "Blind image watermark decoder in NSST-FPCET domain using Weibull Mixtures-HMT",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103986",
    "abstract": "Balancing imperceptibility, robustness, and data payload is a key topic in digital watermarking technology. Many contributions point to statistical modeling as an effective solution to this problem. On the basis of this, in this paper, we devise a hybrid domain image watermark decoder based on Weibull mixtures based hidden Markov tree (Weibull Mixtures-HMT) model. In the embedding phase, Otsu-Canny edge detection method is used to map the positions of high entropy blocks to the target subband obtained by non-subsampled shearlet transform (NSST), and fast polar complex exponential transform (FPCET) is computed in the target blocks to obtain the NSST-FPCET domain. The watermark signals are embedded into NSST-FPCET magnitudes by a linear method. In the extraction phase, NSST-FPCET magnitude coefficients are modeled by Weibull Mixtures-HMT model, which describes the distribution characteristics as well as the dependencies of NSST-FPCET magnitudes. The efficient variance reduced stochastic expectation maximization method is employed for estimating the parameters of Weibull Mixtures-HMT model. A statistical image watermark decoder in NSST-FPCET domain is finally achieved by the maximum likelihood decision. Massive experiments are performed so as to confirm the superiority of the designed scheme in trade-off among the data payload, imperceptibility, and robustness.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002365",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Digital watermarking",
      "Gene",
      "Image (mathematics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Robustness (evolution)",
      "Statistics",
      "Watermark",
      "Weibull distribution"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Xiangyang"
      },
      {
        "surname": "Shen",
        "given_name": "Yixuan"
      },
      {
        "surname": "Wang",
        "given_name": "Tingting"
      },
      {
        "surname": "Niu",
        "given_name": "Panpan"
      }
    ]
  },
  {
    "title": "MIL-ViT: A multiple instance vision transformer for fundus image classification",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103956",
    "abstract": "Despite the great success of deep learning approaches, retinal disease classification is still challenging as the early-stage pathological regions of retinal diseases may be extremely tiny and subtle, which are difficult for networks to detect. The feature representations learnt by deep learning models focusing more on the local view may lead to indiscriminative semantic-level representation. On the contrary, if they focus more on the global semantic-level, they may ignore the discerning subtle local pathological regions. To address this issue, in this paper, we propose a hybrid framework, combining the strong global semantic representation learning capability of the vision Transformer (ViT) and the excellent capacity of local representation extraction from the conventional multiple instance learning (MIL). Particularly, a multiple instance vision Transformer (MIL-ViT) is implemented, where the vanilla ViT branch and the MIL branch generate semantic probability distributions separately, and a bag consistency loss is proposed to minimize the difference between them. Moreover, a calibrated attention mechanism is developed to embed the instance representation into the bag representation in our MIL-ViT. To further improve the feature representation capability for fundus images, we pre-train the vanilla ViT on a large-scale fundus image database. The experimental results validate that the generalization capability of the model using our pre-trained weights for fundus disease diagnosis is better than the one using ImageNet pre-trained weights. Extensive experiments on four publicly available benchmarks demonstrate that our proposed MIL-ViT outperforms latest fundus image classification methods, including various deep learning models and deep MIL methods. All our source code and pre-trained models are publicly available at https://github.com/greentreeys/MIL-VT.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002067",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Deep learning",
      "Feature learning",
      "Fundus (uterus)",
      "Law",
      "Machine learning",
      "Medicine",
      "Ophthalmology",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Bi",
        "given_name": "Qi"
      },
      {
        "surname": "Sun",
        "given_name": "Xu"
      },
      {
        "surname": "Yu",
        "given_name": "Shuang"
      },
      {
        "surname": "Ma",
        "given_name": "Kai"
      },
      {
        "surname": "Bian",
        "given_name": "Cheng"
      },
      {
        "surname": "Ning",
        "given_name": "Munan"
      },
      {
        "surname": "He",
        "given_name": "Nanjun"
      },
      {
        "surname": "Huang",
        "given_name": "Yawen"
      },
      {
        "surname": "Li",
        "given_name": "Yuexiang"
      },
      {
        "surname": "Liu",
        "given_name": "Hanruo"
      },
      {
        "surname": "Zheng",
        "given_name": "Yefeng"
      }
    ]
  },
  {
    "title": "UAV small target detection algorithm based on an improved YOLOv5s model",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103936",
    "abstract": "The targets of UAV target detection are usually small targets, and the backgrounds are complex. In this work, aiming at the problem that small targets are easy to be missed or misdetected during the UAV detection, an improved YOLOv5s_MSES target detection algorithm based on YOLOv5s is proposed. First of all, to solve the problem of UAV’s difficulty in detecting small targets, the detection layer is ameliorated into the small target detection layer STD, which makes the model more easily detect the small targets. Then, the multi-scale feature fusion module is added to improve the detection accuracy of the small targets. Furthermore, by combining multi-scale module and attention module, a new connection method is proposed to retain the large scale of feature information. Finally, in contrast with some existent methods, the experimental results of VisDrone2019 UAV target detection dataset show that our proposed YOLOv5s_MSES can achieve the better detection effect, and more effectively complete the small target detection task for UAV aerial photography images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001864",
    "keywords": [
      "Activity detection",
      "Algorithm",
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Layer (electronics)",
      "Linguistics",
      "Object detection",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "Cao",
        "given_name": "Shihai"
      },
      {
        "surname": "Wang",
        "given_name": "Ting"
      },
      {
        "surname": "Li",
        "given_name": "Tao"
      },
      {
        "surname": "Mao",
        "given_name": "Zehui"
      }
    ]
  },
  {
    "title": "SAE-PPL: Self-guided attention encoder with prior knowledge-guided pseudo labels for weakly supervised video anomaly detection",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103967",
    "abstract": "Recently, many weakly supervised video anomaly detection (WS-VAD) methods focus on generating reasonable pseudo-labels for frames by using video-level annotations. To achieve this goal, some existing label-noise cleaning techniques and pseudo-label generators are used by them, but their performance is limited. The main reason is that the useful prior knowledge, i.e., the graduality of anomaly events, is ignored. Here, we propose a Self-Training Framework, which assumes flexible soft boundaries between abnormal and normal clips. The framework consists of: (1) A prior knowledge guided pseudo-label generator that incorporates prior knowledge of video segment distributions into the MIL framework to generate high-confidence pseudo labels; (2) An improved self-guided attention encoder is developed to capture multiscale long-term spatiotemporal features, where dependencies among anomaly frames are preserved. Moreover, a pseudo-label based self-training scheme is adopted to supervise the encoder. Experimental results verify the superiority of our method over baseline approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002171",
    "keywords": [
      "Anomaly (physics)",
      "Anomaly detection",
      "Artificial intelligence",
      "Computer science",
      "Condensed matter physics",
      "Encoder",
      "Focus (optics)",
      "Generator (circuit theory)",
      "Operating system",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Power (physics)",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Tang",
        "given_name": "Jun"
      },
      {
        "surname": "Wang",
        "given_name": "Zhentao"
      },
      {
        "surname": "Hao",
        "given_name": "Guanyu"
      },
      {
        "surname": "Wang",
        "given_name": "Ke"
      },
      {
        "surname": "Zhang",
        "given_name": "Yan"
      },
      {
        "surname": "Wang",
        "given_name": "Nian"
      },
      {
        "surname": "Liang",
        "given_name": "Dong"
      }
    ]
  },
  {
    "title": "Unsupervised industrial anomaly detection with diffusion models",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103983",
    "abstract": "Due to the limitations of autoencoders and generative adversarial networks, the performance of reconstruction-based unsupervised image anomaly detection methods are not satisfactory. In this paper, we aim to explore the potential of a more powerful generative model, the diffusion model, in the anomaly detection problem. Specifically, we design a Reconstructed Diffusion Models (RecDMs) based on conditional denoising diffusion implicit models for image reconstruction. To eliminate the stochastic nature of the generation process, our key idea is to use a learnable encoder to extract meaningful semantic representations, which are then used as signal conditions in an iterative denoising process to guide the model in recovering the image, while avoiding falling into an ”identical shortcut” to meaningless image reconstruction. To accurately locate anomaly regions, we introduce a discriminative network to obtain the pixel-level anomaly segmentation map based on the reconstructed image. Our experiments demonstrate the effectiveness of the proposed method, achieving a new state-of-the-art image-level AUC score of 98.1% and a pixel-level AUC score of 94.6% on the MVTec AD dataset, among all reconstruction-based methods. We also show the significant potential and promising future of our method on the challenging real-world dataset, the CHL AD dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032300233X",
    "keywords": [
      "Anomaly (physics)",
      "Anomaly detection",
      "Artificial intelligence",
      "Computer science",
      "Condensed matter physics",
      "Discriminative model",
      "Image (mathematics)",
      "Iterative reconstruction",
      "Noise reduction",
      "Pattern recognition (psychology)",
      "Physics",
      "Pixel",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Haohao"
      },
      {
        "surname": "Xu",
        "given_name": "Shuchang"
      },
      {
        "surname": "Yang",
        "given_name": "Wenzhen"
      }
    ]
  },
  {
    "title": "Screen content video quality assessment based on spatiotemporal sparse feature",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103912",
    "abstract": "In recent years, the explosive growth of application scenarios have occurred in screen content videos (SCVs), in which the SCVs are unavoidably suffered from the quality degradation and the video quality assessment (VQA) of the SCVs becomes very essential. In view of this, a full-reference VQA algorithm, called the spatiotemporal sparse feature-based model (SSFM) is proposed in this paper, aiming to give a precise and efficient quality evaluation about the distorted SCVs. Note that the SCVs are full of edge information which the human eyes are highly sensitive to, and the sparse coding can provide accurate quantitative predictions which are consistent with the perception resulted from the cerebral cortex in various receptive field models of the visual cortex. With these considerations, three dimensional Difference of Gaussian (3D-DOG) filter and 3D Sparse Dictionary are developed to extract multi-scale spatiotemporal features and obtain spatiotemporal sparse features respectively, from the reference and distorted SCVs. Based on these features, the spatiotemporal sparse feature similarity can be measured and followed by generating the quality scores of the distorted SCVs under evaluation. Compared to other classic and state-of-the-art image/video quality evaluation metrics, the experimental results of the proposed SSFM on the screen content video database (SCVD) are more consistent with the perceived evaluation of SCVs according to the human visual system (HVS).",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001621",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Blob detection",
      "Coding (social sciences)",
      "Computer science",
      "Computer vision",
      "Edge detection",
      "Feature (linguistics)",
      "Gaussian",
      "Human visual system model",
      "Image (mathematics)",
      "Image processing",
      "Image quality",
      "Linguistics",
      "Mathematics",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Similarity (geometry)",
      "Sparse approximation",
      "Statistics",
      "Visual cortex"
    ],
    "authors": [
      {
        "surname": "Ding",
        "given_name": "Rui"
      },
      {
        "surname": "Zeng",
        "given_name": "Huanqiang"
      },
      {
        "surname": "Wen",
        "given_name": "Hao"
      },
      {
        "surname": "Huang",
        "given_name": "Hailiang"
      },
      {
        "surname": "Cheng",
        "given_name": "Shan"
      },
      {
        "surname": "Hou",
        "given_name": "Junhui"
      }
    ]
  },
  {
    "title": "Multi-stage affine motion estimation fast algorithm for versatile video coding using decision tree",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103910",
    "abstract": "Affine motion estimation (AME) which is newly introduced in versatile video coding (VVC) plays a significant role in the bit-rate reduction for rotation and zooming scenes. However, it brings the complexity extremely increased. In this paper, a fast AME algorithm is designed to cope with the high-complexity problem. Three crucial stages are involved in the fast algorithm, which is the optimal inter-mode decision based on the coding unit (CU) partition, the fast algorithm in affine motion search (AMS), and the fast inter-mode decision based on a decision tree. Specifically, the optimal inter-mode will be determined straightforwardly when the parent CU is checked whether it is translational motion estimation (TME). Then, three fast algorithms are made in AMS, which are early termination based on control point motion vector ( C P M V ), early termination of the iteration process, and fast fine granularity C P M V search. Finally, the optimal inter-mode is predicted using a decision tree based on eight essential features. Experimental results show that the proposed algorithm achieves 10.20% and 10.33% encoding time-saving on average under Low Delay B (LDB) and Random Access (RA) configuration, while the BD-Rate loss is only 0.12% and 0.14%, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001608",
    "keywords": [
      "Affine transformation",
      "Algorithm",
      "Coding (social sciences)",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Mathematics",
      "Motion estimation",
      "Motion vector",
      "Operating system",
      "Pure mathematics",
      "Random access",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Xiaofeng"
      },
      {
        "surname": "Zhou",
        "given_name": "Fangtao"
      },
      {
        "surname": "Niu",
        "given_name": "Weihong"
      },
      {
        "surname": "Li",
        "given_name": "Tianci"
      },
      {
        "surname": "Lu",
        "given_name": "Yu"
      },
      {
        "surname": "Zhou",
        "given_name": "Yang"
      },
      {
        "surname": "Yin",
        "given_name": "Haibing"
      },
      {
        "surname": "Yan",
        "given_name": "Chenggang"
      }
    ]
  },
  {
    "title": "Underwater image enhancement method based on denoising diffusion probabilistic model",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103926",
    "abstract": "Underwater images often suffer from severe distortion, which seriously affects the image quality and the application. Current underwater image enhancement methods have poor generalization capability and cannot be adapted to all types of underwater images. In recent years, the diffusion model based on the denoising diffusion probabilistic model (DDPM) has achieved excellent results in various fields of computer vision. Inspired by the DDPM, an underwater image enhancement method based on the DDPM (UW-DDPM) was proposed in this paper. The UW-DDPM trained on paired datasets and utilized two U-Net networks to complete image denoising as well as image distribution transformation, which effectively improved the quality of underwater images. By testing on real underwater image datasets, UW-DDPM achieved better improvement in visual effects and evaluation metrics than the existing model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001761",
    "keywords": [
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Diffusion",
      "Distortion (music)",
      "Gene",
      "Generalization",
      "Geology",
      "Image (mathematics)",
      "Image denoising",
      "Image quality",
      "Mathematical analysis",
      "Mathematics",
      "Oceanography",
      "Physics",
      "Probabilistic logic",
      "Telecommunications",
      "Thermodynamics",
      "Transformation (genetics)",
      "Underwater"
    ],
    "authors": [
      {
        "surname": "Lu",
        "given_name": "Siqi"
      },
      {
        "surname": "Guan",
        "given_name": "Fengxu"
      },
      {
        "surname": "Zhang",
        "given_name": "Hanyu"
      },
      {
        "surname": "Lai",
        "given_name": "Haitao"
      }
    ]
  },
  {
    "title": "Accurate subvoxel location and characterization of edges in 3D images based on the Partial Volume Effect",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103928",
    "abstract": "An accurate estimation of the position, orientation, principal curvatures, and change in intensity of the edges in a 3D image provides highly useful information for many applications. The use of derivative operators to compute the gradient vector and the Hessian matrix in each voxel usually generates inaccurate results. This paper presents a new edge detector which is derived from the Partial Volume Effect (PVE). Instead of assuming continuity in the image values, edge features are extracted from the distribution of intensities within a neighborhood of each edge voxel. First, the influence of the intensities of the voxels in first- and second-order edges is analyzed to demonstrate that these types of edges can be precisely characterized from the intensity distribution. Afterward, this approach is extended to especially demanding situations by considering how adverse conditions can be tackled. This extension includes filtering noisy images, characterizing edges in blurred regions, and using windows with floating limits for close edges. The proposed technique has been tested on synthetic and real images, including some particularly difficult objects, and achieving a highly accurate subvoxel characterization of the edges. An open source implementation of our method is provided.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001785",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Economics",
      "Edge detection",
      "Enhanced Data Rates for GSM Evolution",
      "Finance",
      "Geometry",
      "Hessian matrix",
      "Image (mathematics)",
      "Image processing",
      "Mathematics",
      "Orientation (vector space)",
      "Pattern recognition (psychology)",
      "Physics",
      "Position (finance)",
      "Quantum mechanics",
      "Volume (thermodynamics)",
      "Voxel"
    ],
    "authors": [
      {
        "surname": "Trujillo-Pino",
        "given_name": "Agustín"
      },
      {
        "surname": "Alemán-Flores",
        "given_name": "Miguel"
      },
      {
        "surname": "Santana-Cedrés",
        "given_name": "Daniel"
      },
      {
        "surname": "Monzón",
        "given_name": "Nelson"
      }
    ]
  },
  {
    "title": "Accurate subvoxel location and characterization of edges in 3D images based on the Partial Volume Effect",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103928",
    "abstract": "An accurate estimation of the position, orientation, principal curvatures, and change in intensity of the edges in a 3D image provides highly useful information for many applications. The use of derivative operators to compute the gradient vector and the Hessian matrix in each voxel usually generates inaccurate results. This paper presents a new edge detector which is derived from the Partial Volume Effect (PVE). Instead of assuming continuity in the image values, edge features are extracted from the distribution of intensities within a neighborhood of each edge voxel. First, the influence of the intensities of the voxels in first- and second-order edges is analyzed to demonstrate that these types of edges can be precisely characterized from the intensity distribution. Afterward, this approach is extended to especially demanding situations by considering how adverse conditions can be tackled. This extension includes filtering noisy images, characterizing edges in blurred regions, and using windows with floating limits for close edges. The proposed technique has been tested on synthetic and real images, including some particularly difficult objects, and achieving a highly accurate subvoxel characterization of the edges. An open source implementation of our method is provided.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001785",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Economics",
      "Edge detection",
      "Enhanced Data Rates for GSM Evolution",
      "Finance",
      "Geometry",
      "Hessian matrix",
      "Image (mathematics)",
      "Image processing",
      "Mathematics",
      "Orientation (vector space)",
      "Pattern recognition (psychology)",
      "Physics",
      "Position (finance)",
      "Quantum mechanics",
      "Volume (thermodynamics)",
      "Voxel"
    ],
    "authors": [
      {
        "surname": "Trujillo-Pino",
        "given_name": "Agustín"
      },
      {
        "surname": "Alemán-Flores",
        "given_name": "Miguel"
      },
      {
        "surname": "Santana-Cedrés",
        "given_name": "Daniel"
      },
      {
        "surname": "Monzón",
        "given_name": "Nelson"
      }
    ]
  },
  {
    "title": "Underwater image enhancement method based on denoising diffusion probabilistic model",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103926",
    "abstract": "Underwater images often suffer from severe distortion, which seriously affects the image quality and the application. Current underwater image enhancement methods have poor generalization capability and cannot be adapted to all types of underwater images. In recent years, the diffusion model based on the denoising diffusion probabilistic model (DDPM) has achieved excellent results in various fields of computer vision. Inspired by the DDPM, an underwater image enhancement method based on the DDPM (UW-DDPM) was proposed in this paper. The UW-DDPM trained on paired datasets and utilized two U-Net networks to complete image denoising as well as image distribution transformation, which effectively improved the quality of underwater images. By testing on real underwater image datasets, UW-DDPM achieved better improvement in visual effects and evaluation metrics than the existing model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001761",
    "keywords": [
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Diffusion",
      "Distortion (music)",
      "Gene",
      "Generalization",
      "Geology",
      "Image (mathematics)",
      "Image denoising",
      "Image quality",
      "Mathematical analysis",
      "Mathematics",
      "Oceanography",
      "Physics",
      "Probabilistic logic",
      "Telecommunications",
      "Thermodynamics",
      "Transformation (genetics)",
      "Underwater"
    ],
    "authors": [
      {
        "surname": "Lu",
        "given_name": "Siqi"
      },
      {
        "surname": "Guan",
        "given_name": "Fengxu"
      },
      {
        "surname": "Zhang",
        "given_name": "Hanyu"
      },
      {
        "surname": "Lai",
        "given_name": "Haitao"
      }
    ]
  },
  {
    "title": "Improving semantic segmentation with knowledge reasoning network",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103923",
    "abstract": "Most methods cannot segment the semantic regions accurately due to the lack of global-level supervision or guidance of external knowledge. To overcome this limitation, we propose a Knowledge Reasoning Network (KRNet) that consists of two crucial modules: (1) a prior knowledge mapping module that incorporates external knowledge by graph convolutional network to guide learning semantic representations and (2) a knowledge reasoning module that correlates these representations with a graph built on the external knowledge and explores their interactions via the knowledge reasoning. In the prior knowledge mapping module, we adopt an algorithm to mine knowledge from an external large-scale relational modeling dataset. In the knowledge reasoning module, we adopt an iterative mechanism to perform knowledge reasoning and explore the interaction between features. Reasoning makes the spatial distribution of categories more significant. We establish state-of-the-art results on Cityscapes and ADE20K datasets, which demonstrates the effectiveness of our methods on semantic segmentation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001736",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Cognition",
      "Computer science",
      "Graph",
      "Knowledge graph",
      "Knowledge representation and reasoning",
      "Neuroscience",
      "Segmentation",
      "Semantic memory",
      "Semantic network",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Shengjia"
      },
      {
        "surname": "Yang",
        "given_name": "Xiwei"
      },
      {
        "surname": "Li",
        "given_name": "Zhixin"
      }
    ]
  },
  {
    "title": "Efficient online real-time video stabilization with a novel least squares formulation and parallel AC-RANSAC",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103922",
    "abstract": "A novel online real-time video stabilization algorithm (LSstab) that suppresses unwanted motion jitters based on cinematography principles is presented. LSstab features a parallel realization of the a-contrario RANSAC (AC-RANSAC) algorithm to estimate the inter-frame camera motion parameters. A novel least squares based smoothing cost function is then proposed to mitigate undesirable camera jitters according to cinematography principles. A recursive least square solver is derived to minimize the smoothing cost function with a linear computation complexity. LSstab is evaluated using a suite of publicly available videos against state-of-the-art video stabilization methods. Results show that LSstab achieves comparable or better performance, which attains real-time processing speed when a GPU is used.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001724",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Frame rate",
      "Image (mathematics)",
      "Programming language",
      "RANSAC",
      "Smoothing",
      "Solver"
    ],
    "authors": [
      {
        "surname": "Ke",
        "given_name": "Jianwei"
      },
      {
        "surname": "Watras",
        "given_name": "Alex J"
      },
      {
        "surname": "Kim",
        "given_name": "Jae-Jun"
      },
      {
        "surname": "Liu",
        "given_name": "Hewei"
      },
      {
        "surname": "Jiang",
        "given_name": "Hongrui"
      },
      {
        "surname": "Hu",
        "given_name": "Yu Hen"
      }
    ]
  },
  {
    "title": "Design of supervision-scalable learning systems: Methodology and performance benchmarking",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103925",
    "abstract": "The design of robust learning systems that offer stable performance under a wide range of supervision degrees is investigated in this work. We choose the image classification problem as an illustrative example and focus on the design of modularized systems that consist of three learning modules: representation learning, feature learning, and decision learning. We discuss ways to adjust each module so that the design is robust with respect to different training sample numbers. Based on these ideas, we propose two families of learning systems. One adopts the classical histogram of oriented gradients (HOG) features, while the other uses successive-subspace-learning (SSL) features. We test their performance against LeNet-5 and ResNet-18, two end-to-end optimized neural networks, for MNIST, Fashion-MNIST and CIFAR-10 datasets. The number of training samples per image class goes from the extremely weak supervision condition (i.e., one labeled sample per class) to the strong supervision condition (i.e., 4096 labeled samples per class) with a gradual transition in between (i.e., 2 n , n = 0 , 1 , … , 12 ). Experimental results show that the two families of modularized learning systems have more robust performance than LeNet-5 and ResNet-18. They both outperform the two deep learning networks by a large margin for small n and have performance comparable for large n .",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032300175X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmarking",
      "Business",
      "Chemistry",
      "Chromatography",
      "Class (philosophy)",
      "Computer science",
      "Database",
      "Deep learning",
      "Feature (linguistics)",
      "Feature learning",
      "Law",
      "Linguistics",
      "MNIST database",
      "Machine learning",
      "Margin (machine learning)",
      "Marketing",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Sample (material)",
      "Scalability",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Yijing"
      },
      {
        "surname": "Fu",
        "given_name": "Hongyu"
      },
      {
        "surname": "Kuo",
        "given_name": "C.-C. Jay"
      }
    ]
  },
  {
    "title": "Indoor dataset for Person Re-Identification: Exploring the impact of backpacks",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103931",
    "abstract": "Person Re-Identification (PR-Id) encounters misclassification issues when re-identifying persons with different backpacks. These bags manifest as large and distinct regions in the images surpassing other finer details of a person. As a result, a CNN model swiftly detects and prioritizes these image regions for re-identification. However, the bags are subject to alterations or may be similar among multiple persons, resulting in misclassification. To ensure that a CNN model does not consider bags as unique features of specific persons and prioritize them for re-identification, images of persons with diverse backpacks are crucial in the training dataset. Moreover, these images enhance the model’s focus on other unique regions of a person. Although the current datasets show potential, incorporating such images could enhance their effectiveness. Therefore, in this paper, we propose an indoor PR-Id dataset named “With Bag/Without Bag-ReID” (WB/WoB-ReID). The set “with_bag” in WB/WoB-ReID dataset includes identities with different backpacks for the first time. We also incorporate identities without bags and with varying numbers of image counts in three other sets, namely “without_bag”, “both_small,” and “both_large”. We assess WB/WoB-ReID and three other PR-Id datasets: Market1501, CUHK03, and DukeMTMC-reID on various existing approaches. The highest mAP achieved on the“with_bag” is 74%, “without_bag” is 96.7% and other datasets are 97.78%, 95.20% and 92.4%. The results show that incorporating identities with diverse bags reduces the mAP, highlighting the misclassifications that arise specifically in the presence of bags.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001815",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Computer vision",
      "Focus (optics)",
      "Identification (biology)",
      "Image (mathematics)",
      "Machine learning",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Programming language",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Singh",
        "given_name": "Divya"
      },
      {
        "surname": "Mathew",
        "given_name": "Jimson"
      },
      {
        "surname": "Agarwal",
        "given_name": "Mayank"
      },
      {
        "surname": "Govind",
        "given_name": "Mahesh"
      }
    ]
  },
  {
    "title": "Measuring dense false positive regions from segmentation result for whole slide tissue histology image",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103929",
    "abstract": "Evaluating a segmentation model is critical for constructing computer-aided diagnosis (CAD) systems, in which the segmented regions will be used for downstream analysis. However, existing segmentation evaluation metrics may not reflect every aspect of a trained segmentation model performance in the context of the whole slide tissue histology images (WSIs). Specifically, existing segmentation metrics generally ignore the impact of densely packed false positive pixels (DFP) in the WSI segmentation result. This study presents a new and efficient metric, named MAFaR, accounting for DFP regions in digital WSI segmentation. The proposed metric consists of two modules: 1) Estimation of DFP regions; 2) Calculation of MAFaR score. In module 1, a Gaussian Kernel Density Estimation method was used to estimate the density of the false positive (FP) regions in segmentation result (SR). Then a two-step Mean-shift clustering algorithm was applied to the high-density FP regions to estimate the DFP regions. In module 2, the ratio of DFP regions area to the positive regions area were used for MAFaR calculation. We conducted two experiments to evaluate the effectiveness of the MAFaR score. In the first experiment, we compared MAFaR with existing metrics related to FP regions, the proposed MAFaR score can reflect the impact of DFP regions. In the second experiment, MAFaR scores were compared with the manual evaluation scores given by three experienced engineers and found high correlation (Spearman’s rank correlation coefficients greater than 0.7) and high agreement (Kendall coefficient = 0.839). Therefore, the MAFaR can be used with other segmentation metrics for evaluating WSI segmentation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001797",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Cluster analysis",
      "Computer science",
      "Context (archaeology)",
      "Economics",
      "Image segmentation",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Pixel",
      "Region growing",
      "Scale-space segmentation",
      "Segmentation",
      "Segmentation-based object categorization",
      "Spearman's rank correlation coefficient",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Zhao"
      },
      {
        "surname": "Feng",
        "given_name": "Qianyu"
      },
      {
        "surname": "Corredor",
        "given_name": "Germán"
      },
      {
        "surname": "Koyuncu",
        "given_name": "Can"
      },
      {
        "surname": "Lu",
        "given_name": "Cheng"
      }
    ]
  },
  {
    "title": "Low-light images enhancement and denoising network based on unsupervised learning multi-stream feature modeling",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103932",
    "abstract": "Inspired by the generative adversarial networks EnlightenGAN, this paper proposes a novel low-light images Enhancement and Denoising model based on unsupervised learning Multi-Stream feature modeling (MSED). The model has two stages: generator network and discriminator network. Generator network includes global and local feature modeling network. First, Swin Transformer Block is innovatively introduced in the global feature modeling of generator network stage. It makes the interaction between the image and the convolutional kernel related to the image content. Its shift window mechanism can model the global feature dependency of the input image with less memory consumption, and extract the color, texture, and shape features of the image, thereby effectively suppressing noise and artifacts. Second, in the local feature modeling, a multi-scale image and feature fusion branch is added. It not only extracts reduced features from large-scale low-light images, but also extracts features from multiple downsampled low-light images, and then combines the two through attention and DSFF. By utilizing the complementary information of the reduced features and downsampled images, various underexposure/ overexposure phenomena caused by low-light images can be effectively avoided. In the discriminator network stage, the deep/shallow feature aggregation module is added to enhance the discrimination ability, and the inconsistencies are suppressed by learning the contradictory information of spatial filter, so that the shallow representation of information and the deep semantic information can guide each other. Thanks to the synergy of the above three innovative work, compared with many existing advanced low-light images enhancement models, MSED can achieve SOTA level performance on several public datasets. However, when dealing with low-light images with blurry content caused by rapid motion, MSED still cannot effectively restore their detailed information.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001827",
    "keywords": [
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Detector",
      "Discriminator",
      "Feature (linguistics)",
      "Feature learning",
      "Geometry",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Han",
        "given_name": "Guang"
      },
      {
        "surname": "Wang",
        "given_name": "Yingfan"
      },
      {
        "surname": "Liu",
        "given_name": "Jixin"
      },
      {
        "surname": "Zeng",
        "given_name": "Fanyu"
      }
    ]
  },
  {
    "title": "Detecting Water in Visual Image Streams from UAV with Flight Constraints",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103933",
    "abstract": "Unmanned Ariel Vehicles (UAVs) require identifying water surfaces during flight maneuvers, mainly for safety in execution and its applications. We introduce two novel techniques to identify water surfaces from front-facing and downward-facing cameras mounted on a UAV. The first method — UNet-RAU, a unique architecture based on UNet and Reflection Attention Units, segments water pixels from front-facing camera views, utilizing the reflection property of water surfaces. On the On-Road and Off-Road datasets of Puddle-1000, UNet-RAU improved its performance by 2% over the state-of-the-art FCN-RAU. Additionally, the UNet-RAU generated an F1-score of 80.97% on our Drone-Water-Front dataset. The second method — Dense Optical Flow based Water Detection (DOF-WD), detects water surfaces in videos of downward-facing cameras. This method utilizes downwash-generated ripples and natural texture features on a water surface to identify water in low and high altitudes, respectively. We empirically validated the performance of the DOF-WD method using our Drone-Water-Down dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001839",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Drone",
      "Environmental science",
      "Front (military)",
      "Genetics",
      "Geology",
      "Image (mathematics)",
      "Oceanography",
      "Optical flow",
      "Pixel",
      "Programming language",
      "Reflection (computer programming)",
      "Remote sensing"
    ],
    "authors": [
      {
        "surname": "Samaranayake",
        "given_name": "Harin"
      },
      {
        "surname": "Mudannayake",
        "given_name": "Oshan"
      },
      {
        "surname": "Perera",
        "given_name": "Dushani"
      },
      {
        "surname": "Kumarasinghe",
        "given_name": "Prabhash"
      },
      {
        "surname": "Suduwella",
        "given_name": "Chathura"
      },
      {
        "surname": "De Zoysa",
        "given_name": "Kasun"
      },
      {
        "surname": "Wimalaratne",
        "given_name": "Prasad"
      }
    ]
  },
  {
    "title": "Making depthwise convolution SR-friendly via kernel attention injection",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103930",
    "abstract": "Despite the remarkable results achieved by deep convolutional neural networks (CNNs) in single image super-resolution (SISR), their computational cost increase exponentially as CNN models get deeper and wider. Consequently, depthwise separable convolutions (DSConvs) have emerged as the fundamental building basis for various contemporary lightweight network architectures. However, depthwise convolution is sub-optimal for restoring missing high-frequency details. In this paper, we commence with the vanilla depthwise convolution and progressively develop it to alleviate the above problem. Specifically, we introduce an effective method, kernel attention injection, to integrate intra-kernel correlation into the depthwise convolution layer by incorporating the learned kernel attention into the depthwise kernel. We find that the determinant is capable of capturing the correlation between diagonal elements of the depthwise kernel by summing the products of the diagonals. Therefore, we utilize the determinant as the kernel pooling method. Furthermore, we propose to remove the non-linear activation function behind the depthwise convolution (i.e., linear depthwise convolution) to preserve informative features for reconstructing faithful high-resolution (HR) images. Built on this recipe, we introduce kernel-attentive linear depthwise separable convolutions (KDSConvs), which exhibits promising super-resolution performance. Our experimental results underline the potential of depthwise convolution in super-resolution tasks by demonstrating that the proposed KDSConvs significantly outperforms DSConvs in terms of both quantitative measurements and visual quality. The code will be released at https://github.com/AwesomeHwang/KALDN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001803",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Diagonal",
      "Discrete mathematics",
      "Geometry",
      "Kernel (algebra)",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pooling",
      "Separable space"
    ],
    "authors": [
      {
        "surname": "Hwang",
        "given_name": "Seongmin"
      },
      {
        "surname": "Han",
        "given_name": "Daeyoung"
      },
      {
        "surname": "Jeon",
        "given_name": "Moongu"
      }
    ]
  },
  {
    "title": "RDD-net: Robust duplicated-diffusion watermarking based on deep network",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103934",
    "abstract": "This paper proposes a novel robust duplicated-diffusion watermarking model based on deep network (RDD-net). RDD-net employs the encoder-noise-decoder structure for end-to-end training to obtain high image watermarking invisibility and robustness. Firstly, a duplicated-diffusion strategy is employed in the encoder to replicate the original watermark iteratively until all copies are diffused to the whole image so that the generalized ability in resisting various noises is obtained. Then, a channel connection technique is designed to extract inherent image features for fusing watermark for robustness by mining correlations of RGB three channels of the color image. Meanwhile, each channel is fused with watermark to increase robustness. Another attempt to improve the watermarking performance is the optimizer, which optimizes the watermark distribution by evaluating the similarities between the encoded image and the original image, as well as between the encoded image and the noised image. Our extensive experimental results demonstrate that the proposed RDD-net not only resists different noises, but also obtains better image quality and higher robustness than the existing watermarking models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001840",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Diffusion",
      "Digital watermarking",
      "Geometry",
      "Image (mathematics)",
      "Mathematics",
      "Net (polyhedron)",
      "Physics",
      "Thermodynamics"
    ],
    "authors": [
      {
        "surname": "Jiang",
        "given_name": "Guowei"
      },
      {
        "surname": "He",
        "given_name": "Zhouyan"
      },
      {
        "surname": "Huang",
        "given_name": "Jiangtao"
      },
      {
        "surname": "Luo",
        "given_name": "Ting"
      },
      {
        "surname": "Xu",
        "given_name": "Haiyong"
      },
      {
        "surname": "Jin",
        "given_name": "Chongchong"
      }
    ]
  },
  {
    "title": "A dual-task region-boundary aware neural network for accurate pulmonary nodule segmentation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103909",
    "abstract": "Recently, the performance of pulmonary nodule segmentation has improved fast because of the development of deep learning-based methods. However, existing CNN-based methods still don’t solve the two issues, leading to poor segmentation: (1) The information of small nodules lost in continuous down-sampling operation; (2) Discriminative features can’t be extracted from non-solid nodules because whose gray values are close to the surrounding environment. This study proposes a novel dual-task region-boundary aware deep convolutional neural network to solve the problems above. A hierarchical feature module is proposed to capture multi-scale information, improving small nodules' segmentation accuracy. Boundary guided module is introduced to extract edge features and predict nodule boundaries directly. In addition, a feature aggregation module is proposed to aggregate features at different levels. Besides, to better optimize the segment results and reduce overfitting, the region-boundary aware loss function is proposed. The method is trained and tested on the LIDC-IDRI dataset and the LUNA16 dataset, and the results demonstrate that our method is efficient. More importantly, our method is suitable for the segmentation of various nodules, especially small nodules, GGO nodules, and part-solid nodules.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001591",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Discriminative model",
      "Feature (linguistics)",
      "Linguistics",
      "Nodule (geology)",
      "Overfitting",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Qiu",
        "given_name": "Junrong"
      },
      {
        "surname": "Li",
        "given_name": "Bin"
      },
      {
        "surname": "Liao",
        "given_name": "Riqiang"
      },
      {
        "surname": "Mo",
        "given_name": "Hongqiang"
      },
      {
        "surname": "Tian",
        "given_name": "Lianfang"
      }
    ]
  },
  {
    "title": "Class semantic enhancement network for semantic segmentation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103924",
    "abstract": "Existing semantic segmentation methods favor class semantic consistency by extracting long-range contextual features through multi-scale and attention strategies. These methods ignore the relations between feature channels and classes, which are essential to represent consistent class semantics. To this end, we propose the Class Semantic Enhancement Network (CSENet) which boosts the segmentation performance of a backbone network in a coarse-to-fine manner. CSENet consists of two basic modules – (1) the Class Semantic Channel Graph Module (CSCG) module, which captures inter-dependencies among channels and strengthens the channel-class relation, and (2) the Class Prior Fully Convolution (CP-FC) module, which utilizes the channel-class relation as class priors to refine the segmentation results. Extensive experiments have demonstrated that the proposed CSENet is able to learn discriminative feature representations and achieves state-of-the-art performance on three benchmark datasets, including PASCAL Context, ADE20K and COCO Stuff.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001748",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Discriminative model",
      "Feature (linguistics)",
      "Linguistics",
      "Natural language processing",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Segmentation",
      "Semantics (computer science)"
    ],
    "authors": [
      {
        "surname": "Fu",
        "given_name": "Siming"
      },
      {
        "surname": "Wang",
        "given_name": "Hualiang"
      },
      {
        "surname": "Hu",
        "given_name": "Haoji"
      },
      {
        "surname": "He",
        "given_name": "Xiaoxuan"
      },
      {
        "surname": "Long",
        "given_name": "Yongwen"
      },
      {
        "surname": "Bai",
        "given_name": "Jianhong"
      },
      {
        "surname": "Ou",
        "given_name": "Yangtao"
      },
      {
        "surname": "Huang",
        "given_name": "Yuanjia"
      },
      {
        "surname": "Zhou",
        "given_name": "Mengqiu"
      }
    ]
  },
  {
    "title": "TASTNet: An end-to-end deep fingerprinting net with two-dimensional attention mechanism and spatio-temporal weighted fusion for video content authentication",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103913",
    "abstract": "In this paper, a robust fingerprinting scheme for video content authentication with two-dimensional attention mechanism and spatio-temporal weighted fusion called TASTNet is proposed, which can automatically extracts key spatio-temporal features from the input video and maps them to the corresponding fingerprint. Detailedly, the two-dimensional attention mechanism is applied to resist different kinds of digital manipulations for robustness enhancement. To incorporate perceptual characteristics, a spatio-temporal weighted fusion method based on LTSM is presented to integrate frame-level features into video-level features while retaining the temporal order. In the process of fusion, key frames are allocated with larger weights according to inter-frame correlation. With these two steps, we can obtain representative video features that contain principal perception information. In addition, the proposed scheme utilizes deep metric learning for training, and we design multiple constraints to make the generated fingerprint more compact and discriminable. Extensive experiments demonstrate that our scheme can achieve superior performances with respect to robustness and discrimination compared with some state-of-the-art schemes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001633",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Economics",
      "Fingerprint (computing)",
      "Frame (networking)",
      "Fusion",
      "Fusion mechanism",
      "Gene",
      "Key (lock)",
      "Linguistics",
      "Lipid bilayer fusion",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Robustness (evolution)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Gejian"
      },
      {
        "surname": "Li",
        "given_name": "Fengyong"
      },
      {
        "surname": "Yao",
        "given_name": "Heng"
      },
      {
        "surname": "Qin",
        "given_name": "Chuan"
      }
    ]
  },
  {
    "title": "Iterative graph filtering network for 3D human pose estimation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103908",
    "abstract": "Graph convolutional networks (GCNs) have proven to be an effective approach for 3D human pose estimation. By naturally modeling the skeleton structure of the human body as a graph, GCNs are able to capture the spatial relationships between joints and learn an efficient representation of the underlying pose. However, most GCN-based methods use a shared weight matrix, making it challenging to accurately capture the different and complex relationships between joints. In this paper, we introduce an iterative graph filtering framework for 3D human pose estimation, which aims to predict the 3D joint positions given a set of 2D joint locations in images. Our approach builds upon the idea of iteratively solving graph filtering with Laplacian regularization via the Gauss–Seidel iterative method. Motivated by this iterative solution, we design a Gauss–Seidel network (GS-Net) architecture, which makes use of weight and adjacency modulation, skip connection, and a pure convolutional block with layer normalization. Adjacency modulation facilitates the learning of edges that go beyond the inherent connections of body joints, resulting in an adjusted graph structure that reflects the human skeleton, while skip connections help maintain crucial information from the input layer’s initial features as the network depth increases. We evaluate our proposed model on two standard benchmark datasets, and compare it with a comprehensive set of strong baseline methods for 3D human pose estimation. Our experimental results demonstrate that our approach outperforms the baseline methods on both datasets, achieving state-of-the-art performance. Furthermore, we conduct ablation studies to analyze the contributions of different components of our model architecture and show that the skip connection and adjacency modulation help improve the model performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032300158X",
    "keywords": [
      "Adjacency list",
      "Adjacency matrix",
      "Algorithm",
      "Anthropology",
      "Artificial intelligence",
      "Computer science",
      "Graph",
      "Laplacian matrix",
      "Normalization (sociology)",
      "Pattern recognition (psychology)",
      "Pose",
      "Sociology",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Islam",
        "given_name": "Zaedul"
      },
      {
        "surname": "Hamza",
        "given_name": "A. Ben"
      }
    ]
  },
  {
    "title": "Boosting separated softmax with discrimination for class incremental learning",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103899",
    "abstract": "Deep neural networks (DNNs) suffer from catastrophic forgetting when learning new classes continually and tend to classify old samples to new classes. Existing methods like exemplar-memory and knowledge distillation alleviate forgetting, but face prediction bias due to data imbalance. Separated softmax (Ahn et al., 2021) was proposed to solve this problem. However, they ignore the discrimination between old classes and new classes, which greatly limits the performance of DNNs. To enhance model’s discrimination, we propose discriminative separated softmax. We divide new samples into two parts: one combined with old samples to build a balanced dataset of all seen classes, and the other combined with old samples as an imbalanced dataset. Furthermore, we apply mixup for these two datasets respectively to enhance discrimination. We evaluate our method on CIFAR100 and ImageNet100. Experimental results show that our method can effectively improve the discrimination and achieves superior performance compared with separated softmax, while outperforming state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001499",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Boosting (machine learning)",
      "Cognitive psychology",
      "Computer science",
      "Discriminative model",
      "Forgetting",
      "Incremental learning",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Psychology",
      "Softmax function"
    ],
    "authors": [
      {
        "surname": "Shi",
        "given_name": "Lei"
      },
      {
        "surname": "Zhao",
        "given_name": "Kai"
      },
      {
        "surname": "Fu",
        "given_name": "Zhenyong"
      }
    ]
  },
  {
    "title": "Learning-based JNCD prediction for quality-wise perceptual quantization in HEVC",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103877",
    "abstract": "In visual perception, human only perceive discrete-scale quality levels over a wide range of coding bitrate. More clearly, the videos compressed with a series of quantization parameters (QPs) only have limited perceived quality levels. In this paper, perceptual quantization is transformed into the problem of how to determine the just perceived QP for each quality level, and a just noticeable coding distortion (JNCD) based perceptual quantization scheme is proposed. Specifically, multiple visual masking effects are analyzed and a linear regression (LR) based JNCD model is proposed to predict JNCD thresholds for all quality levels at first. According to the JNCD prediction model, the frame-level perceptual QPs for all quality levels are then derived on the premise of that coding distortions are infinitely close to the predicted JNCD thresholds. Based on the predicted frame-level perceptual QPs, the perceived QPs of all quality levels for each coding unit (CU) are finally determined according to a perceptual modulation function. Experimental results show that the proposed quality-wise perceptual quantization scheme is superior to the existing perceptual video coding algorithms significantly, i.e., the proposed perceptual quantization could save more bitrate with better quality.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032300127X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Coding (social sciences)",
      "Computer science",
      "Computer vision",
      "Mathematics",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Perception",
      "Psychology",
      "Quantization (signal processing)",
      "Speech recognition",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Hongkui"
      },
      {
        "surname": "Zhang",
        "given_name": "Lianmin"
      },
      {
        "surname": "Yu",
        "given_name": "Li"
      },
      {
        "surname": "Yang",
        "given_name": "Hailang"
      },
      {
        "surname": "Yin",
        "given_name": "Haibing"
      },
      {
        "surname": "Ding",
        "given_name": "Sitong"
      },
      {
        "surname": "Xu",
        "given_name": "Haifeng"
      },
      {
        "surname": "Wang",
        "given_name": "He"
      }
    ]
  },
  {
    "title": "Fast Non-Local Attention network for light super-resolution",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103861",
    "abstract": "Although convolutional neural network-based methods have achieved significant performance improvement for Single Image Super-Resolution (SISR), their vast computational cost hinders real-world environment application. Thus, the interest in light networks for SISR is rising. Since existing SISR light models mainly focus on extracting fine local features using convolution operation, they have a limitation in that networks hardly capture global information. To capture the long-range dependency, Non-Local (NL) attention and Transformers have been explored in the SISR task. However, they are still suffering from a balancing problem between performance and computational cost. In this paper, we propose Fast Non-Local attention NETwork (FNLNET) for a super light SISR, which can capture the global representation. To acquire global information, we propose The Fast Non-Local Attention (FNLA) module that has low computational complexity while capturing global representation that reflects long-distance relationships between patches. Then, FNLA requires only 16 times lower computational cost than conventional NL networks while improving performance. In addition, we propose a powerful module called Global Self-Intension Mining (GSIM) that fuses the multi-information resources such as local, and global representation. Our FNLNET shows outstanding performance with fewer parameters and computational costs in the experiments on the benchmark datasets against state-of-the-art light SISR models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001116",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Attention network",
      "Benchmark (surveying)",
      "Computational complexity theory",
      "Computer science",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Data mining",
      "Geodesy",
      "Geography",
      "Law",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Hong",
        "given_name": "Jonghwan"
      },
      {
        "surname": "Lee",
        "given_name": "Bokyeung"
      },
      {
        "surname": "Ko",
        "given_name": "Kyungdeuk"
      },
      {
        "surname": "Ko",
        "given_name": "Hanseok"
      }
    ]
  },
  {
    "title": "A no-reference panoramic image quality assessment with hierarchical perception and color features",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103885",
    "abstract": "The ultimate goal of no-reference omnidirectional image quality assessment (NR-OIQA) is to design a comprehensive perception method that can accurately assess the quality of damaged omnidirectional images without prior knowledge. However, most existing studies cannot attain credible accuracy because of lacking a non-neuroscience-based or non-biology-based model. Inspired by this, an original visual perception-based and neuroscience-based OIQA model by considering the hierarchical perception features of the HVS, which includes specific information, local saliency information, global information, and color information which is often ignored by researchers is proposed in this work. According to the hierarchical process in neuroscience, the high-frequency co-occurrence matrix (HFCM)-based and variance-based specific features are applied to perceive details that are first distorted in the frequency domain. The entropy-based combination of the paranormal saliency map(PSM) and superpixel segmentation with the simple linear iterative clustering (SLIC) algorithm is employed to emphasize the rich quality-aware local saliency information. The global panoramic statistical (GPS) model is utilized to express global semantics distortion as the high-level feature. Visual-aware color texture descriptor with the cross-channel local binary pattern(CCLBP) which effectively reflects the correlation and dependency of pixels between different color channels is employed to map color information. Finally, all above features have been extracted and combined with subjective scores to assess the objective quality scores by support vector regression (SVR). Experiments express that our method has more accuracy and stronger stability on CVIQD2018 and OIQA databases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001359",
    "keywords": [
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Distortion (music)",
      "Entropy (arrow of time)",
      "Pattern recognition (psychology)",
      "Physics",
      "Pixel",
      "Quantum mechanics",
      "Support vector machine",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Yun"
      },
      {
        "surname": "Yin",
        "given_name": "Xiaohua"
      },
      {
        "surname": "Tang",
        "given_name": "Chang"
      },
      {
        "surname": "Yue",
        "given_name": "Guanghui"
      },
      {
        "surname": "Wang",
        "given_name": "Yan"
      }
    ]
  },
  {
    "title": "Exploring the potential of Siamese network for RGBT object tracking",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103882",
    "abstract": "Siamese tracking is one of the most promising object tracking methods today due to its balance of performance and speed. However, it still performs poorly when faced with some challenges such as low light or extreme weather. This is caused by the inherent limitations of visible images, and a common way to cope with it is to introduce infrared data as an aid to improve the robustness of tracking. However, most of the existing RGBT trackers are variants of MDNet (Hyeonseob Nam and Bohyung Han, Learning multi-domain convolutional neural networks for visual tracking, in: Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 4293–4302.), which have significant limitations in terms of operational efficiency. On the contrary, the potential of Siamese tracking in the field of RGBT tracking has not been effectively exploited due to the reliance on large-scale training data. To solve this dilemma, in this paper, we propose an end-to-end Siamese RGBT tracking framework that is based on cross-modal feature enhancement and self-attention (SiamFEA). We draw on the idea of migration learning and employ local fine-tuning to reduce the dependence on large-scale RGBT data and verify the feasibility of this approach, and then we propose a reliable fusion approach to efficiently fuse the features of different modalities. Specifically, we first propose a cross-modal feature enhancement module to exploit the complementary properties of dual-modality, followed by capturing non-local attention in channel and spatial dimensions for adaptive weighted fusion, respectively. Our network was trained end-to-end on the LasHeR (Chenglong Li, Wanlin Xue, Yaqing Jia, Zhichen Qu, Bin Luo, Jin Tang, LasHeR: A Large-scale High-diversity Benchmark for RGBT Tracking, CoRR abs/2104.13202, 2021) training set and reached new SOTAs on GTOT (C. Li, H. Cheng, S. Hu, X. Liu, J. Tang, L. Lin, Learning collaborative sparse representation for grayscale-thermal tracking, IEEE Trans. Image Process, 25 (12) (2016) 5743–5756.), RGBT234 (C. Li, X. Liang, Y. Lu, N. Zhao, and J. Tang, “Rgb-t object tracking: Benchmark and baseline,” Pattern Recognition, vol. 96, p. 106977, 2019.), and LasHeR (Chenglong Li, Wanlin Xue, Yaqing Jia, Zhichen Qu, Bin Luo, Jin Tang, LasHeR: A Large-scale High-diversity Benchmark for RGBT Tracking, CoRR abs/2104.13202, 2021) while running in real-time.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001323",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "BitTorrent tracker",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Electrical engineering",
      "Engineering",
      "Exploit",
      "Eye tracking",
      "Feature (linguistics)",
      "Fuse (electrical)",
      "Gene",
      "Linguistics",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Robustness (evolution)",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Feng",
        "given_name": "Liangliang"
      },
      {
        "surname": "Song",
        "given_name": "Kechen"
      },
      {
        "surname": "Wang",
        "given_name": "Junyi"
      },
      {
        "surname": "Yan",
        "given_name": "Yunhui"
      }
    ]
  },
  {
    "title": "Depth cue enhancement and guidance network for RGB-D salient object detection",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103880",
    "abstract": "Depth maps have been proven profitable to provide supplements for salient object detection in recent years. However, most RGB-D salient object detection approaches ignore that there are usually low-quality depth maps, which will inevitably result in unsatisfactory results. In this paper, we propose a depth cue enhancement and guidance network (DEGNet) for RGB-D salient object detection by exploring the depth quality enhancement and utilizing the depth cue guidance to generate predictions with highlighted objects and suppressed backgrounds. Specifically, a depth cue enhancement module is designed to generate high-quality depth maps by enhancing the contrast between the foreground and the background. Then considering the different characteristics of unimodal RGB and depth features, we use different feature enhancement strategies to strengthen the representation capability of side-output unimodal features. Moreover, we propose a depth-guided feature fusion module to excavate depth cues provided by the depth stream to guide the fusion of multi-modal features by fully making use of different modal properties, thus generating discriminative cross-modal features. Besides, we aggregate cross-modal features at different levels to obtain the final prediction by adopting a pyramid feature shrinking structure. Experimental results on six benchmark datasets demonstrate that the proposed network DEGNet outperforms 17 state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032300130X",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Contrast (vision)",
      "Depth map",
      "Discriminative model",
      "Feature (linguistics)",
      "Geodesy",
      "Geography",
      "Geometry",
      "Image (mathematics)",
      "Linguistics",
      "Mathematics",
      "Modal",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Polymer chemistry",
      "Pyramid (geometry)",
      "RGB color model",
      "Salient"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Xiang"
      },
      {
        "surname": "Zhang",
        "given_name": "Qing"
      },
      {
        "surname": "Yan",
        "given_name": "Weiqi"
      },
      {
        "surname": "Dai",
        "given_name": "Meng"
      }
    ]
  },
  {
    "title": "Transforming spatio-temporal self-attention using action embedding for skeleton-based action recognition",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103892",
    "abstract": "Over the past few years, skeleton-based action recognition has attracted great success because the skeleton data is immune to illumination variation, view-point variation, background clutter, scaling, and camera motion. However, effective modeling of the latent information of skeleton data is still a challenging problem. Therefore, in this paper, we propose a novel idea of action embedding with a self-attention Transformer network for skeleton-based action recognition. Our proposed technology mainly comprises of two modules as, (i) action embedding and (ii) self-attention Transformer. The action embedding encodes the relationship between corresponding body joints (e.g., joints of both hands move together for performing clapping action) and thus captures the spatial features of joints. Meanwhile, temporal features and dependencies of body joints are modeled using Transformer architecture. Our method works in a single-stream (end-to-end) fashion, where multiple-layer perceptron (MLP) is used for classification. We carry out an ablation study and evaluate the performance of our model on a small-scale SYSU-3D dataset and large-scale NTU-RGB+D and NTU-RGB+D 120 datasets where the results establish that our method performs better than other state-of-the-art architectures.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001426",
    "keywords": [
      "Action recognition",
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Computer vision",
      "Embedding",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "RGB color model",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Ahmad",
        "given_name": "Tasweer"
      },
      {
        "surname": "Rizvi",
        "given_name": "Syed Tahir Hussain"
      },
      {
        "surname": "Kanwal",
        "given_name": "Neel"
      }
    ]
  },
  {
    "title": "MTMVC: Semi-supervised 3D hand pose estimation using multi-task and multi-view consistency",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103902",
    "abstract": "The high performance of state-of-the-art deep learning methods for 3D hand pose estimation heavily depends on a large annotated training set. However, it is difficult and time-consuming to obtain the annotations for 3D hand poses. To leverage unannotated images to reduce the annotation cost, we propose a semi-supervised method based on Multi-Task and Multi-View Consistency (MTMVC) for hand pose estimation. First, we obtain the joints based on heatmap prediction and coordinate regression parallelly and encourage their consistency. Second, we introduce multi-view consistency to encourage the predicted poses to be rotation-invariant. Thirdly, to make the network pay more attention to the hand region, we propose a spatially weighted consistency. Experiments on four public datasets showed that our proposed MTMVC outperformed existing semi-supervised hand pose estimation methods, and by only using half of the annotations, the accuracy of our method was comparable to those of several state-of-the-art fully supervised methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001529",
    "keywords": [
      "Annotation",
      "Artificial intelligence",
      "Computer science",
      "Consistency (knowledge bases)",
      "Economics",
      "Leverage (statistics)",
      "Machine learning",
      "Management",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pose",
      "Programming language",
      "Regression",
      "Set (abstract data type)",
      "Statistics",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Xiang",
        "given_name": "Donghai"
      },
      {
        "surname": "Xu",
        "given_name": "Wei"
      },
      {
        "surname": "Zhang",
        "given_name": "Yuting"
      },
      {
        "surname": "Peng",
        "given_name": "Bei"
      },
      {
        "surname": "Wang",
        "given_name": "Guotai"
      },
      {
        "surname": "Li",
        "given_name": "Kang"
      }
    ]
  },
  {
    "title": "Event-guided low light image enhancement via a dual branch GAN",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103887",
    "abstract": "In the low light conditions, images are corrupted by low contrast and severe noise, but event cameras capture event streams with clear edge structures. Therefore, we propose an Event-Guided Low Light Image Enhancement method using a dual branch generative adversarial networks and recover clear structure with the guide of events. To overcome the lack of paired training datasets, we first synthesize three datasets containing low-light event streams, low-light images, and the ground truth normal-light images. Then, in the generator network, we develop an end-to-end dual branch network consisting of a image enhancement branch and a gradient reconstruction branch. The image enhancement branch is employed to enhance the low light images, and the gradient reconstruction branch is utilized to learn the gradient from events. Moreover, we develops the attention based event-image feature fusion module which selectively fuses the event and low-light image features, and the fused features are concatenated into the image enhancement branch and gradient reconstruction branch, which respectively generate the enhanced images with clear structure and more accurate gradient images. Extensive experiments on synthetic and real datasets demonstrate that the proposed event guided low light image enhancement method produces visually more appealing enhancement images, and achieves a good performance in structure preservation and denoising over state-of-the-arts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001372",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Event (particle physics)",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Jin",
        "given_name": "Haiyan"
      },
      {
        "surname": "Wang",
        "given_name": "Qiaobin"
      },
      {
        "surname": "Su",
        "given_name": "Haonan"
      },
      {
        "surname": "Xiao",
        "given_name": "Zhaolin"
      }
    ]
  },
  {
    "title": "CCA-FPN: Channel and content adaptive object detection",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103903",
    "abstract": "Feature pyramid network (FPN) is a typical detector commonly for solving the issue of object detection at different scales. However, the lateral connections in FPN lead to the loss of feature information due to the reduction of feature channels. Moreover, the top-down feature fusion will weaken the feature representation in the process of feature delivery because of features with different semantic information. In this paper, we propose a feature pyramid network with channel and content adaptive feature enhancement module (CCA-FPN), which uses a channel adaptive guided mechanism module (CAGM) and multi-scale content adaptive feature enhancement module (MCAFEM) to alleviate these problems. We conduct comprehensive experiments on the MS COCO dataset. By replacing FPN with CCA-FPN in ATSS, our models achieve 1.3 percentage points higher Average Precision (AP) when using ResNet50 as backbone. Furthermore, our CCA-FPN achieves 0.3 percentage points higher than the AugFPN which is the state-of-the-art FPN-based detector.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001530",
    "keywords": [
      "Artificial intelligence",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Detector",
      "Feature (linguistics)",
      "Geometry",
      "Law",
      "Linguistics",
      "Mathematics",
      "Object (grammar)",
      "Object detection",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Process (computing)",
      "Pyramid (geometry)",
      "Representation (politics)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Ye",
        "given_name": "Zhiyang"
      },
      {
        "surname": "Lan",
        "given_name": "Chengdong"
      },
      {
        "surname": "Zou",
        "given_name": "Min"
      },
      {
        "surname": "Qiu",
        "given_name": "Xu"
      },
      {
        "surname": "Chen",
        "given_name": "Jian"
      }
    ]
  },
  {
    "title": "CCA-FPN: Channel and content adaptive object detection",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103903",
    "abstract": "Feature pyramid network (FPN) is a typical detector commonly for solving the issue of object detection at different scales. However, the lateral connections in FPN lead to the loss of feature information due to the reduction of feature channels. Moreover, the top-down feature fusion will weaken the feature representation in the process of feature delivery because of features with different semantic information. In this paper, we propose a feature pyramid network with channel and content adaptive feature enhancement module (CCA-FPN), which uses a channel adaptive guided mechanism module (CAGM) and multi-scale content adaptive feature enhancement module (MCAFEM) to alleviate these problems. We conduct comprehensive experiments on the MS COCO dataset. By replacing FPN with CCA-FPN in ATSS, our models achieve 1.3 percentage points higher Average Precision (AP) when using ResNet50 as backbone. Furthermore, our CCA-FPN achieves 0.3 percentage points higher than the AugFPN which is the state-of-the-art FPN-based detector.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001530",
    "keywords": [
      "Artificial intelligence",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Detector",
      "Feature (linguistics)",
      "Geometry",
      "Law",
      "Linguistics",
      "Mathematics",
      "Object (grammar)",
      "Object detection",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Process (computing)",
      "Pyramid (geometry)",
      "Representation (politics)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Ye",
        "given_name": "Zhiyang"
      },
      {
        "surname": "Lan",
        "given_name": "Chengdong"
      },
      {
        "surname": "Zou",
        "given_name": "Min"
      },
      {
        "surname": "Qiu",
        "given_name": "Xu"
      },
      {
        "surname": "Chen",
        "given_name": "Jian"
      }
    ]
  },
  {
    "title": "Dual cross knowledge distillation for image super-resolution",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103858",
    "abstract": "The huge computational requirements and memory footprint limit the practical deployment of super resolution (SR) models. Knowledge distillation (KD) allows student networks to obtain performance improvement by learning from over-parameterized teacher networks. Previous work has attempted to solve SR distillation problem by using feature-based distillation, which ignores the supervisory role of the teacher module itself. In this paper, we introduce a cross knowledge distillation framework to compress and accelerate SR models. Specifically, we propose to obtain supervision by cascading the student into the teacher network for directly utilizing teacher’s well-trained parameters. This not only reduces the difficulty of optimization for students but also avoids designing alignment with obscure feature textures between two networks. To the best of our knowledge, we are the first work to explore the cross distillation paradigm on the SR tasks. Experiments on typical SR networks have shown the superiority of our method in generated images, PSNR and SSIM.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001086",
    "keywords": [
      "Algorithm",
      "Art",
      "Artificial intelligence",
      "Biology",
      "Chemistry",
      "Computer engineering",
      "Computer science",
      "Distillation",
      "Dual (grammatical number)",
      "Engineering",
      "Feature (linguistics)",
      "Footprint",
      "Image (mathematics)",
      "Limit (mathematics)",
      "Linguistics",
      "Literature",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Mechanical engineering",
      "Memory footprint",
      "Organic chemistry",
      "Paleontology",
      "Parameterized complexity",
      "Philosophy",
      "Programming language",
      "Resolution (logic)",
      "Software deployment",
      "Software engineering",
      "Work (physics)"
    ],
    "authors": [
      {
        "surname": "Fang",
        "given_name": "Hangxiang"
      },
      {
        "surname": "Long",
        "given_name": "Yongwen"
      },
      {
        "surname": "Hu",
        "given_name": "Xinyi"
      },
      {
        "surname": "Ou",
        "given_name": "Yangtao"
      },
      {
        "surname": "Huang",
        "given_name": "Yuanjia"
      },
      {
        "surname": "Hu",
        "given_name": "Haoji"
      }
    ]
  },
  {
    "title": "Point cloud-based scene flow estimation on realistically deformable objects: A benchmark of deep learning-based methods",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103893",
    "abstract": "Flow estimation on 3D point clouds is a challenging problem in the field of computer vision, which has great significance in many areas, such as autonomous driving and human interaction applications. Within the last years, the field of motion analysis has made great progress. The evaluation of the existing approaches mostly focuses on scenarios where objects are affected by rigid transformations. However, in many application areas such as gesture recognition or pose tracking, the detection of shape changes is essential and breaking them down to local rigid transformations is accompanied by loss of information. One component of our contributions is that we specifically prepared existing datasets for scene flow estimation on deformable objects. Additionally, we benchmark existing methods and analyze their behavior on various subtasks. The results show that already close to 80% of correct correspondences can be found on synthetic hand data, while only around 50% are found on real hand data. Our experimental validation and analysis help to build an understanding of new possibilities in broader areas. Furthermore, they should help to inspire possible further research directions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001438",
    "keywords": [
      "Artificial intelligence",
      "Astrophysics",
      "Benchmark (surveying)",
      "Component (thermodynamics)",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Field (mathematics)",
      "Flow (mathematics)",
      "Geodesy",
      "Geography",
      "Geometry",
      "Gesture",
      "Image (mathematics)",
      "Machine learning",
      "Mathematics",
      "Optical flow",
      "Pedagogy",
      "Physics",
      "Point (geometry)",
      "Point cloud",
      "Pose",
      "Psychology",
      "Pure mathematics",
      "Thermodynamics",
      "Tracking (education)",
      "Variation (astronomy)"
    ],
    "authors": [
      {
        "surname": "Hermes",
        "given_name": "Niklas"
      },
      {
        "surname": "Bigalke",
        "given_name": "Alexander"
      },
      {
        "surname": "Heinrich",
        "given_name": "Mattias P."
      }
    ]
  },
  {
    "title": "A Dual-Decoding branch U-shaped semantic segmentation network combining Transformer attention with Decoder: DBUNet",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103856",
    "abstract": "Semantic Segmentation is an extremely important medical image auxiliary analysis method. However, existing networks have the following problems: 1) The amount of feature information of Encoder and Decoder is not equal under multi-branch architecture; 2) The direct processing of the original image by ViT Encoder is not sufficient; 3) Multi-channel features are too independent and lack of fusion. Combined with the ViT Encoder framework, this study proposes a 'Single Encoder – Double Decoder' structure: DBUNet. Firstly, ViT Encoder is employed as a part of the Decoder branches to enhance the shallow features. Then, a polarization amplification of channel weights is proposed and placed in front of the ViT Encoder module to achieve early image processing. Finally, a Bottleneck for feature fusion is proposed to solve the problem of channel independence. The comprehensive verification of 13 comparative networks in three aspects, combined with ablation experiments, jointly proves the superiority of DBUNet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001062",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Decoding methods",
      "Encoder",
      "Feature (linguistics)",
      "Linguistics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yuefei"
      },
      {
        "surname": "Yu",
        "given_name": "Xi"
      },
      {
        "surname": "Guo",
        "given_name": "Xiaoyan"
      },
      {
        "surname": "Wang",
        "given_name": "Xilei"
      },
      {
        "surname": "Wei",
        "given_name": "Yuanhong"
      },
      {
        "surname": "Zeng",
        "given_name": "Shijie"
      }
    ]
  },
  {
    "title": "Wavelet-FCWAN: Fast and Covert Watermarking Attack Network in Wavelet Domain",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103875",
    "abstract": "The protection of digital image content has become an important topic of scientific research with the continuous development and updates of Internet and multimedia technology. In the past few years, various watermarking algorithms with good robustness and imperceptibility have been proposed, but the development of watermarking attack techniques has stagnated. In this paper, we have attempted to use deep learning to develop a new watermarking attack scheme and present the Fast and Covert Watermarking Attack Network in Wavelet Domain (Wavelet-FCWAN). The watermarking attack scheme employs noise filling as a preprocessing step for the watermarked image, performs wavelet transform operation on the preprocessed watermarked image, and inputs the wavelet transformed sub-image into Wavelet-FCWAN along with the noise level map in parallel. The network can be trained quickly and produces a better watermarking attack effect while ensuring the visual quality and detail retention of the attacked image. The experiments show that Wavelet-FCWAN demonstrates the superiority of its watermarking attack effect by comparing different attack strategies, and it can produce varying degrees of attack effects on various image watermarking algorithms with high levels of universality and imperceptibility.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001256",
    "keywords": [
      "Artificial intelligence",
      "Attribute-based encryption",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Digital watermarking",
      "Discrete wavelet transform",
      "Encryption",
      "Gene",
      "Image (mathematics)",
      "Lifting scheme",
      "Pattern recognition (psychology)",
      "Public-key cryptography",
      "Robustness (evolution)",
      "Second-generation wavelet transform",
      "Stationary wavelet transform",
      "Watermarking attack",
      "Wavelet",
      "Wavelet packet decomposition",
      "Wavelet transform"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Chunpeng"
      },
      {
        "surname": "Sun",
        "given_name": "Fanran"
      },
      {
        "surname": "Xia",
        "given_name": "Zhiqiu"
      },
      {
        "surname": "Li",
        "given_name": "Qi"
      },
      {
        "surname": "Li",
        "given_name": "Jian"
      },
      {
        "surname": "Han",
        "given_name": "Bing"
      },
      {
        "surname": "Ma",
        "given_name": "Bin"
      }
    ]
  },
  {
    "title": "DB-TASNet for disease diagnosis and lesion segmentation in medical images",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103896",
    "abstract": "Deep learning algorithms have been successfully used in the field of medical image analysis and have greatly improved application of intelligent algorithms to medical diagnosis. However, existing deep-learning-based diagnostic methods still suffer from several drawbacks: (1) In most medical image multi-tasking methods, focus segmentation and disease classification are often performed linearly, resulting in excessive reliance on the final results of focus segmentation. (2) The computational cost of the traditional attention mechanism for performing the segmentation task is very high and the convolutional architecture cannot be used to model long-distance dependencies, which in turn affects the segmentation accuracy. To address these issues, we propose a disease diagnosis and lesion segmentation model, Dual-Branch with Transformer Axial-attention Segmentation Net (DB-TASNet). DB-TASNet is built by the DenseNet-121 classification network and U-Net segmentation network improved using an axial-attention transformer model. Moreover, DB-TASNet also includes a lesion integration module to integrate segmentation results with the classification network in order to increase its attention to lesions and improve the diagnosis results. Experimental results on the Pneumothorax dataset provided by the Society for Imaging Informatics in Medicine (SIIM) show that the average AUC of the DB-TASNet classification task reaches 0.939, and the DICE coefficient of the segmentation task reaches 0.886. Such performance suggests that the proposed model may provide an efficient and effective diagnosis tool for medical personnel.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001463",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Image segmentation",
      "Machine learning",
      "Medical imaging",
      "Pattern recognition (psychology)",
      "Scale-space segmentation",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Yuqing"
      },
      {
        "surname": "He",
        "given_name": "Ping"
      },
      {
        "surname": "Wang",
        "given_name": "Shengrui"
      },
      {
        "surname": "Tian",
        "given_name": "Yu"
      },
      {
        "surname": "Zhang",
        "given_name": "Wei"
      }
    ]
  },
  {
    "title": "YOLOv3-based human detection and heuristically modified-LSTM for abnormal human activities detection in ATM machine",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103901",
    "abstract": "In existing works, accurately analyzing human activities is a complicated problem in public places. Consequently, the detection of human activities becomes challenging the computer vision technology. The major scope of the research is to develop an abnormal Human Activity Recognition (HAR) model using deep structured architectures for detecting the suspicious activities of humans in the ATM using the video surveillance system. The classification phase utilizes the enhanced deep learning approach named improved Long Short-Term Memory (LSTM) by optimizing certain parameters in LSTM by hybrid optimization algorithm for accurately classifying the normal and abnormal activities of humans. This hybrid optimization algorithm is developed and termed Hybrid Spider Monkey-Chicken Swarm Optimization (HSM-CSO) for achieving the effective performance of the deep learning-based classification. Hence, the designed HAR model in ATM is proven that it helps to improve the system performance and also give relief from prohibited activities or crimes and false alarms for humans.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001517",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Deep learning",
      "Long short term memory",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Programming language",
      "Recurrent neural network",
      "Scope (computer science)"
    ],
    "authors": [
      {
        "surname": "Kshirsagar",
        "given_name": "Aniruddha Prakash"
      },
      {
        "surname": "Azath",
        "given_name": "H."
      }
    ]
  },
  {
    "title": "Accumulated micro-motion representations for lightweight online action detection in real-time",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103879",
    "abstract": "In the last decade, the explosive growth of vision sensors and video content has driven numerous application demands for automating human action detection in space and time. Aside from reliable precision, vast real-world scenarios also mandate continuous and instantaneous processing of actions under limited computational budgets. However, existing studies often rely on heavy operations such as 3D convolution and fine-grained optical flow, therefore are hindered in practical deployment. Aiming strictly at a better mixture of detection accuracy, speed, and complexity for online detection, we customize a cost-effective 2D-CNN-based tubelet detection framework coined Accumulated Micro-Motion Action detector (AMMA). It sparsely extracts and fuses visual-dynamic cues of actions spanning a longer temporal window. To lift reliance on expensive optical flow estimation, AMMA efficiently encodes actions’ short-term dynamics as accumulated micro-motion from RGB frames on-the-fly. On top of AMMA’s motion-aware 2D backbone, we adopt an anchor-free detector to cooperatively model action instances as moving points in the time span. The proposed action detector achieves highly competitive accuracy as state-of-the-arts while substantially reducing model size, computational cost, and processing time (6 million parameters, 1 GMACs, and 100 FPS respectively), making it much more appealing under stringent speed and computational constraints. Codes are available on https://github.com/alphadadajuju/AMMA.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001293",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Detector",
      "Image (mathematics)",
      "Motion (physics)",
      "Motion detection",
      "Optical flow",
      "RGB color model",
      "Real-time computing",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Yu"
      },
      {
        "surname": "Yang",
        "given_name": "Fan"
      },
      {
        "surname": "Ginhac",
        "given_name": "Dominique"
      }
    ]
  },
  {
    "title": "HDR video synthesis by a nonlocal regularization variational model",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103883",
    "abstract": "High dynamic range (HDR) video synthesis is a very challenging task. Consecutive frames are acquired with alternate expositions, generally two or three different exposure times. Classical methods aim at registering neighboring frames and fuse them using image HDR techniques. However, the registration often fails to obtain accurate results and the fusion produces ghosting artifacts. Deep learning techniques have recently appeared imitating the structure of existing classical methods. The neural network is intended to estimate the registration function and choose the fusion weights. In this paper, we propose a new method for HDR video synthesis using a variational model. The proposed model uses a nonlocal regularization term to combine pixel information from neighboring frames. The obtained results are competitive with state-of-the-art. Moreover, the proposed method gives a more reliable and understandable solution than deep-learning based ones.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001335",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Dynamic range",
      "Fusion",
      "Ghosting",
      "High dynamic range",
      "High-dynamic-range imaging",
      "Linguistics",
      "Philosophy",
      "Pixel",
      "Regularization (linguistics)"
    ],
    "authors": [
      {
        "surname": "Buades",
        "given_name": "Antoni"
      },
      {
        "surname": "Martorell",
        "given_name": "Onofre"
      },
      {
        "surname": "Pereira-Sánchez",
        "given_name": "Ivan"
      }
    ]
  },
  {
    "title": "Learning knowledge representation with meta knowledge distillation for single image super-resolution",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103874",
    "abstract": "Although the deep CNN-based super-resolution methods have achieved outstanding performance, their memory cost and computational complexity severely limit their practical employment. Knowledge distillation (KD), which can efficiently transfer knowledge from a cumbersome network (teacher) to a compact network (student), has demonstrated its advantages in some computer vision applications. The representation of knowledge is vital for knowledge transferring and student learning, which is generally defined in hand-crafted manners or uses the intermediate features directly. In this paper, we propose a model-agnostic meta knowledge distillation method under the teacher–student architecture for the single image super-resolution task. It provides a more flexible and accurate way to help teachers transmit knowledge in accordance with the abilities of students via knowledge representation networks (KRNets) with learnable parameters. Specifically, the texture-aware dynamic kernels are generated from local information to decompose the distillation problem into texture-wise supervision for further promoting the recovery quality of high-frequency details. In addition, the KRNets are optimized in a meta-learning manner to ensure the knowledge transferring and the student learning are beneficial to improving the reconstructed quality of the student. Experiments conducted on various single image super-resolution datasets demonstrate that our proposed method outperforms existing defined knowledge representation-related distillation methods and can help super-resolution algorithms achieve better reconstruction quality without introducing any extra inference complexity.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001244",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Distillation",
      "Economics",
      "Epistemology",
      "Image (mathematics)",
      "Inference",
      "Knowledge management",
      "Knowledge representation and reasoning",
      "Knowledge transfer",
      "Law",
      "Machine learning",
      "Management",
      "Meta learning (computer science)",
      "Organic chemistry",
      "Philosophy",
      "Political science",
      "Politics",
      "Quality (philosophy)",
      "Representation (politics)",
      "Resolution (logic)",
      "Task (project management)",
      "Texture (cosmology)"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Han"
      },
      {
        "surname": "Chen",
        "given_name": "Zhenzhong"
      },
      {
        "surname": "Liu",
        "given_name": "Shan"
      }
    ]
  },
  {
    "title": "Multi exposure image fusion based on exposure correction and input refinement using limited low dynamic range images",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103907",
    "abstract": "Multi-Exposure Image Fusion (MEF) is an image processing technique that combines several images taken at various exposure levels into one high-quality image. The generation of artifacts and diminished perceptual quality of the fused image under few input images is an inevitable problem in MEF approaches. Although numerous MEF techniques have been proposed in the literature, little effort has been devoted to generate high quality fused images with minimal input images. The proposed work unveils a method for producing a fused image that exhibits superior visual quality with a minimum of two input images. The proposed work begins with refining the input images using an exposure correction strategy and a recursive filter. The refined images along with a selected exposure corrected image form an intermediate exposure stack. A weight map is constructed from the intermediate exposure stack using a set of weighting terms and a fast-guided filter is used to prevent the weight maps from being distorted by external anomalies. Finally, the fusion is performed using the pyramidal decomposition of weight maps and intermediate images. The resultant fused images obtained have been tested and proven to hold superior performance over the state-of-the-art methods in perceptual and empirical analysis. Benefiting from this approach is a framework that seeks fewer input images, an intermediate exposure stack, and a detail-enriched fused image.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001578",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Composite material",
      "Computer science",
      "Computer vision",
      "Dynamic range",
      "Filter (signal processing)",
      "Fusion",
      "Gene",
      "High dynamic range",
      "Image (mathematics)",
      "Image fusion",
      "Image quality",
      "Linguistics",
      "Materials science",
      "Medicine",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Radiology",
      "Range (aeronautics)",
      "Set (abstract data type)",
      "Stack (abstract data type)",
      "Transformation (genetics)",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "C.R.",
        "given_name": "Jishnu"
      },
      {
        "surname": "S.",
        "given_name": "Vishnukumar"
      }
    ]
  },
  {
    "title": "Dictionary-based histogram packing technique for lossless image compression",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103894",
    "abstract": "This paper proposes a dictionary-based histogram packing technique for lossless image compression. It is used to improve the performance of the state-of-the-art lossless image compression standards and methods when compressing sparse and locally sparse histogram images. The proposed method leverages inter-block correlations and similarities not only within the neighborhood but also across the entire image, thereby effectively reducing the block boundary artifacts commonly observed in block-based histogram packing techniques. To achieve this, a dictionary is employed to represent highly correlated blocks using a key that captures the union of their active symbol sets. Experimental results have demonstrated that the proposed method, when applied to sparse and locally sparse histogram images, enhances the performance of various state-of-the-art lossless image compression techniques. Notably, improvements were observed in standards and methods such as JPEG-2000, JPEG-LS, JPEG-XL, PNG, and CALIC.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032300144X",
    "keywords": [
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Computer science",
      "Computer vision",
      "Data compression",
      "Geometry",
      "Histogram",
      "Histogram matching",
      "Image (mathematics)",
      "Image compression",
      "Image histogram",
      "Image processing",
      "Image texture",
      "JPEG",
      "Lossless JPEG",
      "Lossless compression",
      "Lossy compression",
      "Mathematics",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Zouari",
        "given_name": "Sonia"
      },
      {
        "surname": "Masmoudi",
        "given_name": "Atef"
      }
    ]
  },
  {
    "title": "Age estimation by extracting hierarchical age-related features",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103884",
    "abstract": "Image-based facial age estimation is considered an intractable problem because aging characteristics are hard to obtain. Most previous works have focused on extracting age-related features, but rarely explored which local region plays an important role. Several works combine local face regions with global face to estimate age in a heuristic way, where the local regions are uniformly cropped for each individual. In this paper, we design an individual adaptive segmentation of local regions of interest to perform personalized local features extraction and build hierarchical age features by erasing the local regions of interest iteratively for each individual. A joint multi-input and multi-output (MIMO) network for multi-task learning of age classification and regression tasks is designed by combining global features and personalized local features as inputs. In addition, we conduct extensive experiments to validate the effectiveness of the proposed method for age estimation, which beats most state-of-the-art methods in three public datasets and also works well for gender and race estimation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001347",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Economics",
      "Estimation",
      "Face (sociological concept)",
      "Heuristic",
      "Machine learning",
      "Management",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Regression",
      "Segmentation",
      "Social science",
      "Sociology",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Na"
      },
      {
        "surname": "Zhang",
        "given_name": "Fan"
      },
      {
        "surname": "Duan",
        "given_name": "Fuqing"
      }
    ]
  },
  {
    "title": "Bi-READ: Bi-Residual AutoEncoder based feature enhancement for video anomaly detection",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103860",
    "abstract": "Video anomaly detection (VAD) refers to identifying abnormal events in the surveillance video. Typically, reconstruction based video anomaly detection techniques employ convolutional autoencoders with a limited number of layers, which extracts insufficient features leading to improper network training. To address this challenge, an end-to-end unsupervised feature enhancement network, namely Bi-Residual Convolutional AutoEncoder (Bi-ResCAE) has been proposed that can learn normal events with low reconstruction error and detect anomalies with high reconstruction error. The proposed Bi-ResCAE network incorporates long–short residual connections to enhance feature reusability and training stabilization. In addition, we propose to formulate a novel VAD model that can extract appearance and motion features by fusing both the Bi-ResCAE network and optical flow network in the objective function to recognize the anomalous object in the video. Extensive experiments on three benchmark datasets validate the effectiveness of the model. The proposed model achieves an AUC (Area Under the ROC Curve) of 84.7% on Ped1, 97.7% on Ped2, and 86.71% on the Avenue dataset. The results show that the Bi-READ performs better than state-of-the-art techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001104",
    "keywords": [
      "Algorithm",
      "Anomaly detection",
      "Artificial intelligence",
      "Autoencoder",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Feature (linguistics)",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Linguistics",
      "Optical flow",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Residual"
    ],
    "authors": [
      {
        "surname": "Kommanduri",
        "given_name": "Rangachary"
      },
      {
        "surname": "Ghorai",
        "given_name": "Mrinmoy"
      }
    ]
  },
  {
    "title": "Part-attentive kinematic chain-based regressor for 3D human modeling",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103881",
    "abstract": "As the demand for realistic representation and its applications increases rapidly, 3D human modeling via a single RGB image has become the essential technique. Owing to the great success of deep neural networks, various learning-based approaches have been introduced for this task. However, partial occlusions still give the difficulty to accurately estimate the 3D human model. In this letter, we propose the part-attentive kinematic regressor for 3D human modeling. The key idea of the proposed method is to predict body part attentions based on each body center position and estimate parameters of the 3D human model via corresponding attentive features through the kinematic chain-based decoder in a one-stage fashion. One important advantage is that the proposed method has a good ability to yield natural shapes and poses even with severe occlusions. Experimental results on benchmark datasets show that the proposed method is effective for 3D human modeling under complicated real-world environments. The code and model are publicly available at: https://github.com/DCVL-3D/PKCN_release",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001311",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Classical mechanics",
      "Code (set theory)",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Deep learning",
      "Economics",
      "Engineering",
      "Finance",
      "Geodesy",
      "Geography",
      "Key (lock)",
      "Kinematics",
      "Law",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Politics",
      "Position (finance)",
      "Programming language",
      "Representation (politics)",
      "Set (abstract data type)",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Jeonghwan"
      },
      {
        "surname": "Um",
        "given_name": "Gi-Mun"
      },
      {
        "surname": "Seo",
        "given_name": "Jeongil"
      },
      {
        "surname": "Kim",
        "given_name": "Wonjun"
      }
    ]
  },
  {
    "title": "Multi-view convolutional vision transformer for 3D object recognition",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103906",
    "abstract": "With the rapid development of three-dimensional (3D) vision technology and the increasing application of 3D objects, there is an urgent need for 3D object recognition in the fields of computer vision, virtual reality, and artificial intelligence robots. The view-based method projects 3D objects into two-dimensional (2D) images from different viewpoints and applies convolutional neural networks (CNN) to model the projected views. Although these methods have achieved excellent recognition performance, there is not sufficient information interaction between the features of different views in these methods. Inspired by the recent success achieved by vision transformer (ViT) in image recognition, we propose a hybrid network by taking advantage of CNN to extract multi-scale local information of each view, and of transformer to capture the relevance of multi-scale information between different views. To verify the effectiveness of our multi-view convolutional vision transformer (MVCVT), we conduct experiments on two public benchmarks, ModelNet40 and ModelNet10, and compare with those of some state-of-the-art methods. The final results show that MVCVT has competitive performance in 3D object recognition.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001566",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Cognitive neuroscience of visual object recognition",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Electrical engineering",
      "Engineering",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Transformer",
      "Viewpoints",
      "Visual arts",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Jie"
      },
      {
        "surname": "Liu",
        "given_name": "Zhao"
      },
      {
        "surname": "Li",
        "given_name": "Li"
      },
      {
        "surname": "Lin",
        "given_name": "Junqin"
      },
      {
        "surname": "Yao",
        "given_name": "Jian"
      },
      {
        "surname": "Tu",
        "given_name": "Jingmin"
      }
    ]
  },
  {
    "title": "Adaptive multi-teacher softened relational knowledge distillation framework for payload mismatch in image steganalysis",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103900",
    "abstract": "In this paper, we focus on improving the detection accuracy when payload mismatch occurs in steganalysis, by proposing an adaptive multi-teacher softened relational knowledge distillation framework, which combines multiple balanced payload difference networks (BPDNets), adaptive weight allocation (AWA) and softened relational knowledge distillation (SRKD). BPDNets aims to equivalently concern about the payload difference signal, so that the student can still achieve satisfactory detection accuracy even under unseen payloads due to that a stego signal can be considered as the sum of multiple payload difference signals. AWA assigns appropriate weights to each BPDNet based on the confidence evaluated using the cross entropy and batch entropy loss, thus effectively integrating the knowledge of BPDNets. SRKD combining the softmax with temperature τ other than standard logits to guide the student to inherit more knowledge of relationships between images. Extensive experiments verify that the student achieves better detection performance than state-of-the-art networks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001505",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Data mining",
      "Distillation",
      "Embedding",
      "Entropy (arrow of time)",
      "Network packet",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Payload (computing)",
      "Physics",
      "Quantum mechanics",
      "Softmax function",
      "Steganalysis",
      "Steganography"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Lifang"
      },
      {
        "surname": "Li",
        "given_name": "Yunwei"
      },
      {
        "surname": "Weng",
        "given_name": "Shaowei"
      },
      {
        "surname": "Tian",
        "given_name": "Huawei"
      },
      {
        "surname": "Liu",
        "given_name": "Jing"
      }
    ]
  },
  {
    "title": "Intermediate deep feature coding for human–machine vision collaboration",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103859",
    "abstract": "Traditional image coding are mainly designed for human vision. While for collaborative intelligence, deep feature coding is specific for machine vision, which includes feature extraction and compression. Actually, deep features can build a bridge between human and machine vision. Therefore, we focus on generalized deep feature extraction and compression for multitask, which includes image reconstruction task for human vision and computer visual tasks for machine vision. After analyzing correlation among multitask, a reconstruction guided feature extraction strategy and feature fusion based network are proposed to get more generalized intermediate deep feature, which contains sufficient information friendly for human and machine vision. Besides, a non-uniform quantization method based on importance and a compact representation method for feature distribution information protection are proposed for high efficiency feature coding. Eventually, we come up with an entire intermediate deep feature coding framework including feature extraction and compression. Experimental results indicate the performance gains with our framework.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001098",
    "keywords": [
      "Artificial intelligence",
      "Coding (social sciences)",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Feature extraction",
      "Linguistics",
      "Machine vision",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Weiqian"
      },
      {
        "surname": "An",
        "given_name": "Ping"
      },
      {
        "surname": "Huang",
        "given_name": "Xinpeng"
      },
      {
        "surname": "Huang",
        "given_name": "Kunqiang"
      },
      {
        "surname": "Yang",
        "given_name": "Chao"
      }
    ]
  },
  {
    "title": "Deep ensemble-based hard sample mining for food recognition",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103905",
    "abstract": "Deep neural networks represent a compelling technique to tackle complex real-world problems, but are over-parameterized and often suffer from over- or under-confident estimates. Deep ensembles have shown better parameter estimations and often provide reliable uncertainty estimates that contribute to the robustness of the results. In this work, we propose a new metric to identify samples that are hard to classify. Our metric is defined as coincidence score for deep ensembles which measures the agreement of its individual models. The main hypothesis we rely on is that deep learning algorithms learn the low-loss samples better compared to large-loss samples. In order to compensate for this, we use controlled over-sampling on the identified ”hard” samples using proper data augmentation schemes to enable the models to learn those samples better. We validate the proposed metric using two public food datasets on different backbone architectures and show the improvements compared to the conventional deep neural network training using different performance metrics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001554",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Data mining",
      "Deep learning",
      "Deep neural networks",
      "Economics",
      "Gene",
      "Machine learning",
      "Metric (unit)",
      "Operations management",
      "Parameterized complexity",
      "Pattern recognition (psychology)",
      "Robustness (evolution)",
      "Sample (material)"
    ],
    "authors": [
      {
        "surname": "Nagarajan",
        "given_name": "Bhalaji"
      },
      {
        "surname": "Bolaños",
        "given_name": "Marc"
      },
      {
        "surname": "Aguilar",
        "given_name": "Eduardo"
      },
      {
        "surname": "Radeva",
        "given_name": "Petia"
      }
    ]
  },
  {
    "title": "Multiple transformation function estimation for image enhancement",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103863",
    "abstract": "Most deep learning-based image enhancement algorithms have been developed based on the image-to-image translation approach, in which enhancement processes are difficult to interpret. In this paper, we propose a novel interpretable image enhancement algorithm that estimates multiple transformation functions to describe complex color mapping. First, we develop a histogram-based multiple transformation function estimation network (HMTF-Net) to estimate multiple transformation functions by exploiting both the spatial and statistical information of the input images. Second, we estimate pixel-wise weight maps, which indicate the contribution of each transformation function at each pixel, based on the local structures of the input image and the transformed images obtained by each transformation function. Finally, we obtain the enhanced image as the weighted sum of the transformed images using the estimated weight maps. Extensive experiments confirm the effectiveness of the proposed approach and demonstrate that the proposed algorithm outperforms state-of-the-art image enhancement algorithms for different image enhancement tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032300113X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Evolutionary biology",
      "Function (biology)",
      "Gene",
      "Geometric transformation",
      "Histogram",
      "Image (mathematics)",
      "Mathematics",
      "Messenger RNA",
      "Pattern recognition (psychology)",
      "Pixel",
      "Transformation (genetics)",
      "Translation (biology)"
    ],
    "authors": [
      {
        "surname": "Park",
        "given_name": "Jaemin"
      },
      {
        "surname": "Vien",
        "given_name": "An Gia"
      },
      {
        "surname": "Cha",
        "given_name": "Minhee"
      },
      {
        "surname": "Pham",
        "given_name": "Thuy Thi"
      },
      {
        "surname": "Kim",
        "given_name": "Hanul"
      },
      {
        "surname": "Lee",
        "given_name": "Chul"
      }
    ]
  },
  {
    "title": "3D pedestrian localization fusing via monocular camera",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103871",
    "abstract": "In the field of intelligent transportation, autonomous driving technologies, especially visual sensing solutions have attracted increasing attention in recent years. There are still some challenges in pedestrian location based on the monocular camera, as the pedestrian is a non-rigid object and its depth information cannot be obtained from the monocular camera easily and accurately. In this paper, a pedestrian location framework based on monocular cameras is proposed. The framework consists of three parts: coarse positioning, auxiliary information generation and information fusion. In the part of coarse positioning, the human skeleton information is obtained from the monocular images and a light-weight feed-forward neural network is used to predict the pedestrian position based on the skeleton information. In the part of auxiliary information generation, pseudo-LiDAR points with pedestrian depth information are generated from the monocular images through an auxiliary network. Finally, the outputs of the above two parts are fused to achieve the pedestrian location. The experimental results on KITTI dataset show that our method has achieved better performance than other methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001219",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Economics",
      "Finance",
      "Geography",
      "Monocular",
      "Monocular vision",
      "Pedestrian",
      "Pedestrian detection",
      "Position (finance)"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Hao"
      },
      {
        "surname": "Sun",
        "given_name": "Jiande"
      },
      {
        "surname": "Zhang",
        "given_name": "Shanxin"
      },
      {
        "surname": "Yuan",
        "given_name": "Hui"
      },
      {
        "surname": "Zhang",
        "given_name": "Huaxiang"
      },
      {
        "surname": "Zhang",
        "given_name": "Jia"
      }
    ]
  },
  {
    "title": "An efficient optimization of measurement matrix for compressive sensing",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103904",
    "abstract": "Compressive sensing (CS) is a new paradigm for signal acquisition and reconstruction, which can reconstruct the signal at less than the Nyquist sampling rate. The sampling of the signal occurs through a measurement matrix (MM); thus, MM generation is significant in the context of the CS framework. In this paper, an optimization algorithm is introduced for the generation of the MM of CS based on Restricted Isometric Property (RIP) mandates that eigenvalues of the sensing matrix fall within an interval also minimizes the mutual coherence of the sensing matrix (i.e. the product of the MM and sparsifying matrix). A novel gradient-based iterative optimization method is used to reduce the eigenvalues of the sensing matrix by SVD decomposition. Meanwhile, the proposed algorithm can also reduce the operational complexity. Experimental results and analysis prove that the optimized MM reduces the maximum mutual and average mutual coherence between the MM and the sparsifying basis, which shows the effectiveness of the proposed algorithm over some state-of-art works.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001542",
    "keywords": [
      "Algorithm",
      "Biology",
      "Coherence (philosophical gambling strategy)",
      "Composite material",
      "Compressed sensing",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Eigenvalues and eigenvectors",
      "Filter (signal processing)",
      "Materials science",
      "Mathematical optimization",
      "Mathematics",
      "Matrix (chemical analysis)",
      "Mutual coherence",
      "Nyquist rate",
      "Nyquist–Shannon sampling theorem",
      "Paleontology",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "SIGNAL (programming language)",
      "Sampling (signal processing)",
      "Singular value decomposition",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Patel",
        "given_name": "Saumya"
      },
      {
        "surname": "Vaish",
        "given_name": "Ankita"
      }
    ]
  },
  {
    "title": "Global guidance-based integration network for salient object detection in low-light images",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103862",
    "abstract": "Most of current salient object detection (SOD) methods focus on well-lit scenes, and their performance drops when generalized into low-light scenes due to limitations such as blurred boundaries and low contrast. To solve this problem, we propose a global guidance-based integration network (G2INet) customized for low-light SOD. First, we propose a Global Information Flow (GIF) to extract comprehensive global information, for guiding the fusion of multi-level features. To facilitate information integration, we design a Multi-level features Cross Integration (MCI) module, which progressively fuses low-level details, high-level semantics, and global information by interweaving. Furthermore, a U-shaped Attention Refinement (UAR) module is proposed to further refine edges and details for accurate saliency predictions. In terms of five metrics, extensive experimental results demonstrate that our method outperforms the existing twelve state-of-the-art models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001128",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Contrast (vision)",
      "Focus (optics)",
      "Information fusion",
      "Object (grammar)",
      "Object detection",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Programming language",
      "Salient",
      "Semantics (computer science)",
      "State (computer science)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Zenan"
      },
      {
        "surname": "Guo",
        "given_name": "Jichang"
      },
      {
        "surname": "Yue",
        "given_name": "Huihui"
      },
      {
        "surname": "Wang",
        "given_name": "Yudong"
      }
    ]
  },
  {
    "title": "EMHIFormer: An Enhanced Multi-Hypothesis Interaction Transformer for 3D human pose estimation in video",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103890",
    "abstract": "Monocular 3D human pose estimation is a challenging task because of depth ambiguity and occlusion. Recent methods exploit spatio-temporal information and generate different hypotheses for simulating diverse solutions to alleviate these problems. However, these methods do not fully extract spatial and temporal information and the relationship of each hypothesis. To ease these limitations, we propose EMHIFormer (Enhanced Multi-Hypothesis Interaction Transformer) to model 3D human pose with better performance. In detail, we build connections between different Transformer layers so that our model is able to integrate spatio-temporal information from the previous layer and establish more comprehensive hypotheses. Furthermore, a cross-hypothesis model consisting of a parallel Transformer is proposed to strengthen the relationship between various hypotheses. We also design an enhanced regression head which adaptively adjusts the channel weights to export the final 3D human pose. Extensive experiments are conducted on two challenging datasets: Human3.6M and MPI-INF-3DHP to evaluate our EMHIFormer. The results show that EMHIFormer achieves competitive performance on Human3.6M and state-of-the-art performance on MPI-INF-3DHP. Compared with the closest counterpart, MHFormer, our model outperforms it by 0.6% P-MPJPE and 0.5% MPJPE on Human3.6M dataset and 46.0% MPJPE on MPI-INF-3DHP.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001402",
    "keywords": [
      "Ambiguity",
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Exploit",
      "Machine learning",
      "Monocular",
      "Pattern recognition (psychology)",
      "Physics",
      "Pose",
      "Programming language",
      "Quantum mechanics",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Xiang",
        "given_name": "Xuezhi"
      },
      {
        "surname": "Zhang",
        "given_name": "Kaixu"
      },
      {
        "surname": "Qiao",
        "given_name": "Yulong"
      },
      {
        "surname": "Saddik",
        "given_name": "Abdulmotaleb El"
      }
    ]
  },
  {
    "title": "Deep semantic image compression via cooperative network pruning",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103897",
    "abstract": "Incorporating semantic analysis into image compression can significantly reduce the repetitive computation of fundamental semantic analysis in downstream applications such as semantic image retrieval. In this paper, we tackle the semantic image compression task, which embeds semantics in the compressed bitstream. An intuitive solution to this task is joint multi-task training, which generally results in the trade-off of one task to accommodate the other. We thus provide an alternative pilot solution: given a pair of pre-trained teacher networks that specialize in image compression and semantic inference respectively, we first fuse both models to acquire an ensemble model and then leverage cooperative network pruning and retraining to condense the knowledge. Various experiments on five benchmark datasets validate that the proposed method achieves on par and in many cases better performance than the teachers yet comes in a more compact size, and outperforms its multi-task learning and knowledge distillation counterparts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001475",
    "keywords": [
      "Agronomy",
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Bitstream",
      "Computer science",
      "Decoding methods",
      "Economics",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Image compression",
      "Image processing",
      "Leverage (statistics)",
      "Machine learning",
      "Management",
      "Programming language",
      "Pruning",
      "Semantic Web",
      "Semantic compression",
      "Semantic computing",
      "Semantic technology",
      "Semantics (computer science)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Luo",
        "given_name": "Sihui"
      },
      {
        "surname": "Fang",
        "given_name": "Gongfan"
      },
      {
        "surname": "Song",
        "given_name": "Mingli"
      }
    ]
  },
  {
    "title": "Deep semantic image compression via cooperative network pruning",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103897",
    "abstract": "Incorporating semantic analysis into image compression can significantly reduce the repetitive computation of fundamental semantic analysis in downstream applications such as semantic image retrieval. In this paper, we tackle the semantic image compression task, which embeds semantics in the compressed bitstream. An intuitive solution to this task is joint multi-task training, which generally results in the trade-off of one task to accommodate the other. We thus provide an alternative pilot solution: given a pair of pre-trained teacher networks that specialize in image compression and semantic inference respectively, we first fuse both models to acquire an ensemble model and then leverage cooperative network pruning and retraining to condense the knowledge. Various experiments on five benchmark datasets validate that the proposed method achieves on par and in many cases better performance than the teachers yet comes in a more compact size, and outperforms its multi-task learning and knowledge distillation counterparts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001475",
    "keywords": [
      "Agronomy",
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Bitstream",
      "Computer science",
      "Decoding methods",
      "Economics",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Image compression",
      "Image processing",
      "Leverage (statistics)",
      "Machine learning",
      "Management",
      "Programming language",
      "Pruning",
      "Semantic Web",
      "Semantic compression",
      "Semantic computing",
      "Semantic technology",
      "Semantics (computer science)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Luo",
        "given_name": "Sihui"
      },
      {
        "surname": "Fang",
        "given_name": "Gongfan"
      },
      {
        "surname": "Song",
        "given_name": "Mingli"
      }
    ]
  },
  {
    "title": "Detection of GAN generated image using color gradient representation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103876",
    "abstract": "With the development of generative adversarial network (GANs) technology, the technology of GAN generates images has evolved dramatically. Distinguishing these GAN generated images is challenging for the human eye. Moreover, the GAN generated fake images may cause some behaviors that endanger society and bring great security problems to society. Research on GAN generated image detection is still in the exploratory stage and many challenges remain. Motivated by the above problem, we propose a novel GAN image detection method based on color gradient analysis. We consider the difference in color information between real images and GAN generated images in multiple color spaces, and combined the gradient information and the directional texture information of the generated images to extract the gradient texture features for GAN generated images detection. Experimental results on PGGAN and StyleGAN2 datasets demonstrate that the proposed method achieves good performance, and is robust to other various perturbation attacks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001268",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Generative adversarial network",
      "Image (mathematics)",
      "Law",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Texture (cosmology)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Yun"
      },
      {
        "surname": "Wan",
        "given_name": "Zuliang"
      },
      {
        "surname": "Yin",
        "given_name": "Xiaohua"
      },
      {
        "surname": "Yue",
        "given_name": "Guanghui"
      },
      {
        "surname": "Tan",
        "given_name": "Aiping"
      },
      {
        "surname": "Zheng",
        "given_name": "Zhi"
      }
    ]
  },
  {
    "title": "Hierarchical deep semantic alignment for cross-domain 3D model retrieval",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103895",
    "abstract": "With the development of deep learning and the widespread application of 3D modeling technology, image-based cross-domain 3D model retrieval has attracted more and more researchers’ attention. Existing methods have achieved success by aligning the feature distributions from different domains. However, previous methods just statistically align the domain-level or class-level feature distributions, leaving sample discriminability a margin to be improved for retrieval. To address this issue, this paper proposes a Hierarchical Deep Semantic Alignment Network (HDSAN) for cross-domain 3D model retrieval, which combines the proposed sample-level semantic enhancement with global domain alignment and class semantic alignment. Concretely, we adopt adversarial domain adaptation at the domain level and dynamically align the class centers of two domains at the class level. To further improve sample discriminability, we design intra-domain and cross-domain triplet center alignment to enhance the semantic representation ability at the sample level. Experiments on two commonly-used cross-domain 3D model retrieval datasets MI3DOR-1 and MI3DOR-2 demonstrate the effectiveness of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001451",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Chromatography",
      "Class (philosophy)",
      "Classifier (UML)",
      "Computer science",
      "Deep learning",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Feature (linguistics)",
      "Law",
      "Linguistics",
      "Machine learning",
      "Margin (machine learning)",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Sample (material)"
    ],
    "authors": [
      {
        "surname": "Song",
        "given_name": "Dan"
      },
      {
        "surname": "Ling",
        "given_name": "Yuting"
      },
      {
        "surname": "Li",
        "given_name": "Tianbao"
      },
      {
        "surname": "Wang",
        "given_name": "Teng"
      },
      {
        "surname": "Li",
        "given_name": "Xuanya"
      }
    ]
  },
  {
    "title": "Gesture image recognition method based on DC-Res2Net and a feature fusion attention module",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103891",
    "abstract": "To extract decisive features from gesture images and solve the problem of information redundancy in the existing gesture recognition methods, we propose a new multi-scale feature extraction module named densely connected Res2Net (DC-Res2Net) and design a feature fusion attention module (FFA). Firstly, based on the new dimension residual network (Res2Net), the DC-Res2Net uses channel grouping to extract fine-grained multi-scale features, and dense connection has been adopted to extract stronger features of different scales. Then, we apply a selective kernel network (SK-Net) to enhance the representation of effective features. Afterwards, the FFA has been designed to remove redundant information in features by fusing low-level location features with high-level semantic features. Finally, experiments have been conducted to validate our method on the OUHANDS, ASL, and NUS-II datasets. The results demonstrate the superiority of DC-Res2Net and FFA, which can extract more decisive features and remove redundant information while ensuring high recognition accuracy and low computational complexity.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001414",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Dimension (graph theory)",
      "Feature (linguistics)",
      "Feature extraction",
      "Gesture",
      "Gesture recognition",
      "Kernel (algebra)",
      "Linguistics",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pure mathematics",
      "Redundancy (engineering)",
      "Residual"
    ],
    "authors": [
      {
        "surname": "Tian",
        "given_name": "Qiuhong"
      },
      {
        "surname": "Sun",
        "given_name": "Wenxuan"
      },
      {
        "surname": "Zhang",
        "given_name": "Lizao"
      },
      {
        "surname": "Pan",
        "given_name": "Hao"
      },
      {
        "surname": "Chen",
        "given_name": "Qiaohong"
      },
      {
        "surname": "Wu",
        "given_name": "Jialu"
      }
    ]
  },
  {
    "title": "Transformer-based global–local feature learning model for occluded person re-identification",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103898",
    "abstract": "Most recent occluded person re-identification (re-ID) methods usually learn global features directly from pedestrian images, or use additional pose estimation and semantic analysis model to learn local features, while ignoring the relationship between global and local features, thus incorrectly retrieving different pedestrians with similar attributes as the same pedestrian. Moreover, learning local features using auxiliary models brings additional computational cost. In this work, we propose a Transformer-based dual-branch feature learning model for occluded person re-ID. Firstly, we propose a global–local feature interaction module to learn the relationship between global and local features, thus enhancing the richness of information in pedestrian features. Secondly, we randomly erase local areas in the input image to simulate the real occlusion situation, thereby improving the model’s adaptability to the occlusion scene. Finally, a spilt group module is introduced to explore the local distinguishing features of pedestrian. Numerous experiments validate the effectiveness of our proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001487",
    "keywords": [
      "Adaptability",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Ecology",
      "Electrical engineering",
      "Engineering",
      "Feature (linguistics)",
      "Feature learning",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Pedestrian",
      "Philosophy",
      "Transformer",
      "Transport engineering",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Guoqing"
      },
      {
        "surname": "Chen",
        "given_name": "Chao"
      },
      {
        "surname": "Chen",
        "given_name": "Yuhao"
      },
      {
        "surname": "Zhang",
        "given_name": "Hongwei"
      },
      {
        "surname": "Zheng",
        "given_name": "Yuhui"
      }
    ]
  },
  {
    "title": "SFGDO: Smart flower gradient descent optimization enabled generative adversarial network for recognition of Tamil handwritten character",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103878",
    "abstract": "The Tamil language is complicated to identify, and thus more efforts are devised in the literary works. The objective is to develop a model called the Smart Flower Gradient Descent optimization-based Generative Adversarial Network (SFGDO-based GAN) for recognising Tamil handwriting. The dataset is first used to acquire the input image. The undesired noises are then pre-processed using a bilateral filter, and the binarization procedure is carried out using gradient-based thresholding. The necessary characteristics are then extracted for further categorization of Tamil characters, including character length, character width, statistical features, Local Binary Pattern (LBP), Convolutional Neural Network (CNN), density features, and Histogram of Oriented Gradients (HOG) features. Finally, utilising the proposed SFGDO enabled GAN, Tamil handwritten characters are recognised. The proposed Smart Flower Gradient Descent optimization (SFGDO) algorithm is developed by integrating Smart Flower Optimization Algorithm SFOA) and Gradient Descent Optimization (GDO).",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001281",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Character (mathematics)",
      "Computer science",
      "Convolutional neural network",
      "Generative adversarial network",
      "Geometry",
      "Gradient descent",
      "Handwriting",
      "Histogram",
      "Image (mathematics)",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Tamil",
      "Thresholding"
    ],
    "authors": [
      {
        "surname": "Sasipriyaa",
        "given_name": "N."
      },
      {
        "surname": "P.",
        "given_name": "Natesan"
      },
      {
        "surname": "E.",
        "given_name": "Gothai"
      }
    ]
  },
  {
    "title": "Multi-scale convolutional attention network for lightweight image super-resolution",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103889",
    "abstract": "Convolutional neural network (CNN) based methods have recently achieved extraordinary performance in single image super-resolution (SISR) tasks. However, most existing CNN-based approaches increase the model’s depth by stacking massive kernel convolutions, bringing expensive computational costs and limiting their application in mobile devices with limited resources. Furthermore, large kernel convolutions are rarely used in lightweight super-resolution designs. To alleviate the above problems, we propose a multi-scale convolutional attention network (MCAN), a lightweight and efficient network for SISR. Specifically, a multi-scale convolutional attention (MCA) is designed to aggregate the spatial information of different large receptive fields. Since the contextual information of the image has a strong local correlation, we design a local feature enhancement unit (LFEU) to further enhance the local feature extraction. Extensive experimental results illustrate that our proposed MCAN can achieve better performance with lower model complexity compared with other state-of-the-art lightweight methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001396",
    "keywords": [
      "Aggregate (composite)",
      "Artificial intelligence",
      "Combinatorics",
      "Composite material",
      "Computer science",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Feature extraction",
      "Image (mathematics)",
      "Image resolution",
      "Kernel (algebra)",
      "Linguistics",
      "Materials science",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "Xie",
        "given_name": "Feng"
      },
      {
        "surname": "Lu",
        "given_name": "Pei"
      },
      {
        "surname": "Liu",
        "given_name": "Xiaoyong"
      }
    ]
  },
  {
    "title": "A fast full partitioning algorithm for HEVC-to-VVC video transcoding using Bayesian classifiers",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103829",
    "abstract": "The Versatile Video Coding (VVC) standard was released in 2020 to replace the High Efficiency Video Coding (HEVC) standard, making it necessary to convert HEVC encoded content to VCC to exploit its compression performance, which was achieved by using a larger block size of 128 × 128 pixels, among other new coding tools. However, 80.93% of the encoding time is spent on finding a suitable block partitioning. To reduce this time, this proposal presents an HEVC-to-VVC transcoding algorithm focused on accelerating the CTU partitioning decisions. The transcoder takes different information from the input bitstream of HEVC, and feeds it to two Bayes-based models. Experimental results show a time saving in the transcoding process of 45.40%, compared with the traditional cascade transcoder. This time gain has been obtained on average for all test sequences in the Random Access scenario, at the expense of only 1.50% BD-rate.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000792",
    "keywords": [
      "Algorithm",
      "Bit rate",
      "Bitstream",
      "Coding (social sciences)",
      "Computer network",
      "Computer science",
      "Computer security",
      "Decoding methods",
      "Exploit",
      "Mathematics",
      "Real-time computing",
      "Statistics",
      "Transcoding"
    ],
    "authors": [
      {
        "surname": "García-Lucas",
        "given_name": "D."
      },
      {
        "surname": "Cebrián-Márquez",
        "given_name": "G."
      },
      {
        "surname": "Díaz-Honrubia",
        "given_name": "A.J."
      },
      {
        "surname": "Mallikarachchi",
        "given_name": "T."
      },
      {
        "surname": "Cuenca",
        "given_name": "P."
      }
    ]
  },
  {
    "title": "A hybrid indicator for realistic blurred image quality assessment",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103848",
    "abstract": "Blurriness is annoying yet common in digital images. Many sharpness assessment indicators using handcrafted features achieve impressive results on synthesized blurring images, while room exists for improvement on realistic datasets. This study presents a hybrid indicator in which no-reference indicators perform as mid-level feature extractors and their outputs are selected using a consensus-based method for discriminative ones. On realistic image datasets, 15 off-the-shelf indicators are explored, and experimental results reveal that the hybrid indicator obtains considerable improvement ( ≥ 21.5%, BID2011; ≥ 11.6%, CID2013; ≥ 7.1%, LIVE Challenge; and ≥ 11.6%, KonIQ-10k) compared to the baseline indicator. Meanwhile, the indicator requires more features for representation of diverse distortions (CID2013, LIVE Challenge and KonIQ-10k) than different blurriness (BID2011). Four regression models are investigated, and fitting neural network leads to overall better results. Realistic image quality assessment is challenging, fusion of existing indicators improves the performance, while to develop advanced indicators remains desirable.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000986",
    "keywords": [
      "Artificial intelligence",
      "Baseline (sea)",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Discriminative model",
      "Economics",
      "Epistemology",
      "Feature (linguistics)",
      "Geology",
      "Image (mathematics)",
      "Image quality",
      "Law",
      "Linguistics",
      "Machine learning",
      "Management",
      "Oceanography",
      "Pattern recognition (psychology)",
      "Performance indicator",
      "Philosophy",
      "Political science",
      "Politics",
      "Quality (philosophy)",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Shaode"
      },
      {
        "surname": "Wang",
        "given_name": "Jiayi"
      },
      {
        "surname": "Gu",
        "given_name": "Jiacheng"
      },
      {
        "surname": "Jin",
        "given_name": "Mingxue"
      },
      {
        "surname": "Ma",
        "given_name": "Yunling"
      },
      {
        "surname": "Yang",
        "given_name": "Lijuan"
      },
      {
        "surname": "Li",
        "given_name": "Jianguang"
      }
    ]
  },
  {
    "title": "Person re-identification based on improved attention mechanism and global pooling method",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103849",
    "abstract": "Deep network has become a new favorite for person re-identification (Re-ID), whose research focus is how to effectively extract the discriminative feature representation for pedestrians. In the paper, we propose a novel Re-ID network named as improved ReIDNet (iReIDNet), which can effectively extract the local and global multi-granular feature representations of pedestrians by a well-designed spatial feature transform and coordinate attention (SFTCA) mechanism together with improved global pooling (IGP) method. SFTCA utilizes channel adaptability and spatial location to infer a 2D attention map and can help iReIDNet to focus on the salient information contained in pedestrian images. IGP makes iReIDNet capture more effectively the global information of the whole human body. Besides, to boost the recognition accuracy, we develop a weighted joint loss to guide the training of iReIDNet. Comprehensive experiments demonstrate the availability and superiority of iReIDNet over other Re-ID methods. The code is available at https://github.com/XuRuyu66/ iReIDNet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000998",
    "keywords": [
      "Adaptability",
      "Artificial intelligence",
      "Attention network",
      "Biology",
      "Botany",
      "Channel (broadcasting)",
      "Code (set theory)",
      "Computer science",
      "Discriminative model",
      "Ecology",
      "Engineering",
      "Feature (linguistics)",
      "Feature learning",
      "Focus (optics)",
      "Identification (biology)",
      "Law",
      "Linguistics",
      "Machine learning",
      "Optics",
      "Pattern recognition (psychology)",
      "Pedestrian",
      "Philosophy",
      "Physics",
      "Political science",
      "Politics",
      "Pooling",
      "Programming language",
      "Representation (politics)",
      "Salient",
      "Set (abstract data type)",
      "Telecommunications",
      "Transport engineering"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Ruyu"
      },
      {
        "surname": "Zheng",
        "given_name": "Yueyang"
      },
      {
        "surname": "Wang",
        "given_name": "Xiaoming"
      },
      {
        "surname": "Li",
        "given_name": "Dong"
      }
    ]
  },
  {
    "title": "A secure fingerprint template generation mechanism using visual secret sharing with inverse halftoning",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103854",
    "abstract": "Fingerprints are the most popular and widely practiced biometric trait for human recognition and authentication. Due to the wide approval, reliable fingerprint template generation and secure saving of the generated templates are highly vital. Since fingers are permanently connected to the human body, loss of fingerprint data is irreversible. Cancelable fingerprint templates are used to overcome this problem. This paper introduces a novel cancelable fingerprint template generation mechanism using Visual Secret Sharing (VSS), data embedding, inverse halftoning, and super-resolution. During the fingerprint template generation, VSS shares with some hidden information are formulated as the secure cancelable template. Before authentication, the secret fingerprint image is reconstructed back from the VSS shares. The experimental results show that the proposed cancelable templates are simple, secure, and fulfill all the properties of the ideal cancelable templates, such as security, accuracy, non-invertibility, diversity, and revocability. The experimental analysis shows that the reconstructed fingerprint images are similar to the original fingerprints in terms of visual parameters and matching error rates.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001049",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Authentication (law)",
      "Biometrics",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Cryptography",
      "Fingerprint (computing)",
      "Image (mathematics)",
      "Matching (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Programming language",
      "Secret sharing",
      "Statistics",
      "Template",
      "Template matching"
    ],
    "authors": [
      {
        "surname": "Muhammed",
        "given_name": "Ajnas"
      },
      {
        "surname": "Pais",
        "given_name": "Alwyn Roshan"
      }
    ]
  },
  {
    "title": "Fast saliency prediction based on multi-channels activation optimization",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103831",
    "abstract": "The saliency prediction precision has improved rapidly with the development of deep learning technology, but the inference speed is slow due to the continuous deepening of networks. Hence, this paper proposes a fast saliency prediction model. Concretely, the siamese network backbone based on tailored EfficientNetV2 accelerates the inference speed while maintaining high performance. The shared parameters strategy further curbs parameter growth. Furthermore, we add multi-channel activation maps to optimize the fine features considering different channels and low-level visual features, which improves the interpretability of the model. Extensive experiments show that the proposed model achieves competitive performance on the standard benchmark datasets, and prove the effectiveness of our method in striking a balance between prediction accuracy and inference speed. Moreover, the small model size allows our method to be applied in edge devices. The code is available at: https://github.com/lscumt/fast-fixation-prediction.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000810",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Channel (broadcasting)",
      "Cloud computing",
      "Code (set theory)",
      "Computer network",
      "Computer science",
      "Data mining",
      "Deep learning",
      "Edge device",
      "Enhanced Data Rates for GSM Evolution",
      "Geodesy",
      "Geography",
      "Inference",
      "Interpretability",
      "Machine learning",
      "Operating system",
      "Pattern recognition (psychology)",
      "Programming language",
      "Set (abstract data type)",
      "Source code"
    ],
    "authors": [
      {
        "surname": "Liang",
        "given_name": "Song"
      },
      {
        "surname": "Liu",
        "given_name": "Ruihang"
      },
      {
        "surname": "Qian",
        "given_name": "Jiansheng"
      }
    ]
  },
  {
    "title": "Multi-loop graph convolutional network for multimodal conversational emotion recognition",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103846",
    "abstract": "Emotion recognition in conversations (ERC) has gained increasing research attention in recent years due to its wide applications in a surge of emerging tasks, such as social media analysis, dialog generation, and recommender systems. Since constituent utterances in a conversation are closely semantic-related, the constituent utterances’ emotional states are also closely related. In our consideration, this correlation could serve as a guide for the emotion recognition of constituent utterances. Accordingly, we propose a novel approach named Semantic-correlation Graph Convolutional Network (SC-GCN) to take advantage of this correlation for the ERC task in multimodal scenario. Specifically, we first introduce a hierarchical fusion module to model the dynamics among the textual, acoustic and visual features and fuse the multimodal information. Afterward, we construct a graph structure based on the speaker and temporal dependency of the dialog. We put forward a novel multi-loop architecture to explore the semantic correlations by the self-attention mechanism and enhance the correlation information via multiple loops. Through the graph convolution process, the proposed SC-GCN finally obtains a refined representation of each utterance, which is used for the final prediction. Extensive experiments are conducted on two benchmark datasets and the experimental results demonstrate the superiority of our SC-GCN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000962",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Conversation",
      "Correlation",
      "Dialog box",
      "Geometry",
      "Graph",
      "Linguistics",
      "Mathematics",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Speech recognition",
      "Theoretical computer science",
      "Utterance",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Ren",
        "given_name": "Minjie"
      },
      {
        "surname": "Huang",
        "given_name": "Xiangdong"
      },
      {
        "surname": "Li",
        "given_name": "Wenhui"
      },
      {
        "surname": "Liu",
        "given_name": "Jing"
      }
    ]
  },
  {
    "title": "Unsupervised sub-domain adaptation using optimal transport",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103857",
    "abstract": "We focus on domain adaptation, a branch of transfer learning that concentrates on transferring knowledge from one domain to another when the data distributions differ. Specifically, we investigate unsupervised domain adaptation methods, which have abundant labeled examples from a source domain and unlabeled examples from a target domain available. We aim to minimize the distribution divergences between the domains using optimal transport with subdomain adaptation. Previous methods have mainly focused on reducing global distribution discrepancies between the domains, but these approaches cannot capture fine-grained information and do not consider the structure or geometry of the data. To handle these limitations, we propose Optimal Transport via Subdomain Adaptation (OTSA). Our method utilizes the sliced Wasserstein metric to reduce transportation costs while preserving geometrical data information and the Local Maximum Discrepancy (LMMD) to compute the local discrepancy in each domain category, which helps capture relevant features. Experiments were conducted on six standard domain adaptation datasets, and our method outperformed the majority of baselines. Our approach increased the average accuracy when compared with baselines on the OfficeHome (67.7% to 68.31%), Office-Caltech10 (91.8% to 96.33%), IMAGECLEF-DA (87.9% to 89.9%), VisDA-2017 (79.6% to 81.83%), Office31 (88.17% to 89.11%), and PACS (69.08% to 83.72%) datasets, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001074",
    "keywords": [
      "Adaptation (eye)",
      "Algorithm",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Distribution (mathematics)",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Economics",
      "Focus (optics)",
      "Labeled data",
      "Mathematical analysis",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Gilo",
        "given_name": "Obsa"
      },
      {
        "surname": "Mathew",
        "given_name": "Jimson"
      },
      {
        "surname": "Mondal",
        "given_name": "Samrat"
      },
      {
        "surname": "Kumar Sanodiya",
        "given_name": "Rakesh"
      }
    ]
  },
  {
    "title": "Screen-shooting resistant image watermarking based on lightweight neural network in frequency domain",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103837",
    "abstract": "Currently, digital mobile devices, especially smartphones, can be used to acquire information conveniently through photograph taking. To protect information security in this case, we propose an efficient screen-shooting resistant watermarking scheme via deep neural network (DNN) in the frequency domain to achieve additional information embedding and source tracing. Specifically, we enhance the imperceptibility of watermarked images and the robustness against various attacks in real scene by computing the residual watermark message and encoding it with the original image using a lightweight neural network in the DCT domain. In addition, a noise layer is designed to simulate the photometric and radiometric effects of screen-shooting transfer. During the training process, the enhancing network is used to highlight the coding features of distorted images and improve the accuracy of extracted watermark message. Experimental results demonstrate that our scheme not only effectively ensures the balance between the imperceptibility of watermark embedding and the robustness of watermark extraction, but also significantly improves computational efficiency compared with some state-of-the-art schemes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000871",
    "keywords": [
      "Anthropology",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Digital watermarking",
      "Discrete cosine transform",
      "Embedding",
      "Encryption",
      "Frequency domain",
      "Gene",
      "Image (mathematics)",
      "Normalization (sociology)",
      "Operating system",
      "Robustness (evolution)",
      "Sociology",
      "Tracing",
      "Watermark"
    ],
    "authors": [
      {
        "surname": "Cao",
        "given_name": "Fang"
      },
      {
        "surname": "Wang",
        "given_name": "Tianjun"
      },
      {
        "surname": "Guo",
        "given_name": "Daidou"
      },
      {
        "surname": "Li",
        "given_name": "Jian"
      },
      {
        "surname": "Qin",
        "given_name": "Chuan"
      }
    ]
  },
  {
    "title": "A super-resolution-based license plate recognition method for remote surveillance",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103844",
    "abstract": "With the continuous development of deep learning, neural networks have made great progress in license plate recognition (LPR). Nevertheless, there is still room to improve the performance of license plate recognition for low-resolution and relatively blurry images in remote surveillance scenarios. When it is difficult to enhance the recognition algorithm, we choose super-resolution (SR) to improve the quality of license plate images and thereby provide clearer input for the subsequent recognition stage. In this paper, we propose an automatic super-resolution license plate recognition (SRLPR) network which consists of four parts separately: license plate detection, character detection, single character super-resolution, and recognition. In the training stage, firstly, LP detection model needs to be trained alone and then its detection results will be used to successively train the three subsequent modules. During the test phase, for each input image, the network can get its LP number automatically. We also collect an applicable and challenging LPR dataset called SRLP, which is collected from real remote traffic surveillance. The experimental results demonstrate that our method achieves comprehensive quality of SR images and higher recognition accuracy compared with state-of-the-art methods. The SRLP dataset and the code for training and testing SRLPR network are available at https://pan.baidu.com/s/1vnhRa-c-dBj6jlfBZV5w4g.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000949",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Geography",
      "High resolution",
      "License",
      "Low resolution",
      "Operating system",
      "Pattern recognition (psychology)",
      "Remote sensing"
    ],
    "authors": [
      {
        "surname": "Pan",
        "given_name": "Sen"
      },
      {
        "surname": "Chen",
        "given_name": "Si-Bao"
      },
      {
        "surname": "Luo",
        "given_name": "Bin"
      }
    ]
  },
  {
    "title": "Multi-layer and Multi-scale feature aggregation for DIBR-Synthesized image quality assessment",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103851",
    "abstract": "Depth-Image-Based Rendering (DIBR) is one of the main fundamental techniques for generating new viewpoints in 3D video applications such as multi-viewpoint video (MVV), free viewpoint video (FVV) and virtual reality (VR). Due to the imperfections of color images, depth maps or texture restoration techniques, several types of distortions occur in synthesized views. However, most of related works evaluated the quality of DIBR-synthesized views by only detecting a specific type of distortion, such as stretching, black holes, blurring, etc., which were unable to accurately evaluate the quality of DIBR-synthesized views. In this paper, a new no-reference image quality assessment method is proposed to evaluate the quality of DIBR-synthesized images by combining multi-layer and multi-scale features of images. To be specific, the distortions introduced by different stages of virtual viewpoint synthesis are first analyzed, and then multi-layer and multi-scale features are extracted to estimate the degree of texture and structure distortions. As a result, individual quality scores associated with two types of distortions (e.g., structural distortion and texture distortion) are aggregated to an overall image quality. Experimental results on two publicly available DIBR datasets show that the method has better performance than the state-of-the-art models. Index Terms: image quality assessment, DIBR-synthesized image, distortion correction, BIQA.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001013",
    "keywords": [
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Distortion (music)",
      "Epistemology",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Image quality",
      "Image-based modeling and rendering",
      "Linguistics",
      "Liquid-crystal display",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quality (philosophy)",
      "Quantum mechanics",
      "Rendering (computer graphics)",
      "Scale (ratio)",
      "Texture (cosmology)",
      "View synthesis",
      "Viewing angle"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Kexin"
      },
      {
        "surname": "Wang",
        "given_name": "Xuejin"
      },
      {
        "surname": "Chai",
        "given_name": "Xiongli"
      },
      {
        "surname": "Shao",
        "given_name": "Feng"
      }
    ]
  },
  {
    "title": "UAV image stitching by estimating orthograph with RGB cameras",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103835",
    "abstract": "In the field of image stitching, cases with large camera optical center movement and large parallax have been the virgin territory of research. The goal of image stitching is to overcome the parallax and stitch a natural image. We look into this problem in the context of ultra-low altitude flight of a UAV. We model the 3D world in this scenario and quickly estimate orthographic projection by pairs of homography matrices. Our stitching method can achieve precise alignment since it takes parallax well into consideration. The stitching results are natural and the extra time consumed is short.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000858",
    "keywords": [
      "Algorithm",
      "Archaeology",
      "Artificial intelligence",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Field of view",
      "Geography",
      "Homography",
      "Image stitching",
      "Mathematics",
      "Orthographic projection",
      "Parallax",
      "Projection (relational algebra)",
      "Projective space",
      "Projective test",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Cai",
        "given_name": "Wenxiao"
      },
      {
        "surname": "Du",
        "given_name": "Songlin"
      },
      {
        "surname": "Yang",
        "given_name": "Wankou"
      }
    ]
  },
  {
    "title": "BFANet: Effective segmentation network for low altitude high-resolution urban scene image",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103847",
    "abstract": "The semantic segmentation of low altitude high-resolution urban scene images taken by UAV plays an important role in city management. However, such images have the characteristics of inter-class homogeneity and intra-class heterogeneity. How to segment these images quickly and accurately is still challenging. In this paper, we propose a novel double-branch network. For the challenge of inter-class homogeneity, a boundary flow module is designed to enhance the flow of latent semantic information between two branches by imposing boundary constraints between classes. To alleviate intra-class heterogeneity, a context extraction module based on adaptive dynamic fusion is designed, which effectively captures the long-term relationship of features with very low parameters. Experiments on two typical datasets show that our approach achieves the best balance between accuracy and speed. Specifically, we achieve 65.8% mIoU and 74.1% mIoU on UAVid test set and UDD validation set respectively, and 60FPS on an NVIDIA TITAN Xp.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000974",
    "keywords": [
      "Altitude (triangle)",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Geometry",
      "Homogeneity (statistics)",
      "Image segmentation",
      "Low altitude",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Segmentation",
      "Test set"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Letian"
      },
      {
        "surname": "Zhang",
        "given_name": "Xian"
      },
      {
        "surname": "Zhu",
        "given_name": "Dejun"
      },
      {
        "surname": "Yang",
        "given_name": "Wankou"
      }
    ]
  },
  {
    "title": "DCAM: Disturbed class activation maps for weakly supervised semantic segmentation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103852",
    "abstract": "In the field of weakly supervised semantic segmentation (WSSS), Class Activation Maps (CAM) are typically adopted to generate pseudo masks. Yet, we find that the crux of the unsatisfactory pseudo masks is the incomplete CAM. Specifically, as convolutional neural networks tend to be dominated by the specific regions in the high-confidence channels of feature maps during prediction, the extracted CAM contains only parts of the object. To address this issue, we propose the Disturbed CAM (DCAM), a simple yet effective method for WSSS. Following CAM, we adopt a binary cross-entropy (BCE) loss to train a multi-label classification model. Then, we disturb the feature map with retraining to enhance the high-confidence channels. In addition, a softmax cross-entropy (SCE) loss branch is employed to increase the model attention to the target classes. Once converged, we extract DCAM in the same way as in CAM. The evaluation on both PASCAL VOC and MS COCO shows that DCAM not only generates high-quality masks (6.2% and 1.4% higher than the benchmark models), but also enables more accurate activation in object regions. The code is available at https://github.com/gyyang23/DCAM.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001025",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Binary number",
      "Computer science",
      "Convolutional neural network",
      "Cross entropy",
      "Entropy (arrow of time)",
      "Feature (linguistics)",
      "Linguistics",
      "Mathematics",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Segmentation",
      "Softmax function"
    ],
    "authors": [
      {
        "surname": "Lei",
        "given_name": "Jie"
      },
      {
        "surname": "Yang",
        "given_name": "Guoyu"
      },
      {
        "surname": "Wang",
        "given_name": "Shuaiwei"
      },
      {
        "surname": "Feng",
        "given_name": "Zunlei"
      },
      {
        "surname": "Liang",
        "given_name": "Ronghua"
      }
    ]
  },
  {
    "title": "Aethra-net: Single image and video dehazing using autoencoder",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103855",
    "abstract": "A fast and efficient video dehazing system with low computational complexity has a huge demand among drivers during hazy winter nights. There are only a few video dehazing models that exist in literature. Video dehazing requires the sequential extraction and processing of frames. The processed frames must be restored in the same sequence as the original video. However, the existing video dehazing algorithms suffer from color distortion due to the continuous processing of frames. They are not suitable for videos with dense haze. Furthermore, some dehazing systems require hardware, whereas the proposed model is completely software-based to reduce the computational costs. In this paper, an image and video dehazing system called Aethra-Net is developed. A gush enhancer-based autoencoder is modified to obtain the transmission map. The structure of gush enhancement module resembles the processing of light entering the human eye from different paths. The multiple blocks of Resnet-101 layers are employed to overcome vanishing gradient problem. The vessel enhancement filter is also incorporated to enhance the performance of the proposed system. The proposed model has a susceptibility to compute the dehazed images effectively. The proposed model is evaluated on various benchmark datasets and compared with the existing dehazing techniques. Experimental results reveal that the performance of Aethra-Net is found superior as compared to the existing dehazing models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001050",
    "keywords": [
      "Amplifier",
      "Artificial intelligence",
      "Autoencoder",
      "Bandwidth (computing)",
      "Benchmark (surveying)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Distortion (music)",
      "Geodesy",
      "Geography",
      "Video processing"
    ],
    "authors": [
      {
        "surname": "Juneja",
        "given_name": "Akshay"
      },
      {
        "surname": "Kumar",
        "given_name": "Vijay"
      },
      {
        "surname": "Singla",
        "given_name": "Sunil Kumar"
      }
    ]
  },
  {
    "title": "Dual-branch vision transformer for blind image quality assessment",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103850",
    "abstract": "Blind image quality assessment (BIQA) has always been a challenging problem due to the absence of reference images. In this paper, we propose a novel dual-branch vision transformer for BIQA, which simultaneously considers both local distortions and global semantic information. It first extracts dual-scale features from the backbone network, and then each scale feature is fed into one of the transformer encoder branches as a local feature embedding to consider the scale-variant local distortions. Each transformer branch obtains the context of global image distortion as well as the local distortion by adopting content-aware embedding. Finally, the outputs of the dual-branch vision transformer are combined by using multiple feed-forward blocks to predict the image quality scores effectively. Experimental results demonstrate that the proposed BIQA method outperforms the conventional methods on the six public BIQA datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001001",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Electrical engineering",
      "Embedding",
      "Encoder",
      "Engineering",
      "Image (mathematics)",
      "Image quality",
      "Operating system",
      "Pattern recognition (psychology)",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Lee",
        "given_name": "Se-Ho"
      },
      {
        "surname": "Kim",
        "given_name": "Seung-Wook"
      }
    ]
  },
  {
    "title": "DDFP:A data driven filter pruning method with pruning compensation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103833",
    "abstract": "Neural network pruning techniques can be effective in accelerating neural network models, making it possible to deploy them on edge devices. In this paper, we propose to prune neural networks using data variance. Unlike other existing methods, this method is somewhat robust and does not invalidate our criteria depending on the number of data batches and the number of training sessions. We also propose a pruning compensation technique. This technique fuses the pruned convolutional information into the remaining convolutional kernel close to it. This fusion operation can effectively help retain the pruned information. We evaluate the proposed method on a number of standard datasets and compare it with several current state-of-the-art methods. Our method always achieves better performance. For example, on Tiny ImageNet, our method can prune 54.2% FLOPs of ResNet50 while obtaining a 0.22% accuracy improvement.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000834",
    "keywords": [
      "Accounting",
      "Agronomy",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Business",
      "Combinatorics",
      "Compensation (psychology)",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Enhanced Data Rates for GSM Evolution",
      "FLOPS",
      "Filter (signal processing)",
      "Kernel (algebra)",
      "Machine learning",
      "Mathematics",
      "Parallel computing",
      "Pattern recognition (psychology)",
      "Pruning",
      "Psychoanalysis",
      "Psychology",
      "Variance (accounting)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Guoqiang"
      },
      {
        "surname": "Liu",
        "given_name": "Bowen"
      },
      {
        "surname": "Chen",
        "given_name": "Anbang"
      }
    ]
  },
  {
    "title": "CrowdFormer: Weakly-supervised crowd counting with improved generalizability",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103853",
    "abstract": "Convolutional neural networks (CNNs) have dominated the field of computer vision for nearly a decade. However, due to their limited receptive field, CNNs fail to model the global context. On the other hand, transformers, an attention-based architecture, can model the global context easily. Despite this, there are limited studies that investigate the effectiveness of transformers in crowd counting. In addition, the majority of the existing crowd-counting methods are based on the regression of density maps which requires point-level annotation of each person present in the scene. This annotation task is laborious and also error-prone. This has led to an increased focus on weakly-supervised crowd-counting methods, which require only count-level annotations. In this paper, we propose a weakly-supervised method for crowd counting using a pyramid vision transformer. We have conducted extensive evaluations to validate the effectiveness of the proposed method. Our method achieves state-of-the-art performance. More importantly, it shows remarkable generalizability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323001037",
    "keywords": [
      "Annotation",
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Generalizability theory",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Regression",
      "Statistics",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Savner",
        "given_name": "Siddharth Singh"
      },
      {
        "surname": "Kanhangad",
        "given_name": "Vivek"
      }
    ]
  },
  {
    "title": "CB-D2RNet – An efficient context bridge network for glioma segmentation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103836",
    "abstract": "The recent automatic glioma segmentation and localization techniques obtained promising results, but there is much scope for improvement in execution complexity and segmentation efficiency. These methods often fail to pinpoint small and isolated target locations in necrotic and enhancing glioma sub-regions. Moreover, the computational complexity and number of model parameters utilized in these techniques are also high. To address such issues, a Context Bridge-Dense Dilated Residual Net (CB-D2RNet) is proposed in this paper which reflects the five novel contributions. Firstly, a Dense Dilated Convolutional (DDC) block is formed with four cascade branches to cope with large morphological differences in gliomas. Secondly, the skip connections in traditional UNet are redesigned to overcome the large contextual gap between encoder-decoder. Thirdly, a new loss function is proposed that handles unequal class distribution in gliomas and provides a regularization impact. Fourthly, the precise selection of dilation rates is made for each dilated convolutional block in the feature encoder to gather a more receptive view of complex and multiple tumor regions. Lastly, only a single convolutional operation is included in the feature encoder and decoder, unlike other state-of-the-art models. The experiments are conducted on BraTS 2018 and BraTS 2019 benchmarks, demonstrating that the proposed model performs competitively in all three glioma sub-regions. It achieves dice similarity coefficient for the whole tumor, tumor core, and enhancing tumor as 0.982, 0.987, and 0.976, respectively for the BraTS 2018 dataset, whereas 0.983, 0.989, and 0.977 respectively for the BraTS 2019 dataset. Besides this, the model uses only 6.7 million parameters, the lowest among other compared models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032300086X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Block (permutation group theory)",
      "Combinatorics",
      "Computer science",
      "Context (archaeology)",
      "Convolutional neural network",
      "Dilation (metric space)",
      "Encoder",
      "Feature (linguistics)",
      "Geometry",
      "Linguistics",
      "Mathematics",
      "Operating system",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Regularization (linguistics)",
      "Residual",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Malhotra",
        "given_name": "Radhika"
      },
      {
        "surname": "Singh Saini",
        "given_name": "Barjinder"
      },
      {
        "surname": "Gupta",
        "given_name": "Savita"
      }
    ]
  },
  {
    "title": "DDFP:A data driven filter pruning method with pruning compensation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103833",
    "abstract": "Neural network pruning techniques can be effective in accelerating neural network models, making it possible to deploy them on edge devices. In this paper, we propose to prune neural networks using data variance. Unlike other existing methods, this method is somewhat robust and does not invalidate our criteria depending on the number of data batches and the number of training sessions. We also propose a pruning compensation technique. This technique fuses the pruned convolutional information into the remaining convolutional kernel close to it. This fusion operation can effectively help retain the pruned information. We evaluate the proposed method on a number of standard datasets and compare it with several current state-of-the-art methods. Our method always achieves better performance. For example, on Tiny ImageNet, our method can prune 54.2% FLOPs of ResNet50 while obtaining a 0.22% accuracy improvement.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000834",
    "keywords": [
      "Accounting",
      "Agronomy",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Business",
      "Combinatorics",
      "Compensation (psychology)",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Enhanced Data Rates for GSM Evolution",
      "FLOPS",
      "Filter (signal processing)",
      "Kernel (algebra)",
      "Machine learning",
      "Mathematics",
      "Parallel computing",
      "Pattern recognition (psychology)",
      "Pruning",
      "Psychoanalysis",
      "Psychology",
      "Variance (accounting)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Guoqiang"
      },
      {
        "surname": "Liu",
        "given_name": "Bowen"
      },
      {
        "surname": "Chen",
        "given_name": "Anbang"
      }
    ]
  },
  {
    "title": "Accelerating QTMT-based CU partition and intra mode decision for versatile video coding",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103832",
    "abstract": "The H.266/VVC achieves about 50% bitrate saving compared to its predecessor H.265/HEVC at the expense of exponentially increased computational complexity. The most efficient but complex technique for H.266/VVC intra frame coding is the QuadTree with a nested Multi-type Tree encoding structure (QTMT), which usually requires traversing the Rate-Distortion (R-D) cost of each partition and each mode for the best option. To alleviate such computational burden while preserving the coding efficiency as much as possible, this paper develops a multi-feature guided Fast CU Partition (FCP) and Laplacian guided Fast Mode Selection (FMS) to accelerate the intra QTMT decision together. For FCP, we regard the CU partition as a classification problem and adopt the Support Vector Machine (SVM) for its low-complexity implementation; after evaluating the contribution of a set of features, three representative features of video textures are selected and used to train the SVM model. Additionally, an advanced technique is applied by adopting a soft decision in SVM for a more flexible trade-off between the complexity and R-D performance. For FMS, we utilize the Laplace operator to determine the most probable directions of the current CU and skip half of the candidate modes for runtime saving. Experimental results demonstrate that the proposed FCP reduces the encoding time of H.266/VVC by 51.03% with 1.65% Bjøntegaard Delta Bit-Rate (BDBR) increase; the proposed FMS reduces the encoding time by 12.68% with 0.09% BDBR loss. Their direct combination and advanced combination finally lead to 54.84% encoding time reduction with 1.74% BDBR increase and 40.39% encoding time reduction with 1.33% BDBR increase, respectively, outperforming state-of-the-art approaches significantly.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000822",
    "keywords": [
      "Algorithm",
      "Algorithmic efficiency",
      "Artificial intelligence",
      "Coding (social sciences)",
      "Combinatorics",
      "Computational complexity theory",
      "Computer science",
      "Geodesy",
      "Geography",
      "Mathematics",
      "Partition (number theory)",
      "Quadtree",
      "Rate distortion",
      "Statistics",
      "Support vector machine",
      "Traverse"
    ],
    "authors": [
      {
        "surname": "Ding",
        "given_name": "Gongchun"
      },
      {
        "surname": "Lin",
        "given_name": "Xiujun"
      },
      {
        "surname": "Wang",
        "given_name": "Junjie"
      },
      {
        "surname": "Ding",
        "given_name": "Dandan"
      }
    ]
  },
  {
    "title": "Multi-scale spatial–temporal attention graph convolutional networks for driver fatigue detection",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103826",
    "abstract": "With the development of deep learning, fatigue detection technology for drivers has achieved remarkable achievements. Although the image-based approach achieves good accuracy, it inevitably leads to greater model complexity, which is unsuitable for mobile terminal devices. Luckily, human skeletal data significantly reduces the impact of noise and input data volume while retaining valid information, and it can better deal with real-world driving scenarios with the benefit of robustness in complex driving situations. This paper proposes a lightweight multi-scale spatio-temporal attention graph convolutional network (MS-STAGCN) to efficiently utilize skeleton data to identify driver states by aggregating locally and globally valid face information, which achieves good performance even for lightweight design. The experimental results show that the method achieves 92.4% accuracy on the NTHU-DDD dataset, which can be applied to fatigue detection tasks of the driver in real-world driving scenarios in the future.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000767",
    "keywords": [
      "Artificial intelligence",
      "Attention network",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Deep learning",
      "Gene",
      "Graph",
      "Real-time computing",
      "Robustness (evolution)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Fa",
        "given_name": "Shuxiang"
      },
      {
        "surname": "Yang",
        "given_name": "Xiaohui"
      },
      {
        "surname": "Han",
        "given_name": "Shiyuan"
      },
      {
        "surname": "Feng",
        "given_name": "Zhiquan"
      },
      {
        "surname": "Chen",
        "given_name": "Yuehui"
      }
    ]
  },
  {
    "title": "Robust reversible image watermarking scheme based on spread spectrum",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103808",
    "abstract": "Robust reversible watermarking can provide robustness against various attacks besides the ability to recover the cover image. However, robustness and reversibility are somewhat separate in many schemes. The original cover image cannot be recovered even if the watermarked image suffers from a tiny distortion. This paper presents a new robust reversible watermarking scheme by exploring the reversibility of spread-spectrum codes. Watermark bits are embedded by a suggested adaptive spread-spectrum code. The embedding amplitude used in the algorithms is determined by quantizing the source interference of the cover. The proposed scheme is robust to various attacks. Furthermore, since the embedding amplitude is available at the receiver, the original image can be recovered losslessly when there is no attack. Even in the presence of attacks, the original cover images can still be partially recovered. Experimental results demonstrate that the proposed scheme performs well on robustness and watermarked image quality, and provide extra reversibility that resists image distortions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000585",
    "keywords": [
      "Algorithm",
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Biochemistry",
      "Chemistry",
      "Code (set theory)",
      "Code division multiple access",
      "Computer science",
      "Computer vision",
      "Cover (algebra)",
      "Digital watermarking",
      "Distortion (music)",
      "Embedding",
      "Engineering",
      "Gene",
      "Image (mathematics)",
      "Image quality",
      "Mathematics",
      "Mechanical engineering",
      "Programming language",
      "Robustness (evolution)",
      "Set (abstract data type)",
      "Spread spectrum",
      "Telecommunications",
      "Watermark"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Ziquan"
      },
      {
        "surname": "Feng",
        "given_name": "Bingwen"
      },
      {
        "surname": "Xiang",
        "given_name": "Shijun"
      }
    ]
  },
  {
    "title": "Benford’s law: What does it say on adversarial images?",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103818",
    "abstract": "Convolutional neural networks (CNNs) are fragile to small perturbations in the input images. These networks are thus prone to malicious attacks that perturb the inputs to force a misclassification. Such slightly manipulated images aimed at deceiving the classifier are known as adversarial images. In this work, we investigate statistical differences between natural images and adversarial ones. More precisely, we show that employing a proper image transformation for a class of adversarial attacks, the distribution of the leading digit of the pixels in adversarial images deviates from Benford’s law. The stronger the attack, the more distant the resulting distribution is from Benford’s law. Our analysis provides a detailed investigation of this new approach that can serve as a basis for alternative adversarial example detection methods that do not need to modify the original CNN classifier neither work on the high-dimensional pixel space for features to defend against attacks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000688",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Benford's law",
      "Class (philosophy)",
      "Classifier (UML)",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Digital image",
      "Image (mathematics)",
      "Image processing",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pixel",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Zago",
        "given_name": "João G."
      },
      {
        "surname": "Antonelo",
        "given_name": "Eric A."
      },
      {
        "surname": "Baldissera",
        "given_name": "Fabio L."
      },
      {
        "surname": "Saad",
        "given_name": "Rodrigo T."
      }
    ]
  },
  {
    "title": "Real-time and robust visual tracking with scene-perceptual memory",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103825",
    "abstract": "Unmanned aerial vehicle (UAV) based aerial visual tracking is one of the research hotspots in computer vision. However, the mainstream trackers for UAV still have two shortcomings: (1) the accuracy of correlation filter tracker is greatly improved with more complex model, it impedes accuracy-speed trade-off. (2) object occlusion and camera motion in the aerial tracking scene also seriously restrict the application of aerial tracking. To address these problems, and inspired by AutoTrack tracker, we propose an aerial correlation filtering tracker based on scene-perceptual memory, Fast-AutoTrack. Firstly, to perceive and judge tracking anomalies, such as object occlusion and camera motion, inspired by the peak sidelobe ratio and AutoTrack, a confidence score is designed by perceiving and remembering the changing trend of the confidence and the local historical confidence. Secondly, after tracking anomaly occurring, several search regions are predicted based on the local object motion trend and the Spatio-temporal context information for object re-detection. Finally, to accelerate the model updating, the perceptual hashing algorithm (PHA) is used to obtain the similarity of the search regions between two adjacent frames. On typical aerial tracking datasets UAVDT, UAV123@10fps, and DTB70, Fast-AutoTrack run 71.4% faster than AutoTrack with almost equal accuracy and show favorable accuracy-speed trade-off.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000755",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Biology",
      "BitTorrent tracker",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Eye tracking",
      "Geography",
      "Neuroscience",
      "Object (grammar)",
      "Pedagogy",
      "Perception",
      "Psychology",
      "Tracking (education)",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Shao",
        "given_name": "Yanhua"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiao"
      },
      {
        "surname": "Liao",
        "given_name": "Kuisheng"
      },
      {
        "surname": "Chu",
        "given_name": "Hongyu"
      }
    ]
  },
  {
    "title": "An efficient lightweight network for single image super-resolution",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103834",
    "abstract": "The outstanding performance of convolutional neural networks (CNNs) shown in single image super-resolution (SISR) strongly depends on network’s depth, which hampers its application in low-power computing devices. In this paper, a lightweight and efficient network (LESR) is proposed for SISR by constructing the shallow feature extraction block (SFBlock), the cascaded sparse mask blocks (SMBlocks) and the feature fusion block (FFBlock). The SFBlock efficiently extracts global informative features from the original low resolution image using sparse self-attention, SMBlock skips the redundant computation in extracted features, and more meaningful information is distilled for the sequential reconstruction block by the FFBlock. In addition, a recently proposed activation function called ACON-C is used to replace the ReLU function to ease the training difficulty. Extensive experiments show that our proposed network performs better than most advanced lightweight SISR algorithms with comparable parameters and less FLOPs on benchmark database for × 2 / 3 / 4 SISR.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000846",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Block (permutation group theory)",
      "Computer science",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Feature extraction",
      "Geodesy",
      "Geography",
      "Geometry",
      "Image (mathematics)",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Tang",
        "given_name": "Yinggan"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiang"
      },
      {
        "surname": "Zhang",
        "given_name": "Xuguang"
      }
    ]
  },
  {
    "title": "CANet: Context-aware Aggregation Network for Salient Object Detection of Surface Defects",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103820",
    "abstract": "Surface defect detection has become more and more important in the industrial manufacture and engineering application in recent years. However, due to the lack of overall perception and interaction among features layers, lots of computer vision-based detection methods cannot grab the complete details of defects when dealing with complex scenes, such as low contrast and irregular shape. Therefore, in this paper, we propose a Context-aware Aggregation Network (CANet) to accurately pop-out the defects, where we focus on the mining of context cues and the fusion of multiple context features. To be specific, embarking on the multi-level deep features extracted by encoder, we first deploy a sufficient exploration to dig the context information by deploying the weighted convolution pyramid (WCP) module, which extracts multi-scale context features, transfers the information flow between different resolution features, and fuses the features with same resolution. By this way, we can obtain the effective context pyramid features. Then, the decoder deploys the weighted context attention (WCA) module to filter the irrelevant information from context features and employs the cascaded fusion structure (CFS) to aggregate the multiple context cues in a hierarchical way. Following this way, the generated high-quality saliency maps can highlight the defects accurately and completely. Extensive experiments are performed on four public datasets, and the results firmly prove the effectiveness and superiority of the proposed CANet under different evaluation metrics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000706",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Context model",
      "Encoder",
      "Feature (linguistics)",
      "Filter (signal processing)",
      "Geometry",
      "Linguistics",
      "Mathematics",
      "Object (grammar)",
      "Operating system",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pyramid (geometry)"
    ],
    "authors": [
      {
        "surname": "Wan",
        "given_name": "Bin"
      },
      {
        "surname": "Zhou",
        "given_name": "Xiaofei"
      },
      {
        "surname": "Zhu",
        "given_name": "Bin"
      },
      {
        "surname": "Xiao",
        "given_name": "Mang"
      },
      {
        "surname": "Sun",
        "given_name": "Yaoqi"
      },
      {
        "surname": "Zheng",
        "given_name": "Bolun"
      },
      {
        "surname": "Zhang",
        "given_name": "Jiyong"
      },
      {
        "surname": "Yan",
        "given_name": "Chenggang"
      }
    ]
  },
  {
    "title": "LiDAR-only 3D object detection based on spatial context",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103805",
    "abstract": "LiDAR-based 3D Object detection is one of the popular topics in recent years, and it is widely used in the fields of autonomous driving and robot controlling. However, due to the scanning pattern of LiDAR, the point clouds of objects at far distance are sparse and more difficult to be detected. To solve this problem, we propose a two-stage network based on spatial context information, named SC-RCNN (Spatial Context RCNN), for object detection in 3D point cloud scenes. SC-RCNN first uses a backbone with sparse convolutions and submanifold sparse convolutions to extract the voxel features of point scenes and generate a series of candidate boxes. For the sparsity of far-distance point clouds, we design the local grid point pooling (LGP Pooling) to extract features and spatial context information around candidate regions for subsequent box refinement. In addition, we propose the pyramid candidate box augmentation (PCB Augmentation) to expand the candidate boxes with a multi-scale style, enriching the feature encoding. The experimental results show that SC-RCNN significantly outperforms previous methods on KITTI dataset and Waymo dataset, and is particularly robust to the sparsity of point clouds.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032300055X",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Feature (linguistics)",
      "Geography",
      "Geometry",
      "Lidar",
      "Linguistics",
      "Mathematics",
      "Object detection",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Point (geometry)",
      "Point cloud",
      "Pooling",
      "Pyramid (geometry)",
      "Remote sensing",
      "Spatial contextual awareness"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Qiang"
      },
      {
        "surname": "Li",
        "given_name": "Ziyu"
      },
      {
        "surname": "Zhu",
        "given_name": "Dejun"
      },
      {
        "surname": "Yang",
        "given_name": "Wankou"
      }
    ]
  },
  {
    "title": "3D Deformable Convolution Temporal Reasoning network for action recognition",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103804",
    "abstract": "Modeling and reasoning of the interactions between multiple entities (actors and objects) are beneficial for the action recognition task. In this paper, we propose a 3D Deformable Convolution Temporal Reasoning (DCTR) network to model and reason about the latent relationship dependencies between different entities in videos. The proposed DCTR network consists of a spatial modeling module and a temporal reasoning module. The spatial modeling module uses 3D deformable convolution to capture relationship dependencies between different entities in the same frame, while the temporal reasoning module uses Conv-LSTM to reason about the changes of multiple entity relationship dependencies in the temporal dimension. Experiments on the Moments-in-Time dataset, UCF101 dataset and HMDB51 dataset demonstrate that the proposed method outperforms several state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000548",
    "keywords": [
      "Action (physics)",
      "Action recognition",
      "Artificial intelligence",
      "Artificial neural network",
      "Class (philosophy)",
      "Computer science",
      "Convolution (computer science)",
      "Dimension (graph theory)",
      "Economics",
      "Frame (networking)",
      "Management",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Pure mathematics",
      "Quantum mechanics",
      "Spatial intelligence",
      "Task (project management)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Ou",
        "given_name": "Yangjun"
      },
      {
        "surname": "Chen",
        "given_name": "Zhenzhong"
      }
    ]
  },
  {
    "title": "HSP-MFL: A High-level Semantic Property driven Multi-task Feature Learning Network for unsupervised person Re-ID",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103828",
    "abstract": "Unsupervised person re-identification aims to distinguish different pedestrians from discriminative representations on the basis of unlabeled data. Currently, most unsupervised Re-ID approaches explore visual representations to generate pseudo-labels for model’s training, which may suffer from background noise and semantic loss. To tackle this problem, this paper proposes a High-level Semantic Property driven Multi-task Feature Learning Network (HSP-MFL) to firstly introduce three high-level semantic properties for unsupervised person Re-ID. Technically, we design a novel Multiple Feature Fusion Module (MFFM) to deeply explore the complex correlation among multiple semantic and visual features to capture the discriminative feature cues, as well as a multi-task training scheme to generate robust fusion features. The architecture is quite simple and does not consume extra labeling costs. Extensive experiments on three datasets demonstrate that both high-level semantic properties and multi-task learning are effective in performance improvement, yielding SOTA mAPs for unsupervised person Re-ID.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000780",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Discriminative model",
      "Economics",
      "Epistemology",
      "Feature (linguistics)",
      "Feature learning",
      "Linguistics",
      "Machine learning",
      "Management",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Property (philosophy)",
      "Task (project management)",
      "Unsupervised learning"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Jing"
      },
      {
        "surname": "Liao",
        "given_name": "Jie"
      },
      {
        "surname": "Yuan",
        "given_name": "Jin"
      }
    ]
  },
  {
    "title": "Uncertainty-guided joint attention and contextual relation network for person re-identification",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103822",
    "abstract": "Due to the influence of factors such as camera angle and pose changes, some salient local features are often suppressed in person re-identification tasks. Moreover, many existing person re-identification methods do not consider the relation between features. To address these issues, this paper proposes two novel approaches: (1) To solve the problem of being confused and misidentified when local features of different individuals have similar attributes, we design a contextual relation network that focuses on establishing the relationship between local features and contextual features, so that all local features of the same person both contain contextual information. (2) To fully and correctly express key local features, we propose an uncertainty-guided joint attention module. The module focuses on the joint representation of individual pixels and local spatial features to enhance the credibility of local features. Finally, our method achieves competitive performance on four widely recognized datasets compared with state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032300072X",
    "keywords": [
      "Architectural engineering",
      "Artificial intelligence",
      "Attention network",
      "Biology",
      "Botany",
      "Computer science",
      "Computer security",
      "Credibility",
      "Data mining",
      "Engineering",
      "Feature (linguistics)",
      "Identification (biology)",
      "Joint (building)",
      "Key (lock)",
      "Law",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Relation (database)",
      "Representation (politics)",
      "Salient"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Dengwen"
      },
      {
        "surname": "Chen",
        "given_name": "Yanbing"
      },
      {
        "surname": "Wang",
        "given_name": "Wangmeng"
      },
      {
        "surname": "Tie",
        "given_name": "Zhixin"
      },
      {
        "surname": "Fang",
        "given_name": "Xian"
      },
      {
        "surname": "Ke",
        "given_name": "Wei"
      }
    ]
  },
  {
    "title": "Rotational Voxels Statistics Histogram for both real-valued and binary feature representations of 3D local shape",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103817",
    "abstract": "3D Local feature description is an active and fundamental task in 3D computer vision. However, most of the existing descriptors fail to simultaneously achieve satisfactory performance among descriptiveness, robustness, efficiency, and compactness. To address these limitations, we first propose a real-valued descriptor named Rotational Voxels Statistics Histogram (RoVo), which exploits the novel 3D multi-pose processing mechanism proposed in this paper to calculate the 3D voxel density distribution in different 3D poses. Moreover, through well-designed binary encoding algorithms, we conduct the seamless extension of the real-valued RoVo descriptor to three binary representations that have different performance characteristics. Extensive evaluation experiments validate the superiority of the real-valued and three binary RoVo descriptors concerning descriptiveness, robustness, and efficiency. Furthermore, the three binary RoVo descriptors extend the performance of high compactness. Lastly, we perform the experiments of 3D scene registration and 3D object recognition to intuitively present the effectiveness of the four proposed RoVo descriptors.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000676",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Binary number",
      "Biochemistry",
      "Chemistry",
      "Compact space",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Feature extraction",
      "Gene",
      "Histogram",
      "Image (mathematics)",
      "Linguistics",
      "Local binary patterns",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pure mathematics",
      "Robustness (evolution)",
      "Transformation geometry",
      "Voxel"
    ],
    "authors": [
      {
        "surname": "Hao",
        "given_name": "Linbo"
      },
      {
        "surname": "Yang",
        "given_name": "Xuefeng"
      },
      {
        "surname": "Xu",
        "given_name": "Ke"
      },
      {
        "surname": "Yi",
        "given_name": "Wentao"
      },
      {
        "surname": "Shen",
        "given_name": "Ying"
      },
      {
        "surname": "Wang",
        "given_name": "Huaming"
      }
    ]
  },
  {
    "title": "Corrigendum to “Generative detect for occlusion object based on occlusion generation and feature completing” [J. Visual Commun. Image Represent. 78 (2021) 103189]",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103809",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000597",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Generative grammar",
      "Image (mathematics)",
      "Internal medicine",
      "Linguistics",
      "Medicine",
      "Object (grammar)",
      "Occlusion",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Can"
      },
      {
        "surname": "Lang",
        "given_name": "Wenxi"
      },
      {
        "surname": "Xin",
        "given_name": "Rui"
      },
      {
        "surname": "Mao",
        "given_name": "Kaichen"
      },
      {
        "surname": "Jiang",
        "given_name": "Haiyan"
      }
    ]
  },
  {
    "title": "Low calculation cost of HEVC coding unit size based on spatial homogeneity detection",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103819",
    "abstract": "For the same video quality, HEVC gives 25% to 50% bitrate savings, compared to its predecessor the Advanced Video Coding H.264 and thus supports resolutions up to 8 K UHD. However, the reduction in bitrates provided by the HEVC leads to an increase in the computational cost of the encoding operation. This complexity can become a true handicap especially for real-time video streaming and also for VANET (Vehicular Ad-Hoc Network) applications such as traffic safety and Video surveillance. The improvement in the bitrates and also the increase in the calculation cost are due to the use of large and multi-sized coding, prediction and transform blocks. Indeed, the H264 coder is based on structure macroblocks with sizes 4 × 4, 8 × 8 and 16 × 16, while H.265 depends on Coding Tree Units (CTUs), CTUs select sizes 4 × 4, 8 × 8, 16 × 16, 32 × 32 and 64 × 64 blocks. This paper proposes a fast CU (Coding Unit) size decision method to reduce the HEVC calculation cost based on spatial homogeneity. Compared with the HM16.13 benchmark test model, the average coding time is reduced by around 40% for CIF / QCIF video sequences, 35% to 43% for class A, B and C test sequences. These important reductions in coding time are obtained with negligible loss of quality and an average increase in bitrates which does not exceed 0.89% for the three configuration modes (All intra, Random Access and Low Delay).",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032300069X",
    "keywords": [
      "Algorithm",
      "Algorithmic efficiency",
      "Coding (social sciences)",
      "Coding tree unit",
      "Computer science",
      "Decoding methods",
      "Economics",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Real-time computing",
      "Statistics",
      "Video quality"
    ],
    "authors": [
      {
        "surname": "Brai",
        "given_name": "Radhia."
      },
      {
        "surname": "Bekhouch",
        "given_name": "Amara"
      },
      {
        "surname": "Doghmane",
        "given_name": "Noureddine"
      },
      {
        "surname": "Harize",
        "given_name": "Saliha"
      },
      {
        "surname": "Kouadria",
        "given_name": "Nasreddine"
      }
    ]
  },
  {
    "title": "Joint face completion and super-resolution using multi-scale feature relation learning",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103806",
    "abstract": "Previous research on face restoration often focused on repairing specific types of low-quality facial images such as low-resolution (LR) or occluded facial images. However, in the real world, both the above-mentioned forms of image degradation often coexist. Therefore, it is important to design a model that can repair images that are LR and occluded simultaneously. This paper proposes a multi-scale feature graph generative adversarial network (MFG-GAN) to carry out face restoration in contexts in which both LR and occluded degradation modes coexist, and also to repair images with a single type of degradation. Based on the GAN, the MFG-GAN integrates the graph convolution and feature pyramid networks to restore occluded low-resolution face images to non-occluded high-resolution face images. The MFG-GAN uses a set of customized losses to ensure that high-quality images are generated. In addition, we designed the network in an end-to-end format. We conduct experiments on general face image restoration and facial expression restoration. Experimental results on the public-domain databases show that the proposed approach outperforms state-of-the-art methods in performing face super resolution (up to 4 × or 8 × ) and face completion simultaneously and can recover better facial expression details.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000561",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Expression (computer science)",
      "Face (sociological concept)",
      "Facial expression",
      "Feature (linguistics)",
      "Generative adversarial network",
      "Graph",
      "Image (mathematics)",
      "Image processing",
      "Image restoration",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Social science",
      "Sociology",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Zhilei"
      },
      {
        "surname": "Zhang",
        "given_name": "Chenggong"
      },
      {
        "surname": "Wu",
        "given_name": "Yunpeng"
      },
      {
        "surname": "Zhang",
        "given_name": "Cuicui"
      }
    ]
  },
  {
    "title": "Human object interaction detection based on feature optimization and key human-object enhancement",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103824",
    "abstract": "Aiming at the problem of unclear or missing human object interaction behavior objects in complex background, we propose a human object interaction detection algorithm based on feature optimization and key human-object enhancement. In order to solve the problem of missing human behavior objects, we propose Feature Optimized Faster Region Convolutional Neural Network (FOFR-CNN). FOFR-CNN is an object detection network optimized by multi-scale feature optimization algorithm, taking into account both image semantics and image structure. In order to reduce the interference of complex background, we propose a Key Human-Object Enhancement Network. The network uses an instance-based method to enhance the features of interactive objects. In order to enrich the interaction information, we use the graph convolutional network. Experimental results on HICO-DET, V-COCO and HOI-A datasets show that the proposed algorithm has significantly improved accuracy and multi-scale object detection ability compared with other human object interaction algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000743",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Key (lock)",
      "Linguistics",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Ye",
        "given_name": "Qing"
      },
      {
        "surname": "Wang",
        "given_name": "Xikun"
      },
      {
        "surname": "Li",
        "given_name": "Rui"
      },
      {
        "surname": "Zhang",
        "given_name": "Yongmei"
      }
    ]
  },
  {
    "title": "Global-guided weakly-supervised learning for multi-label image classification",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103823",
    "abstract": "Multi-label classification with region-free labels is attracting increasing attention compared to that with region-based labels due to the time-consuming manual region-labeling process. Existing methods usually employ attention-based technology to discover the conspicuous label-related regions in a weakly-supervised manner with only image-level region-free labels, while the region covering is not precise without exploring global clues of multi-level features. To address this issue, a novel Global-guided Weakly-Supervised Learning (GWSL) method for multi-label classification is proposed. The GWSL first extracts the multi-level features to estimate their global correlation map which is further utilized to guide feature disentanglement in the proposed Feature Disentanglement and Localization (FDL) networks. Specifically, the FDL networks then adaptively combine the different correlated features and localize the fine-grained features for identifying multiple labels. The proposed method is optimized in an end-to-end manner under weakly supervision with only image-level labels. Experimental results demonstrate that the proposed method outperforms the state-of-the-arts for multi-label learning problems on several publicly available image datasets. To facilitate similar researches in the future, the codes are directly available online at https://github.com/Yong-DAI/GWSL.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000731",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Correlation",
      "Feature (linguistics)",
      "Geometry",
      "Image (mathematics)",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Process (computing)"
    ],
    "authors": [
      {
        "surname": "Dai",
        "given_name": "Yong"
      },
      {
        "surname": "Song",
        "given_name": "Weiwei"
      },
      {
        "surname": "Gao",
        "given_name": "Zhi"
      },
      {
        "surname": "Fang",
        "given_name": "Leyuan"
      }
    ]
  },
  {
    "title": "Dual-branch deep image prior for image denoising",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103821",
    "abstract": "In this work, we propose a two-stage denoising approach, which includes generation and fusion stages. Specifically, in the generation stage, we first split the expanding path of the UNet backbone of the standard DIP (deep image prior) network into two branches, converting it into a Y-shaped network (YNet). Then we adopt the initial denoised images obtained with DAGL (dynamic attentive graph learning) and Restormer methods together with the given noisy image as the target images. Finally, we utilize the standard DIP on-line training routine to generate two complementary basic images, whose image quality is quite improved, with the help of a novel automatic iteration termination mechanism. In the fusion stage, we first split the contracting path of the standard UNet network into two branches for receiving the two basic images generated in the previous stage, and obtain a fused image as the final denoised image in a fully unsupervised manner. Extensive experimental results confirm that our method has a significant improvement over the standard DIP or other unsupervised methods, and outperforms recently proposed supervised denoising models. The noticeable performance improvement is attributed to the proposed hybrid strategy, i.e., we first adopt the supervised denoising methods to process the common content of images substantially, then utilize the unsupervised method to fine-tune the specific details. In other words, we take full advantage of the high performance of the supervised methods and the flexibility of the unsupervised methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000718",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Image (mathematics)",
      "Noise reduction",
      "Pattern recognition (psychology)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Shaoping"
      },
      {
        "surname": "Cheng",
        "given_name": "Xiaohui"
      },
      {
        "surname": "Luo",
        "given_name": "Jie"
      },
      {
        "surname": "Xiao",
        "given_name": "Nan"
      },
      {
        "surname": "Xiong",
        "given_name": "Minghai"
      },
      {
        "surname": "Zhou",
        "given_name": "Changfei"
      }
    ]
  },
  {
    "title": "Semantical video coding: Instill static-dynamic clues into structured bitstream for AI tasks",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103816",
    "abstract": "Traditional media coding schemes typically encode image or video into a semantic-unknown binary stream, which fails to directly support downstream intelligent tasks at the bitstream level. Semantically Structured Image Coding (SSIC) (Sun et al., 2020) makes the first attempt to enable partial-decoding image intelligent task analysis via a Semantically Structured Bitstream (SSB). However, the SSIC considers image coding and its generated SSB only contains the static object information. In this paper, we propose an advanced Semantically Structured Video Coding (SSVC). Video signals contain more rich dynamic motion information and redundancy. Thus, we present a reformulation of semantically structured bitstream (SSB) in SSVC which contains both static object characteristics and dynamic motion clues. Specifically, we introduce optical flow to encode continuous motion information and reduce cross-frame redundancy via a predictive coding architecture, then the optical flow and residual information are reorganized into SSB, which enables the proposed SSVC could better adaptively support video-based downstream intelligent applications. Extensive experiments on various vision tasks demonstrate that the proposed SSVC framework could directly support multiple intelligent tasks just depending on a partially decoded bitstream, saving bitrate consumption for intelligent analytics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000664",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Bitstream",
      "Chemistry",
      "Coding (social sciences)",
      "Computer science",
      "Computer vision",
      "Decoding methods",
      "ENCODE",
      "Gene",
      "Mathematics",
      "Operating system",
      "Real-time computing",
      "Redundancy (engineering)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Jin",
        "given_name": "Xin"
      },
      {
        "surname": "Feng",
        "given_name": "Ruoyu"
      },
      {
        "surname": "Sun",
        "given_name": "Simeng"
      },
      {
        "surname": "Feng",
        "given_name": "Runsen"
      },
      {
        "surname": "He",
        "given_name": "Tianyu"
      },
      {
        "surname": "Chen",
        "given_name": "Zhibo"
      }
    ]
  },
  {
    "title": "Corrigendum to “FFTI: Image inpainting algorithm via features fusion and two-steps inpainting” [J. Visual Commun. Image Represent. 91 (2023) 103776]",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103802",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000524",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Fusion",
      "Image (mathematics)",
      "Image fusion",
      "Inpainting",
      "Linguistics",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Yuantao"
      },
      {
        "surname": "Xia",
        "given_name": "Runlong"
      },
      {
        "surname": "Zou",
        "given_name": "Ke"
      },
      {
        "surname": "Yang",
        "given_name": "Kai"
      }
    ]
  },
  {
    "title": "Semantic-embedding Guided Graph Network for cross-modal retrieval",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103807",
    "abstract": "Many methods focus on aligning image regions with the corresponding text fragments, and ignore that images contain fragments that cannot be expressed by texts. To fully express the information of images and prevent the performance degradation caused by fine-grained information deviating from the core meaning of images, we propose Semantic-embedding Guided Graph Network (SGGN) for cross-modal retrieval. It learns the detail representations of each modality, with an integrated semantic information, by guiding local fragments to capture the internal correlation of cross-modal data and effectively convey the information. To further bridge the semantic gap between different modalities, SGGN uses adversarial network to play a game, and uses graph aggregation network to absorb complementary information of neighbor samples. We evaluate our approach on two datasets. Our method (based on R @ 10 ) achieves 97.2% on Flickr30k dataset. On MS-COCO dataset, it reaches 99.2% using 1 K test set and 92.0% using 5 K test set.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000573",
    "keywords": [
      "Artificial intelligence",
      "Bridging (networking)",
      "Chemistry",
      "Computer network",
      "Computer science",
      "Embedding",
      "Focus (optics)",
      "Graph",
      "Graph embedding",
      "Information retrieval",
      "Modal",
      "Natural language processing",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Polymer chemistry",
      "Programming language",
      "Set (abstract data type)",
      "Test set",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Yuan",
        "given_name": "Mengru"
      },
      {
        "surname": "Zhang",
        "given_name": "Huaxiang"
      },
      {
        "surname": "Liu",
        "given_name": "Dongmei"
      },
      {
        "surname": "Wang",
        "given_name": "Lin"
      },
      {
        "surname": "Liu",
        "given_name": "Li"
      }
    ]
  },
  {
    "title": "Multitone reconstruction visual cryptography based on phase periodicity",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103827",
    "abstract": "The current no-computation grayscale image visual cryptography (VC) can only achieve halftone reconstruction but cannot truly achieve multitone. To solve this problem, we propose the concept of phase periodicity of the λ / 2 retarder film and calculate the optical axis angle set with phase periodicity. According to the phase periodicity, we propose a λ / 2 retarder film phase periodicity visual cryptography (RPP-VC). In RPP-VC, the secret pixels are encoded as the optical axis angles of n λ / 2 retarder films and distributed to n shares. The decoding process does not require computation. The reconstructed image has no pixel expansion and can reach up to 23 tones. The quality of the reconstructed images has been greatly improved and the evaluation indicators of perceived quality are nearly doubled compared with other grayscale image VC schemes. The experimental results verify the feasibility of RPP-VC.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000779",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computation",
      "Computer science",
      "Computer vision",
      "Cryptography",
      "Decoding methods",
      "Encryption",
      "Grayscale",
      "Halftone",
      "Image (mathematics)",
      "Image quality",
      "Mathematics",
      "Operating system",
      "Phase (matter)",
      "Physics",
      "Pixel",
      "Process (computing)",
      "Quantum mechanics",
      "Secret sharing",
      "Visual cryptography"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Zi-Nan"
      },
      {
        "surname": "Liu",
        "given_name": "Tao"
      },
      {
        "surname": "Yan",
        "given_name": "Bin"
      },
      {
        "surname": "Pan",
        "given_name": "Jeng-Shyang"
      },
      {
        "surname": "Yang",
        "given_name": "Hong-Mei"
      }
    ]
  },
  {
    "title": "Rethinking PASCAL-VOC and MS-COCO dataset for small object detection",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103830",
    "abstract": "The data and the algorithm are critical to deep learning-based small object detectors. In this paper, we rethink the PASCAL-VOC and MS-COCO dataset for small object detection. By visual analysis of the original annotations, we find that there are different labeling errors in these two datasets. To solve these problems, we build specific datasets, including SDOD, Mini6K, Mini2022 and Mini6KClean. The experimental results of several typical algorithms (e.g. SSD, YOLOv5, Faster RCNN and Deformable DETR) on the datasets show that data labeling errors (such as missing labels, category label errors, inappropriate labels) are another factor that affects the detection performance of small objects.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000809",
    "keywords": [
      "Artificial intelligence",
      "Coco",
      "Computer science",
      "Detector",
      "Object (grammar)",
      "Object detection",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Programming language",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Tong",
        "given_name": "Kang"
      },
      {
        "surname": "Wu",
        "given_name": "Yiquan"
      }
    ]
  },
  {
    "title": "A computationally efficient moving object detection technique using tensor QR decomposition based TRPCA framework",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103785",
    "abstract": "Advancements in high-quality video cameras and the consequent capture of minute details of the scene have led the field of computer vision to remarkable heights. This paper develops a tensor QR decomposition-based approach for Moving Object Detection (MOD), which aims to reduce the computational complexity without disturbing the structural framework of the input video frames. The increased performance and efficiency of the proposed method lie in the usage of tensor QR decomposition along with l 2 , 1 norm and l 1 / 2 norm. It is designed on top of a tensor-based Robust Principal Component Analysis (TRPCA) framework. In addition, this work safeguards the variation along the spatio-temporal directions with the effective use of Tensor Total Variation (TTV) regularization. The results and the analysis prove that the proposed method improves the F-measure by 15%–45% and reduces the computational complexity by 75%–85% with respect to the counterparts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000354",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computational complexity theory",
      "Computer science",
      "Computer vision",
      "Eigenvalues and eigenvectors",
      "Geometry",
      "Law",
      "Mathematical optimization",
      "Mathematics",
      "Matrix decomposition",
      "Norm (philosophy)",
      "Physics",
      "Political science",
      "Principal component analysis",
      "QR decomposition",
      "Quantum mechanics",
      "Regularization (linguistics)",
      "Singular value decomposition",
      "Tensor (intrinsic definition)"
    ],
    "authors": [
      {
        "surname": "Sabat",
        "given_name": "Neelesh"
      },
      {
        "surname": "M.S.",
        "given_name": "Subodh Raj"
      },
      {
        "surname": "George",
        "given_name": "Sudhish N."
      },
      {
        "surname": "T.K.",
        "given_name": "Sunil Kumar"
      }
    ]
  },
  {
    "title": "A bilateral attention based generative adversarial network for DIBR 3D image watermarking",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103794",
    "abstract": "This paper presents a bilateral attention based generative adversarial network (BAGAN) for depth-image-based rendering (DIBR) 3D image watermarking to protect the image copyright. Convolutional block operations are employed to extract main image features for robust watermarking, but embedding watermark into some features will degrade image quality much. To relieve this kind of image distortion, the bilateral attention module (BAM) is utilized by mining correlations of the center view and the depth map to compute attention of the 3D image for guiding watermark to distribute over different image regions. Since a modality gap exists between the center view and the depth map, a cross-modal feature fusion module (CMFFM) is designed for BAM to bridge the cross-view gap. Because the depth map has lots of flat background information including many redundant features, to prune them, the depth redundancy elimination module (DREM) is used for cross-view feature fusion. In the decoder, two extractors with the same structure are built to recover watermark from the center view and the synthesized view, respectively. In addition, the discriminator is supposed to build a competitive relationship with the encoder to increase the image quality. The noise sub-network is used to train different image attacks for robustness. Extensive experimental results have demonstrated that the proposed BAGAN can obtain higher watermarking invisibility and robustness compared with existing DIBR 3D watermarking methods. Ablation experiments have also proven the effectiveness of DREM, CMFFM and BAM on BAGAN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000445",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Detector",
      "Digital watermarking",
      "Discriminator",
      "Feature (linguistics)",
      "Gene",
      "Generative adversarial network",
      "Image (mathematics)",
      "Image quality",
      "Inpainting",
      "Linguistics",
      "Philosophy",
      "Rendering (computer graphics)",
      "Robustness (evolution)",
      "Telecommunications",
      "Watermark"
    ],
    "authors": [
      {
        "surname": "He",
        "given_name": "Zhouyan"
      },
      {
        "surname": "He",
        "given_name": "Lingqiang"
      },
      {
        "surname": "Xu",
        "given_name": "Haiyong"
      },
      {
        "surname": "Chai",
        "given_name": "Tong-Yuen"
      },
      {
        "surname": "Luo",
        "given_name": "Ting"
      }
    ]
  },
  {
    "title": "A simple and effective static gesture recognition method based on attention mechanism",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103783",
    "abstract": "To solve the problem of low sign language recognition rate under the condition of small samples, a simple and effective static gesture recognition method based on an attention mechanism is proposed. The method proposed in this paper can enhance the features of both the details and the subject of the gesture image. The input of the proposed method depends on the intermediate feature map generated by the original network. Also, the proposed convolutional model is a lightweight general module, which can be seamlessly integrated into any CNN(Convolutional Neural Network) architecture and achieve significant performance gains with minimal overhead. Experiments on two different datasets show that the proposed method is effective and can improve the accuracy of sign language recognition of the benchmark model, making its performance better than the existing methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000330",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Convolutional neural network",
      "Epistemology",
      "Feature (linguistics)",
      "Geodesy",
      "Geography",
      "Gesture",
      "Gesture recognition",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Overhead (engineering)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Sign (mathematics)",
      "Sign language",
      "Simple (philosophy)",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Lizao"
      },
      {
        "surname": "Tian",
        "given_name": "Qiuhong"
      },
      {
        "surname": "Ruan",
        "given_name": "Qionglu"
      },
      {
        "surname": "Shi",
        "given_name": "Zhixiang"
      }
    ]
  },
  {
    "title": "Deep interactive image segmentation based on region and Boundary-click guidance",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103797",
    "abstract": "In interactive image segmentation, the target object of interest can be extracted based on the guidance of user interactions. One of the main goals in this task is to reduce the user interaction burden and ensure satisfactory segmentation with as few interactions as possible. Thanks to the development of deep learning technology, neural network-based interactive approaches have significantly improved the segmentation performance through powerful feature representation. Only limited point (click) interaction is required for the user to complete the segmentation. This paper mainly follows the deep learning-based interactive segmentation methods and explores more efficient interaction strategies and effective segmentation models. We further simplify user interaction to two clicks, where the first click is utilized to select the target region and the other aims to determine the target boundary. Based on the region and boundary clicks, an interactive two-stream network structure is naturally derived to learn the region and boundary features of interest. Furthermore, we also construct an information diffusion module to better propagate the region and boundary-click labels, which helps to enhance the similarity within the region and the discrimination between boundaries. Vast experiments on the popular GrabCut, Berkeley, DAVIS, MS COCO and SBD datasets verified the effectiveness of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000470",
    "keywords": [
      "Artificial intelligence",
      "Boundary (topology)",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Feature (linguistics)",
      "Geometry",
      "Image (mathematics)",
      "Image segmentation",
      "Law",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Point (geometry)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Scale-space segmentation",
      "Segmentation",
      "Segmentation-based object categorization",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Qian",
        "given_name": "Yuxiang"
      },
      {
        "surname": "Xue",
        "given_name": "Yang"
      },
      {
        "surname": "Wang",
        "given_name": "Tao"
      }
    ]
  },
  {
    "title": "Reversible data hiding based on automatic contrast enhancement using histogram expansion",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103798",
    "abstract": "A lot of Reversible Data Hiding (RDH) methods aim to generate a stego image infinitely approaches the original image while the quality of the original image is leaved out of consideration. Juxtaposed with a plain image, a contrast enhanced version always improves the user experience significantly. Reversible Data Hiding with Contrast Enhancement (RDHCE) enhances the stego image contrast combined with its payloads and enables the cover image to be regained accurately after the payloads have been extracted. This study presents a novel RDHCE method using histogram expansion. First, a new local histogram selecting strategy is proposed to improve the contrast of the whole image. Meanwhile, the global average brightness is used as a reference to determine the shifting direction of the local histogram to prevent the image from being over-enhanced. Moreover, the contrast can be improved adaptively when a reasonable number of data is embedded at the selected embedding points. Experimental results show that, with a given payload, the proposed method achieves better contrast and maintains good visual quality compared with state-of-the-arts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000482",
    "keywords": [
      "Adaptive histogram equalization",
      "Artificial intelligence",
      "Brightness",
      "Color image",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Contrast (vision)",
      "Cover (algebra)",
      "Embedding",
      "Engineering",
      "Histogram",
      "Histogram equalization",
      "Histogram matching",
      "Image (mathematics)",
      "Image histogram",
      "Image processing",
      "Image quality",
      "Information hiding",
      "Mathematics",
      "Mechanical engineering",
      "Network packet",
      "Optics",
      "Pattern recognition (psychology)",
      "Payload (computing)",
      "Physics"
    ],
    "authors": [
      {
        "surname": "Lyu",
        "given_name": "WanLi"
      },
      {
        "surname": "Yue",
        "given_name": "YaJie"
      },
      {
        "surname": "Yin",
        "given_name": "Zhaoxia"
      }
    ]
  },
  {
    "title": "Compressive Spectral Video Sensing using the Convolutional Sparse Coding framework CSC4D",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103782",
    "abstract": "Spectral Videos (SV) contain a scene’s spatial–spectral-time information. Just as with Spectral Images (SI), SVs require expensive sensing hardware, storage plus high frame ratios. Although Super Resolution techniques improve the quality of low-resolution SVs, Compressive Spectral Video Sensing (CSVS) senses high-quality SVs by extending the Compressive Sensing Image (CSI) techniques. CSI uses the universal Sparse Signal Representation (SSR) model for SVs and SIs despite the limited quality of the recovered signals. On the other hand, dictionaries synthesis models are used successfully for representing SIs, SVs, and in CSI. This work proposes the 4D convolutional sparse representation (CSC4D) for recovering full-resolution SV from CSVS measurements. It is based on a multidimensional formulation of the CSC model, profiting from its robustness without additional optical flow information. Extensive numerical simulations (two CSI architectures and noise models) show that the proposed CSC4D+CSVS improves the state-of-the-art in both quality and border sharpness by up to 1.5 dB.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000329",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Coding (social sciences)",
      "Compressed sensing",
      "Computer science",
      "Computer vision",
      "Convolutional code",
      "Decoding methods",
      "Gene",
      "Image resolution",
      "Mathematics",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Robustness (evolution)",
      "Sparse approximation",
      "Statistics",
      "Temporal resolution"
    ],
    "authors": [
      {
        "surname": "Barajas-Solano",
        "given_name": "Crisostomo"
      },
      {
        "surname": "Ramirez",
        "given_name": "Juan-Marcos"
      },
      {
        "surname": "Torre",
        "given_name": "José Ignacio Martínez"
      },
      {
        "surname": "Arguello",
        "given_name": "Henry"
      }
    ]
  },
  {
    "title": "TransCAM: Transformer attention-based CAM refinement for Weakly supervised semantic segmentation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103800",
    "abstract": "Weakly supervised semantic segmentation (WSSS) with only image-level supervision is a challenging task. Most existing methods exploit Class Activation Maps (CAM) to generate pixel-level pseudo labels for supervised training. However, due to the local receptive field of Convolution Neural Networks (CNN), CAM applied to CNNs often suffers from partial activation — highlighting the most discriminative part instead of the entire object area. In order to capture both local features and global representations, the Conformer has been proposed to combine a visual transformer branch with a CNN branch. In this paper, we propose TransCAM, a Conformer-based solution to WSSS that explicitly leverages the attention weights from the transformer branch of the Conformer to refine the CAM generated from the CNN branch. TransCAM is motivated by our observation that attention weights from shallow transformer blocks are able to capture low-level spatial feature similarities while attention weights from deep transformer blocks capture high-level semantic context. Despite its simplicity, TransCAM achieves competitive performance of 69.3% and 69.6% on the respective PASCAL VOC 2012 validation and test sets, showing the effectiveness of transformer attention-based refinement of CAM for WSSS.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000500",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Discriminative model",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Physics",
      "Pixel",
      "Programming language",
      "Quantum mechanics",
      "Segmentation",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Ruiwen"
      },
      {
        "surname": "Mai",
        "given_name": "Zheda"
      },
      {
        "surname": "Zhang",
        "given_name": "Zhibo"
      },
      {
        "surname": "Jang",
        "given_name": "Jongseong"
      },
      {
        "surname": "Sanner",
        "given_name": "Scott"
      }
    ]
  },
  {
    "title": "Boosting semantic segmentation via feature enhancement",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103796",
    "abstract": "Semantic segmentation aims to map each pixel of an image into its corresponding semantic label. Most existing methods either mainly concentrate on high-level features or simple combination of low-level and high-level features from backbone convolutional networks, which may weaken or even ignore the compensation between different levels. To effectively take advantages from both shallow (textural) and deep (semantic) features, this paper proposes a novel plug-and-play module, namely feature enhancement module (FEM). The proposed FEM first uses an information extractor to extract the desired details or semantics from different stages, and then enhances target features by taking in the extracted message. Two types of FEM, i.e., detail FEM and semantic FEM, can be customized. Concretely, the former type strengthens textural information to protect key but tiny/low-contrast details from suppression/removal, while the other one highlights structural information to boost segmentation performance. By equipping a given backbone network with FEMs, there might contain two information flows, i.e., detail flow and semantic flow. Extensive experiments on the Cityscapes, ADE20K and PASCAL Context datasets are conducted to validate the effectiveness of our design. The code has been released at https://github.com/SuperZ-Liu/FENet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000469",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Feature (linguistics)",
      "Key (lock)",
      "Linguistics",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Segmentation",
      "Semantic feature",
      "Semantics (computer science)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Zhi"
      },
      {
        "surname": "Zhang",
        "given_name": "Yi"
      },
      {
        "surname": "Guo",
        "given_name": "Xiaojie"
      }
    ]
  },
  {
    "title": "A convolutional autoencoder model with weighted multi-scale attention modules for 3D skeleton-based action recognition",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103781",
    "abstract": "The 3D skeleton sequences of action can be recognized based on series of meaningful movements including changes in the direction and geometry features of the body pose. In this paper, we introduce the 3DPo-CDP descriptor, which incorporates the change direction patterns of body joints and pose features in a unified deep structure to learn more adequate features for the action recognition problem. To this end, two types of features are extracted. First, Change Direction Patterns (CDPs) are extracted by following the important points of motion trajectories where a significant change of direction has occurred using two filtering phases. The CDPs capture the global features which are invariant to noise and insignificant temporal dynamics of joints. Second, Pose Features are employed to learn the intrinsic connectivity relationships of adjacent limbs and the variance distances of body joints from representative joints to concentrate on key-frames and informative joints. The complementary features of CDPs and 3D pose, which are transformed into images, are combined in a unified representation and fed into a new convolutional autoencoder. Unlike conventional convolutional autoencoders that focus on frames, high-level discriminative features of spatiotemporal relationships of whole body joints are extracted by introducing weighted multi-scale channel and spatial attention modules. In this paper, we show that adjacent and non-adjacent neighbors can be effectively used to compute different weights for extracting cross-interaction channels and multi-scale spatial relationships of the current pixel. The extracted features are combined with the wavelet representation of statistical body information and then classified with a multi-class SVM classifier. The experimental results demonstrate the effectiveness of the augmented 3DPo-CDP descriptor using an attentional convolution autoencoder structure on five challenging 3D action recognition datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000317",
    "keywords": [
      "Artificial intelligence",
      "Autoencoder",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Discriminative model",
      "Invariant (physics)",
      "Law",
      "Mathematical physics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Politics",
      "Quantum mechanics",
      "Representation (politics)",
      "Scale (ratio)",
      "Wavelet"
    ],
    "authors": [
      {
        "surname": "Khezerlou",
        "given_name": "F."
      },
      {
        "surname": "Baradarani",
        "given_name": "A."
      },
      {
        "surname": "Balafar",
        "given_name": "M.A."
      }
    ]
  },
  {
    "title": "Global attention retinex network for low light image enhancement",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103795",
    "abstract": "Most low-light image enhancement methods only adjust the brightness, contrast and noise reduction of low-light images, making it difficult to recover the lost information in darker areas of the image, and even cause color distortion and blurring. To solve the above problems, a global attention-based Retinex network (GARN) for low-light image enhancement is proposed in this paper. We propose a novel global attention module which computes multiple dimensional information in the channel attention module to help facilitate inference learning. Then the global attention module is embedded into different layers of the network to extract richer shallow texture features and deep semantic features. This means that the rich features are more conducive to learning the mapping relationship between low-light images to normal-light images, so that the detail recovery of dark regions is enhanced in low-light images. We also collected a low/normal light image dataset with multiple scenes, in which the images paired as training set can succeed to be applied to low-light image enhancement under different lighting conditions. Experimental results on publicly available datasets show that our method has better effectiveness and generality than the state-of-the-art methods in terms of evaluations metrics such as PSNR, SSIM, NIQE, Entropy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000457",
    "keywords": [
      "Artificial intelligence",
      "Brightness",
      "Color constancy",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yongnian"
      },
      {
        "surname": "Zhang",
        "given_name": "Zhibin"
      }
    ]
  },
  {
    "title": "Virtual view synthesis using joint information from multi-view",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103799",
    "abstract": "Immersive media has attracted widespread attention with the development of virtual reality. Three Degree of Freedom Plus media greatly enhances the user experience by allowing users’ head motion and viewpoint switching within a certain range. Due to the limitation of panoramic video acquisition and transmission, it is impossible to obtain videos from any viewpoint directly. Virtual view synthesis is the general solution to this problem. However, existing algorithms do not adequately consider the pixel correlation between multiple views. Thus, we propose a virtual view synthesis algorithm using joint information from multi-view panoramic videos to further explore the pixel correlation. Specifically, sub-pixels from different reference views in the virtual view are obtained by performing multi-view three-dimensional image warping. Dedicated area division and interpolation methods are then designed to improve the synthesized quality. Experimental results show that the proposed algorithm outperforms the state-of-the-art virtual view synthesis algorithms in performance and efficiency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000494",
    "keywords": [
      "Algorithm",
      "Architectural engineering",
      "Arithmetic",
      "Artificial intelligence",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Division (mathematics)",
      "Engineering",
      "Image warping",
      "Interpolation (computer graphics)",
      "Joint (building)",
      "Mathematics",
      "Motion (physics)",
      "Pixel",
      "Rendering (computer graphics)",
      "View synthesis",
      "Virtual reality"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yifan"
      },
      {
        "surname": "Yang",
        "given_name": "Fuzheng"
      },
      {
        "surname": "Chen",
        "given_name": "Ying"
      },
      {
        "surname": "Zhang",
        "given_name": "Wei"
      }
    ]
  },
  {
    "title": "Automatic fake document identification and localization using DE-Net and color-based features of foreign inks",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103801",
    "abstract": "Document examination is a vital mission for revealing illegal modifications that assist in the detection and resolution of criminal acts. Addition and alteration are more frequently used in handwritten documents. However, most of the documents have been modified with similar inks, and it is tough to detect or observe them with human eyes. As a result, there is a need for methods to automatically detect handwriting forgery to reach an accurate detection efficiently. In this paper, a novel and efficient method is proposed for automatically detecting altered handwritten documents and locating the fake part. Therefore, DE-Net is proposed to identify the forged document using a digitally scanned version of the document. Unlike the existing methods, a further localization schema is applied to locate the forged parts in the candidate forged document accurately. Where each forged document is segmented into objects. Color histograms of R, G, and B channels are used to generate a fused feature vector for each object. Then a structural similarity index (SSIM) is applied to detect the lower similarity parts as forged. The experimental results demonstrate that the proposed method can identify and localize foreign ink in handwritten documents with high performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000512",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Feature vector",
      "Handwriting",
      "Histogram",
      "Identification (biology)",
      "Image (mathematics)",
      "Information retrieval",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Schema (genetic algorithms)",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Fadl",
        "given_name": "Sondos"
      },
      {
        "surname": "Hosny",
        "given_name": "Khalid M."
      },
      {
        "surname": "Hammad",
        "given_name": "Mohamed"
      }
    ]
  },
  {
    "title": "Learning dynamic relationship between joints for 3D hand pose estimation from single depth map",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103803",
    "abstract": "3D hand pose estimation from a single depth map is an essential topic in computer vision. Most existing methods are devoted to designing a model to capture more spatial information or designing loss functions based on prior knowledge to constrain the estimated pose with prior spatial information. In this work, we focus on constraining the estimation process with spatial information adaptively by learning the mutual position relationship between joint pairs. Specifically, we propose a dynamic relationship network (DRN) with dynamic anchors. The preset fixed anchors are employed to estimate the position of each joint initially. Then, each joint is considered a dynamic anchor, which plays the role of a dynamic regressor to adjust the initially estimated position of each joint. The final estimation of each joint is the weighted sum of the results from all the dynamic anchors. Extensive experiments on benchmarks demonstrate that our method provides competitive results compared with state-of-the-arts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000536",
    "keywords": [
      "Architectural engineering",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Economics",
      "Engineering",
      "Estimation",
      "Finance",
      "Focus (optics)",
      "Joint (building)",
      "Mutual information",
      "Operating system",
      "Optics",
      "Physics",
      "Pose",
      "Position (finance)",
      "Process (computing)",
      "Systems engineering"
    ],
    "authors": [
      {
        "surname": "Xing",
        "given_name": "Huiqin"
      },
      {
        "surname": "Yang",
        "given_name": "Jianyu"
      },
      {
        "surname": "Xiao",
        "given_name": "Yang"
      }
    ]
  },
  {
    "title": "Image fusion based on discrete Chebyshev moments",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103784",
    "abstract": "Though deep learning-based methods have demonstrated strong capabilities on image fusion, they usually improve the fusion performance by increasing the width and depth of the network, increasing the computational effort and being unsuitable for industrial applications. In this paper, an end-to-end network based on fixed convolution module of discrete Chebyshev moments is proposed, which does not need any pre- or post-processing. The proposed network is roughly composed of three parts: feature extraction module, fusion module and feature reconstruction module. In the feature extraction module, a novel fixed convolution module based on discrete Chebyshev moments is proposed to obtain different frequency components in a short time. To improve the image sharpness and fuse more details, a spatial attention mechanism based on average gradient is proposed in fusion module. Extensive results demonstrate that the proposed network can achieve remarkable fusion performance, high time efficiency and strong generalization ability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000342",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Chebyshev filter",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Electrical engineering",
      "Engineering",
      "Feature (linguistics)",
      "Feature extraction",
      "Fuse (electrical)",
      "Fusion",
      "Generalization",
      "Image (mathematics)",
      "Image fusion",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Upsampling"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Xiaoxuan"
      },
      {
        "surname": "Xu",
        "given_name": "Shuwen"
      },
      {
        "surname": "Hu",
        "given_name": "Shaohai"
      },
      {
        "surname": "Ma",
        "given_name": "Xiaole"
      }
    ]
  },
  {
    "title": "Cross-view information interaction and feedback network for face hallucination",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103758",
    "abstract": "Hallucinating a photo-realistic frontal face image from a low-resolution (LR) non-frontal face image is beneficial for a series of face-related applications. However, previous efforts either focus on super-resolving high-resolution (HR) face images from nearly frontal LR counterparts or frontalizing non-frontal HR faces. It is necessary to address all these challenges jointly for real-world face images in unconstrained environment. In this paper, we develop a novel Cross-view Information Interaction and Feedback Network (CVIFNet), which simultaneously handles the non-frontal LR face image super-resolution (SR) and frontalization in a unified framework and interacts them with each other to further improve their performance. Specifically, the CVIFNet is composed of two feedback sub-networks for frontal and profile face images. Considering the reliable correspondence between frontal and non-frontal face images can be crucial and contribute to face hallucination in a different manner, we design a cross-view information interaction module (CVIM) to aggregate HR representations of different views produced by the SR and frontalization processes to generate finer face hallucination results. Besides, since 3D rendered facial priors contain rich hierarchical features, such as low-level (e.g., sharp edge and illumination) and perception level (e.g., identity) information, we design an identity-preserving consistency loss based on 3D rendered facial priors, which can ensure that the high-frequency details of frontal face hallucination result are consistent with the profile. Extensive experiments demonstrate the effectiveness and advancement of CVIFNet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000081",
    "keywords": [
      "Acoustics",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Face (sociological concept)",
      "Face detection",
      "Face hallucination",
      "Facial recognition system",
      "Hallucinating",
      "Identity (music)",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Huan"
      },
      {
        "surname": "Chi",
        "given_name": "Jianning"
      },
      {
        "surname": "Wu",
        "given_name": "Chengdong"
      },
      {
        "surname": "Yu",
        "given_name": "Xiaosheng"
      },
      {
        "surname": "Wu",
        "given_name": "Hao"
      }
    ]
  },
  {
    "title": "IPC-Net: Incomplete point cloud classification network based on data augmentation and similarity measurement",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103769",
    "abstract": "Existing point cloud classification researches are usually conducted on datasets with complete structure and clear semantics. However, in real point cloud scenes, the occlusion and truncation may destroy the completeness of objects affecting the classification performance. To solve this problem, we propose an incomplete point cloud classification network (IPC-Net) with data augmentation and similarity measurement. The proposed network learns the feature representation of incomplete point clouds and the semantic differences compared to the complete ones for classification. Specifically, IPC-Net adopts a random erasing-based data augmentation to deal with incomplete point clouds. IPC-Net also introduces an auxiliary loss function weighted by attention scores to measure the similarity between the incomplete and the complete point clouds. Extensive experiments verify that IPC-Net has the ability to classify incomplete point clouds and significantly improves the robustness of point cloud classification under different completeness.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000196",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Cloud computing",
      "Completeness (order theory)",
      "Computer science",
      "Data mining",
      "Gene",
      "Geometry",
      "Image (mathematics)",
      "Mathematical analysis",
      "Mathematics",
      "Net (polyhedron)",
      "Operating system",
      "Point cloud",
      "Robustness (evolution)",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "He",
        "given_name": "Yunqian"
      },
      {
        "surname": "Zhang",
        "given_name": "Zhi"
      },
      {
        "surname": "Wang",
        "given_name": "Zhe"
      },
      {
        "surname": "Luo",
        "given_name": "Yongkang"
      },
      {
        "surname": "Su",
        "given_name": "Li"
      },
      {
        "surname": "Li",
        "given_name": "Wanyi"
      },
      {
        "surname": "Wang",
        "given_name": "Peng"
      },
      {
        "surname": "Zhang",
        "given_name": "Wen"
      }
    ]
  },
  {
    "title": "Blind omnidirectional image quality assessment with representative features and viewport oriented statistical features",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103770",
    "abstract": "With the development of information technologies, various types of streaming images are generated, such as videos, graphics, Virtual Reality (VR)/omnidirectional images (OIs), etc. Among them, the OIs usually have a broader view and a higher resolution, which provides human an immersive visual experience in a head-mounted display. However, the current image quality assessment works cannot achieve good performance without considering representative human visual features and visual viewing characteristics of OIs, which limited OIs’ further development. Motivated by the above problem, this work proposes a blind omnidirectional image quality assessment (BOIQA) model based on representative features and viewport oriented statistical features. Specifically, we apply the local binary pattern operator to encoder the cross-channel color information, and apply the weighted LBP to extract the structural features. Then the local natural scene statistics (NSS) features are extracted by using the viewport sampling to boost the performance. Finally, we apply support vector regression to predict the OIs’ quality score, and experimental results on CVIQD2018 and OIQA2018 Databases prove that the proposed model achieves better performance than state-of-the-art OIQA models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000202",
    "keywords": [
      "Antenna (radio)",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Epistemology",
      "Image (mathematics)",
      "Image quality",
      "Omnidirectional antenna",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Quality (philosophy)",
      "Telecommunications",
      "Viewport"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Yun"
      },
      {
        "surname": "Yin",
        "given_name": "Xiaohua"
      },
      {
        "surname": "Yue",
        "given_name": "Guanghui"
      },
      {
        "surname": "Zheng",
        "given_name": "Zhi"
      },
      {
        "surname": "Jiang",
        "given_name": "Jinhe"
      },
      {
        "surname": "He",
        "given_name": "Quangui"
      },
      {
        "surname": "Li",
        "given_name": "Xinzhuang"
      }
    ]
  },
  {
    "title": "Omnidirectional 2.5D representation for COVID-19 diagnosis using chest CTs",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103775",
    "abstract": "The Coronavirus Disease 2019 (COVID-19) has drastically overwhelmed most countries in the last two years, and image-based approaches using computerized tomography (CT) have been used to identify pulmonary infections. Recent methods based on deep learning either require time-consuming per-slice annotations (2D) or are highly data- and hardware-demanding (3D). This work proposes a novel omnidirectional 2.5D representation of volumetric chest CTs that allows exploring efficient 2D deep learning architectures while requiring volume-level annotations only. Our learning approach uses a siamese feature extraction backbone applied to each lung. It combines these features into a classification head that explores a novel combination of Squeeze-and-Excite strategies with Class Activation Maps. We experimented with public and in-house datasets and compared our results with state-of-the-art techniques. Our analyses show that our method provides better or comparable prediction quality and accurately distinguishes COVID-19 infections from other kinds of pneumonia and healthy lungs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000251",
    "keywords": [
      "Antenna (radio)",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Coronavirus disease 2019 (COVID-19)",
      "Deep learning",
      "Disease",
      "Feature (linguistics)",
      "Feature extraction",
      "Infectious disease (medical specialty)",
      "Law",
      "Linguistics",
      "Medicine",
      "Omnidirectional antenna",
      "Pathology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "da Silveira",
        "given_name": "Thiago L.T."
      },
      {
        "surname": "Pinto",
        "given_name": "Paulo G.L."
      },
      {
        "surname": "Lermen",
        "given_name": "Thiago S."
      },
      {
        "surname": "Jung",
        "given_name": "Cláudio R."
      }
    ]
  },
  {
    "title": "TsrNet: A two-stage unsupervised approach for clothing region-specific textures style transfer",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103778",
    "abstract": "Style transfer has been widely applied in the fashion industry for design assistance. This paper focuses on object-level image style transfer for specific texture regions on fashion images. The current methods mostly utilize extra conditional inputs to obtain regions of interest in fashion images, which take extensive time and cost to prepare these data. To address this issue, we propose a two-stage unsupervised approach named TsrNet for clothing texture style transfer between images. Firstly, we construct a network named Mask-GAN to unsupervisedly split out the clothing texture region. Secondly, we improve CycleGAN to interchange the texture on specific target regions of the two images. Specifically, considering the diversity of fashion texture, we construct a multiscale module (Drf-module) with dynamic perceptual fields and design an image gradient-based texture structure loss (GTS) to perform texture transfer. Qualitative and quantitative experimental results show the effectiveness of our algorithm for clothing-specific region style transfer.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000287",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Clothing",
      "Computer science",
      "Computer vision",
      "Construct (python library)",
      "Geography",
      "Image (mathematics)",
      "Neuroscience",
      "Object (grammar)",
      "Parallel computing",
      "Pattern recognition (psychology)",
      "Perception",
      "Programming language",
      "Psychology",
      "Style (visual arts)",
      "Texture (cosmology)",
      "Transfer (computing)"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Kexin"
      },
      {
        "surname": "Zhang",
        "given_name": "Jie"
      },
      {
        "surname": "Zhang",
        "given_name": "Peng"
      },
      {
        "surname": "Yuan",
        "given_name": "Kexin"
      },
      {
        "surname": "Li",
        "given_name": "Geng"
      }
    ]
  },
  {
    "title": "Image watermarking using DNST-PHFMs magnitude domain vector AGGM-HMT",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103779",
    "abstract": "Invisibility, robustness and payload are three indispensable and contradictory properties for any image watermarking systems. Therefore, in this paper a novel statistical image watermark decoder based on robust discrete nonseparable Shearlet transform (DNST)-polar harmonic Fourier moments (PHFMs) magnitude and effective vector anisotropic generalized Gaussian mixtures (AGGM)-hidden Markov tree (HMT). We begin with a detailed study on the robustness and statistical characteristics of local DNST- PHFMs magnitudes of natural images. This study reveals the excellent robustness, highly non-Gaussian marginal statistics and strong dependencies of local DNST-PHFMs magnitudes. We also find that conditioned on their generalized neighborhoods, the local DNST-PHFMs magnitudes can be approximately modeled as anisotropic generalized Gaussian variables. Based on these findings, we model local DNST-PHFMs magnitudes using a vector AGGM-HMT that can capture all interscale, interdirection, and interlocation dependencies. Meanwhile, model parameters can be estimated effectively by using localization clues guided expectation–maximization (LCGEM) approach. Finally, we develop a new statistical image watermark decoder using the vector AGGM-HMT and maximum likelihood (ML) decision rule. Extensive experimental results show the superiority of the proposed watermark decoder over several state-of-the-art statistical watermarking methods and some approaches based on convolutional neural networks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000299",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Digital watermarking",
      "Gaussian",
      "Gene",
      "Image (mathematics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Robustness (evolution)",
      "Statistical model",
      "Watermark"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Xiangyang"
      },
      {
        "surname": "Ma",
        "given_name": "Runtong"
      },
      {
        "surname": "Shen",
        "given_name": "Yixuan"
      },
      {
        "surname": "Niu",
        "given_name": "Panpan"
      }
    ]
  },
  {
    "title": "Multi-scale Superpixel based Hierarchical Attention model for brain CT classification",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103773",
    "abstract": "Brain CT image classification is critical for assisting brain disease diagnosis. The brain CT images contain much noisy information, and the lesions are unstable in shape and location, making the classification task more difficult when using conventional CNN models. In this paper, we propose a novel Multi-scale Superpixel based Hierarchical Attention (MSHA) model for brain CT classification by introducing the multi-scale superpixels to a hierarchical fusion structure to remove noise and help the model focus on the lesion areas. MSHA contains three modules: (1) a Semantic-level Information Extractor that extracts appearance and geometry information based on the superpixel of the image, (2) a Mixed Multi-head Attention module that obtains the mixed attention features from the semantic-level information, and (3) a Hierarchical Fusion Structure that fuses the multi-scale attention features from coarse to fine. Experiments on the brain CT dataset demonstrate the effectiveness of the proposed model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000238",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Engineering",
      "Extractor",
      "Focus (optics)",
      "Hierarchical database model",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Process engineering",
      "Quantum mechanics",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "Song",
        "given_name": "Xiao"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiaodan"
      },
      {
        "surname": "Ji",
        "given_name": "Junzhong"
      },
      {
        "surname": "Liu",
        "given_name": "Ying"
      }
    ]
  },
  {
    "title": "Improved traffic sign recognition algorithm based on YOLOv4-tiny",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103774",
    "abstract": "This study offers an enhanced yolov4-tiny traffic sign identification method for easy deployment on mobile or embedded devices to address the difficulties of a high number of parameters, low recognition accuracy, and poor real-time performance of traffic sign recognition models in complex scenarios. The yolov4-tiny network serves as the model’s foundation. To begin, Octave Convolution is incorporated into the backbone network to eliminate low-frequency feature redundancy, lowering the number of parameters in the model and enhancing computational efficiency. Second, the convolutional block attention module is employed to improve the recognition accuracy of small and medium-sized targets by strengthening the weights of traffic sign regions and suppressing the weights of invalid features. Finally, in the feature fusion stage, the Feature Pyramid Networks structure is replaced with the Simplified Path Aggregation Network structure to improve the fusing of shallow feature information with deep semantic knowledge and lower the miss detection rate even more On the TT100K data set as well as on CCTSDB dataset, the experimental results suggest that our technique can achieve good recognition performance. With a 16MB model size, our solution improves the mean average precision by 3.5 percent and the Frame Per Second by 12.5 f/s when compared to the yolov4-tiny algorithm. Our method outperforms yolov4-tiny in terms of recognition accuracy and detection speed, and it can easily meet the real-time requirements for traffic sign recognition.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032300024X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Data mining",
      "Feature (linguistics)",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Sign (mathematics)",
      "Traffic sign",
      "Traffic sign recognition"
    ],
    "authors": [
      {
        "surname": "Sharma",
        "given_name": "Vipal Kumar"
      },
      {
        "surname": "Dhiman",
        "given_name": "Pankaj"
      },
      {
        "surname": "Rout",
        "given_name": "Ranjeet Kumar"
      }
    ]
  },
  {
    "title": "Research on a face recognition algorithm based on 3D face data and 2D face image matching",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103757",
    "abstract": "Under the condition of weak light or no light, the recognition accuracy of the mature 2D face recognition technology decreases sharply. In this paper, a face recognition algorithm based on the matching of 3D face data and 2D face images is proposed. Firstly, 3D face data is reconstructed from the 2D face in the database based on the 3DMM algorithm, and the face depth image is obtained through orthogonal projection. Then, the average curvature map of the face depth image is used to enhance the data of the depth image. Finally, an improved residual neural network based on the depth image and curvature is designed to compare the scanned face with the face in the database. The method proposed in this paper is tested on the 3D face data in three public face datasets (Texas 3DFRD, FRGC v2.0, and Lock3DFace), and the recognition accuracy is 84.25%, 83.39%, and 78.24%, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032300007X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Curvature",
      "Face (sociological concept)",
      "Face detection",
      "Facial recognition system",
      "Geometry",
      "Image (mathematics)",
      "Matching (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Projection (relational algebra)",
      "Residual",
      "Social science",
      "Sociology",
      "Statistics",
      "Three-dimensional face recognition"
    ],
    "authors": [
      {
        "surname": "Niu",
        "given_name": "Wenjie"
      },
      {
        "surname": "Zhao",
        "given_name": "Yuankun"
      },
      {
        "surname": "Yu",
        "given_name": "Zhiyan"
      },
      {
        "surname": "Liu",
        "given_name": "Yu"
      },
      {
        "surname": "Gong",
        "given_name": "Yu"
      }
    ]
  },
  {
    "title": "Classification networks for continuous automatic pain intensity monitoring in video using facial expression on the X-ITE Pain Database",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2022.103743",
    "abstract": "So far, the current methods in the clinical application do not facilitate continuous monitoring for pain and are unreliable, especially for vulnerable patients. In contrast, several automated methods have been proposed for this task by using facial features that were extracted independently from every frame of a given sequence. However, the obtained results were poor due to the failure to represent movement dynamics. To solve this problem, this work introduces three distinct methods regarding classification to monitor continuous pain intensity: (1) A Random Forest classifier (RFc) baseline method, (2) Long-Short Term Memory (LSTM) method, and (3) LSTM using sample weighting method (LSTM-SW). In this study, we conducted experiments with 11 datasets regarding classification, then compared results to regression results in Othman et al. (2021). Experimental results showed that the LSTM & LSTM-SW methods for continuous automatic pain intensity recognition performed better than guessing and RFc except with small datasets such as the reduced tonic datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322002632",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Database",
      "Facial expression",
      "Frame (networking)",
      "Medicine",
      "Pattern recognition (psychology)",
      "Radiology",
      "Random forest",
      "Telecommunications",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Othman",
        "given_name": "Ehsan"
      },
      {
        "surname": "Werner",
        "given_name": "Philipp"
      },
      {
        "surname": "Saxen",
        "given_name": "Frerk"
      },
      {
        "surname": "Al-Hamadi",
        "given_name": "Ayoub"
      },
      {
        "surname": "Gruss",
        "given_name": "Sascha"
      },
      {
        "surname": "Walter",
        "given_name": "Steffen"
      }
    ]
  },
  {
    "title": "FFTI: Image inpainting algorithm via features fusion and two-steps inpainting",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103776",
    "abstract": "In view of the faultiness that the existing image inpainting methods fail to make full use of the complete region to predict the missing region features when the object features are seriously missing, resulting in discontinuous features and fuzzy detail texture of the inpainting results, a fine inpainting method of incomplete image based on features fusion and two-steps inpainting (FFTI) is proposed in this paper. Firstly, the dynamic memory networks (DMN+) are used to fuse the external features and internal features of the incomplete image to generate the incomplete image optimization map. Secondly, a generation countermeasure generative network with gradient penalty constraints is constructed to guide the generator to rough repair the optimized incomplete image and obtain the rough repair map of the target to be repaired. Finally, the coarse repair graph is further optimized by the idea of coherence of relevant features to obtain the final fine repair graph. It is verified by simulation on three image data sets with different complexity, and compared with the existing dominant repair model for visual effect and objective data. The experimental results show that the results of the model repair in this paper are more reasonable in texture structure, better than other models in visual effect and objective data, and the Peak Signal-to-Noise Ratio of the proposed algorithm in the most challenging underwater targe dataset is 27.01, the highest Structural Similarity Index is 0.949.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000263",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Generator (circuit theory)",
      "Graph",
      "Image (mathematics)",
      "Inpainting",
      "Pattern recognition (psychology)",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Yuantao"
      },
      {
        "surname": "Xia",
        "given_name": "Runlong"
      },
      {
        "surname": "Zou",
        "given_name": "Ke"
      },
      {
        "surname": "Yang",
        "given_name": "Kai"
      }
    ]
  },
  {
    "title": "Semantic prior-driven fused contextual transformation network for image inpainting",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103777",
    "abstract": "Recent advances in image inpainting have achieved impressive performance for generating plausible visual details on small regular image defects or simple backgrounds. However, current solution suffers from the lack of semantic priors for the image and the inability to deduce the image content from distant background, leading to distorted structures and artifacts in the results when inpainting large random irregular complicated images. To address these problems, a semantic prior-driven fused contextual transformation network for image inpainting is proposed as a promise solution. First, the semantic prior generator is put forward to map the semantic features of ground truth images and the low-level features of broken images to semantic priors. Subsequently, an image split-transform-aggregated strategy, named fusion context transformation block, is presented to infer rich multi-scale remote texture features and thus to improve the restored image finesse. Thereafter, an aggregated semantic attention-aware module, consisting of spatially adaptive normalization and enhanced spatial attention is designed to aggregate semantic priors and multi-scale texture features into the decoder to restore reasonable structure. Finally, the mask guided discriminator is developed to effectively discriminate between real and false pixels in the output image to improve the capability of the discriminator and hence to reduce the probability of artifacts containing in the output image. Comprehensive experimental results on CelebA-HQ, Paris Street View, and Places2 datasets demonstrate the superiority of the proposed network over the state-of-the-arts, whose PSNR, SSIM and MAE are improved about 20 %, 12.6 %, and 42 % gains, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000275",
    "keywords": [
      "Anthropology",
      "Artificial intelligence",
      "Bayesian probability",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Detector",
      "Discriminator",
      "Gene",
      "Generator (circuit theory)",
      "Image (mathematics)",
      "Inpainting",
      "Normalization (sociology)",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Physics",
      "Pixel",
      "Power (physics)",
      "Prior probability",
      "Quantum mechanics",
      "Sociology",
      "Telecommunications",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Haiyan"
      },
      {
        "surname": "Song",
        "given_name": "Yingqing"
      },
      {
        "surname": "Li",
        "given_name": "Haijiang"
      },
      {
        "surname": "Wang",
        "given_name": "Zhengyu"
      }
    ]
  },
  {
    "title": "Feature generation based on relation learning and image partition for occluded person re-identification",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103772",
    "abstract": "In order to solve the challenging tasks of person re-identification(Re-ID) in occluded scenarios, we propose a novel approach which divides local units by forming high-level semantic information of pedestrians and generates features of occluded parts. The approach uses CNN and pose estimation to extract the feature map and key points, and a graph convolutional network to learn the relation of key points. Specifically, we design a Generating Local Part (GLP) module to divide the feature map into different units. Based on different occluded conditions, the partition mode of GLP has high flexibility and variability. The features of the non-occluded parts are clustered into an intermediate node, and then the spatially correlated features of the occluded parts are generated according to the de-clustering operation. We conduct experiments on both the occluded and the holistic datasets to demonstrate its effectiveness.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000226",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Combinatorics",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Data mining",
      "Feature (linguistics)",
      "Graph",
      "Key (lock)",
      "Linguistics",
      "Mathematics",
      "Partition (number theory)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Relation (database)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Pang",
        "given_name": "Yunxiao"
      },
      {
        "surname": "Zhang",
        "given_name": "Huaxiang"
      },
      {
        "surname": "Zhu",
        "given_name": "Lei"
      },
      {
        "surname": "Liu",
        "given_name": "Dongmei"
      },
      {
        "surname": "Liu",
        "given_name": "Li"
      }
    ]
  },
  {
    "title": "MT-Net: Fast video instance lane detection based on space time memory and template matching",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103771",
    "abstract": "Currently, only a few lane detection methods focus on the dynamic characteristics of a video. In continuous prediction, single-frame detection results produce different degrees of jitter, resulting in poor robustness. We propose a new fast video instance lane detection network, called MT-Net, based on space–time memory and template matching. Memory templates were used to establish feature associations between past and current frames from a local–global perspective to mitigate jitter from scene changes and other disturbances. Moreover, we also investigated the sources and spreading mechanism of memory errors. We designed new query frame and memory encoders to obtain higher-precision memory and query frame features. The experimental results showed that, compared with state-of-the-art models, the proposed model can reduce the number of parameters by 62.28% and the unnecessary jitter and unstable factors in muti-frame lane prediction results by 12.70%, and increases the muti-frame lane detection speed by 1.79. Our proposed methods has obvious advantages in maintaining multi-frame instance lane stability and reducing errors.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000214",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Encoder",
      "Focus (optics)",
      "Frame (networking)",
      "Gene",
      "Image (mathematics)",
      "Jitter",
      "Matching (statistics)",
      "Mathematics",
      "Operating system",
      "Optics",
      "Physics",
      "Real-time computing",
      "Reference frame",
      "Residual frame",
      "Robustness (evolution)",
      "Statistics",
      "Telecommunications",
      "Template matching"
    ],
    "authors": [
      {
        "surname": "Shi",
        "given_name": "Peicheng"
      },
      {
        "surname": "Zhang",
        "given_name": "Chenghui"
      },
      {
        "surname": "Xu",
        "given_name": "Shucai"
      },
      {
        "surname": "Qi",
        "given_name": "Heng"
      },
      {
        "surname": "Chen",
        "given_name": "Xinhe"
      }
    ]
  },
  {
    "title": "Hierarchical attention vision transformer for fine-grained visual classification",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103755",
    "abstract": "Recently, vision transformer has gained a breakthrough in image recognition. Its self-attention mechanism (MSA) can extract discriminative tokens information from different patches to improve image classification accuracy. However, the classification token in its deep layer ignore the local features between layers. In addition, the patch embedding layer feeds fixed-size patches into the network, which inevitably introduces additional image noise. Therefore, we propose a hierarchical attention vision transformer (HAVT) based on the transformer framework. We present a data augmentation method for attention cropping to crop and drop image noise and force the network to learn key features. Second, the hierarchical attention selection (HAS) module is proposed, which improves the network's ability to learn discriminative tokens between layers by filtering and fusing tokens between layers. Experimental results show that the proposed HAVT outperforms state-of-the-art approaches and significantly improves the accuracy to 91.8% and 91.0% on CUB-200–2011 and Stanford Dogs, respectively. We have released our source code on GitHub https://github.com/OhJackHu/HAVT.git.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000056",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Discriminative model",
      "Embedding",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Security token",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Xiaobin"
      },
      {
        "surname": "Zhu",
        "given_name": "Shining"
      },
      {
        "surname": "Peng",
        "given_name": "Taile"
      }
    ]
  },
  {
    "title": "DSE-Net: Deep simultaneous estimation network for low-light image enhancement",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103780",
    "abstract": "This paper presents a novel approach for low-light image enhancement. We propose a deep simultaneous estimation network (DSE-Net), which simultaneously estimates the reflectance and illumination for low-light image enhancement. The proposed network contains three modules: image decomposition, illumination adjustment, and image refinement module. The DSE-Net uses a novel branched encoder–decoder based image decomposition module for simultaneous estimation. The proposed decomposition module uses a separate decoder to estimate illumination and reflectance. DSE-Net improves the estimated illumination using the illumination adjustment module and feeds it to the proposed refinement module. The image refinement module aims to produce sharp and natural-looking output. Extensive experiments conducted on a range of low-light images demonstrate the efficacy of the proposed model and show its supremacy over various state-of-the-art alternatives.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000305",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Composite material",
      "Computer science",
      "Computer vision",
      "Decomposition",
      "Ecology",
      "Encoder",
      "Image (mathematics)",
      "Image synthesis",
      "Materials science",
      "Operating system",
      "Range (aeronautics)"
    ],
    "authors": [
      {
        "surname": "Singh",
        "given_name": "Kavinder"
      },
      {
        "surname": "Parihar",
        "given_name": "Anil Singh"
      }
    ]
  },
  {
    "title": "Image downscaling via co-occurrence learning",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103766",
    "abstract": "Image downscaling is one of the widely used operations in image processing and computer graphics. It was recently demonstrated in the literature that kernel-based convolutional filters could be modified to develop efficient image downscaling algorithms. In this work, we present a new downscaling technique which is based on kernel-based image filtering concept. We propose to use pairwise co-occurrence similarity of the pixelpairs as the range kernel similarity in the filtering operation. The co-occurrence of the pixel-pair is learned directly from the input image. This co-occurrence learning is performed in a neighborhood based fashion all over the image. The proposed method can preserve the high-frequency structures, which were present in the input image, into the downscaled image. The idea is further extended to the case of fractions factor of downscaling. The resulting images retain visually-important details and do not suffer from edge-blurring artifact. We demonstrate the effectiveness of our proposed approach with extensive experiments on a large number of images downscaled with various downscaling factors.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000160",
    "keywords": [
      "Artifact (error)",
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Downscaling",
      "Geography",
      "Image (mathematics)",
      "Image processing",
      "Kernel (algebra)",
      "Mathematics",
      "Meteorology",
      "Pairwise comparison",
      "Pattern recognition (psychology)",
      "Precipitation",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Ghosh",
        "given_name": "Sanjay"
      },
      {
        "surname": "Garai",
        "given_name": "Arpan"
      }
    ]
  },
  {
    "title": "Residual spatiotemporal convolutional networks for face anti-spoofing",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2022.103744",
    "abstract": "Previous deep learning studies on Face Anti-Spoofing (FAS) systems have exploited many aspects of spatial data for face anti-spoofing detection, but few have used end-to-end spatiotemporal approaches to solving FAS problems. This paper aims to provide new perspectives for end-to-end spatiotemporal systems to deal with FAS problems, using five residual spatiotemporal convolutional models. This work analyzes and detects which network is the most appropriate for identifying spoofing on video-based identification systems. These five models were adapted to specific features of the FAS problem and its performance (accuracy and computational cost) were tested with OULU-NPU and SiW datasets. In addition, a cross-dataset validation was carried out. The experimentation shows the strengths and weaknesses of each model against the dependency on the temporal dimension, data initialization and different FAS environment conditions. According to experimentation, residual networks outperform the state-of-the-art, being the model based on decomposing spatial and temporal flow the best option.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322002644",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer network",
      "Computer science",
      "Convolutional neural network",
      "Data mining",
      "Deep learning",
      "Dependency (UML)",
      "Face (sociological concept)",
      "Initialization",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Programming language",
      "Residual",
      "Social science",
      "Sociology",
      "Spoofing attack"
    ],
    "authors": [
      {
        "surname": "da Silva",
        "given_name": "Vitor Luiz"
      },
      {
        "surname": "Lérida",
        "given_name": "Josep Luis"
      },
      {
        "surname": "Sarret",
        "given_name": "Marta"
      },
      {
        "surname": "Valls",
        "given_name": "Magda"
      },
      {
        "surname": "Giné",
        "given_name": "Francesc"
      }
    ]
  },
  {
    "title": "A novel Siamese network object tracking algorithm based on tensor space mapping and memory-learning mechanism",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2022.103742",
    "abstract": "The tracker is a core component of the tracking algorithm, but it is difficult to identify the object, which is a challenge to improve the tracking accuracy. This paper proposes a Siamese network-based tracking algorithm based on tensor space mapping and memory-learning mechanisms. Firstly, the source image is mapped to the tensor space to serialize the feature distributions. Then the gating mechanism is used to extract the association information about the adjacent state, which guides the update of the subsequent state, and the interactive information on the objects is used to locate the object. On this basis, a memory-learning module is built to traverse and extract the fine-grained features, which can filter the semantic information of the object learned by the tracker. As a result, the tracking accuracy is enhanced. The experiments show that the proposed algorithm has better performance than that of the comparison methods in the OTB100 data set and the VOT data set.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322002620",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Mathematics",
      "Object (grammar)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Pedagogy",
      "Programming language",
      "Psychology",
      "Pure mathematics",
      "Serialization",
      "Set (abstract data type)",
      "Tensor (intrinsic definition)",
      "Tracking (education)",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Yongqiang"
      },
      {
        "surname": "Zhang",
        "given_name": "Baohua"
      },
      {
        "surname": "Lu",
        "given_name": "Xiaoqi"
      },
      {
        "surname": "Gu",
        "given_name": "Yu"
      },
      {
        "surname": "Wang",
        "given_name": "Yueming"
      },
      {
        "surname": "Liu",
        "given_name": "Xin"
      },
      {
        "surname": "Ren",
        "given_name": "Yan"
      },
      {
        "surname": "Li",
        "given_name": "Jianjun"
      }
    ]
  },
  {
    "title": "Eyes gaze detection based on multiprocess of ratio parameters for smart wheelchair menu selection in different screen size",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103756",
    "abstract": "Smart wheelchairs support paralysis disability people, especially paralysis of the hands and feet. Paralysis disability cannot operate menu selection with a touch screen. This research proposes a menu selection to select features in a smart wheelchair using the eye’s gaze and execute the menu by blinking the left eye. Multiprocess of ratio parameter is the methodology used and expanded in this research. The proposed method compared the ratio between the midpoint on the iris contour and registered midpoint on the selection step. This research is conducted under two conditions: the screen size being started in 14 and 17 inches. This research resulted in 91.48% and 90.68% accuracy in 14 and 17-inch screens for users without glasses. Meanwhile, 89.55% and 86.99% were obtained for wearing glasses users. The experiments were conducted where the position of a user is 30–39, 40–49, and 50–59 cm from the camera.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000068",
    "keywords": [
      "Artificial intelligence",
      "Blindness",
      "Computer science",
      "Computer vision",
      "Gaze",
      "Medicine",
      "Optometry",
      "Selection (genetic algorithm)",
      "Simulation",
      "Wheelchair",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Hutamaputra",
        "given_name": "William"
      },
      {
        "surname": "Utaminingrum",
        "given_name": "Fitri"
      },
      {
        "surname": "Budi",
        "given_name": "Agung Setia"
      },
      {
        "surname": "Ogata",
        "given_name": "Kohichi"
      }
    ]
  },
  {
    "title": "Attention meets involution in visual tracking",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2022.103746",
    "abstract": "In visual tracking, both convolution and attention are widely employed for feature enhancement and fusion. However, convolution does not adequately model global dependencies of samples due to its operation on local neighbors, while attention gives too much attention to global dependencies and too little to local dependencies. It is intrinsically infeasible to combine both methods to integrate global and local information. However, a recently-proposed model called involution uses kernels differing in spatial extent but sharing across channels, making it possible to take advantage of both convolution and attention. We propose an attention-involution (Att-Inv) model that uses an attention mechanism to generate involution kernels to take both global and local dependencies of samples into account. To improve the performance of our tracker, we develop and implement strategies of backbone network modification, template updates, and regression of bounding box distributions. We evaluate our tracker using benchmarks such as GOT10k, LaSOT, TrackingNet and OxUvA. Experimental results show that it is competitive with state-of-the-art trackers.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322002668",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "BitTorrent tracker",
      "Bounding overwatch",
      "Computer science",
      "Eye tracking",
      "Image (mathematics)",
      "Involution (esoterism)",
      "Law",
      "Minimum bounding box",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics"
    ],
    "authors": [
      {
        "surname": "Hou",
        "given_name": "Yueen"
      },
      {
        "surname": "Luo",
        "given_name": "Zhijian"
      },
      {
        "surname": "Deng",
        "given_name": "Jiaming"
      },
      {
        "surname": "Gao",
        "given_name": "Yanzeng"
      },
      {
        "surname": "Huang",
        "given_name": "Kekun"
      },
      {
        "surname": "Li",
        "given_name": "Weiguang"
      }
    ]
  },
  {
    "title": "Hierarchical context-agnostic network with contrastive feature diversity for one-shot semantic segmentation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103754",
    "abstract": "One-shot semantic segmentation aims at distinguishing pixels of an unseen category from the background, using merely one annotated image from the same category. However, most previous works neglect the feature diversity of foreground and the context information of background by using the masked average pooling. To solve these issues, we propose the Hierarchical Context-Agnostic Network (HCNet). It mainly includes two modules: (1) a Hierarchical Pyramid Supportive (HPS) module that generate the hierarchical supportive prototypes from coarse to fine to ensure feature diversity, and (2) a Background Exclusion Supportive (BES) module that explicitly introduces the contrastive information from the background for more precise category features. We conduct extensive experiments on Pascal-5 i and COCO-20 i to evaluate the performance of HCNet. HCNet achieves the mIoU score of 62.1% on Pascal-5 i and 40.7% on COCO-20 i and outperforms other works for the challenging one-shot segmentation, which has proved the efficiency of the whole network. Code is available at https://github.com/fangzy97/hcnet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000044",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Feature (linguistics)",
      "Geometry",
      "Linguistics",
      "Mathematics",
      "Natural language processing",
      "Paleontology",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pooling",
      "Programming language",
      "Pyramid (geometry)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Fang",
        "given_name": "Zhiyuan"
      },
      {
        "surname": "Gao",
        "given_name": "Guangyu"
      },
      {
        "surname": "Zhang",
        "given_name": "Zekang"
      },
      {
        "surname": "Zhang",
        "given_name": "Anqi"
      }
    ]
  },
  {
    "title": "Proposal of a new fidelity measure between computed image quality and observers quality scores accounting for scores variability",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2022.103704",
    "abstract": "Assessment of the visual quality of colour images is usually a difficult process, validated through hard-to-carry-out psychophysical experiments, used to record observer quality scores. Visual image quality metrics aim to maximise the agreement between computed indexes and observer scores, or opinions. Therefore, in this area, it is of critical importance to have appropriate measures of this agreement (i.e. performance) between the computed image quality metric values and observer’s quality scores, both for the development, as well as for the use of image quality metrics. Among the measures of agreement, the most used one nowadays is the well-known Pearson correlation coefficient, while Spearman rank correlation coefficient is also commonly used. The aim of this paper is two-fold. First, to introduce the Standardised Residual Sum of Squares ( S T R E S S ) as an alternative metric for the agreement between computed image quality and observers quality scores and analyse its properties and advantages in front of Pearson, Spearman and Kendall correlation coefficients; Second, to introduce a new version of S T R E S S (called U S T R E S S ) that takes observers’ scores variability into account. The results on synthetic and real datasets support that S T R E S S has a series of benefits in front of the classical approaches and that the inclusion of uncertainty in S T R E S S has an important effect on the results, quantified by statistical significance tests. A free to download MATLAB code version of U S T R E S S is available at https://viplab.webs.upv.es/resources/",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322002243",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Correlation",
      "Correlation coefficient",
      "Data mining",
      "Economics",
      "Epistemology",
      "Fidelity",
      "Geometry",
      "Image (mathematics)",
      "Image quality",
      "Mathematics",
      "Metric (unit)",
      "Observer (physics)",
      "Operations management",
      "Pearson product-moment correlation coefficient",
      "Philosophy",
      "Physics",
      "Quality (philosophy)",
      "Quality Score",
      "Quantum mechanics",
      "Rank correlation",
      "Spearman's rank correlation coefficient",
      "Statistics",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Latorre-Carmona",
        "given_name": "Pedro"
      },
      {
        "surname": "Huertas",
        "given_name": "Rafael"
      },
      {
        "surname": "Pedersen",
        "given_name": "Marius"
      },
      {
        "surname": "Morillas",
        "given_name": "Samuel"
      }
    ]
  },
  {
    "title": "Dynamic representation-based tracker for long-term pedestrian tracking with occlusion",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2022.103710",
    "abstract": "This paper presents a dynamic representation-based tracker (DRT) to handle occlusions in the long-term pedestrian tracking of a single target. In our DRT, an adaptive representation network (ARN) is first constructed to extract multiple features, including classical features such as appearance and pose as well as some vector-format deep features. These features are then stacked to form a dynamic representation so as to convert the target tracking into a matching problem between the target features and candidate features, where the Euclidean distance (ED) and locality-constrained linear coding (LLC) are used as measurements in the decision-making. Next, the target state is determined through a voting procedure according to the feature matching error. Finally, a pose supervised module (PSM) and an IOU filtering module (IFM) are applied, respectively, to refine the target state and to filter out some invalid candidate targets that have been detected. Experimental results on public benchmark datasets show that our DRT is quite robust to complex environments with long-term pedestrian occlusions, and outperforms several existing state-of-the-arts trackers as it produces the best performance on both the pedestrian tracking dataset with occlusion (PTDO) and the pedestrian tracking dataset with occlusion plus (PTDO Plus).",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322002309",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "BitTorrent tracker",
      "Computer science",
      "Computer vision",
      "Eye tracking",
      "Feature (linguistics)",
      "Geodesy",
      "Geography",
      "Law",
      "Linguistics",
      "Matching (statistics)",
      "Mathematics",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Pedagogy",
      "Philosophy",
      "Political science",
      "Politics",
      "Psychology",
      "Representation (politics)",
      "Statistics",
      "Tracking (education)",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Zhen"
      },
      {
        "surname": "Huang",
        "given_name": "Zhiyi"
      },
      {
        "surname": "He",
        "given_name": "Dunyun"
      },
      {
        "surname": "Zhang",
        "given_name": "Tao"
      },
      {
        "surname": "Yang",
        "given_name": "Fan"
      }
    ]
  },
  {
    "title": "Illumination-aware window transformer for RGBT modality fusion",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2022.103725",
    "abstract": "Combination of RGB and thermal sensors has been proven to be useful for many vision applications. However, how to effectively fuse the information of two modalities remains a challenging problem. In this paper, we propose an Illumination-Aware Window Transformer (IAWT) fusion module to handle the RGB and thermal multi-modality fusion. Specifically, the IAWT fusion module adopts a window-based multi-modality attention combined with additional estimated illumination information. The window-based multi-modality attention infers dependency cross modalities within a local window, thus implicitly alleviate the problem caused by weakly spatial misalignment of the RGB and thermal image pairs within specific dataset. The introduction of estimated illumination feature enables the fusion module to adaptively merge the two modalities according to illumination conditions so as to make full use of the complementary characteristics of RGB and thermal images under different environments. Besides, our proposed fusion module is task-agnostic and data-specific, which means it can be used for different tasks with RGBT inputs. To evaluate the advances of the proposed fusion method, we embed the IAWT fusion module into different networks and conduct the experiments on various RGBT tasks, including pedestrian detection, semantic segmentation and crowd counting. Extensive results demonstrate the superior performance of our method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322002450",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Electrical engineering",
      "Engineering",
      "Fuse (electrical)",
      "Fusion",
      "Fusion mechanism",
      "Image (mathematics)",
      "Image fusion",
      "Information retrieval",
      "Linguistics",
      "Lipid bilayer fusion",
      "Merge (version control)",
      "Modalities",
      "Modality (human–computer interaction)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "RGB color model",
      "Segmentation",
      "Social science",
      "Sociology",
      "Window (computing)"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Lin"
      },
      {
        "surname": "Chen",
        "given_name": "Zhenzhong"
      }
    ]
  },
  {
    "title": "Multi-stage feature-fusion dense network for motion deblurring",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2022.103717",
    "abstract": "Although convolutional neural networks (CNNs) have recently shown considerable progress in motion deblurring, most existing methods that adopt multi-scale input schemes are still challenging in accurately restoring the heavily-blurred regions in blurry images. Several recent methods aim to further improve the deblurring effect using larger and more complex models, but these methods inevitably result in huge computing costs. To address the performance-complexity trade-off, we propose a multi-stage feature-fusion dense network (MFFDNet) for motion deblurring. Each sub-network of our MFFDNet has the similar structure and the same scale of input. Meanwhile, we propose a feature-fusion dense connection structure to reuse the extracted features, thereby improving the deblurring effect. Moreover, instead of using the multi-scale loss function, we only calculate the loss function at the output of the last stage since the input scale of our sub-network is invariant. Experimental results show that MFFDNet maintains a relatively small computing cost while outperforming state-of-the-art motion-deblurring methods. The source code is publicly available at: https://github.com/CaiGuoHS/MFFDNet_release.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322002371",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deblurring",
      "Feature (linguistics)",
      "Fusion",
      "Image (mathematics)",
      "Image processing",
      "Image restoration",
      "Linguistics",
      "Motion blur",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "Guo",
        "given_name": "Cai"
      },
      {
        "surname": "Wang",
        "given_name": "Qian"
      },
      {
        "surname": "Dai",
        "given_name": "Hong-Ning"
      },
      {
        "surname": "Li",
        "given_name": "Ping"
      }
    ]
  },
  {
    "title": "Denoising image by matrix factorization in U-shaped convolutional neural network",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2022.103729",
    "abstract": "Image denoising requires both spatial details and global contextualized information to recover a clean version from the deteriorative one. Previous deep convolution networks usually focus on modeling the local feature and stacked convolution blocks to expand the receptive field, which can catch the long-distance dependencies. However, contrary to the expectation, the extracted local feature incapacity recovers the global details by traditional convolution while the stacked blocks hinder the information flow. To tackle these issues, we introduce the Matrix Factorization Denoising Module (MD) to model the interrelationship between the global context aggregating process and the reconstructed process to attain the context details. Besides, we redesign a new basic block to ease the information flow and maintain the network performance. In addition, we conceive the Feature Fusion Module (FFU) to fuse the information from the different sources. Inspired by the multi-stage progressive restoration architecture, we adopt two-stage convolution branches progressively reconstructing the denoised image. In this paper, we propose an original and efficient neural convolution network dubbed MFU. Experimental results on various image denoising datasets: SIDD, DND, and synthetic Gaussian noise datasets show that our MFU can produce comparable visual quality and accuracy results with state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322002498",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Block (permutation group theory)",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Geometry",
      "Image (mathematics)",
      "Linguistics",
      "Mathematics",
      "Noise (video)",
      "Noise reduction",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "li",
        "given_name": "Qifan"
      }
    ]
  },
  {
    "title": "Anomaly detection with dual-stream memory network",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2022.103739",
    "abstract": "Anomaly detection is an essential but challenging task. Existing DNN-based approaches tend to ignore the impact of network history state on extracting spatio-temporal correlations between video events. To address this problem, a Dual-Stream Memory Network (DSM-Net) has been proposed. It leverages historical information from the network to create a dual-stream memory module serving as complementary knowledge for the anomaly detection network. The memory module performs writing and reading in the form of a queue of data features. The writing records the historic information of video events through a moving average encoder, and the reading uses optical flow to uncover behavioral patterns in RGB images. Using a memory sharing strategy, the semantic information of the appearance branch and the motion branch can be integrated to reinforce the network. Results demonstrate that the proposed method on various standard datasets performs favorably when compared to existing methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322002590",
    "keywords": [
      "Anomaly (physics)",
      "Anomaly detection",
      "Art",
      "Artificial intelligence",
      "Auxiliary memory",
      "Computer hardware",
      "Computer network",
      "Computer science",
      "Condensed matter physics",
      "Dual (grammatical number)",
      "Economics",
      "Encoder",
      "Law",
      "Literature",
      "Management",
      "Operating system",
      "Physics",
      "Political science",
      "Queue",
      "RGB color model",
      "Reading (process)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Zhongyue"
      },
      {
        "surname": "Chen",
        "given_name": "Ying"
      }
    ]
  },
  {
    "title": "Single image deraining using multi-scales context information and attention network",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2022.103695",
    "abstract": "The existing deraining methods based on convolutional neural networks (CNNs) have made great success, but some remaining rain streaks can degrade images drastically. In this work, we proposed an end-to-end multi-scale context information and attention network, called MSCIANet. The proposed network consists of multi-scale feature extraction (MSFE) and multi-receptive fields feature extraction (MRFFE). Firstly, the MSFE can pick up features of rain streaks in different scales and propagate deep features of the two layers across stages by skip connections. Secondly, the MRFFE can refine details of the background by attention mechanism and the depthwise separable convolution of different receptive fields with different scales. Finally, the fusion of these outputs of two subnetworks can reconstruct the clean background image. Extensive experimental results have shown that the proposed network achieves a good effect on the deraining task on synthetic and real-world datasets. The demo can be available at https://github.com/CoderLi365/MSCIANet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322002152",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Cartography",
      "Computer science",
      "Context (archaeology)",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Economics",
      "Feature (linguistics)",
      "Feature extraction",
      "Geography",
      "Geology",
      "Image (mathematics)",
      "Linguistics",
      "Management",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Scale (ratio)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Pengcheng"
      },
      {
        "surname": "Gai",
        "given_name": "Shan"
      }
    ]
  },
  {
    "title": "Hyperspectral image classification based on three-dimensional adaptive sampling and improved iterative shrinkage-threshold algorithm",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2022.103693",
    "abstract": "Abundant spectral information of hyperspectral images (HSI) provides rich information for HSI classification, which often brings high dimensional data resulting in the dilemma between the demand for fine data and the limited resources such as computation, storage as well as transmission band-width. To address this issue, we propose a deep hierarchical feature representation model based on three-dimensional adaptive sampling and improved iterative shrinkage-threshold algorithm (ISTA) for HSI classification. Due to the adaptive sampling, we improve ISTA with deep learning network for spectral–spatial feature representation since the ISTA is no longer applicable for the sampled data reconstruction. Through end-to-end joint learning, the proposed method can not only effectively reduce the required data, but also learn discriminative features for HSI classification, which will be meaningful for the HSI’s transmission from the space satellites and fast classification. Experimental results demonstrate the effectiveness and superiority of the proposed method on three public HSI datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322002139",
    "keywords": [
      "Adaptive sampling",
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Discriminative model",
      "Feature (linguistics)",
      "Filter (signal processing)",
      "Hyperspectral imaging",
      "Law",
      "Linguistics",
      "Mathematics",
      "Monte Carlo method",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Sampling (signal processing)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Cao",
        "given_name": "Chunhong"
      },
      {
        "surname": "Duan",
        "given_name": "Hongxuan"
      },
      {
        "surname": "Gao",
        "given_name": "Xieping"
      }
    ]
  },
  {
    "title": "Edge-aware object pixel-level representation tracking",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2022.103735",
    "abstract": "Recently, there has been a trend in tracking to use more refined segmentation mask instead of coarse bounding box to represent the target object. Some trackers proposed segmentation branches based on the tracking framework and maintain real-time speed. However, those trackers use a simple FCNs structure and lack of the edge information modeling. This makes performance quite unsatisfactory. In this paper, we propose an edge-aware segmentation network, which uses the complementarity between target information and edge information to provide a more refined representation of the target. Firstly, We use the high-level features of the tracking backbone network and the correlation features of the classification branch of the tracking framework to fuse, and use the target edge and target segmentation mask for simultaneous supervision to obtain an optimized high-level feature with rough edge information and target information. Secondly, we use the optimized high-level features to guide the low-level features of the tracking backbone network to generate more refined edge features. Finally, we use the refined edge features to fuse with the target features of each layer to generate the final mask. Our approach has achieved leading performance on recent pixel-wise object tracking benchmark VOT2020 and segmentation datasets DAVIS2016 and DAVIS2017 while running on 47 fps. Code is available at https://github.com/TJUMMG/EATtracker.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322002553",
    "keywords": [
      "Artificial intelligence",
      "Backbone network",
      "Benchmark (surveying)",
      "BitTorrent tracker",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Electrical engineering",
      "Engineering",
      "Enhanced Data Rates for GSM Evolution",
      "Eye tracking",
      "Feature (linguistics)",
      "Fuse (electrical)",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Law",
      "Linguistics",
      "Minimum bounding box",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Pedagogy",
      "Philosophy",
      "Pixel",
      "Political science",
      "Politics",
      "Psychology",
      "Representation (politics)",
      "Segmentation",
      "Tracking (education)",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Jing",
        "given_name": "Peiguang"
      },
      {
        "surname": "Huang",
        "given_name": "Zijian"
      },
      {
        "surname": "Liu",
        "given_name": "Jing"
      },
      {
        "surname": "Wang",
        "given_name": "Yating"
      },
      {
        "surname": "Yu",
        "given_name": "Jiexiao"
      }
    ]
  },
  {
    "title": "Improving video quality by predicting inter-frame residuals based on an additive 3D-CNN model",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2022.103734",
    "abstract": "Video compression is essential for uploading videos to online platforms which usually have bandwidth limitations. However, the compression reduces the visual quality. To overcome this problem, the visual quality of the low bitrate compressed videos for various standards, including H.264 and HEVC in decoders, needs to be improved. Accordingly, this paper proposes a novel method for improving video quality based on 3D convolutional neural networks (CNNs). This method is totally compatible with the encoders of video compression standards, i.e., H.264, VVC, and HEVC, and can be implemented easily. In particular, the proposed neural network model receives five frames of the low bitrate compressed video as input and subsequently predicts the compression error of frames using the first and fifth frames. Finally, it reconstructs an improved version of the frame with high quality. The CNN is an Additive (3D) model that can predict the eliminated inter-frame redundancies resulting from compression. Our goal is to increase the peak signal to noise ratio (PSNR) and structural index similarity (SSIM) of the luminance (Y) and chrominance (U, V) frames in the video. Additive 3D-CNN achieves an average of 12.4%, 9.9% and 5% BD-rate increases for LP, LB and RA for the Y component. The results indicate that the new proposed algorithm outperforms the previous methods in terms of PSNR, SSIM, and BD-rate.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322002541",
    "keywords": [
      "Artificial intelligence",
      "Automotive engineering",
      "Chrominance",
      "Compression artifact",
      "Compression ratio",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Data compression",
      "Economics",
      "Encoder",
      "Engineering",
      "Frame (networking)",
      "Image (mathematics)",
      "Image compression",
      "Image processing",
      "Inter frame",
      "Internal combustion engine",
      "Intra-frame",
      "Luminance",
      "Metric (unit)",
      "Operating system",
      "Operations management",
      "Peak signal-to-noise ratio",
      "Pixel",
      "Reference frame",
      "Residual frame",
      "Telecommunications",
      "Video quality"
    ],
    "authors": [
      {
        "surname": "Azadegan",
        "given_name": "Hamid"
      },
      {
        "surname": "Shirazi",
        "given_name": "Ali-Asghar Beheshti"
      }
    ]
  },
  {
    "title": "Adaptive smoothness evaluation and multiple asymmetric histogram modification for reversible data hiding",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2022.103732",
    "abstract": "In this paper, an adaptive reversible data hiding (RDH) algorithm based on multiple asymmetric histograms is proposed by making full use of the image content. Different from existing multiple prediction error histogram (PEHs) modification methods that directly cluster all the pixels of a cover image into multiple categories, we firstly utilize a smoothness threshold to exclude as many pixels in complex regions as possible for reducing unnecessary pixel shifting, and then exploit fuzzy C-means with multiple deliberately-designed features to construct multiple sharply-distributed categories, which helps in increasing the subsequent embedding performance. Two asymmetric PEHs for each class are generated using a pair of asymmetric predictors, and the short part of each asymmetric PEH is modified to reduce the number of invalid modifications. The improved discrete particle swarm optimization is used to adaptively select the best bin while reducing computational complexity. The experimental results show that the proposed method outperforms several state-of-the-art RDH methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322002528",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Embedding",
      "Histogram",
      "Image (mathematics)",
      "Information hiding",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pixel",
      "Smoothness"
    ],
    "authors": [
      {
        "surname": "Weng",
        "given_name": "Shaowei"
      },
      {
        "surname": "Hou",
        "given_name": "Tanshuai"
      },
      {
        "surname": "Zhang",
        "given_name": "Tiancong"
      },
      {
        "surname": "Pan",
        "given_name": "Jeng-Shyang"
      }
    ]
  },
  {
    "title": "Multi-Scale and spatial position-based channel attention network for crowd counting",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2022.103718",
    "abstract": "Crowd counting algorithms have recently incorporated attention mechanisms into convolutional neural networks (CNNs) to achieve significant progress. The channel attention model (CAM), as a popular attention mechanism, calculates a set of probability weights to select important channel-wise feature responses. However, most CAMs roughly assign a weight to the entire channel-wise map, which makes useful and useless information being treat indiscriminately, thereby limiting the representational capacity of networks. In this paper, we propose a multi-scale and spatial position-based channel attention network (MS-SPCANet), which integrates spatial position-based channel attention models (SPCAMs) with multiple scales into a CNN. SPCAM assigns different channel attention weights to different positions of channel-wise maps to capture more informative features. Furthermore, an adaptive loss, which uses adaptive coefficients to combine density map loss and headcount loss, is constructed to improve network performance in sparse crowd scenes. Experimental results on four public datasets verify the superiority of the scheme.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322002383",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Attention network",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Convolutional neural network",
      "Economics",
      "Engineering",
      "Feature (linguistics)",
      "Finance",
      "Limiting",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Mechanical engineering",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Position (finance)",
      "Programming language",
      "Quantum mechanics",
      "Scale (ratio)",
      "Scheme (mathematics)",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Lin"
      },
      {
        "surname": "Li",
        "given_name": "Jie"
      },
      {
        "surname": "Zhang",
        "given_name": "Siqi"
      },
      {
        "surname": "Qi",
        "given_name": "Chun"
      },
      {
        "surname": "Wang",
        "given_name": "Pan"
      },
      {
        "surname": "Wang",
        "given_name": "Fengping"
      }
    ]
  },
  {
    "title": "An improved all-optical diffractive deep neural network with less parameters for gesture recognition",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2022.103688",
    "abstract": "As a framework of optical machine learning, all-optical diffractive neural network (D 2 NN) has delivered an ideal outcome of feature detection and target classification, currently raising high interest in the optics and photonics community. In this paper, we applied an improved D 2 NN architecture to the field of gesture recognition, which features more complicated contour than the common MNIST handwriting recognition in the previous literature. The proposed network structure incorporates the wavelet-like phase modulation pattern technique and the highway network on the basis of all-optical neural network. Through modulating the phase of incident light, the wavelet-like pattern can substantially reduce the parameters in the network layer. In addition, a highway network is employed to address the vanishing gradient phenomenon in the training process. In the experiment, we numerically achieved blind testing accuracy of 95.6% for identifying ten different gestures, and the number of parameters is only 3% of the regular D 2 NN. Reliability test and analysis show that the proposed method is a high-efficiency solution with low-parameters expecting for implementation of various machine learning tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322002085",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Deep neural networks",
      "Gesture",
      "Pattern recognition (psychology)",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Yuanguo"
      },
      {
        "surname": "Shui",
        "given_name": "Shan"
      },
      {
        "surname": "Cai",
        "given_name": "Yijun"
      },
      {
        "surname": "Chen",
        "given_name": "Chengying"
      },
      {
        "surname": "Chen",
        "given_name": "Yingshi"
      },
      {
        "surname": "Abdi-Ghaleh",
        "given_name": "Reza"
      }
    ]
  },
  {
    "title": "Image quality assessment based on self-supervised learning and knowledge distillation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2022.103708",
    "abstract": "Deep neural networks have achieved great success in a wide range of machine learning tasks due to their excellent ability to learn rich semantic features from high-dimensional data. Deeper networks have been successful in the field of image quality assessment to improve the performance of image quality assessment models. The success of deep neural networks majorly comes along with both big models with hundreds of millions of parameters and the availability of numerous annotated datasets. However, the lack of large-scale labeled data leads to the problems of over-fitting and poor generalization of deep learning models. Besides, these models are huge in size, demanding heavy computation power and failing to be deployed on edge devices. To deal with the challenge, we propose an image quality assessment based on self-supervised learning and knowledge distillation. First, the self-supervised learning of soft target prediction given by the teacher network is carried out, and then the student network is jointly trained to use soft target and label on knowledge distillation. Experiments on five benchmark databases show that the proposed method is superior to the teacher network and even outperform the state-of-the-art strategies. Furthermore, the scale of our model is much smaller than the teacher model and can be deployed in edge devices for smooth inference.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322002280",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Composite material",
      "Computer science",
      "Deep learning",
      "Enhanced Data Rates for GSM Evolution",
      "Epistemology",
      "Field (mathematics)",
      "Generalization",
      "Geodesy",
      "Geography",
      "Inference",
      "Machine learning",
      "Materials science",
      "Mathematical analysis",
      "Mathematics",
      "Philosophy",
      "Physics",
      "Pure mathematics",
      "Quality (philosophy)",
      "Quantum mechanics",
      "Range (aeronautics)",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "Sang",
        "given_name": "Qingbing"
      },
      {
        "surname": "Shu",
        "given_name": "Ziru"
      },
      {
        "surname": "Liu",
        "given_name": "Lixiong"
      },
      {
        "surname": "Hu",
        "given_name": "Cong"
      },
      {
        "surname": "Wu",
        "given_name": "Qin"
      }
    ]
  },
  {
    "title": "Online relational tracking with camera motion suppression",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2022.103750",
    "abstract": "To overcome challenges in multiple-object tracking (MOT) tasks, recent algorithms use interaction cues alongside motion and appearance features. These algorithms use graph neural networks or transformers to extract interaction features that lead to high computation costs. In this paper, a novel interaction cue based on geometric features is presented aiming to detect occlusion and reidentify lost targets with low computational costs. Moreover, in the majority of algorithms, camera motion is considered negligible, which is a strong assumption that is not always true and can lead to identity (ID) switching or mismatching of targets. In this paper, a method for measuring camera motion is presented that efficiently reduces its effect on tracking. The proposed algorithm is evaluated on MOT17 and MOT20 datasets and achieves state-of-the-art performance on MOT17 with comparable results on MOT20. The code is also publicly available. 1 1 https://github.com/mhnasseri/for_tracking.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032200270X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Code (set theory)",
      "Computation",
      "Computer science",
      "Computer vision",
      "Graph",
      "Match moving",
      "Motion (physics)",
      "Object (grammar)",
      "Pedagogy",
      "Programming language",
      "Psychology",
      "Set (abstract data type)",
      "Theoretical computer science",
      "Tracking (education)",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Nasseri",
        "given_name": "Mohammad Hossein"
      },
      {
        "surname": "Babaee",
        "given_name": "Mohammadreza"
      },
      {
        "surname": "Moradi",
        "given_name": "Hadi"
      },
      {
        "surname": "Hosseini",
        "given_name": "Reshad"
      }
    ]
  },
  {
    "title": "TMSO-Net: Texture adaptive multi-scale observation for light field image depth estimation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2022.103731",
    "abstract": "Light field can record the four-dimensional information of light rays, i.e. the position and direction information in which depth information is implied. To improve the depth estimation accuracy, we propose a depth estimation algorithm based on convolutional neural network (CNN). First, a single image super resolution algorithm is adopted to spatially super resolve the sub-aperture images (SAIs). Second, to adapt the texture complexity, the SAIs are partitioned into two regions, i.e., simple texture region and complex texture region, based on the texture analysis of the central SAI. Third, the epipolar plane images (EPIs) in horizontal, vertical, 45 degree diagonal, and 135 degree diagonal directions for both complex and simple texture regions are extracted, and the corresponding EPIs for the simple and complex texture regions are fed into the specified network branches. Finally, a fusion module is designed to generate the depth map. Experimental results show that the quality of the estimated depth maps by the proposed method is better than the state-of-the-art methods in terms of both objective quality and subjective quality. Moreover, the proposed method is more robust to noise.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322002516",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Diagonal",
      "Epipolar geometry",
      "Geometry",
      "Image (mathematics)",
      "Image processing",
      "Image texture",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Texture (cosmology)",
      "Texture compression"
    ],
    "authors": [
      {
        "surname": "Fu",
        "given_name": "Congrui"
      },
      {
        "surname": "Yuan",
        "given_name": "Hui"
      },
      {
        "surname": "Xu",
        "given_name": "Hongji"
      },
      {
        "surname": "Zhang",
        "given_name": "Hao"
      },
      {
        "surname": "Shen",
        "given_name": "Liquan"
      }
    ]
  },
  {
    "title": "Recaptured screen image identification based on vision transformer",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2022.103692",
    "abstract": "Due to the copyright issues often involved in the recapture of LCD screen content, recaptured screen image identification has received lots of concerns in image source forensics. This paper analyzes the characteristics of convolutional neural network (CNN) and vision transformer (ViT) in extracting features and proposes a cascaded network structure that combines local-feature and global-feature extraction modules to detect the recaptured screen image from original images with or without demoiréing operation. We first extract the local features of the input images with five convolutional layers and feed the local features into the ViT to enhance the local perception capability of the ViT module, and further extract the global features of the input images. Through thorough experiments, our method achieves a detection accuracy rate of 0.9691 in our generated dataset and 0.9940 in the existing mixture dataset, both showing the best performance among the compared methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322002127",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Electrical engineering",
      "Engineering",
      "Feature (linguistics)",
      "Feature extraction",
      "Identification (biology)",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Guihao"
      },
      {
        "surname": "Yao",
        "given_name": "Heng"
      },
      {
        "surname": "Le",
        "given_name": "Yanfen"
      },
      {
        "surname": "Qin",
        "given_name": "Chuan"
      }
    ]
  },
  {
    "title": "HD-YOLO: Using radius-aware loss function for head detection in top-view fisheye images",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2022.103715",
    "abstract": "People detection is commonly used in computer vision systems, particularly for video surveillance and passenger flow statistics. Unlike standard cameras, fisheye cameras offer a large field of view and reduce occlusions when mounted overhead. However, due to the orientation variation of people in fisheye images, head detection models suffer from severe distortion when applied to fisheye images captured by top-view fisheye cameras. This work develops an end-to-end head detection method named HD-YOLO against complex situations in top-view fisheye images. The radius-aware loss function is designed to make HD-YOLO adapt to the impact of fisheye distortion, and the channel attention module is added to the model. We have also created new fisheye-image datasets for evaluation. Experiments showed that HD-YOLO outperforms other baseline methods on public and self-built datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322002358",
    "keywords": [
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Distortion (music)",
      "Geometry",
      "Image (mathematics)",
      "Mathematics",
      "Operating system",
      "Orientation (vector space)",
      "Overhead (engineering)",
      "RADIUS"
    ],
    "authors": [
      {
        "surname": "Wei",
        "given_name": "Xuan"
      },
      {
        "surname": "Wei",
        "given_name": "Yun"
      },
      {
        "surname": "Lu",
        "given_name": "Xiaobo"
      }
    ]
  },
  {
    "title": "Adaptive smoothness evaluation and multiple asymmetric histogram modification for reversible data hiding",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2022.103732",
    "abstract": "In this paper, an adaptive reversible data hiding (RDH) algorithm based on multiple asymmetric histograms is proposed by making full use of the image content. Different from existing multiple prediction error histogram (PEHs) modification methods that directly cluster all the pixels of a cover image into multiple categories, we firstly utilize a smoothness threshold to exclude as many pixels in complex regions as possible for reducing unnecessary pixel shifting, and then exploit fuzzy C-means with multiple deliberately-designed features to construct multiple sharply-distributed categories, which helps in increasing the subsequent embedding performance. Two asymmetric PEHs for each class are generated using a pair of asymmetric predictors, and the short part of each asymmetric PEH is modified to reduce the number of invalid modifications. The improved discrete particle swarm optimization is used to adaptively select the best bin while reducing computational complexity. The experimental results show that the proposed method outperforms several state-of-the-art RDH methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322002528",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Embedding",
      "Histogram",
      "Image (mathematics)",
      "Information hiding",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pixel",
      "Smoothness"
    ],
    "authors": [
      {
        "surname": "Weng",
        "given_name": "Shaowei"
      },
      {
        "surname": "Hou",
        "given_name": "Tanshuai"
      },
      {
        "surname": "Zhang",
        "given_name": "Tiancong"
      },
      {
        "surname": "Pan",
        "given_name": "Jeng-Shyang"
      }
    ]
  },
  {
    "title": "R2RNet: Low-light image enhancement via Real-low to Real-normal Network",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2022.103712",
    "abstract": "Images captured in weak illumination conditions could seriously degrade the image quality. Solving a series of degradation of low-light images can effectively improve the visual quality of images and the performance of high-level visual tasks. In this study, a novel Retinex-based Real-low to Real-normal Network (R2RNet) is proposed for low-light image enhancement, which includes three subnets: a Decom-Net, a Denoise-Net, and a Relight-Net. These three subnets are used for decomposing, denoising, contrast enhancement and detail preservation, respectively. Our R2RNet not only uses the spatial information of the image to improve the contrast but also uses the frequency information to preserve the details. Therefore, our model achieved more robust results for all degraded images. Unlike most previous methods that were trained on synthetic images, we collected the first Large-Scale Real-World paired low/normal-light images dataset (LSRW dataset) to satisfy the training requirements and make our model have better generalization performance in real-world scenes. Extensive experiments on publicly available datasets demonstrated that our method outperforms the existing state-of-the-art methods both quantitatively and visually. In addition, our results showed that the performance of the high-level visual task (i.e., face detection) can be effectively improved by using the enhanced results obtained by our method in low-light conditions. Our codes and the LSRW dataset are available at: https://github.com/JianghaiSCU/R2RNet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322002322",
    "keywords": [
      "Artificial intelligence",
      "Color constancy",
      "Computer science",
      "Computer vision",
      "Contrast (vision)",
      "Face (sociological concept)",
      "Generalization",
      "Image (mathematics)",
      "Image enhancement",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Hai",
        "given_name": "Jiang"
      },
      {
        "surname": "Xuan",
        "given_name": "Zhu"
      },
      {
        "surname": "Yang",
        "given_name": "Ren"
      },
      {
        "surname": "Hao",
        "given_name": "Yutong"
      },
      {
        "surname": "Zou",
        "given_name": "Fengzhu"
      },
      {
        "surname": "Lin",
        "given_name": "Fang"
      },
      {
        "surname": "Han",
        "given_name": "Songchen"
      }
    ]
  },
  {
    "title": "VICTOR: Visual incompatibility detection with transformers and fashion-specific contrastive pre-training",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2022.103741",
    "abstract": "For fashion outfits to be considered aesthetically pleasing, the garments that constitute them need to be compatible in terms of visual aspects, such as style, category and color. Previous works have defined visual compatibility as a binary classification task with items in a garment being considered as fully compatible or fully incompatible. However, this is not applicable to Outfit Maker applications where users create their own outfits and need to know which specific items may be incompatible with the rest of the outfit. To address this, we propose the Visual InCompatibility TransfORmer (VICTOR) that is optimized for two tasks: 1) overall compatibility as regression and 2) the detection of mismatching items and utilize fashion-specific contrastive language-image pre-training for fine tuning computer vision neural networks on fashion imagery. We build upon the Polyvore outfit benchmark to generate partially mismatching outfits, creating a new dataset termed Polyvore-MISFITs, that is used to train VICTOR. A series of ablation and comparative analyses show that the proposed architecture can compete and even surpass the current state-of-the-art on Polyvore datasets while reducing the instance-wise floating operations by 88%, striking a balance between high performance and efficiency. We release our code at https://github.com/stevejpapad/Visual-InCompatibility-Transformer",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322002619",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Electrical engineering",
      "Engineering",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Papadopoulos",
        "given_name": "Stefanos-Iordanis"
      },
      {
        "surname": "Koutlis",
        "given_name": "Christos"
      },
      {
        "surname": "Papadopoulos",
        "given_name": "Symeon"
      },
      {
        "surname": "Kompatsiaris",
        "given_name": "Ioannis"
      }
    ]
  },
  {
    "title": "Cartoon-Texture decomposition with patch-wise decorrelation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2022.103726",
    "abstract": "Cartoon-Texture decomposition (CTD) is a fundamental task and has wide applications in image processing and computer vision. To enhance separation of the cartoon and texture, existing models explicitly introduce correlation terms to decorrelate the two components. However, existing correlations usually ignore the local geometric structure information, thus insufficient to decorrelate cartoon and texture. In this work, we propose the patch-wise cosine similarity to decorrelate the cartoon and texture. The proposed decorrelation term takes the local geometric information into account and is more effective in separating cartoon and texture. Combining our decorrelation term with the regularities for cartoon (Relative Total Variation (RTV)) and texture (div( L 1 )-norm), we propose a new CTD model. Extended experiments show that the proposed model outperforms existing methods in CTD, especially in preserving edges of the cartoon.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322002462",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Decomposition",
      "Decorrelation",
      "Ecology",
      "Image (mathematics)",
      "Law",
      "Mathematics",
      "Norm (philosophy)",
      "Pattern recognition (psychology)",
      "Political science",
      "Similarity (geometry)",
      "Texture (cosmology)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Xiaofang"
      },
      {
        "surname": "Wang",
        "given_name": "Weiwei"
      },
      {
        "surname": "Feng",
        "given_name": "Xiangchu"
      },
      {
        "surname": "Qi",
        "given_name": "Tingting"
      }
    ]
  },
  {
    "title": "CCFNet: Cross-Complementary fusion network for RGB-D scene parsing of clothing images",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2022.103727",
    "abstract": "Schemes to complement context relationships by cross-scale feature fusion have appeared in many RGB-D scene parsing algorithms; however, most of these works conduct multi-scale information interaction after multi-modal feature fusion, which ignores the information loss of the two modes in the original coding. Therefore, a cross-complementary fusion network (CCFNet) is designed in this paper to calibrate the multi-modal information before feature fusion, so as to improve the feature quality of each mode and the information complementarity ability of RGB and the depth map. First, we divided the features into low, middle, and high levels, among which the low-level features contain the global details of the image and the main learning features include texture, edge, and other features. The middle layer features contain not only some global detail features but also some local semantic features. Additionally, the high-level features contain rich local semantic features. Then, the feature information lost in the coding process of low and middle level features is supplemented and extracted through the designed cross feature enhancement module, and the high-level features are extracted through the feature enhancement module. In addition, the cross-modal fusion module is designed to integrate multi-modal features of different levels. The experimental results verify that the proposed CCFNet achieves excellent performance on the RGB-D scene parsing dataset containing clothing images, and the generalization ability of the model is verified by the dataset NYU Depth V2.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322002474",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Feature (linguistics)",
      "Linguistics",
      "Paleontology",
      "Parsing",
      "Pattern recognition (psychology)",
      "Philosophy",
      "RGB color model"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Gao"
      },
      {
        "surname": "Zhou",
        "given_name": "Wujie"
      },
      {
        "surname": "Qian",
        "given_name": "Xiaohong"
      },
      {
        "surname": "Ye",
        "given_name": "Lv"
      },
      {
        "surname": "Lei",
        "given_name": "Jingsheng"
      },
      {
        "surname": "Yu",
        "given_name": "Lu"
      }
    ]
  },
  {
    "title": "Green learning: Introduction, examples and outlook",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2022.103685",
    "abstract": "Rapid advances in artificial intelligence (AI) in the last decade have been largely built upon the wide applications of deep learning (DL). However, the high carbon footprint yielded by larger and larger DL networks has become a concern for sustainability. Furthermore, DL decision mechanism is somewhat obscure in that it can only be verified by test data. Green learning (GL) is being proposed as an alternative paradigm to address these concerns. GL is characterized by low carbon footprints, lightweight model, low computational complexity, and logical transparency. It offers energy-efficient solutions in cloud centers as well as mobile/edge devices. GL also provides a more transparent, logical decision-making process which is essential to gaining people’s trust. Several statistical tools such as unsupervised representation learning, supervised feature learning, and supervised decision learning, have been developed to achieve this goal in recent years. We have seen a few successful GL examples with performance comparable with state-of-the-art DL solutions. This paper introduces the key characteristics of GL, its demonstrated applications, and future outlook.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032200205X",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Carbon footprint",
      "Computer science",
      "Computer security",
      "Data science",
      "Deep learning",
      "Ecology",
      "Feature learning",
      "Greenhouse gas",
      "Key (lock)",
      "Law",
      "Machine learning",
      "Operating system",
      "Political science",
      "Politics",
      "Process (computing)",
      "Representation (politics)",
      "Sustainability",
      "Transparency (behavior)"
    ],
    "authors": [
      {
        "surname": "Kuo",
        "given_name": "C.-C. Jay"
      },
      {
        "surname": "Madni",
        "given_name": "Azad M."
      }
    ]
  },
  {
    "title": "Object semantic-guided graph attention feature fusion network for Siamese visual tracking",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2022.103705",
    "abstract": "The similarity matching between the template and the search area plays a key role in Siamese-based trackers. Most Siamese-based trackers adopt correlation operation to perform feature fusion on the template branch and search branch for similarity matching. However, the correlation operation directly uses the template feature to slide the window on the search area feature without distinguishing the discriminant part of the target and the background noise, which blurs the spatial information of the response feature. To address this issue, this work proposes a novel object semantic-guided graph attention feature fusion network that both removes background information and focuses on the discriminative part of the object. The proposed network effectively removes background noise by utilizing an adaptive template instead of the fixed-size template used by the correlation operation. The network also models the contextual semantic relations of the target and uses the resulting semantic relations to guide the feature fusion process in a part-based manner, thereby accurately highlighting the discriminative parts of the target. Therefore, the problem of blurring response feature caused by correlation operation is effectively resolved. Furthermore, we propose an object-aware prediction network to learn object-aware features for classification and regression task, which effectively improves the discriminative ability of the prediction network. Experiments on many challenging benchmarks like OTB-100, LaSOT, TColor-128, GOT-10k and VOT2019, show that our methods achieves excellent performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322002255",
    "keywords": [
      "Artificial intelligence",
      "BitTorrent tracker",
      "Computer science",
      "Computer vision",
      "Discriminative model",
      "Eye tracking",
      "Feature (linguistics)",
      "Graph",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Semantic feature",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Jianwei"
      },
      {
        "surname": "Miao",
        "given_name": "Mengen"
      },
      {
        "surname": "Zhang",
        "given_name": "Huanlong"
      },
      {
        "surname": "Wang",
        "given_name": "Jingchao"
      },
      {
        "surname": "Zhao",
        "given_name": "Yanchun"
      },
      {
        "surname": "Chen",
        "given_name": "Zhiwu"
      },
      {
        "surname": "Qiao",
        "given_name": "Jianwei"
      }
    ]
  },
  {
    "title": "Real-world image dehazing with improved joint enhancement and exposure fusion",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2022.103720",
    "abstract": "In this work, a single image dehazing method that improves the haze removal capacity of the Joint Contrast Enhancement and Exposure Fusion (CEEF) method with Smoothing-Sharpening Image Filter (SSIF) is presented. In this method, the hazy image is first sharpened with SSIF to obtain a sharper image. In this way, the difference between haze and objects is amplified. Then, the AHE procedure in CEEF is replaced by CLAHE to obtain an enhanced CEEF. The enhanced CEEF is applied to the filtering result to obtain the final dehazed image. Observations demonstrate that the proposed method obtains enhanced results while reducing the amount of haze. The visual and quantitative comparisons between the proposed method and state-of-the-art dehazing methods show that the proposed method has better dehazing performance and has a 50% improvement in terms of the FADE metric compared to the closest result.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322002401",
    "keywords": [
      "Adaptive histogram equalization",
      "Architectural engineering",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Contrast (vision)",
      "Economics",
      "Engineering",
      "Filter (signal processing)",
      "Fusion",
      "Haze",
      "Histogram",
      "Histogram equalization",
      "Image (mathematics)",
      "Image processing",
      "Image restoration",
      "Joint (building)",
      "Linguistics",
      "Meteorology",
      "Metric (unit)",
      "Operations management",
      "Philosophy",
      "Physics",
      "Sharpening",
      "Smoothing"
    ],
    "authors": [
      {
        "surname": "Kaplan",
        "given_name": "Nur Huseyin"
      }
    ]
  },
  {
    "title": "SRI-Net: Similarity retrieval-based inference network for light field salient object detection",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2022.103721",
    "abstract": "The cutting-edge RGB saliency models are prone to fail for some complex scenes, while RGB-D saliency models are often affected by inaccurate depth maps. Fortunately, light field images can provide a sufficient spatial layout depiction of 3D scenes. Therefore, this paper focuses on salient object detection of light field images, where a Similarity Retrieval-based Inference Network (SRI-Net) is proposed. Due to various focus points, not all focal slices extracted from light field images are beneficial for salient object detection, thus, the key point of our model lies in that we attempt to select the most valuable focal slice, which can contribute more complementary information for the RGB image. Specifically, firstly, we design a focal slice retrieval module (FSRM) to choose an appropriate focal slice by measuring the foreground similarity between the focal slice and RGB image. Secondly, in order to combine the original RGB image and the selected focal slice, we design a U-shaped saliency inference module (SIM), where the two-stream encoder is used to extract multi-level features, and the decoder is employed to aggregate multi-level deep features. Extensive experiments are conducted on two widely used light field datasets, and the results firmly demonstrate the superiority and effectiveness of the proposed SRI-Net.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322002413",
    "keywords": [
      "Artificial intelligence",
      "Cardinal point",
      "Computer science",
      "Computer vision",
      "Field (mathematics)",
      "Focal point",
      "Focus (optics)",
      "Image (mathematics)",
      "Inference",
      "Light field",
      "Mathematics",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Pure mathematics",
      "RGB color model",
      "Salient",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Lv",
        "given_name": "Chengtao"
      },
      {
        "surname": "Zhou",
        "given_name": "Xiaofei"
      },
      {
        "surname": "Zhu",
        "given_name": "Bin"
      },
      {
        "surname": "Liu",
        "given_name": "Deyang"
      },
      {
        "surname": "Zheng",
        "given_name": "Bolun"
      },
      {
        "surname": "Zhang",
        "given_name": "Jiyong"
      },
      {
        "surname": "Yan",
        "given_name": "Chenggang"
      }
    ]
  },
  {
    "title": "Chosen plaintext attack on JPEG image encryption with adaptive key and run consistency",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2022.103733",
    "abstract": "A JPEG image encryption with the adaptive key and run consistency of MCUs is proposed. The chosen-plaintext attack (CPA) is given here on this encryption scheme. First, the adaptive key can be reproduced from the encrypted image, so that the plaintext images with the same adaptive key can be constructed. Second, the existence of run consistency of MCUs (RCM) between the original image and the encrypted image facilitates rapid estimation. In addition, the single swap for the runs of MCUs with RCM is designed for more accurate estimation. Detailed cryptanalytic results suggest that this encryption scheme can only be used to realize perceptual encryption but not to provide content protection for digital images. Furthermore, applications of the CPA to break other encryption schemes with RCM are presented.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032200253X",
    "keywords": [
      "Algorithm",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Deterministic encryption",
      "Encryption",
      "Image (mathematics)",
      "JPEG",
      "Key (lock)",
      "Multiple encryption",
      "On-the-fly encryption",
      "Plaintext",
      "Probabilistic encryption",
      "Theoretical computer science",
      "Watermarking attack"
    ],
    "authors": [
      {
        "surname": "He",
        "given_name": "Hongjie"
      },
      {
        "surname": "Yuan",
        "given_name": "Yuan"
      },
      {
        "surname": "Ye",
        "given_name": "Yuyun"
      },
      {
        "surname": "Tai",
        "given_name": "Heng-Ming"
      },
      {
        "surname": "Chen",
        "given_name": "Fan"
      }
    ]
  },
  {
    "title": "Design of anchor boxes and data augmentation for transformer-based vehicle localization",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2022.103711",
    "abstract": "Vehicle localization is an important task in the signal processing field. In recent years, context exploration has been widely studied, especially the nonlocal dependencies in an image, using, for example, attention and transformer mechanisms. However, these approaches encounter difficulties in achieving accurate localization owing to ineffective design and use of queries. Motivated by the fact that spatial information is determined by decoder embeddings and details of reference boxes, we propose a method of explicitly and dynamically modeling anchor boxes in the query generation module. Moreover, we design a geometry-aware data augmentation approach to increase the diversity of the data by employing multiple augmentation methods on an image. Experiments conducted on public datasets show that our approach can improve the average precision by approximately 1.1%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322002310",
    "keywords": [
      "Artificial intelligence",
      "Computer engineering",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Economics",
      "Management",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Task (project management)",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Rui"
      },
      {
        "surname": "Tian",
        "given_name": "Yan"
      },
      {
        "surname": "Xu",
        "given_name": "Zhaocheng"
      },
      {
        "surname": "Liu",
        "given_name": "Dongsheng"
      }
    ]
  },
  {
    "title": "Spatial-invariant convolutional neural network for photographic composition prediction and automatic correction",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103751",
    "abstract": "“Composition” determines the vividness of the image and its narrative power. Current research on image aesthetics implicitly considers simple composition rules, but no reliable composition classification and image optimization method explicitly considers composition rules. The existing composition classification models are not suitable for snapshots. We propose a composition classification model based on spatial-invariant convolutional neural networks (RSTN) with translation invariance and rotation invariance. It enhances the generalization of the model for snapshots or skewed images. Ultimately, the accuracy of the RSTN model improved by 3% over the Baseline to 90.8762%, and the rotation consistency improved by 16.015%. Furthermore, we classify images into three categories based on their sensitivity to editing: skew-sensitive, translation-sensitive, and non-space-sensitive. We design a set of composition optimization strategies for each composition that can effectively adjust the composition to beautify the image.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000019",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Composition (language)",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Gene",
      "Image (mathematics)",
      "Image translation",
      "Invariant (physics)",
      "Linguistics",
      "Mathematical physics",
      "Mathematics",
      "Messenger RNA",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Rotation (mathematics)",
      "Skew",
      "Telecommunications",
      "Translation (biology)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yaoting"
      },
      {
        "surname": "Ke",
        "given_name": "Yongzhen"
      },
      {
        "surname": "Wang",
        "given_name": "Kai"
      },
      {
        "surname": "Guo",
        "given_name": "Jing"
      },
      {
        "surname": "Yang",
        "given_name": "Shuai"
      }
    ]
  },
  {
    "title": "Depth estimation of supervised monocular images based on semantic segmentation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103753",
    "abstract": "In recent years, the research method of depth estimation of target images using Convolutional Neural Networks (CNN) has been widely recognized in the fields of artificial intelligence, scene understanding and three-dimensional (3D) reconstruction. The fusion of semantic segmentation information and depth estimation will further improve the quality of acquired depth images. However, how to deeply combine image semantic information with image depth information and use image edge information more accurately to improve the accuracy of depth image is still an urgent problem to be solved. For this purpose, we propose a novel depth estimation model based on semantic segmentation to estimate the depth of monocular images in this paper. Firstly, a shared parameter model of semantic segmentation information and depth estimation information is built, and the semantic segmentation information is used to guide depth acquisition in an auxiliary way. Then, through the multi-scale feature fusion module, the feature information contained in the neural network on different layers is fused, and the local feature information and global feature information are effectively used to generate high-resolution feature maps, so as to achieve the goal of improving the quality of depth image by optimizing the semantic segmentation model. The experimental results show that the model can fully extract and combine the image feature information, which improves the quality of monocular depth vision estimation. Compared with other advanced models, our model has certain advantages.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000032",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Image segmentation",
      "Linguistics",
      "Monocular",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Qi"
      },
      {
        "surname": "Piao",
        "given_name": "Yan"
      }
    ]
  },
  {
    "title": "Spatial and temporal information fusion for human action recognition via Center Boundary Balancing Multimodal Classifier",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2022.103716",
    "abstract": "This paper proposes a novel multimodal data classifier named Center Boundary Balancing Multimodal Classifier (CBBMC) to fuse and classify the spatial and temporal descriptors for recognizing human actions from depth video sequences. CBBMC is a composite algorithm integrating feature fusion and feature classification, in which Center Boundary Balancing Projection (CBBP) is used to balance the center and boundary information of feature class spaces. In order to solve the problem of multimodal information redundancy and isolation, two feature selection and fusion schemes of CBBMC based on embedded feature selection are presented. Moreover, two new action descriptors called Gaussian Pyramid Depth Motion Images (GP-DMI) and Depth Temporal Maps (DTM) are introduced to capture the multi-scale spatial and fine-grained temporal information of human activities. Finally, we present an effective spatial and temporal information fusion framework based on CBBMC for human action recognition. In order to evaluate the performance of the proposed approach, extensive experiments are conducted. The proposed method achieved impressive results on four benchmark datasets, namely MSR Action3D (96.33%), UTD-MHAD (94.41%), DHA (95.65%), and NTU RGB+D (83.31% cross-subject and 87.66% cross-view), even though only the depth modality was used. The experimental results demonstrate the effectiveness of our method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032200236X",
    "keywords": [
      "Activity recognition",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Computer vision",
      "Feature extraction",
      "Feature selection",
      "Mathematics",
      "Mutual information",
      "Pattern recognition (psychology)",
      "RGB color model",
      "Spatial analysis",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Xing"
      },
      {
        "surname": "Huang",
        "given_name": "Qian"
      },
      {
        "surname": "Wang",
        "given_name": "Zhijian"
      }
    ]
  },
  {
    "title": "Optimized video compression with residual split attention and swin-block artifact contraction",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2022.103737",
    "abstract": "Research in video compression has seen significant advancement in the last several years. However, the existing deep learning-based algorithms continue to be plagued by erroneous motion compression and ineffective motion compensation architectures, resulting in compression errors with a lower rate–distortion trade-off. To overcome these challenges, we present an end-to-end purely deep learning-based video compression method through a set of primary operations (e.g., motion estimation, motion compression, motion compensation, residual compression, and artifact contraction) differently. A deep residual attention split (DRAS) block is introduced for motion compression networks to pay more attention to certain image regions to create more effective features for the decoder while boosting the rate–distortion optimization (RDO) efficiency. A channel residual block (CRB) is proposed in motion compensation to yield a more accurate predicted frame, potentially improving the residual frame. To mitigate the compression errors, an artifact contraction module (ACM) by residual swin convolution UNet block is included in this model to improve the reconstruction quality. To improve the final frame, a buffer is added to fine-tune the previous reference frames. These modules combine with a loss function by assessing the trade-off and enhancing the decoded video quality. A comprehensive ablation study demonstrates the effectiveness of the proposed blocks and modules for video compression. Experimental results show the competitive performance of the proposed method on four benchmark datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322002577",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Block-matching algorithm",
      "Compression artifact",
      "Computer science",
      "Computer vision",
      "Data compression",
      "Frame (networking)",
      "Image (mathematics)",
      "Image compression",
      "Image processing",
      "Inter frame",
      "Motion compensation",
      "Quarter-pixel motion",
      "Reference frame",
      "Residual",
      "Telecommunications",
      "Video processing",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Jeny",
        "given_name": "Afsana Ahsan"
      },
      {
        "surname": "Islam",
        "given_name": "Md Baharul"
      }
    ]
  },
  {
    "title": "Attention-adaptive multi-scale feature aggregation dehazing network",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2022.103706",
    "abstract": "In this paper, we propose an end-to-end Attention-adaptive Multi-scale Feature Aggregation Dehazing Network (AMA-Net). The AMA-Net is based on U-Net and designs with three attention-driven modules, Joint Attention Residual Block (JAB), Joint Attention Feature Aggregation Group (JAAG), and Layer Adaptive Attention Feature Aggregation Module (LAA). To be more specific, considering the unevenly distributed haze in images, we introduce the JAB, which adaptively assigns weights to make networks pay attention to important features; to fully utilize the residual features, we propose the residual aggregation (via three JABs) in JAAG; since most feature aggregation methods for dehazing networks do not filter and refine features at different layers, we add LAA to the decoder to weight the features at different layers for aggregation. Through the ablation studies, we verify the effectiveness of the JAB, JAAG, and LAA. Experimental results on synthetic and real-world datasets show that the proposed AMA-Net outperforms relevant state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322002267",
    "keywords": [
      "Algorithm",
      "Architectural engineering",
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Composite material",
      "Computer science",
      "Engineering",
      "Feature (linguistics)",
      "Geometry",
      "Joint (building)",
      "Layer (electronics)",
      "Linguistics",
      "Materials science",
      "Mathematics",
      "Net (polyhedron)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Residual",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "Su",
        "given_name": "Zhuo"
      },
      {
        "surname": "Liu",
        "given_name": "Ruizhi"
      },
      {
        "surname": "Feng",
        "given_name": "Yuxin"
      },
      {
        "surname": "Zhou",
        "given_name": "Fan"
      }
    ]
  },
  {
    "title": "Improved action proposals using fine-grained proposal features with recurrent attention models",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2022.103709",
    "abstract": "Recent models for the temporal action proposal task show that local properties can be an alternative to the region proposal network (RPN) for generating good proposal candidates on untrimmed videos. In this study, we devise an RPN model with a new two-stage pipeline and a new joint scoring function for temporal proposals. The evaluation of local properties is integrated into our RPN model to search for the best proposal candidates that can be distinguished mainly in fine details of proposal regions. Our network models proposals in multiple scales using two recurrent neural network layers with attention mechanisms. We observe that joint training of the RPN with local clues and multi-scale modeling of proposals with recurrent attention mechanisms improve the performance of the proposal generation task. Our model yields state-of-the-art results on the THUMOS-14 and comparable results on the ActivityNet-1.3 datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322002292",
    "keywords": [
      "Action (physics)",
      "Architectural engineering",
      "Artificial intelligence",
      "Artificial neural network",
      "Attention network",
      "Biology",
      "Computer science",
      "Economics",
      "Engineering",
      "Evolutionary biology",
      "Function (biology)",
      "Joint (building)",
      "Machine learning",
      "Management",
      "Physics",
      "Pipeline (software)",
      "Programming language",
      "Quantum mechanics",
      "Recurrent neural network",
      "Scale (ratio)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Pehlivan",
        "given_name": "Selen"
      },
      {
        "surname": "Laaksonen",
        "given_name": "Jorma"
      }
    ]
  },
  {
    "title": "AMSFF-Net: Attention-Based Multi-Stream Feature Fusion Network for Single Image Dehazing",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2022.103748",
    "abstract": "In this paper, an end-to-end convolutional neural network is proposed to recover haze-free image named as Attention-Based Multi-Stream Feature Fusion Network (AMSFF-Net). The encoder-decoder network structure is used to construct the network. An encoder generates features at three resolution levels. The multi-stream features are extracted using residual dense blocks and fused by feature fusion blocks. AMSFF-Net has ability to pay more attention to informative features at different resolution levels using pixel attention mechanism. A sharp image can be recovered by the good kernel estimation. Further, AMSFF-Net has ability to capture semantic and sharp textural details from the extracted features and retain high-quality image from coarse-to-fine using mixed-convolution attention mechanism at decoder. The skip connections decrease the loss of image details from the larger receptive fields. Moreover, deep semantic loss function emphasizes more semantic information in deep features. Experimental findings prove that the proposed method outperforms in synthetic and real-world images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322002681",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Attention network",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Encoder",
      "Feature (linguistics)",
      "Geometry",
      "Image (mathematics)",
      "Kernel (algebra)",
      "Linguistics",
      "Mathematics",
      "Net (polyhedron)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Residual",
      "Semantic feature"
    ],
    "authors": [
      {
        "surname": "Memon",
        "given_name": "Sanaullah"
      },
      {
        "surname": "Arain",
        "given_name": "Rafaqat Hussain"
      },
      {
        "surname": "Mallah",
        "given_name": "Ghulam Ali"
      }
    ]
  },
  {
    "title": "Exploring visual relationship for social media popularity prediction",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2022.103738",
    "abstract": "Social media popularity prediction is an important channel to explore content sharing and communication on social networks. It aims to capture informative cues by analyzing multi-type data (such as images, user profiles, and text) to decide the popularity of a specified post. Intuitively, given an image, humans can volitionally focus on salient objects and relationships that are associated with their interests. For example, when we see the image including the relationship “elephant-attack-van”, it is more natural to increase our interest than the image with “elephant-near-van”. Therefore, exploiting such structural relationships is expected to help the prediction model search for evidence in support of the popularity of posts. However, most current works only focus on the global representation or the isolated objects, while ignoring the structure knowledge contained in images. To address this problem, we propose the relationship-aware social media popularity predictor. First, we extract inter-object relationships via a pre-trained scene graph generator. Then, we design a content-based filtering module to filter redundant relationships and capture the key 〈 subject–predicate–object 〉 information. Finally, we integrate relationship information with multi-type heterogeneous data and feed them into the CatBoost model for regression. Moreover, our predictor is capable of generating more intuitive interpretations by analyzing visual relationships in images to reasonably infer popularity scores. Extensive experiments conducted on the Social Media Prediction Dataset demonstrate that the proposed method can outperform other state-of-the-art models. Additional ablation studies and visualizations further validate the effectiveness and interpretability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322002589",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Focus (optics)",
      "Information retrieval",
      "Interpretability",
      "Machine learning",
      "Object (grammar)",
      "Optics",
      "Physics",
      "Popularity",
      "Psychology",
      "Social media",
      "Social psychology",
      "Visualization",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "An-An"
      },
      {
        "surname": "Du",
        "given_name": "Hongwei"
      },
      {
        "surname": "Xu",
        "given_name": "Ning"
      },
      {
        "surname": "Zhang",
        "given_name": "Quan"
      },
      {
        "surname": "Zhang",
        "given_name": "Shenyuan"
      },
      {
        "surname": "Tang",
        "given_name": "Yejun"
      },
      {
        "surname": "Li",
        "given_name": "Xuanya"
      }
    ]
  },
  {
    "title": "SE-PSNet: Silhouette-based Enhancement Feature for Panoptic Segmentation Network",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2022.103736",
    "abstract": "In this work, we propose a panoptic segmentation model that integrates bottom-up and top-down methods. Our framework is designed to guarantee both the performance and the inference speed. We also focus on improving the quality of semantic and instance masks. The proposed auxiliary task with the silhouette-based enhanced features can help the model improve the prediction quality of mask contours. Additionally, we introduce a new mask quality score intending to solve the occlusion problem. The model has less chance of ignoring small objects, which often have lower confidence scores than larger objects behind them. The results show that the proposed mask quality score can better distinguish the priority of objects when the occlusion occurs. We demonstrate the results of our work on two datasets: the COCO dataset and the CityScapes dataset. Via our approach, we obtained competitive results with fast inference time.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322002565",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Economics",
      "Epistemology",
      "Feature (linguistics)",
      "Focus (optics)",
      "Inference",
      "Linguistics",
      "Machine learning",
      "Management",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quality (philosophy)",
      "Segmentation",
      "Silhouette",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Chang",
        "given_name": "Shuo-En"
      },
      {
        "surname": "Chen",
        "given_name": "Yi"
      },
      {
        "surname": "Yang",
        "given_name": "Yi-Cheng"
      },
      {
        "surname": "Lin",
        "given_name": "En-Ting"
      },
      {
        "surname": "Hsiao",
        "given_name": "Pei-Yung"
      },
      {
        "surname": "Fu",
        "given_name": "Li-Chen"
      }
    ]
  },
  {
    "title": "MADPL-net: Multi-layer attention dictionary pair learning network for image classification",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2022.103728",
    "abstract": "With the great success of deep neural networks, combining deep learning with traditional dictionary learning has become a hot issue. However, the performance of these methods is still limited for several reasons. First, some existing methods update dictionary learning and classifier as two independent modules, which limits the classification performance. Second, the non-attention dictionary is learned to represent all images, reducing the model representation flexibility. In this paper, we design a novel end-to-end model named Multi-layer Attention Dictionary Pair Learning Network (MADPL-net), which integrates the learning schemes of the convolutional neural network, deep encoder learning, and attention dictionary pair learning (ADicL) into a unified framework. The encoder layer contains the ADicL block, which selects more image-attentive atoms in the dictionary pair block via the softmax function to ensure MADPL-net classification capability. In addition, ADicL schema can yield discriminative dictionary atoms and feature maps with high inter-class separation and high intra-class compactness. To improve the sparse representation learning performance, MADPL-net adds l 1 − norm constraint of the analysis dictionary to the cross-entropy loss function. Extensive experiments show that MADPL-net can achieve excellent performance over other state-of-the-arts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322002486",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Contextual image classification",
      "Convolutional neural network",
      "Deep learning",
      "Discriminative model",
      "Feature learning",
      "Image (mathematics)",
      "K-SVD",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Softmax function",
      "Sparse approximation"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Yulin"
      },
      {
        "surname": "Shi",
        "given_name": "Guangming"
      },
      {
        "surname": "Dong",
        "given_name": "Weisheng"
      },
      {
        "surname": "Xie",
        "given_name": "Xuemei"
      }
    ]
  },
  {
    "title": "DRBR-HDR: Dual-Branch recursive band reconstruction network for HDR with large motions",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2022.103713",
    "abstract": "Ghosting artifacts due to misaligned imaging and missing content of the moving regions are major challenges of synthesizing high dynamic range (HDR) images from multiple low-dynamic range (LDR) with different exposures in dynamic scenes. Therefore, it hopes the HDR reconstruction model can align the LDRs’ features and restore the missing content without artifacts. In the paper, a new dual-branch recursive band reconstruction network for high dynamic range (DRBR-HDR) is proposed to generate credible result in missing content regions, which not only uses global features as supplementary information to help local features from different receptive fields for efficient feature alignment but also designs a series of coarse-to-fine band representation to better repair missing areas in the process of recursion. In addition, we introduce an interactive attention mechanism for local branches to alleviate ghosting artifacts. The experimental results demonstrate that DRBR-HDR achieves state-of-the-art performance compared with that of the prevailing HDR reconstruction methods in various challenging scenes. Index Terms—inverse tone mapping, band reconstruction, global features, high dynamic range images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322002334",
    "keywords": [
      "Artificial intelligence",
      "Composite material",
      "Computer science",
      "Computer vision",
      "Dynamic range",
      "Ghosting",
      "High dynamic range",
      "High-dynamic-range imaging",
      "Law",
      "Materials science",
      "Political science",
      "Politics",
      "Range (aeronautics)",
      "Representation (politics)",
      "Tone mapping"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Yingjie"
      },
      {
        "surname": "Wang",
        "given_name": "Yongfang"
      },
      {
        "surname": "Zhang",
        "given_name": "Han"
      }
    ]
  },
  {
    "title": "SE-PSNet: Silhouette-based Enhancement Feature for Panoptic Segmentation Network",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2022.103736",
    "abstract": "In this work, we propose a panoptic segmentation model that integrates bottom-up and top-down methods. Our framework is designed to guarantee both the performance and the inference speed. We also focus on improving the quality of semantic and instance masks. The proposed auxiliary task with the silhouette-based enhanced features can help the model improve the prediction quality of mask contours. Additionally, we introduce a new mask quality score intending to solve the occlusion problem. The model has less chance of ignoring small objects, which often have lower confidence scores than larger objects behind them. The results show that the proposed mask quality score can better distinguish the priority of objects when the occlusion occurs. We demonstrate the results of our work on two datasets: the COCO dataset and the CityScapes dataset. Via our approach, we obtained competitive results with fast inference time.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322002565",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Economics",
      "Epistemology",
      "Feature (linguistics)",
      "Focus (optics)",
      "Inference",
      "Linguistics",
      "Machine learning",
      "Management",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quality (philosophy)",
      "Segmentation",
      "Silhouette",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Chang",
        "given_name": "Shuo-En"
      },
      {
        "surname": "Chen",
        "given_name": "Yi"
      },
      {
        "surname": "Yang",
        "given_name": "Yi-Cheng"
      },
      {
        "surname": "Lin",
        "given_name": "En-Ting"
      },
      {
        "surname": "Hsiao",
        "given_name": "Pei-Yung"
      },
      {
        "surname": "Fu",
        "given_name": "Li-Chen"
      }
    ]
  },
  {
    "title": "MADPL-net: Multi-layer attention dictionary pair learning network for image classification",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2022.103728",
    "abstract": "With the great success of deep neural networks, combining deep learning with traditional dictionary learning has become a hot issue. However, the performance of these methods is still limited for several reasons. First, some existing methods update dictionary learning and classifier as two independent modules, which limits the classification performance. Second, the non-attention dictionary is learned to represent all images, reducing the model representation flexibility. In this paper, we design a novel end-to-end model named Multi-layer Attention Dictionary Pair Learning Network (MADPL-net), which integrates the learning schemes of the convolutional neural network, deep encoder learning, and attention dictionary pair learning (ADicL) into a unified framework. The encoder layer contains the ADicL block, which selects more image-attentive atoms in the dictionary pair block via the softmax function to ensure MADPL-net classification capability. In addition, ADicL schema can yield discriminative dictionary atoms and feature maps with high inter-class separation and high intra-class compactness. To improve the sparse representation learning performance, MADPL-net adds l 1 − norm constraint of the analysis dictionary to the cross-entropy loss function. Extensive experiments show that MADPL-net can achieve excellent performance over other state-of-the-arts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322002486",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Contextual image classification",
      "Convolutional neural network",
      "Deep learning",
      "Discriminative model",
      "Feature learning",
      "Image (mathematics)",
      "K-SVD",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Softmax function",
      "Sparse approximation"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Yulin"
      },
      {
        "surname": "Shi",
        "given_name": "Guangming"
      },
      {
        "surname": "Dong",
        "given_name": "Weisheng"
      },
      {
        "surname": "Xie",
        "given_name": "Xuemei"
      }
    ]
  },
  {
    "title": "Image compressive sensing via hybrid regularization combining centralized group sparse representation and deep denoiser prior",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2022.103723",
    "abstract": "To effectively solve the ill-posed image compressive sensing (CS) reconstruction problem, it is essential to properly exploit image prior knowledge. In this paper, we propose an efficient hybrid regularization approach for image CS reconstruction, which can simultaneously exploit both internal and external image priors in a unified framework. Specifically, a novel centralized group sparse representation (CGSR) model is designed to more effectively exploit internal image sparsity prior by suppressing the group sparse coding noise (GSCN), i.e., the difference between the group sparse coding coefficients of the observed image and those of the original image. Meanwhile, by taking advantage of the plug-and-play (PnP) image restoration framework, a state-of-the-art deep image denoiser is plugged into the optimization model of image CS reconstruction to implicitly exploit external deep denoiser prior. To make our hybrid internal and external image priors regularized image CS method (named as CGSR-D-CS) tractable and robust, an efficient algorithm based on the split Bregman iteration is developed to solve the optimization problem of CGSR-D-CS. Experimental results demonstrate that our CGSR-D-CS method outperforms some state-of-the-art image CS reconstruction methods (either model-based or deep learning-based methods) in terms of both objective quality and visual perception.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322002437",
    "keywords": [
      "Artificial intelligence",
      "Bayesian probability",
      "Compressed sensing",
      "Computer science",
      "Computer security",
      "Exploit",
      "Image (mathematics)",
      "Image processing",
      "Image restoration",
      "Iterative reconstruction",
      "Neural coding",
      "Pattern recognition (psychology)",
      "Prior probability",
      "Regularization (linguistics)",
      "Sparse approximation"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Jin"
      },
      {
        "surname": "Fu",
        "given_name": "Zhizhong"
      }
    ]
  },
  {
    "title": "FE-YOLOv5: Feature enhancement network based on YOLOv5 for small object detection",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2023.103752",
    "abstract": "Due to their inherent characteristics, small objects have weaker feature representation after multiple down-sampling and are even annihilated in the background. FPN’s simple feature concatenation does not fully utilize multi-scale information and introduces irrelevant context into the information transfer, further reducing the detection performance of the small object. To address the above issues, we propose the simple but effective FE-YOLOv5. (1) We designed the feature enhancement module (FEM) to capture more discriminative features of the small object. Global attention and high-level global contextual information are used to guide shallow, high-resolution features. Global attention interacts with cross-dimensional feature interaction and reduces information loss. High-level context complements more detailed semantic information by modeling global relationships through non-local networks. (2) We design the spatially aware module (SAM) to filter spatial information and enhance the robustness of features. Deformable convolution performs sparse sampling and adaptive spatial learning to better focus on foreground objects. According to the experimental results, our proposed FE-YOLOv5 outperforms the other architectures in the VisDrone2019 dataset and Tsinghua-Tencent100K dataset. Compared to YOLOv5, the A P S was improved by 2.8% and 2.9%, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323000020",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Linguistics",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Min"
      },
      {
        "surname": "Yang",
        "given_name": "Wenzhong"
      },
      {
        "surname": "Wang",
        "given_name": "Liejun"
      },
      {
        "surname": "Chen",
        "given_name": "Danny"
      },
      {
        "surname": "Wei",
        "given_name": "Fuyuan"
      },
      {
        "surname": "KeZiErBieKe",
        "given_name": "HaiLaTi"
      },
      {
        "surname": "Liao",
        "given_name": "Yuanyuan"
      }
    ]
  },
  {
    "title": "Reference picture selection with decreased temporal dependency for HEVC error resilience",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2022.103724",
    "abstract": "The high level of compression efficiency achieved by the High-Efficiency Video Coding standard (HEVC) decreases the robustness of the encoded bitstreams. This increased susceptibility to network errors leads to end video quality degradation. Moreover, due to the high computational complexity of HEVC, high-resolution video transmission with time constraints over hostile channels such as wireless networks becomes more challenging. This paper proposes a reference picture selection-based error-resilient method to reduce the temporal error propagation due to high-trip delay and frame-copy concealment error. First, the encoder selects the reference pictures based on the error status received from the feedback channel, taking into consideration the Rate-Distortion-Optimization (RDO). Second, the temporal information mismatch prediction resulting from the error concealment is reduced by decreasing the temporal dependency between adjacent frames based on new motion-estimation tools. Results show a PSNR gain of about 6.13 dB, 5.20 dB and 4.72 dB for 1080p, 720p and 480p resolutions respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322002449",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Coding (social sciences)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Data compression",
      "Decoding methods",
      "Economics",
      "Encoder",
      "Error concealment",
      "Frame (networking)",
      "Gene",
      "Mathematics",
      "Mean squared prediction error",
      "Metric (unit)",
      "Motion compensation",
      "Multiview Video Coding",
      "Operating system",
      "Operations management",
      "Propagation of uncertainty",
      "Rate–distortion optimization",
      "Real-time computing",
      "Reference frame",
      "Robustness (evolution)",
      "Statistics",
      "Video processing",
      "Video quality",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Mansri",
        "given_name": "Islem"
      },
      {
        "surname": "Kouadria",
        "given_name": "Nasreddine"
      },
      {
        "surname": "Doghmane",
        "given_name": "Noureddine"
      },
      {
        "surname": "Harize",
        "given_name": "Saliha"
      },
      {
        "surname": "Bekhouch",
        "given_name": "Amara"
      }
    ]
  },
  {
    "title": "MTNet: Mutual tri-training network for unsupervised domain adaptation on person re-identification",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2022.103749",
    "abstract": "The existing unsupervised domain adaptation (UDA) methods on person re-identification (re-ID) often employ clustering to assign pseudo labels for unlabeled target domain samples. However, it is difficult to give accurate pseudo labels to unlabeled samples in the clustering process. To solve this problem, we propose a novel mutual tri-training network, termed MTNet, for UDA person re-ID. The MTNet method can avoid noisy labels and enhance the complementarity of multiple branches by collaboratively training the three different branch networks. Specifically, the high-confidence pseudo labels are used to update each network branch according to the joint decisions of the other two branches. Moreover, inspired by self-paced learning, we employ a sample filtering scheme to feed unlabeled samples into the network from easy to hard, so as to avoid trapping in the local optimal solution. Extensive experiments show that the proposed method can achieve competitive performance compared with the state-of-the-art person re-ID methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322002693",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Cluster analysis",
      "Complementarity (molecular biology)",
      "Computer science",
      "Data mining",
      "Domain (mathematical analysis)",
      "Genetics",
      "Identification (biology)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Si"
      },
      {
        "surname": "Qiu",
        "given_name": "Liuxiang"
      },
      {
        "surname": "Tian",
        "given_name": "Zimin"
      },
      {
        "surname": "Yan",
        "given_name": "Yan"
      },
      {
        "surname": "Wang",
        "given_name": "Da-Han"
      },
      {
        "surname": "Zhu",
        "given_name": "Shunzhi"
      }
    ]
  },
  {
    "title": "SCPNet: Self-constrained parallelism network for keypoint-based lightweight object detection",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2022.103719",
    "abstract": "Keypoint-based object detection achieves better performance without positioning calculations and extensive prediction. However, they have heavy backbone, and high-resolution is restored using upsampling that obtain unreliable features. We propose a self-constrained parallelism keypoint-based lightweight object detection network (SCPNet), which speeds inference, drops parameters, widens receptive fields, and makes prediction accurate. Specifically, the parallel multi-scale fusion module (PMFM) with parallel shuffle blocks (PSB) adopts parallel structure to obtain reliable features and reduce depth, adopts repeated multi-scale fusion to avoid too many parallel branches. The self-constrained detection module (SCDM) has a two-branch structure, with one branch predicting corners, and employing entad offset to match high-quality corner pairs, and the other branch predicting center keypoints. The distances between the paired corners’ geometric centers and the center keypoints are used for self-constrained detection. On MS-COCO 2017 and PASCAL VOC, SCPNet’s results are competitive with the state-of-the-art lightweight object detection. https://github.com/mengdie-wang/SCPNet.git.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322002395",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Fusion",
      "Image (mathematics)",
      "Linguistics",
      "Object detection",
      "Offset (computer science)",
      "Parallel computing",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Upsampling"
    ],
    "authors": [
      {
        "surname": "Zhong",
        "given_name": "Xian"
      },
      {
        "surname": "Wang",
        "given_name": "Mengdie"
      },
      {
        "surname": "Liu",
        "given_name": "Wenxuan"
      },
      {
        "surname": "Yuan",
        "given_name": "Jingling"
      },
      {
        "surname": "Huang",
        "given_name": "Wenxin"
      }
    ]
  },
  {
    "title": "Perceptual quality assessment for fine-grained compressed images",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2022.103696",
    "abstract": "Recent years have witnessed the rapid development of image storage and transmission systems, in which image compression plays an important role. Generally speaking, image compression algorithms are developed to ensure good visual quality at limited bit rates. However, due to the different compression optimization methods, the compressed images may have different levels of quality, which needs to be evaluated quantificationally. Nowadays, the mainstream full-reference (FR) metrics are effective to predict the quality of compressed images at coarse-grained levels (the bit rates differences of compressed images are obvious), however, they may perform poorly for fine-grained compressed images whose bit rates differences are quite subtle. Therefore, to better improve the Quality of Experience (QoE) and provide useful guidance for compression algorithms, we propose a full-reference image quality assessment (FR-IQA) method for compressed images of fine-grained levels. Specifically, the reference images and compressed images are first converted to Y C b C r color space. The gradient features are extracted from regions that are sensitive to compression artifacts. Then we employ the Log-Gabor transformation to further analyze the texture difference. Finally, the obtained features are fused into a quality score. The proposed method is validated on the fine-grained compression image quality assessment (FGIQA) database, which is especially constructed for assessing the quality of compressed images with close bit rates. The experimental results show that our metric outperforms mainstream FR-IQA metrics on the FGIQA database. We also test our method on other commonly used compression IQA databases and the results show that our method obtains competitive performance on the coarse-grained compression IQA databases as well.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322002164",
    "keywords": [
      "Artificial intelligence",
      "Automotive engineering",
      "Composite material",
      "Compressed sensing",
      "Compression (physics)",
      "Compression artifact",
      "Compression ratio",
      "Computer science",
      "Computer vision",
      "Data compression",
      "Economics",
      "Engineering",
      "Epistemology",
      "Image (mathematics)",
      "Image compression",
      "Image processing",
      "Image quality",
      "Internal combustion engine",
      "Materials science",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Quality (philosophy)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Zicheng"
      },
      {
        "surname": "sun",
        "given_name": "Wei"
      },
      {
        "surname": "Wu",
        "given_name": "Wei"
      },
      {
        "surname": "Cheng",
        "given_name": "Ying"
      },
      {
        "surname": "Min",
        "given_name": "Xiongkuo"
      },
      {
        "surname": "Zhai",
        "given_name": "Guangtao"
      }
    ]
  },
  {
    "title": "A deep recursive multi-scale feature fusion network for image super-resolution",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2022.103730",
    "abstract": "Recently, Convolutional Neural Networks (CNNs) have achieved great success in Single Image Super-Resolution (SISR). In particular, the recursive networks are now widely used. However, existing recursion-based SISR networks can only make use of multi-scale features in a layer-wise manner. In this paper, a Deep Recursive Multi-Scale Feature Fusion Network (DRMSFFN) is proposed to address this issue. Specifically, we propose a Recursive Multi-Scale Feature Fusion Block (RMSFFB) to make full use of multi-scale features. Besides, a Progressive Feature Fusion (PFF) technique is proposed to take advantage of the hierarchical features from the RMSFFB in a global manner. At the reconstruction stage, we use a deconvolutional layer to upscale the feature maps to the desired size. Extensive experimental results on benchmark datasets demonstrate the superiority of the proposed DRMSFFN in comparison with the state-of-the-art methods in both quantitative and qualitative evaluations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322002504",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Block (permutation group theory)",
      "Chemistry",
      "Computer science",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Fusion",
      "Geodesy",
      "Geography",
      "Geometry",
      "Image (mathematics)",
      "Layer (electronics)",
      "Linguistics",
      "Mathematics",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Recursion (computer science)",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Feiqiang"
      },
      {
        "surname": "Yang",
        "given_name": "Xiaomin"
      },
      {
        "surname": "De Baets",
        "given_name": "Bernard"
      }
    ]
  },
  {
    "title": "Unsupervised self-attention lightweight photo-to-sketch synthesis with feature maps",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2022.103747",
    "abstract": "Face-sketch synthesis is important for gaining a clear portrait photo of suspects when solving crimes. Recent research has made a great process in self-attention generative adversarial networks. We propose a method of unsupervised learning in the synthesis of face sketch-to-photo using a new attention module. The method of processing on a small reference set of photo-sketch pairs adds to the attention module, a focus on the regions distinguishing photos from sketches on the basis of the feature maps obtained by the auxiliary classifier. Unlike previous attention-based methods, which cannot handle the geometric changes between domains, our model can translate images requiring holistic changes. At the same time, we reduce the layers of the discriminator according to different residual layers to optimize our network. With the proposed approach, we can train our networks using a small reference set of photo-sketch pairs together with a large number of face-photo datasets and more distinguishing facial-feature regions in the self-attention model. Experiments have shown the superiority of the proposed method to existing face sketch-to-photo synthesis models using fixed network architectures and hyper-parameters.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032200267X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Detector",
      "Discriminator",
      "Face (sociological concept)",
      "Feature (linguistics)",
      "Generative grammar",
      "Generative model",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Set (abstract data type)",
      "Sketch",
      "Social science",
      "Sociology",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Zhong",
        "given_name": "Kunru"
      },
      {
        "surname": "Chen",
        "given_name": "Zhenxue"
      },
      {
        "surname": "Liu",
        "given_name": "Chengyun"
      },
      {
        "surname": "Wu",
        "given_name": "Q. M. Jonathan"
      },
      {
        "surname": "Duan",
        "given_name": "Shuchao"
      }
    ]
  },
  {
    "title": "Low complexity inter coding scheme for Versatile Video Coding (VVC)",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2022.103683",
    "abstract": "The latest coding standard Versatile Video Coding (VVC) developed by the Joint Video Experts Team (JVET) and Video Coding Experts Group (VCEG) was finalized in 2020. By introducing several new coding techniques, VVC improves the compression efficiency by 50% compared with H.265/HEVC. However, the coding complexity increases dramatically, which obstructs it from real-time application. To tackle this issue, a fast inter coding algorithm utilizing coding information is proposed to speed up the coding process. First, by analyzing the coding areas of the neighboring CUs, we predict the coding area of the current CU to terminate unnecessary splitting modes. Then, the temporally optimal coding mode generated during the prediction process is further utilized to shrink the candidate modes to speed up the coding process. Finally, the distribution of neighboring prediction modes are exploited to measure the motion complexity of the current CU, based on which the unnecessary prediction modes can be skipped earlier. Experimental results demonstrate that the proposed method can reduce the coding complexity by 40.08% on average with 0.07 dB BDPSNR decrease and 1.56% BDBR increase, which outperforms the state-of-the-art approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322002036",
    "keywords": [
      "Algorithm",
      "Coding (social sciences)",
      "Coding tree unit",
      "Computer science",
      "Context-adaptive binary arithmetic coding",
      "Data compression",
      "Decoding methods",
      "Mathematical analysis",
      "Mathematics",
      "Scheme (mathematics)",
      "Statistics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Shang",
        "given_name": "Xiwu"
      },
      {
        "surname": "Li",
        "given_name": "Guoping"
      },
      {
        "surname": "Zhao",
        "given_name": "Xiaoli"
      },
      {
        "surname": "Zuo",
        "given_name": "Yifan"
      }
    ]
  },
  {
    "title": "Densely connected convolutional transformer for single image dehazing",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2022.103722",
    "abstract": "Image Dehazing is an important low-level vision task that aims to remove the haze from an image. In this paper, we proposed Densely Connected Convolutional Transformer (DCCT) for single image dehazing. DCCT is an efficient architecture that combines the multi-head Performer with the local dependencies. To prevent loss of information between features at different levels, we propose a learnable connection layer that is used to fuse features at different levels across the entire architecture. We guide the training of DCCT through a joint loss considering a supervised metric learning approach that allows us to consider both negative and positive features for a multi-image perceptual loss. We validate the design choices and the effectiveness of the proposed DCCT through ablation studies. Through comparison with the representative techniques, we establish that the proposed DCCT is highly competitive with the state of the art.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322002425",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Electrical engineering",
      "Engineering",
      "Fuse (electrical)",
      "Pattern recognition (psychology)",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Parihar",
        "given_name": "Anil Singh"
      },
      {
        "surname": "Java",
        "given_name": "Abhinav"
      }
    ]
  },
  {
    "title": "Multiscale Global-Aware Channel Attention for Person Re-identification",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2022.103714",
    "abstract": "Most person re-identification methods are researched under various assumptions. However, viewpoint variations or occlusions are often encountered in practical scenarios. These are prone to intra-class variance. In this paper, we propose a multiscale global-aware channel attention (MGCA) model to solve this problem. It imitates the process of human visual perception, which tends to observe things from coarse to fine. The core of our approach is a multiscale structure containing two key elements: the global-aware channel attention (GCA) module for capturing the global structural information and the adaptive selection feature fusion (ASFF) module for highlighting discriminative features. Moreover, we introduce a bidirectional guided pairwise metric triplet (BPM) loss to reduce the effect of outliers. Extensive experiments on Market-1501, DukeMTMC-reID, and MSMT17, and achieve the state-of-the-art results on mAP. Especially, our approach exceeds the current best method by 2.0% on the most challenging MSMT17 dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322002346",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Computer security",
      "Discriminative model",
      "Economics",
      "Feature (linguistics)",
      "Identification (biology)",
      "Key (lock)",
      "Linguistics",
      "Machine learning",
      "Metric (unit)",
      "Operating system",
      "Operations management",
      "Outlier",
      "Pairwise comparison",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Process (computing)"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Yingjie"
      },
      {
        "surname": "Yang",
        "given_name": "Wenzhong"
      },
      {
        "surname": "Wang",
        "given_name": "Liejun"
      },
      {
        "surname": "Chen",
        "given_name": "Danny"
      },
      {
        "surname": "Wang",
        "given_name": "Min"
      },
      {
        "surname": "Wei",
        "given_name": "Fuyuan"
      },
      {
        "surname": "KeZiErBieKe",
        "given_name": "HaiLaTi"
      },
      {
        "surname": "Liao",
        "given_name": "Yuanyuan"
      }
    ]
  },
  {
    "title": "Efficient image dehazing algorithm using multiple priors constraints",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2022.103694",
    "abstract": "In this study, a robust and efficient image dehazing technique based on the atmospheric scattering model is proposed, which effectively overcomes the limitations of a single prior condition. It is composed of a transmission estimation module and an atmospheric light estimation module. The transmission estimation module integrates multiple dehazing prior strategies and effectively optimises transmission estimation and application range. The atmospheric light estimation module uses the fuzzy C-means clustering algorithm (FCM) to estimate the atmospheric light of different scenes in an image. Unlike in the previous work, the atmospheric light in this module is a nonglobal value, and a pixel-level atmospheric light value matrix is obtained. Numerous experiments show that the proposed dehazing algorithm is superior to state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322002140",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Bayesian probability",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Pattern recognition (psychology)",
      "Prior probability"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Zilong"
      },
      {
        "surname": "Jing",
        "given_name": "Hongyuan"
      },
      {
        "surname": "Chen",
        "given_name": "Aidong"
      },
      {
        "surname": "Hong",
        "given_name": "Chen"
      },
      {
        "surname": "Shang",
        "given_name": "Xinna"
      }
    ]
  },
  {
    "title": "Visual video evaluation association modeling based on chaotic pseudo-random multi-layer compressed sensing for visual privacy-protected keyframe extraction",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2023",
    "doi": "10.1016/j.jvcir.2022.103691",
    "abstract": "In current society, artificial intelligence processing technology offers convenient video monitoring, but also raises the risk of privacy leakage. Theoretically, the data used in intelligent video processing methods may directly convey visual information containing private content. For the above problem, this paper uses a multi-layer visual privacy-protected (VPP) coding method to blur private content in the video at the visual level, while avoiding the loss of important visual features contained in the video as much as possible. And this provides a guarantee of the quality of the subsequent keyframe extraction step. Then a visual evaluation algorithm is proposed for assessing the quality of VPP-encoded video privacy protection. And the experiment shows that the results are consistent with those of subjective evaluation. In addition, for VPP-encoded video, we propose an unsupervised two-layer clustering keyframe extraction method with corresponding performance evaluation index. Finally, an association model is established to balance the privacy protection quality and the keyframe extraction performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320322002115",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Coding (social sciences)",
      "Computer science",
      "Computer vision",
      "Economics",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Statistics",
      "Video quality"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Jixin"
      },
      {
        "surname": "Li",
        "given_name": "Yicong"
      },
      {
        "surname": "Han",
        "given_name": "Guang"
      },
      {
        "surname": "Sun",
        "given_name": "Ning"
      }
    ]
  }
]