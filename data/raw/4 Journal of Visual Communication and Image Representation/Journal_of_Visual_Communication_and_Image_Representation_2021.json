[
  {
    "title": "Dynamic texture representation based on oriented magnitudes of Gaussian gradients",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103330",
    "abstract": "Efficiently capturing shape and turbulent motions of dynamic textures (DTs) for video description is a challenge in real applications due to the negative influences of the well-known problems: environmental elements, illumination, scale, and noise. In this paper, we propose an efficient and simple framework for DT representation based on the oriented features of high-order Gaussian gradients. Firstly, 2D/3D Gaussian-based filtering kernels in high-order partial derivatives are taken into account the video analysis as a preprocessing step to obtain corresponding gradient-filtered images/volumes. After that, the oriented features, which are robust against the above issues, are extracted by decomposing the Gaussian derivative magnitudes into oriented components. Finally, a shallow local encoding is utilized for structuring spatio-temporal features from these oriented magnitudes. This allows constructing discriminative descriptors with promising performances compared to those based on the non-oriented ones. Experimental results for DT classification task on benchmark datasets have verified the interest of our proposal.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002182",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Discriminative model",
      "Gaussian",
      "Gaussian filter",
      "Geodesy",
      "Geology",
      "Image (mathematics)",
      "Law",
      "Noise (video)",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Politics",
      "Preprocessor",
      "Quantum mechanics",
      "Representation (politics)",
      "Texture (cosmology)"
    ],
    "authors": [
      {
        "surname": "Nguyen",
        "given_name": "Thanh Tuan"
      },
      {
        "surname": "Nguyen",
        "given_name": "Thanh Phuong"
      },
      {
        "surname": "Bouchara",
        "given_name": "Frédéric"
      }
    ]
  },
  {
    "title": "Gaze prediction for first-person videos based on inverse non-negative sparse coding with determinant sparse measure",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103367",
    "abstract": "Gaze prediction is a significant approach for processing a large amount of incoming visual information of videos. Recent gaze prediction algorithms often employ sparse models with the assumption that every superpixel in the video frames can be represented as linear combinations of a few salient superpixels. However, they are not actuated enough because of the insufficient knowledge that video signals contain a non-negative request. Hence, we develop a novel gaze prediction based on an inverse sparse coding framework with a determinant sparse measure. By introducing this sparse measure, the solutions are non-negative and sparser than conventional sparse constraints. However, the proposed optimization problem becomes nonconvex, which is difficult to solve. To efficiently address the corresponding nonconvex optimization problem, we propose a novel algorithm based on the difference in convex function programming, which can yield the global solutions. Experimental results indicate the improved accuracy of the proposed approach compared with state-of-the-art algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002418",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Coding (social sciences)",
      "Computer science",
      "Convex optimization",
      "Data mining",
      "Gaze",
      "Geometry",
      "Mathematics",
      "Measure (data warehouse)",
      "Neural coding",
      "Pattern recognition (psychology)",
      "Regular polygon",
      "Salient",
      "Sparse approximation",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Yujie"
      },
      {
        "surname": "Tan",
        "given_name": "Benying"
      },
      {
        "surname": "Akaho",
        "given_name": "Shotaro"
      },
      {
        "surname": "Asoh",
        "given_name": "Hideki"
      },
      {
        "surname": "Ding",
        "given_name": "Shuxue"
      }
    ]
  },
  {
    "title": "YOLSO: You Only Look Small Object",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103348",
    "abstract": "Small object detection is challenging and far from satisfactory. Most general object detectors suffer from two critical issues with small objects: (1) Feature extractor based on classification network cannot express the characteristics of small objects reasonably due to insufficient appearance information of targets and a large amount of background interference around them. (2) The detector requires a much higher location accuracy for small objects than for general objects. This paper proposes an effective and efficient small object detector YOLSO to address the above problems. For feature representation, we analyze the drawbacks in previous backbones and present a Half-Space Shortcut(HSSC) module to build a background-aware backbone. Furthermore, a coarse-to-fine Feature Pyramid Enhancement(FPE) module is introduced for layer-wise aggregation at a granular level to enhance the semantic discriminability. For loss function, we propose an exponential L1 loss to promote the convergence of regression, and a focal IOU loss to focus on prime samples with high classification confidence and high IOU. Both of them significantly improves the location accuracy of small objects. The proposed YOLSO sets state-of-the-art results on two typical small object datasets, MOCOD and VeDAI, at a speed of over 200 FPS. In the meantime, it also outperforms the baseline YOLOv3 by a wide margin on the common COCO dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002297",
    "keywords": [
      "Artificial intelligence",
      "Backbone network",
      "Computer network",
      "Computer science",
      "Detector",
      "Feature (linguistics)",
      "Focus (optics)",
      "Geometry",
      "Image (mathematics)",
      "Law",
      "Linguistics",
      "Machine learning",
      "Margin (machine learning)",
      "Mathematics",
      "Object (grammar)",
      "Object detection",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Political science",
      "Politics",
      "Pyramid (geometry)",
      "Representation (politics)",
      "Similarity (geometry)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Jinpu"
      },
      {
        "surname": "Zhang",
        "given_name": "Lei"
      },
      {
        "surname": "Liu",
        "given_name": "Tianyu"
      },
      {
        "surname": "Wang",
        "given_name": "Yuehuan"
      }
    ]
  },
  {
    "title": "Structure-prior deep neural network for lane detection",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103373",
    "abstract": "Lane detection is an important task of road environment perception for autonomous driving. Deep learning methods based on semantic segmentation have been successfully applied to lane detection, but they require considerable computational cost for high complexity. The lane detection is treated as a particular semantic segmentation task due to the prior structural information of lane markings which have long continuous shape. Most traditional CNN are designed for the representation learning of semantic information, while this prior structural information is not fully exploited. In this paper, we propose a recurrent slice convolution module (called RSCM) to exploit the prior structural information of lane markings. The proposed RSCM is a special recurrent network structure with several slice convolution units (called SCU). The RSCM could obtain stronger semantic representation through the propagation of the prior structural information in SCU. Furthermore, we design a distance loss in consideration of the prior structure of lane markings. The lane detection network can be trained more steadily via the overall loss function formed by combining segmentation loss with the distance loss. The experimental results show the effectiveness of our method. We achieve excellent computation efficiency while keeping decent detection quality on lane detection benchmarks and the computational cost of our method is much lower than the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002467",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computation",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Deep learning",
      "Engineering",
      "Evolutionary biology",
      "Function (biology)",
      "Law",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Perception",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Segmentation",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Xiao",
        "given_name": "Degui"
      },
      {
        "surname": "Zhuo",
        "given_name": "Lin"
      },
      {
        "surname": "Li",
        "given_name": "Jianfang"
      },
      {
        "surname": "Li",
        "given_name": "Jiazhi"
      }
    ]
  },
  {
    "title": "Discrete hashing with triple supervision learning",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103355",
    "abstract": "In recent years, discrete supervised hashing methods have attracted increasing attention because of their high retrieval efficiency and precision. However, in these methods, some effective semantic information is typically neglected, which means that all the information is not sufficiently utilized. Moreover, these methods often only decompose the first-order features of the original data, ignoring the more fine-grained higher-order features. To address these problems, we propose a supervised hashing learning method called discrete hashing with triple supervision learning (DHTSL). Specifically, we integrate three aspects of semantic information into this method: (1) the bidirectional mapping of semantic labels; (2) pairwise similarity relations; (3) second-order features from the original data. We also design a discrete optimization method to solve the proposed objective function. Moreover, an out-of-sample extension strategy that can better maintain the independence and balance of hash codes is employed to improve retrieval performance. Extensive experiments on three widely used datasets demonstrate its superior performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002340",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Data mining",
      "Double hashing",
      "Dynamic perfect hashing",
      "Extension (predicate logic)",
      "Hash function",
      "Hash table",
      "Image (mathematics)",
      "Independence (probability theory)",
      "Machine learning",
      "Mathematics",
      "Pairwise comparison",
      "Pattern recognition (psychology)",
      "Programming language",
      "Similarity (geometry)",
      "Statistics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Shaohua"
      },
      {
        "surname": "Kang",
        "given_name": "Xiao"
      },
      {
        "surname": "Liu",
        "given_name": "Fasheng"
      },
      {
        "surname": "Nie",
        "given_name": "Xiushan"
      },
      {
        "surname": "Liu",
        "given_name": "Xingbo"
      }
    ]
  },
  {
    "title": "A discriminant kernel entropy-based framework for feature representation learning",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103366",
    "abstract": "The intelligent multimedia processing community has developed increasing interest in kernel entropy component analysis (KECA) due to its abilities in effective data transformation and dimensionality reduction. However, since only the unsupervised structural information of Renyi entropy from the given data set is utilized, KECA alone is incapable of generating high quality discriminant features. Aiming to develop a new and generic approach for feature representation learning, this paper proposes a discriminant kernel entropy-based framework, which integrates KECA and a complete discriminant strategy (consisting of regular and irregular discriminant information), to explore discriminant feature representations from the given data set. The framework is realized and further optimized to generate a more powerful discriminant descriptor for feature representation learning, leading to improved performance. Since the joint utilization of kernel entropy estimation and the complete discriminant strategy is able to reveal the distribution and semantic information of the given data, the proposed framework opens up a new front for discriminant feature representation learning via information theoretic learning (ITL). To demonstrate the generic nature and effectiveness of the proposed framework, experiments are conducted on two different data sources; the visual data source (e.g., University of California Irvine (UCI) database, Olivetti Research Lab (ORL) database, Caltech 256 database) and the audio data source (Ryerson Multimedia Lab (RML) audio emotion database). The results show this framework yields superior performance over other methods on the data sets evaluated for feature representation learning.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032100239X",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Dimensionality reduction",
      "Discriminant",
      "Entropy (arrow of time)",
      "Facial recognition system",
      "Feature learning",
      "Kernel (algebra)",
      "Kernel Fisher discriminant analysis",
      "Linear discriminant analysis",
      "Machine learning",
      "Mathematics",
      "Optimal discriminant analysis",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Gao",
        "given_name": "Lei"
      },
      {
        "surname": "Qi",
        "given_name": "Lin"
      },
      {
        "surname": "Guan",
        "given_name": "Ling"
      }
    ]
  },
  {
    "title": "3-D Epanechnikov Mixture Regression in integral imaging compression",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103332",
    "abstract": "Integral imaging is a kind of 3D display with no glasses, which represents the future developments. Elementary image array (EIA) is an essential component of integral imaging. Our coding framework includes pre-processing, modeling, and reconstruction. We acquire the sub-EIA from the original EIA and get the offsets between adjacent elementary images (EIs) through pre-processing. As for modeling, we get the optimal combination of 3-D Epanechnikov Mixture Regression (3-D EMR) or 3-D Gaussian Mixture Regression (3-D GMR) by Elementary Image Adaptive Model Selection (EI-AMLS) algorithm to achieve the best modeling of sub-EIA. Finally, the linear-based reconstruction is completed according to the correlation between adjacent EIs. Our decoded images realize a clearer outline reconstruction and more superior coding efficiency than HEVC and JPEG2000 below about 0.05bpp. Furthermore, the proposed method can achieve the same visual effect as HEVC with only 15% to 80% time consumed.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002170",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Automotive engineering",
      "Coding (social sciences)",
      "Compression ratio",
      "Computer science",
      "Computer vision",
      "Engineering",
      "Gaussian",
      "Image (mathematics)",
      "Image compression",
      "Image processing",
      "Integral imaging",
      "Internal combustion engine",
      "Iterative reconstruction",
      "JPEG 2000",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Regression",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Boning"
      },
      {
        "surname": "Zhao",
        "given_name": "Yan"
      },
      {
        "surname": "Jiang",
        "given_name": "Xiaomeng"
      },
      {
        "surname": "Wang",
        "given_name": "Shigang"
      },
      {
        "surname": "Wei",
        "given_name": "Jian"
      }
    ]
  },
  {
    "title": "MultiTempGAN: Multitemporal multispectral image compression framework using generative adversarial networks",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103385",
    "abstract": "Multispectral satellites that measure the reflected energy from the different regions on the Earth generate the multispectral (MS) images continuously. The following MS image for the same region can be acquired with respect to the satellite revisit period. The images captured at different times over the same region are called multitemporal images. Traditional compression methods generally benefit from spectral and spatial correlation within the MS image. However, there is also a temporal correlation between multitemporal images. To this end, we propose a novel generative adversarial network (GAN) based prediction method called MultiTempGAN for compression of multitemporal MS images. The proposed method defines a lightweight GAN-based model that learns to transform the reference image to the target image. Here, the generator parameters of MultiTempGAN are saved for the reconstruction purpose in the receiver system. Due to MultiTempGAN has a low number of parameters, it provides efficiency in multitemporal MS image compression. Experiments were carried out on three Sentinel-2 MS image pairs belonging to different geographical regions. We compared the proposed method with JPEG2000-based conventional compression methods and three deep learning methods in terms of signal-to-noise ratio, mean spectral angle, mean spectral correlation, and laplacian mean square error metrics. Additionally, we have also evaluated the change detection performances and visual maps of the methods. Experimental results demonstrate that MultiTempGAN not only achieves the best metric values among the other methods at high compression ratios but also presents convincing performances in change detection applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002546",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Data compression",
      "Geology",
      "Image (mathematics)",
      "Image compression",
      "Image processing",
      "Mathematics",
      "Mean squared error",
      "Multispectral image",
      "Pattern recognition (psychology)",
      "Remote sensing",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Karaca",
        "given_name": "Ali Can"
      },
      {
        "surname": "Kara",
        "given_name": "Ozan"
      },
      {
        "surname": "Güllü",
        "given_name": "Mehmet Kemal"
      }
    ]
  },
  {
    "title": "Low-light image enhancement by diffusion pyramid with residuals",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103364",
    "abstract": "With the advancement of the camera-related technology in mobile devices, the vast amount of photos have been taken and shared in our daily life. However, many users still have unsatisfactory experiences with low-visible photos, which are frequently acquired under complicated real-world environments. In this paper, a novel yet simple method for low-light image enhancement has been proposed without any learning procedure. The key idea of the proposed method is to estimate properties of the scene illumination both in global and local manner by exploiting the diffusion pyramid with residuals. Specifically, the residual of each scale level in the diffusion pyramid is combined with the corresponding input. This restored result efficiently highlights local details across different scale spaces, thus it is helpful for preserving the boundary of illuminations. By conducting max-pooling with restored results from different levels of the diffusion pyramid, which are resized to the original resolution, the illumination component is accurately inferred from a given image. Compared to recent learning-based approaches, one important advantage of the proposed method is to effectively avoid the overfitting problem to the specific training dataset. Experimental results on various benchmark datasets demonstrate the efficiency and robustness of the proposed method for low-light image enhancement in real-world scenarios.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002406",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Gene",
      "Geodesy",
      "Geography",
      "Geometry",
      "Image (mathematics)",
      "Image enhancement",
      "Mathematics",
      "Overfitting",
      "Pooling",
      "Pyramid (geometry)",
      "Residual",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Wonjun"
      }
    ]
  },
  {
    "title": "No-reference stereoscopic image quality evaluator based on human visual characteristics and relative gradient orientation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103354",
    "abstract": "Stereoscopic image quality assessment (SIQA) is of great significance to the development of modern three-dimensional (3D) display technology. In this work, by further mining the relationship between visual features and stereoscopic image quality perception, we build a new no-reference SIQA model, which combines the monocular and binocular features. Statistical quality-aware structural features from relative gradient orientation (RGO) map and texture features from the histogram of the weighted local binary pattern (LBP) in the texture image (TLBP) are not only extracted from both monocular view, but also extracted from binocular views to predict binocular quality perception. Meanwhile, the color statistical features ignored by most models and the binocularity feature is extracted to complement the monocular features and the above binocular features, respectively. Finally, all the extracted features and subjective scores are used to predict the objective quality score through the support vector regression (SVR) model. Experiments on four popular stereoscopic image databases show that the proposed model achieves high consistency with subjective assessment, and the performance of the model is very competitive with the latest models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002327",
    "keywords": [
      "Artificial intelligence",
      "Binocular vision",
      "Computer science",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Feature (linguistics)",
      "Geometry",
      "Histogram",
      "Image (mathematics)",
      "Image quality",
      "Linguistics",
      "Local binary patterns",
      "Mathematics",
      "Monocular",
      "Orientation (vector space)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Stereoscopy",
      "Support vector machine",
      "Texture (cosmology)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Yun"
      },
      {
        "surname": "Huang",
        "given_name": "Baoqing"
      },
      {
        "surname": "Yu",
        "given_name": "Hongwei"
      },
      {
        "surname": "Zheng",
        "given_name": "Zhi"
      }
    ]
  },
  {
    "title": "A comparative study between PVO-based framework and multi-predictor mechanism in reversible data hiding",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103349",
    "abstract": "Sorting-based reversible data hiding (RDH) methods like pixel-value-ordering (PVO) can predict pixel values accurately and achieve an extremely low distortion on the embedded image. However, the excellent performance of these methods was not well explained in previous works, and there are unexploited common points among them. In this paper, we propose a general multi-predictor (GMP) framework to summarize PVO-based RDH methods and explain their high prediction accuracy. Moreover, by utilizing the proposed GMP framework, a more efficient sorting-based RDH method is given as an example to show the generality and applicability of our framework. Comparing with other PVO-based methods, the proposed example method can achieve significant improvement in embedding performance. It is hopeful that more efficient sorting-based RDH algorithms can be designed according to our proposed framework by designing better predictors and their combination methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002315",
    "keywords": [
      "Algorithm",
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Computer network",
      "Computer science",
      "Data mining",
      "Distortion (music)",
      "Embedding",
      "Generality",
      "Image (mathematics)",
      "Information hiding",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Pixel",
      "Psychology",
      "Psychotherapist",
      "Sorting",
      "Value (mathematics)"
    ],
    "authors": [
      {
        "surname": "Fan",
        "given_name": "Guojun"
      },
      {
        "surname": "Pan",
        "given_name": "Zhibin"
      },
      {
        "surname": "Zhou",
        "given_name": "Quan"
      },
      {
        "surname": "Gao",
        "given_name": "Xinyi"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiaoran"
      }
    ]
  },
  {
    "title": "Weighted truncated nuclear norm regularization for low-rank quaternion matrix completion",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103335",
    "abstract": "In recent years, quaternion matrix completion (QMC) based on low-rank regularization has been gradually used in image processing. Unlike low-rank matrix completion (LRMC) which handles RGB images by recovering each color channel separately, QMC models retain the connection of three channels and process them as a whole. Most of the existing quaternion-based methods formulate low-rank QMC (LRQMC) as a quaternion nuclear norm (a convex relaxation of the rank) minimization problem. The main limitation of these approaches is that they minimize the singular values simultaneously such that cannot approximate low-rank attributes efficiently. To achieve a more accurate low-rank approximation, we introduce a quaternion truncated nuclear norm (QTNN) for LRQMC and utilize the alternating direction method of multipliers (ADMM) to get the optimization in this paper. Further, we propose weights to the residual error quaternion matrix during the update process for accelerating the convergence of the QTNN method with admissible performance. The weighted method utilizes a concise gradient descent strategy which has a theoretical guarantee in optimization. The effectiveness of our method is illustrated by experiments on real visual data sets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002200",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Artificial intelligence",
      "Combinatorics",
      "Composite material",
      "Computer science",
      "Eigenvalues and eigenvectors",
      "Gaussian",
      "Geometry",
      "Law",
      "Low-rank approximation",
      "Materials science",
      "Mathematical optimization",
      "Mathematics",
      "Matrix (chemical analysis)",
      "Matrix completion",
      "Matrix norm",
      "Norm (philosophy)",
      "Physics",
      "Political science",
      "Pure mathematics",
      "Quantum mechanics",
      "Quaternion",
      "Rank (graph theory)",
      "Regularization (linguistics)",
      "Tensor (intrinsic definition)"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Liqiao"
      },
      {
        "surname": "Kou",
        "given_name": "Kit Ian"
      },
      {
        "surname": "Miao",
        "given_name": "Jifei"
      }
    ]
  },
  {
    "title": "A multi-exposure fusion framework for contrast enhancement of hazy images employing dynamic stochastic resonance",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103376",
    "abstract": "Current imaging devices coupled with advanced hardware and software are smart enough to enhance low light images taken in clear weather. But in hazy or foggy environments, the captured images are of degraded quality. To address this issue, image processing algorithms are employed to enhance the degraded images to make useful for extracting meaningful features. In this study, we propose a haze removal algorithm to improve the color and contrast of images captured in hazy environments. The first step involves generation of images with various exposures using the theory of dynamic stochastic resonance. The images are then fused in a multi-scale fusion framework crafting weight maps viz. haze density, chromaticity, and luminance gradient. The fusion process focuses on uniformly enhancing the dark and bright regions of the image. However, it may overemphasize haze affected regions. Therefore, in the second step, the atmospheric scattering equation is referred and its modified version is applied that accomplishes the haze removal task. Quantitative and qualitative analyses demonstrate the effectiveness of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002492",
    "keywords": [
      "Artificial intelligence",
      "Chromaticity",
      "Computer science",
      "Computer vision",
      "Contrast (vision)",
      "Fusion",
      "Gamma correction",
      "Haze",
      "Image (mathematics)",
      "Image fusion",
      "Linguistics",
      "Luminance",
      "Meteorology",
      "Operating system",
      "Philosophy",
      "Physics",
      "Process (computing)"
    ],
    "authors": [
      {
        "surname": "Kumar",
        "given_name": "Avishek"
      },
      {
        "surname": "Jha",
        "given_name": "Rajib Kumar"
      },
      {
        "surname": "Nishchal",
        "given_name": "Naveen K."
      }
    ]
  },
  {
    "title": "Multi-frame co-saliency spatio-temporal regularization correlation filters for object tracking",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103329",
    "abstract": "The spatial regularization weight of the correlation filter is not related to the object content and the model degradation in the tracking process. To solve this problem, a new multi-frame co-saliency spatio-temporal regularization correlation filters (MCSRCF) is proposed for visual object tracking. To the best our knowledge, this is the first application of co-saliency regularization to CF-based tracking. In MCSRCF, grayscale features, directional gradient histogram (HOG) features and CNN features are extracted to improve the tracking precision of the tracker. Secondly, the three-dimensional spatial saliency and semantic saliency are introduced to obtain the initial weight of the spatial regularization with object content information. Then, the heterogeneous saliency fusion method is exploited to add a co-saliency spatial regularization term to the objective function to make the spatial penalty weight learn the change of the object region. In additional, the temporal saliency regularization is introduced to learn the information between adjacent frames, which reduces the overfitting effect caused by inaccurate samples. A variety of evaluations are conducted on public benchmarks, and the experimental results show that the proposed tracker achieves good robustness against many state-of-the-art trackers in various complex scenarios.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002145",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Correlation",
      "Gene",
      "Geometry",
      "Histogram",
      "Image (mathematics)",
      "Mathematics",
      "Overfitting",
      "Pattern recognition (psychology)",
      "Regularization (linguistics)",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Xi"
      },
      {
        "surname": "Li",
        "given_name": "Shaoyi"
      },
      {
        "surname": "Ma",
        "given_name": "Jun"
      },
      {
        "surname": "Liu",
        "given_name": "Hao"
      },
      {
        "surname": "Yan",
        "given_name": "Jie"
      }
    ]
  },
  {
    "title": "Multi-focus image fusion based on multi-scale sparse representation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103328",
    "abstract": "Although colorful information in natural scenes can be collected, due to the limitation of camera depth of field, it is hard to capture an image with all-in-focus. Sparse representation (SR)-based methods have shown their powerful potentiality and ability in multi-focus image fusion. However, because of sparse coding and information compress, the existing fusion methods based on SR are imperfect to seize the rich details and significant texture information in source images. As a result, a fusion method based on multi-scale sparse representation for registered multi-focus images (MIF-MsSR) is proposed in this paper, where an adaptive fusion rule for sparse coefficients is presented. At first, source images are processed by multi-scale decomposition and sub-images with different scales can be obtained. According to image features with different richness in these sub-images, dictionaries with different sizes and redundancy are thereby trained. By comprehensively considering the relationships of focused areas, out-of-focused areas and boundary areas between the source images, an adaptive fusion rule based on l 0 − max and Sum Modified Laplacian (SML) is proposed. Finally, a fused image with all-in-focus can be obtained by sparse reconstruction and inverse multi-scale decomposition. Excessive experiments on multi-focus images have demonstrated that the proposed MIF-MsSR not only reserves the integrity of the information in source images, but also has better fusion performance on subjective and objective indicators than other state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002169",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Focus (optics)",
      "Image (mathematics)",
      "Image fusion",
      "Neural coding",
      "Operating system",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Redundancy (engineering)",
      "Scale (ratio)",
      "Sparse approximation"
    ],
    "authors": [
      {
        "surname": "Ma",
        "given_name": "Xiaole"
      },
      {
        "surname": "Wang",
        "given_name": "Zhihai"
      },
      {
        "surname": "Hu",
        "given_name": "Shaohai"
      }
    ]
  },
  {
    "title": "Augmented two stream network for robust action recognition adaptive to various action videos",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103344",
    "abstract": "In video-based action recognition, using videos with different frame numbers to train a two-stream network can result in data skew problems. Moreover, extracting the key frames from a video is crucial for improving the training and recognition efficiency of action recognition systems. However, previous works suffer from problems of information loss and optical-flow interference when handling videos with different frame numbers. In this paper, an augmented two-stream network (ATSNet) is proposed to achieve robust action recognition. A frame-number-unified strategy is first incorporated into the temporal stream network to unify the frame numbers of videos. Subsequently, the grayscale statistics of the optical-flow images are extracted to filter out any invalid optical-flow images and produce the dynamic fusion weights for the two branch networks to adapt to different action videos. Experiments conducted on the UCF101 dataset demonstrate that ATSNet outperforms previously defined methods, improving the recognition accuracy by 1.13%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002273",
    "keywords": [
      "Action (physics)",
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Frame (networking)",
      "Image (mathematics)",
      "Key (lock)",
      "Optical flow",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Skew",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Leng",
        "given_name": "Chuanjiang"
      },
      {
        "surname": "Ding",
        "given_name": "Qichuan"
      },
      {
        "surname": "Wu",
        "given_name": "Chengdong"
      },
      {
        "surname": "Chen",
        "given_name": "Ange"
      }
    ]
  },
  {
    "title": "Spatial self-attention network with self-attention distillation for fine-grained image recognition",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103368",
    "abstract": "The underlining task for fine-grained image recognition captures both the inter-class and intra-class discriminate features. Existing methods generally use auxiliary data to guide the network or a complex network comprising multiple sub-networks. They have two significant drawbacks: (1) Using auxiliary data like bounding boxes requires expert knowledge and expensive data annotation. (2) Using multiple sub-networks make network architecture complex and requires complicated training or multiple training steps. We propose an end-to-end Spatial Self-Attention Network (SSANet) comprising a spatial self-attention module (SSA) and a self-attention distillation (Self-AD) technique. The SSA encodes contextual information into local features, improving intra-class representation. Then, the Self-AD distills knowledge from the SSA to a primary feature map, obtaining inter-class representation. By accumulating classification losses from these two modules enables the network to learn both inter-class and intra-class features in one training step. The experiment findings demonstrate that SSANet is effective and achieves competitive performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032100242X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Attention network",
      "Bounding overwatch",
      "Class (philosophy)",
      "Computer science",
      "Data mining",
      "Economics",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Law",
      "Linguistics",
      "Machine learning",
      "Management",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Spatial analysis",
      "Statistics",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Baffour",
        "given_name": "Adu Asare"
      },
      {
        "surname": "Qin",
        "given_name": "Zhen"
      },
      {
        "surname": "Wang",
        "given_name": "Yong"
      },
      {
        "surname": "Qin",
        "given_name": "Zhiguang"
      },
      {
        "surname": "Choo",
        "given_name": "Kim-Kwang Raymond"
      }
    ]
  },
  {
    "title": "Learning to capture dependencies between global features of different convolution layers",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103360",
    "abstract": "NLNet has been considered as one milestone in the study of capturing long-range dependencies. Many recent studies modify the internal structure of NLNet directly and apply them to video object detection and semantic segmentation tasks. The dependencies between local and global features have been well developed, but the dependencies between global features of different convolution layers are rarely considered. Convolution is a local operation, so the global features of different convolution layers cannot be directly related, resulting in the loss of dependencies between global features. Given the vulnerability, this study designs a network that can efficiently capture the dependencies between the global features of different convolution layers, potentially further improving the accuracy. Furthermore, for the calculation of the dependency matrix, based on the Dot-product used in NLNet, we propose RELU-Dot-product, which can achieve higher accuracy. We evaluatethe proposed method on image classification and object detection tasks. The data sets involved are CIFAR10, CIFAR100, Tiny-imagenet, VOC2007, VOC2012 and MS COCO. Experiments show that our method can significantly improve network performance by introducing a few parameters.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002388",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Composite material",
      "Computer science",
      "Convolution (computer science)",
      "Data mining",
      "Geometry",
      "Materials science",
      "Mathematics",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Product (mathematics)",
      "Range (aeronautics)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Zhangwei"
      },
      {
        "surname": "Hu",
        "given_name": "Anshun"
      },
      {
        "surname": "Wang",
        "given_name": "Xiaofei"
      },
      {
        "surname": "Hu",
        "given_name": "Jun"
      },
      {
        "surname": "Zhang",
        "given_name": "Guijun"
      }
    ]
  },
  {
    "title": "Stitched image quality assessment based on local measurement errors and global statistical properties",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103324",
    "abstract": "Image stitching is developed to generate wide-field images or panoramic images for virtual reality applications. However, the quality assessment of stitched images with respect to various stitching algorithms has been less studied. Effective stitched image quality assessment (SIQA) is advantageous to evaluate the performance of various stitching methods and optimize the design of stitching methods. In this paper, we propose a novel SIQA method by exploiting local measurement errors and global statistical properties for feature extraction. Comprehensive image attributes including ghosting, misalignment, structural distortion, geometric error, chromatic aberrations and blur are considered either locally or globally. The extracted local and global features are aggregated into an overall quality via regression. Experimental results on two benchmark databases demonstrate the superiority of the proposed metric over both the state-of-the-art quality models designed for natural images and stitched images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002157",
    "keywords": [
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Benchmark (surveying)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Distortion (music)",
      "Economics",
      "Feature (linguistics)",
      "Feature extraction",
      "Geodesy",
      "Geography",
      "Ghosting",
      "Image (mathematics)",
      "Image quality",
      "Image stitching",
      "Linguistics",
      "Metric (unit)",
      "Operations management",
      "Panorama",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Tian",
        "given_name": "Chongzhen"
      },
      {
        "surname": "Chai",
        "given_name": "Xiongli"
      },
      {
        "surname": "Shao",
        "given_name": "Feng"
      }
    ]
  },
  {
    "title": "Fixation prediction for advertising images: Dataset and benchmark",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103356",
    "abstract": "Existing saliency prediction methods focus on exploring a universal saliency model for natural images, relatively few on advertising images which typically consists of both textual regions and pictorial regions. To fill this gap, we first build an advertising image database, named ADD1000, recording 57 subjects’ eye movement data of 1000 ad images. Compared to natural images, advertising images contain more artificial scenarios and show stronger persuasiveness and deliberateness, while the impact of this scene heterogeneity on visual attention is rarely studied. Moreover, text elements and picture elements express closely related semantic information to highlight product or brand in ad images, while their respective contribution to visual attention is also less known. Motivated by these, we further propose a saliency prediction model for advertising images based on text enhanced learning (TEL-SP), which comprehensively considers the interplay between textual region and pictorial region. Extensive experiments on the ADD1000 database show that the proposed model outperforms existing state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002339",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Cartography",
      "Computer science",
      "Computer vision",
      "Environmental health",
      "Fixation (population genetics)",
      "Geography",
      "Medicine",
      "Pattern recognition (psychology)",
      "Population"
    ],
    "authors": [
      {
        "surname": "Liang",
        "given_name": "Song"
      },
      {
        "surname": "Liu",
        "given_name": "Ruihang"
      },
      {
        "surname": "Qian",
        "given_name": "Jiansheng"
      }
    ]
  },
  {
    "title": "Gaze estimation via bilinear pooling-based attention networks",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103369",
    "abstract": "Attention mechanism has been found effective for human gaze estimation, and the attention and diversity of learned features are two important aspects of attention mechanism. However, the traditional attention mechanism used in existing gaze model is more prone to utilize first-order information that is attentive but not diverse. Though the existing bilinear pooling-based attention could overcome the shortcoming of traditional attention, it is limited to extract high-order contextual information. Thus we introduce a novel bilinear pooling-based attention mechanism, which could extract the second-order contextual information by the interaction between local deep learned features. To make the gaze-related features robust for spatial misalignment, we further propose an attention-in-attention method, which consists of a global average pooling and an inner attention on the second-order features. For the purpose of gaze estimation, a new bilinear pooling-based attention networks with attention-in-attention is further proposed. Extensive evaluation shows that our method surpasses the state-of-the-art by a big margin.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002431",
    "keywords": [
      "Artificial intelligence",
      "Bilinear interpolation",
      "Computer science",
      "Computer vision",
      "Epistemology",
      "Gaze",
      "Machine learning",
      "Margin (machine learning)",
      "Mechanism (biology)",
      "Philosophy",
      "Pooling"
    ],
    "authors": [
      {
        "surname": "Ren",
        "given_name": "Dakai"
      },
      {
        "surname": "Chen",
        "given_name": "Jiazhong"
      },
      {
        "surname": "Zhong",
        "given_name": "Jian"
      },
      {
        "surname": "Lu",
        "given_name": "Zhaoming"
      },
      {
        "surname": "Jia",
        "given_name": "Tao"
      },
      {
        "surname": "Li",
        "given_name": "Zongyi"
      }
    ]
  },
  {
    "title": "An adaptive two phase blind image deconvolution algorithm for an iterative regularization model",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103370",
    "abstract": "This paper proposes a blind image deconvolution method which consists of two sequential phases, i.e., blur kernel estimation and image restoration. In the first phase, we adopt the L 0-norm of image gradients and total variation (TV) to regularize the latent image and blur kernel, respectively. Then we design an alternating optimization algorithm which jointly incorporates the estimation of intermediately restored image, blur kernel and regularization parameters into account. In the second phase, we propose to take the mixture of L 0-norm of image gradients and TV to regularize the latent image, and design an efficient non-blind deconvolution algorithm to achieve the restored image. Experimental results on both a benchmark image dataset and real-world blurred images show that the proposed method can effectively restore image details while suppress noise and ringing artifacts, the result is of high quality which is competitive with some state of the art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002443",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Blind deconvolution",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Deconvolution",
      "Image (mathematics)",
      "Image processing",
      "Image restoration",
      "Kernel (algebra)",
      "Latent image",
      "Law",
      "Mathematics",
      "Norm (philosophy)",
      "Political science",
      "Regularization (linguistics)",
      "Ringing artifacts",
      "Wiener deconvolution"
    ],
    "authors": [
      {
        "surname": "Tao",
        "given_name": "Shuyin"
      },
      {
        "surname": "Dong",
        "given_name": "Wende"
      },
      {
        "surname": "Xu",
        "given_name": "Jian"
      },
      {
        "surname": "Lu",
        "given_name": "Jianfeng"
      },
      {
        "surname": "Xu",
        "given_name": "Guili"
      },
      {
        "surname": "Chen",
        "given_name": "Yueting"
      }
    ]
  },
  {
    "title": "Reversible data hiding with automatic contrast enhancement using two-sided histogram expansion",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103359",
    "abstract": "Recently, reversible data hiding (RDH) has emerged into a new class of data hiding methods that enables exact retrieving of both embedded data and cover medium. In the present study, a novel automatic RDH method with contrast enhancement is proposed, in which the data is embedded through two-sided histogram expansion. Two-sided histogram shifting doubles the number of bits embedded at each iteration. Moreover, it preserves the mean brightness of the cover image and prevents it from over enhancement with less calculation. Experimental results on two sets of images show that the proposed method enhances the image contrast at an appropriate level without using a mean brightness controller during data embedding and provides higher information security compared to the existing RDH approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002376",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Brightness",
      "Computer science",
      "Computer vision",
      "Contrast (vision)",
      "Contrast enhancement",
      "Cover (algebra)",
      "Embedding",
      "Engineering",
      "Histogram",
      "Histogram matching",
      "Image (mathematics)",
      "Information hiding",
      "Magnetic resonance imaging",
      "Mathematics",
      "Mechanical engineering",
      "Medicine",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Radiology"
    ],
    "authors": [
      {
        "surname": "Mansouri",
        "given_name": "Saeideh"
      },
      {
        "surname": "Khaleghi Bizaki",
        "given_name": "Hossein"
      },
      {
        "surname": "Fakhredanesh",
        "given_name": "Mohammad"
      }
    ]
  },
  {
    "title": "Geographical position spoofing detection based on camera sensor fingerprint",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103320",
    "abstract": "Currently, position check-in on mobile devices has become a fashionable social activity. Meanwhile, criminals probably tamper the geographical position (geo-position) information to provide an alibi. Therefore, it is of importance to identify the authenticity of geo-position. To our knowledge, many current methods for geo-position spoofing detection mainly rely on geo-position information in the database. However, these methods possibly fail in the case of missing prior information or lacking rich training samples. To address that challenge, this paper proposes an alternative manner for detecting the geo-position spoofing via camera sensor fingerprint. In particular, the camera sensor fingerprint is first extracted through the images posted by an inquiry user based on the well-designed denoising filter. Second, the authenticity of the geo-position is verified by comparing the consistency of the residual noise from newly-posted images with position check-in and the unique camera sensor fingerprint from an inquiry user. Finally, the extensive experiments are conducted on the image database, that empirically indicates the relevance of our proposed simple but effective method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032100211X",
    "keywords": [
      "Artificial intelligence",
      "Biometrics",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Economics",
      "Filter (signal processing)",
      "Finance",
      "Fingerprint (computing)",
      "Image (mathematics)",
      "Noise (video)",
      "Position (finance)",
      "Spoofing attack"
    ],
    "authors": [
      {
        "surname": "Qiao",
        "given_name": "Tong"
      },
      {
        "surname": "Zhao",
        "given_name": "Qianru"
      },
      {
        "surname": "Zheng",
        "given_name": "Ning"
      },
      {
        "surname": "Xu",
        "given_name": "Ming"
      },
      {
        "surname": "Zhang",
        "given_name": "Li"
      }
    ]
  },
  {
    "title": "Improving UNIWARD distortion function via isotropic construction and hierarchical merging",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103333",
    "abstract": "Distortion function is designed for evaluating the cost of modifications in adaptive steganography. UNIWARD is a successful and popular distortion scheme which achieves high performance both for spatial and JPEG images. In this paper, we analyze the UNIWARD scheme with some empirical rules of distortion function designation. Based on that we propose our scheme to improve UNIWARD distortion. In our scheme, we focus on the symmetric characteristic of UNIWARD, and suggest that not only use original wavelet filters but also their flippings to calculate sub-models of UNIWARD distortion to maintain its isotropic properties. Moreover, we design several schemes to merge sub-models, which could maintain its invariance regard to flipping or rotation and improve its security against steganalysis detection. Experimental results show our revised UNIWARD achieves better performance for spatial and JPEG image in comparison with original UNIWARD.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002194",
    "keywords": [
      "Algorithm",
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Computer science",
      "Computer vision",
      "Decoding methods",
      "Distortion (music)",
      "Distortion function",
      "Embedding",
      "Image (mathematics)",
      "Information retrieval",
      "Isotropy",
      "JPEG",
      "Mathematics",
      "Merge (version control)",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Steganalysis",
      "Steganography",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Guan",
        "given_name": "Qingxiao"
      },
      {
        "surname": "Chen",
        "given_name": "Hefeng"
      },
      {
        "surname": "Zhang",
        "given_name": "Weiming"
      },
      {
        "surname": "Yu",
        "given_name": "Nenghai"
      }
    ]
  },
  {
    "title": "Global and local information aggregation network for edge-aware salient object detection",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103350",
    "abstract": "Aggregation of local and global contextual information by exploiting multi-level features in a fully convolutional network is a challenge for the pixel-wise salient object detection task. Most existing methods still suffer from inaccurate salient regions and blurry boundaries. In this paper, we propose a novel edge-aware global and local information aggregation network (GLNet) to fully exploit the integration of side-output local features and global contextual information and utilization of contour information of salient objects. The global guidance module (GGM) is proposed to learn discriminative multi-level information with the direct guidance of global semantic knowledge for more accurate saliency prediction. Specifically, the GGM consists of two key components, where the global feature discrimination module exploits the inter-channel relationship of global semantic features to boost representation power, and the local feature discrimination module enables different side-output local features to selectively learn informative locations by fusing with global attentive features. Besides, we propose an edge-aware aggregation module (EAM) to employ the correlation between salient edge information and salient object information for generating estimated saliency maps with explicit boundaries. We evaluate our proposed GLNet on six widely-used saliency detection benchmark datasets by comparing with 17 state-of-the-art methods. Experimental results show the effectiveness and superiority of our proposed method on all the six benchmark datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002303",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Cloud computing",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Convolutional neural network",
      "Discriminative model",
      "Edge device",
      "Enhanced Data Rates for GSM Evolution",
      "Exploit",
      "Feature (linguistics)",
      "Geodesy",
      "Geography",
      "Key (lock)",
      "Law",
      "Linguistics",
      "Object (grammar)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Salient"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Qing"
      },
      {
        "surname": "Zhang",
        "given_name": "Liqian"
      },
      {
        "surname": "Wang",
        "given_name": "Dong"
      },
      {
        "surname": "Shi",
        "given_name": "Yanjiao"
      },
      {
        "surname": "Lin",
        "given_name": "Jiajun"
      }
    ]
  },
  {
    "title": "Video summary generation by visual shielding compressed sensing coding and double-layer affinity propagation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103321",
    "abstract": "Video summary technology based on keyframe extraction is an effective means to rapidly access video content. Traditional video summary generation technology requires high video resolution, which poses a problem as most existing studies have no targeted solutions for videos that are subject to privacy protection. We propose a novel keyframe extraction algorithm for video data in the visual shielding domain, named visual shielding compressed sensing coding and double-layer affinity propagation (VSCS-DAP). VSCS-DAP involves three main steps. First, the video is compressed by compressed sensing technology to provide a visual shielding effect (protecting the privacy of monitored figures), while the data volume is significantly reduced. Then, pyramid histogram of oriented gradients (PHOG) features are extracted from the compressed video to be clustered by the first step affinity propagation (AP) to gain the summaries of the first stage. Finally, the PHOG and Hist fusion features are extracted from the keyframes of the first stage, and they cluster the fused PHOG-Hist features by the second step AP algorithm to obtain the final output summaries. Experimental results obtained on two common video datasets show that our method exhibits advantages including low redundancy and few missing frames, low computational complexity, strong real-time performance, and robustness to vision-shielded video.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002121",
    "keywords": [
      "Affinity propagation",
      "Algorithm",
      "Artificial intelligence",
      "Canopy clustering algorithm",
      "Cluster analysis",
      "Coding (social sciences)",
      "Computer hardware",
      "Computer science",
      "Computer vision",
      "Correlation clustering",
      "Electrical engineering",
      "Electromagnetic shielding",
      "Engineering",
      "MPEG-2",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Redundancy (engineering)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Jixin"
      },
      {
        "surname": "Yu",
        "given_name": "Dan"
      },
      {
        "surname": "Tang",
        "given_name": "Zheng"
      }
    ]
  },
  {
    "title": "Shape transformer nets: Generating viewpoint-invariant 3D shapes from a single image",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103345",
    "abstract": "Single-view 3D shapes generation has achieved great success in recent years. However, current methods always blind the learning of shapes and viewpoints. The generated shape only fit the observed viewpoints and would not be optimal from unknown viewpoints. In this paper, we propose a novel encoder–decoder based network which contains a disentangled transformer to generate the viewpoint-invariant 3D shapes. The differentiable and parametric Non-uniform B-spline (NURBS) surface generation and 3D-to-3D viewpoint transformation are incorporated to learn the viewpoint-invariant shape and the camera viewpoint, respectively. Our new framework allows us to learn the latent geometric parameters of shapes and viewpoints without knowing the ground truth viewpoint. That can simultaneously generate camera-viewpoint and viewpoint-invariant 3D shapes of the object. We analyze the effects of disentanglement and show both quantitative and qualitative results of shapes generated at various unknown viewpoints.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002285",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Differentiable function",
      "Encoder",
      "Invariant (physics)",
      "Mathematical physics",
      "Mathematics",
      "Operating system",
      "Parametric statistics",
      "Physics",
      "Pure mathematics",
      "Quantum mechanics",
      "Statistics",
      "Transformer",
      "Viewpoints",
      "Visual arts",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Jinglun"
      },
      {
        "surname": "Li",
        "given_name": "Youhua"
      },
      {
        "surname": "Yang",
        "given_name": "Lu"
      }
    ]
  },
  {
    "title": "Multispectral background subtraction with deep learning",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103267",
    "abstract": "In this paper, we follow the trend of deep learning and make an attempt to investigate the potential benefit of using multispectral images via convolutional neural networks for background subtraction task. The major contributions of this work lie in two aspects, based on the impressive algorithm FgSegNet _ v2. Firstly, we extract three channels out of the seven of the FluxData FD-1665 multispectral dataset to match the number of input channels of the VGG16 deep model. Some combinations of three-channel based multispectral images perform better than RGB images. Secondly, a new convolutional encoder is designed to use all the multispectral channels available to further explore the information of multispectral images. The results outperform the RGB images and also other approaches using the same multispectral dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001747",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Channel (broadcasting)",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Mathematics",
      "Multispectral image",
      "Multispectral pattern recognition",
      "Pattern recognition (psychology)",
      "RGB color model",
      "Subtraction",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Rongrong"
      },
      {
        "surname": "Ruichek",
        "given_name": "Yassine"
      },
      {
        "surname": "El Bagdouri",
        "given_name": "Mohammed"
      }
    ]
  },
  {
    "title": "Multi-scale attention network for image super-resolution",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103300",
    "abstract": "The power of convolutional neural networks (CNN) has demonstrated irreplaceable advantages in super-resolution. However, many CNN-based methods need large model sizes to achieve superior performance, making them difficult to apply in the practical world with limited memory footprints. To efficiently balance model complexity and performance, we propose a multi-scale attention network (MSAN) by cascading multiple multi-scale attention blocks (MSAB), each of which integrates a multi-scale cross block (MSCB) and a multi-path wide-activated attention block (MWAB). Specifically, MSCB initially connects three parallel convolutions with different dilation rates hierarchically to aggregate the knowledge of features at different levels and scales. Then, MWAB split the channel features from MSCB into three portions to further improve performance. Rather than being treated equally and independently, each portion is responsible for a specific function, enabling internal communication among channels. Experimental results show that our MSAN outperforms most state-of-the-art methods with relatively few parameters and Mult-Adds.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001978",
    "keywords": [
      "Aggregate (composite)",
      "Algorithm",
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Combinatorics",
      "Composite material",
      "Computer science",
      "Convolutional neural network",
      "Dilation (metric space)",
      "Geometry",
      "Materials science",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Li"
      },
      {
        "surname": "Shen",
        "given_name": "Jie"
      },
      {
        "surname": "Tang",
        "given_name": "E."
      },
      {
        "surname": "Zheng",
        "given_name": "Shengnan"
      },
      {
        "surname": "Xu",
        "given_name": "Lizhong"
      }
    ]
  },
  {
    "title": "Unsupervised person re-identification by Intra–Inter Camera Affinity Domain Adaptation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103310",
    "abstract": "Person re-identification (re-ID) based on unsupervised domain adaptation intends to distill knowledge from annotated source dataset to identify target persons in another dataset. Although the advanced UDA re-ID models are dominated by pseudo-label methods, they almost transform images from various camera views into the same feature space, without considering the camera distribution gaps, which may lead to generate considerably noisy pseudo-labels. In this study, we develop an Intra–Inter Camera Affinity Domain Adaptation (I 2 CADA) to tackle these problems for UDA person re-ID. Precisely, I 2 CADA framework is composed of two modules. The first one is generative adversarial learning module, aiming to train a feature extractor that can map target data to source feature space by supervised learning and adversarial learning, which can relieve the distribution gap between different datasets (domains). The second one is affinity transfer learning module, which simultaneously considers intra-camera clustering and inter-camera separation among persons with similar appearances in the target domain, thus mitigating the distribution inconsistency among person images collected from multiple target camera views. Besides, comprehensive experiments exhibit that I 2 CADA outperforms the existing UDA person re-identification approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002042",
    "keywords": [
      "Adaptation (eye)",
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Cluster analysis",
      "Computer science",
      "Computer vision",
      "Domain (mathematical analysis)",
      "Engineering",
      "Extractor",
      "Feature (linguistics)",
      "Feature vector",
      "Identification (biology)",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Process engineering",
      "Transfer of learning",
      "Unsupervised learning"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Guiqing"
      },
      {
        "surname": "Wu",
        "given_name": "Jinzhao"
      }
    ]
  },
  {
    "title": "Effective compressed sensing MRI reconstruction via hybrid GSGWO algorithm",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103274",
    "abstract": "In a bio-imaging context, the main issues which obstruct the CS (Compressed sensing) application are image reconstruction time and computational cost. This paper presents an effective compressed sensing-based MRI reconstruction through a hybrid optimization algorithm. Initially, the preprocessing stage is performed using Cross guided bilateral filter. Then the K-space is generated by the Fourier transform. The hybrid Walsh Hadamard Transform and Discrete Wavelet Transform (HWHDWT) is utilized for the compressive sensing of the images. Finally, the Hybrid Galactic Swarm Optimization and Grey Wolf Optimization (HGSGWO) algorithm are developed for MRI reconstruction. The dataset collected from a hospital which contains MRI images both in JPEG and DICOM format. The performance of SSIM (Structural Similarity Index), PSNR (Peak Signal to Noise Ratio), MSE (mean square error) and reconstruction time are evaluated for images and it is compared with the existing methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001796",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Compressed sensing",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Hadamard transform",
      "Image (mathematics)",
      "Iterative reconstruction",
      "Mathematical analysis",
      "Mathematics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Peak signal-to-noise ratio",
      "Reconstruction algorithm"
    ],
    "authors": [
      {
        "surname": "Guruprasad",
        "given_name": "Shrividya"
      },
      {
        "surname": "Bharathi",
        "given_name": "S.H."
      },
      {
        "surname": "Anto Ramesh Delvi",
        "given_name": "D."
      }
    ]
  },
  {
    "title": "A novel deep learning framework for double JPEG compression detection of small size blocks",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103269",
    "abstract": "Double JPEG compression detection plays a vital role in multimedia forensics, to find out whether a JPEG image is authentic or manipulated. However, it still remains to be a challenging task in the case when the quality factor of the first compression is much higher than that of the second compression, as well as in the case when the targeted image blocks are quite small. In this work, we present a novel end-to-end deep learning framework taking raw DCT coefficients as input to distinguish between single and double compressed images, which performs superior in the above two cases. Our proposed framework can be divided into two stages. In the first stage, we adopt an auxiliary DCT layer with sixty-four 8 × 8 DCT kernels. Using a specific layer to extract DCT coefficients instead of extracting them directly from JPEG bitstream allows our proposed framework to work even if the double compressed images are stored in spatial domain, e.g. in PGM, TIFF or other bitmap formats. The second stage is a deep neural network with multiple convolutional blocks to extract more effective features. We have conducted extensive experiments on three different image datasets. The experimental results demonstrate the superiority of our framework when compared with other state-of-the-art double JPEG compression detection methods either hand-crafted or learned using deep networks in the literature, especially in the two cases mentioned above. Furthermore, our proposed framework can detect triple and even multiple JPEG compressed images, which is scarce in the literature as far as we know.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001759",
    "keywords": [
      "Artificial intelligence",
      "Automotive engineering",
      "Bitmap",
      "Compression artifact",
      "Compression ratio",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Data compression",
      "Deep learning",
      "Discrete cosine transform",
      "Engineering",
      "Image (mathematics)",
      "Image compression",
      "Image processing",
      "Internal combustion engine",
      "JPEG",
      "JPEG 2000",
      "Lossless JPEG",
      "Lossy compression",
      "Pattern recognition (psychology)",
      "Quantization (signal processing)"
    ],
    "authors": [
      {
        "surname": "Hussain",
        "given_name": "Israr"
      },
      {
        "surname": "Tan",
        "given_name": "Shunquan"
      },
      {
        "surname": "Li",
        "given_name": "Bin"
      },
      {
        "surname": "Qin",
        "given_name": "Xinghong"
      },
      {
        "surname": "Hussain",
        "given_name": "Dostdar"
      },
      {
        "surname": "Huang",
        "given_name": "Jiwu"
      }
    ]
  },
  {
    "title": "Residual attention-based tracking-by-detection network with attention-driven data augmentation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103312",
    "abstract": "Tracking-by-detection (TBD) is a significant framework for visual object tracking. However, current trackers are usually updated online based on random sampling with a probability distribution. The performance of the learning-based TBD trackers is limited by the lack of discriminative features, especially when the background is full of semantic distractors. We propose an attention-driven data augmentation method, in which a residual attention mechanism is integrated into the TBD tracking network as supplementary references to identify discriminative image features. A mask generating network is used to simulate changes in target appearances to obtain positive samples, where attention information and image features are combined to identify discriminative features. In addition, we propose a method for mining hard negative samples, which searches for semantic distractors with the response of the attention module. The experiments on the OTB2015, UAV123, and LaSOT benchmarks show that this method achieves competitive performance in terms of accuracy and robustness.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002066",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "BitTorrent tracker",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Discriminative model",
      "Eye tracking",
      "Gene",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Residual",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Shi",
        "given_name": "Zaifeng"
      },
      {
        "surname": "Sun",
        "given_name": "Cheng"
      },
      {
        "surname": "Cao",
        "given_name": "Qingjie"
      },
      {
        "surname": "Wang",
        "given_name": "Zhe"
      },
      {
        "surname": "Fan",
        "given_name": "Qiangqiang"
      }
    ]
  },
  {
    "title": "Attention guided feature pyramid network for crowd counting",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103319",
    "abstract": "Crowd counting has become a hot topic because of its wide applications in video surveillance and public security. However, one main problem of the deep learning methods for crowd counting is that the location information about the crowd is degraded irreversibly due to the spatial down-sampling of convolutional neural networks, which degrades the quality of generated density maps. To remedy the above problem, we propose an attention guided feature pyramid network (AG-FPN) for crowd counting, which can adaptively generate a high-quality density map with accurate spatial locations by combining the high- and low-level features. An attention block is added to each encoder layer to further emphasize the crowd regions and suppress the background clutters in feature extraction. Experimental results on the ShanghaiTech, UCF_CC_50, WorldExpo’10 and UCF-QNRF datasets demonstrate the superiority of the proposed method over state-of-the-art approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002108",
    "keywords": [
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Encoder",
      "Feature (linguistics)",
      "Feature extraction",
      "Geometry",
      "Layer (electronics)",
      "Linguistics",
      "Mathematics",
      "Operating system",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pyramid (geometry)"
    ],
    "authors": [
      {
        "surname": "Chu",
        "given_name": "Huanpeng"
      },
      {
        "surname": "Tang",
        "given_name": "Jilin"
      },
      {
        "surname": "Hu",
        "given_name": "Haoji"
      }
    ]
  },
  {
    "title": "Scale aware remote sensing image enhancement using rolling guidance",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103315",
    "abstract": "Enhancement of remotely sensed images is a challenging problem, since the enhanced image has to have an improved contrast and edge information while preserving the original radiance values as much as possible. In this paper, a scale aware enhancement method based on rolling guidance is proposed for remotely sensed images. For each scale, a guidance image is defined and the approximation image is provided by an iterative joint filtering of the approximation and guidance images. Then the extracted details are amplified through an adaptive scheme and added to the final level approximation layer to provide the resulting enhanced image. A comparative study between the proposed methods with classical edge preserving filters and traditional methods have been carried out by using several criteria. The proposed methods have an average of 12% improvement for contrast gain (CG) metric and 81% improvement for enhancement measurement (EME) metric compared to the closest comparison method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002078",
    "keywords": [
      "Artificial intelligence",
      "Cartography",
      "Computer science",
      "Computer vision",
      "Contrast (vision)",
      "Economics",
      "Enhanced Data Rates for GSM Evolution",
      "Geography",
      "Image (mathematics)",
      "Metric (unit)",
      "Operations management",
      "Radiance",
      "Remote sensing",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "Kaplan",
        "given_name": "N.H."
      },
      {
        "surname": "Erer",
        "given_name": "I."
      }
    ]
  },
  {
    "title": "Using curved angular intra-frame prediction to improve video coding efficiency",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103291",
    "abstract": "This article presents a new curved-based intra-frame prediction method for current and upcoming video coding standards. Our proposal extends conventional straight-line angular modes found on intra-prediction tools to model curved texture characteristics, enhancing the intra-frame prediction process. Our work targets the High Efficiency Video Coding (HEVC) standard for evaluation, although our curved-based method can be used by any other video coding standard. We model curved intra-frame prediction using an offset-based displacement calculation to each predicted sample. The proposal incurs a small bitstream overhead for transmitting the displacement information, which is offset by encoding efficiency gains. Experimental results demonstrate reduced residual energy; consequently, improving BD-Rate for the tested sequences. Evaluations applying eight curve displacement values show an average BD-Rate reduction of 2.69%, 2.49%, and 0.86% for All-Intra-8, All-Intra 10, and Random-Access configurations, respectively. The proposal allows further BD-Rate improvements, albeit at higher encoding complexity.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001929",
    "keywords": [
      "Algorithm",
      "Algorithmic efficiency",
      "Artificial intelligence",
      "Bit rate",
      "Bitstream",
      "Coding (social sciences)",
      "Coding tree unit",
      "Computer science",
      "Computer vision",
      "Decoding methods",
      "Frame (networking)",
      "Inter frame",
      "Mathematics",
      "Offset (computer science)",
      "Operating system",
      "Programming language",
      "Random access",
      "Real-time computing",
      "Reference frame",
      "Residual",
      "Residual frame",
      "Statistics",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Fernandes",
        "given_name": "Ramon"
      },
      {
        "surname": "Sanchez",
        "given_name": "Gustavo"
      },
      {
        "surname": "Cataldo",
        "given_name": "Rodrigo"
      },
      {
        "surname": "Agostini",
        "given_name": "Luciano"
      },
      {
        "surname": "Marcon",
        "given_name": "César"
      }
    ]
  },
  {
    "title": "Occlusion-robust online multi-object visual tracking using a GM-PHD filter with CNN-based re-identification",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103279",
    "abstract": "We propose a novel online multi-object visual tracker using a Gaussian mixture Probability Hypothesis Density (GM-PHD) filter and deep appearance learning. The GM-PHD filter has a linear complexity with the number of objects and observations while estimating the states and cardinality of time-varying number of objects, however, it is susceptible to miss-detections and does not include the identity of objects. We use visual-spatio-temporal information obtained from object bounding boxes and deeply learned appearance representations to perform estimates-to-tracks data association for target labeling as well as formulate an augmented likelihood and then integrate into the update step of the GM-PHD filter. We also employ additional unassigned tracks prediction after the data association step to overcome the susceptibility of the GM-PHD filter towards miss-detections caused by occlusion. Extensive evaluations on MOT16, MOT17 and HiEve benchmark data sets show that our tracker significantly outperforms several state-of-the-art trackers in terms of tracking accuracy and identification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001814",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "BitTorrent tracker",
      "Botany",
      "Cardinality (data modeling)",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Eye tracking",
      "Filter (signal processing)",
      "Gaussian",
      "Geodesy",
      "Geography",
      "Identification (biology)",
      "Image (mathematics)",
      "Minimum bounding box",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Pedagogy",
      "Physics",
      "Psychology",
      "Quantum mechanics",
      "Tracking (education)",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Baisa",
        "given_name": "Nathanael L."
      }
    ]
  },
  {
    "title": "Depth completion towards different sensor configurations via relative depth map estimation and scale recovery",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103272",
    "abstract": "Depth completion, which combines additional sparse depth information from the range sensors, substantially improves the accuracy of monocular depth estimation, especially using the deep-learning-based methods. However, these methods can hardly produce satisfactory depth results when the sensor configuration changes at test time, which is important for real-world applications. In this paper, the problem is tackled by our proposed novel two-stage mechanism, which decomposes depth completion into two subtasks, namely relative depth map estimation and scale recovery. The relative depth map is first estimated from a single color image with our designed scale-invariant loss function. Then the scale map is recovered with the additional sparse depth. Experiments on different densities and patterns of the sparse depth input show that our model always produces satisfactory depth results. Besides, our approach achieves state-of-the-art performance on the indoor NYUv2 dataset and performs competitively on the outdoor KITTI dataset, demonstrating the effectiveness of our method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001772",
    "keywords": [
      "Aerospace engineering",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Depth map",
      "Depth perception",
      "Engineering",
      "Geology",
      "Geophysics",
      "Image (mathematics)",
      "Measured depth",
      "Monocular",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Perception",
      "Physics",
      "Quantum mechanics",
      "Range (aeronautics)",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "Long",
        "given_name": "Yangqi"
      },
      {
        "surname": "Yu",
        "given_name": "Huimin"
      },
      {
        "surname": "Liu",
        "given_name": "Biyang"
      }
    ]
  },
  {
    "title": "PSGU: Parametric self-circulation gating unit for deep neural networks",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103294",
    "abstract": "Activation functions are of great importance for the performance and training of deep neural networks. High-performance activation function is expected to effectively prevent the gradient from vanishing and help network converge. This paper provides a novel smooth activation function, called Parameterized Self-circulating Gating Unit (PSGU), aiming to train an adaptive activation function to improve the performance of deep networks. Compared with other works, we propose and study the self-circulation gating property of activation function, and analyze its influence on the signal transmission in network by controlling the flow of information. Specifically, we theoretically analyze and propose the initialization based on PSGU, which adequately explores the properties in neighborhood of the origin. Finally, the proposed activation function and initialization are compared with other methods on commonly-used network architectures, the achieved performances of using PSGU alone or combining with our proposed initialization are over par with the state of the art.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001942",
    "keywords": [
      "Activation function",
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Epistemology",
      "Evolutionary biology",
      "Function (biology)",
      "Gating",
      "Initialization",
      "Mathematics",
      "Neuroscience",
      "Parameterized complexity",
      "Parametric statistics",
      "Philosophy",
      "Programming language",
      "Property (philosophy)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Zhengze"
      },
      {
        "surname": "Yang",
        "given_name": "Xiaoyuan"
      },
      {
        "surname": "Shen",
        "given_name": "Kangqing"
      },
      {
        "surname": "Jiang",
        "given_name": "Fazhen"
      },
      {
        "surname": "Jiang",
        "given_name": "Jin"
      },
      {
        "surname": "Ren",
        "given_name": "Huwei"
      },
      {
        "surname": "Li",
        "given_name": "Yixiao"
      }
    ]
  },
  {
    "title": "What and how well you exercised? An efficient analysis framework for fitness actions",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103304",
    "abstract": "Human action analysis has been an active research area in computer vision, and has many useful applications such as human computer interaction. Most of the state-of-the-art approaches of human action analysis are data-driven and focus on general action recognition. In this paper, we aim to analyze fitness actions with skeleton sequences and propose an efficient and robust fitness action analysis framework. Firstly, fitness actions from 15 subjects are captured and built to a fitness action dataset (Fitness-28). Secondly, skeleton information is extracted and made alignment with a simplified human skeleton model. Thirdly, the aligned skeleton information is transformed to an uniform human center coordinate system with the proposed spatial–temporal skeleton encoding method. Finally, the action classifier and local–global geometrical registration strategy are constructed to analyze the fitness actions. Experimental results demonstrate that our method can effectively assess fitness action, and have a good performance on artificial intelligence fitness system.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002017",
    "keywords": [
      "Action (physics)",
      "Action recognition",
      "Artificial intelligence",
      "Class (philosophy)",
      "Classifier (UML)",
      "Computer science",
      "Fitness approximation",
      "Fitness function",
      "Genetic algorithm",
      "Human skeleton",
      "Machine learning",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Skeleton (computer programming)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Jianwei"
      },
      {
        "surname": "Hu",
        "given_name": "Qingrui"
      },
      {
        "surname": "Guo",
        "given_name": "Tianxiao"
      },
      {
        "surname": "Wang",
        "given_name": "Siqi"
      },
      {
        "surname": "Shen",
        "given_name": "Yanfei"
      }
    ]
  },
  {
    "title": "Sequential alignment attention model for scene text recognition",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103289",
    "abstract": "Scene text recognition has been a hot research topic in computer vision due to its various applications. The state-of-the-art solutions usually depend on the attention-based encoder-decoder framework that learns the mapping between input images and output sequences in a purely data-driven way. Unfortunately, there often exists severe misalignment between feature areas and text labels in real-world scenarios. To address this problem, this paper proposes a sequential alignment attention model to enhance the alignment between input images and output character sequences. In this model, an attention gated recurrent unit (AGRU) is first devised to distinguish the text and background regions, and further extract the localized features focusing on sequential text regions. Furthermore, CTC guided decoding strategy is integrated into the popular attention-based decoder, which not only helps to boost the convergence of the training but also enhances the well-aligned sequence recognition. Extensive experiments on various benchmarks, including the IIIT5k, SVT, and ICDAR datasets, show that our method substantially outperforms the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001917",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Convergence (economics)",
      "Decoding methods",
      "Economic growth",
      "Economics",
      "Encoder",
      "Encoding (memory)",
      "Feature (linguistics)",
      "Genetics",
      "Linguistics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Sequence (biology)",
      "State (computer science)"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Yan"
      },
      {
        "surname": "Fan",
        "given_name": "Jiaxin"
      },
      {
        "surname": "Tao",
        "given_name": "Renshuai"
      },
      {
        "surname": "Wang",
        "given_name": "Jiakai"
      },
      {
        "surname": "Qin",
        "given_name": "Haotong"
      },
      {
        "surname": "Liu",
        "given_name": "Aishan"
      },
      {
        "surname": "Liu",
        "given_name": "Xianglong"
      }
    ]
  },
  {
    "title": "Video object segmentation via random walks on two-frame graphs comprising superpixels",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103293",
    "abstract": "We propose a novel video object segmentation method employing random walkers to travel on graphs constructed on two consecutive frames. First, we estimate the initial foreground and background distributions by minimising an energy function that incorporates the stationary distributions of the random walks. The random walkers frequently travel between similar nodes of the graph constructed on two adjacent frames, which enables the incorporation of the inter-frame information into the energy function effectively and elegantly. Then, we refine the initial results by simulating the movements of multiple random walkers. We process the sequence in a recursive manner, which naturally propagates the previous segmentation labels to the subsequent frames. Additionally, we develop a strategy for adjusting the superpixel number using region similarity and the average Frobenius norm of optical flow gradient. This strategy can improve performance significantly. Furthermore, we discuss the feature selection problem in the method to select a more effective feature representation. Extensive and comparable experiments on Segtrack and Segtrack v2 demonstrate that the proposed algorithm yields higher performance than several recent state-of-the-art approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001930",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Frame (networking)",
      "Graph",
      "Image (mathematics)",
      "Law",
      "Linguistics",
      "Mathematics",
      "Object (grammar)",
      "Optical flow",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Random walk",
      "Random walker algorithm",
      "Representation (politics)",
      "Segmentation",
      "Similarity (geometry)",
      "Statistics",
      "Telecommunications",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Hui"
      },
      {
        "surname": "Liu",
        "given_name": "Weibin"
      },
      {
        "surname": "Xing",
        "given_name": "Weiwei"
      }
    ]
  },
  {
    "title": "Visible and thermal images fusion architecture for few-shot semantic segmentation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103306",
    "abstract": "Few-shot semantic segmentation (FSS) has drawn great attention in the community of computer vision, due to its remarkable potential for segmenting novel objects with few pixel-annotated samples. However, some interference factors, such as insufficient illumination and complex background, can impose more challenge to the segmentation performance than fully-supervised when the number of samples is insufficient. Therefore, we propose the visible and thermal (V-T) few-shot semantic segmentation task, which utilize the complementary and similar information of visible and thermal images to boost few-shot segmentation performance. As the first step, we build a novel outdoor city dataset Tokyo Multi-Spectral-4i for the V-T few-shot semantic segmentation task. In addition, a fusion architecture is proposed, which consists of an Edge Similarity fusion module (ES) and a Texture Edge Prototype module (TEP). The ES module fuses the bi-modal information by exploiting the edge similarity in the visible and thermal images. The TEP module extracts the prototype from two models by collaborating the representativeness and complementarity of the visible and thermal feature. Finally, extensive experiments conducted on the proposed datasets demonstrate that our architecture can achieve state-of-the-arts results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002030",
    "keywords": [
      "Artificial intelligence",
      "Business",
      "Computer science",
      "Computer vision",
      "Image segmentation",
      "Market segmentation",
      "Marketing",
      "Pattern recognition (psychology)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Bao",
        "given_name": "Yanqi"
      },
      {
        "surname": "Song",
        "given_name": "Kechen"
      },
      {
        "surname": "Wang",
        "given_name": "Jie"
      },
      {
        "surname": "Huang",
        "given_name": "Liming"
      },
      {
        "surname": "Dong",
        "given_name": "Hongwen"
      },
      {
        "surname": "Yan",
        "given_name": "Yunhui"
      }
    ]
  },
  {
    "title": "Detection of moving objects using adaptive multi-feature histograms",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103278",
    "abstract": "This paper presents a moving object detection scheme that incorporates three innovations. First, considering the inter-frame consistency of pixels, we extend the compact binary face descriptor (CBFD) to the temporal domain and propose a novel local binary descriptor named temporally-consistent local compact binary descriptor (TC-LCBD), which exploits the useful correlation of the intensities of inter-frame pixels to guarantee good performance in complex scenes. We do this mainly because the background scene between frames has a significant coherence. Second, both color and TC-LCBD features are modeled as a group of adaptive histograms for characterizing each pixel, which can enhance the robustness to dynamic backgrounds and illumination changes. Third, by comparing changes in histogram proximity between two adjacent frames, we can dynamically adjust the model sensitivity and adaptation rate without user intervention. Experimental results on well-known, challenging data sets demonstrate that the proposed method significantly outperforms many state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001826",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Binary number",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Frame (networking)",
      "Gene",
      "Histogram",
      "Image (mathematics)",
      "Linguistics",
      "Local binary patterns",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Robustness (evolution)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "He",
        "given_name": "Wei"
      },
      {
        "surname": "Li",
        "given_name": "Wujing"
      },
      {
        "surname": "Zhang",
        "given_name": "Guoyun"
      },
      {
        "surname": "Tu",
        "given_name": "Bing"
      },
      {
        "surname": "Kwan Kim",
        "given_name": "Yong"
      },
      {
        "surname": "Wu",
        "given_name": "Jianhui"
      },
      {
        "surname": "Qi",
        "given_name": "Qi"
      }
    ]
  },
  {
    "title": "Blind image quality assessment with channel attention based deep residual network and extended LargeVis dimensionality reduction",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103296",
    "abstract": "Image Quality Assessment (IQA) is one of the fundamental problems in the fields of image processing, image/video coding and transmission, and so on. In this paper, a Blind Image Quality Assessment (BIQA) approach with channel attention based deep Residual Network (ResNet)and extended LargeVis dimensionality reduction is proposed. Firstly, ResNet50 with channel attention mechanism is used as the backbone network to extract the deep features from the image. In order to reduce the dimensionality of the deep features, LargeVis, which is originally designed for the visualization of large scale high-dimensional data, is extended by using Support Vector Regression (SVR) to perform on a single feature vector data. The extended LargeVis can remove the redundant information of the deep features so as to obtain a low-dimensional and discriminative feature representation. Finally, the quality prediction model is established by using SVR as the fitting method. The low-dimensional feature representation and quality score of the image form the pair-wise data samples to train the fitting model. Experimental results on authentic distortions datasets and synthetic distortions datasets show that our proposed method can achieve superior performance compared with the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001966",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Curse of dimensionality",
      "Dimensionality reduction",
      "Discriminative model",
      "Feature (linguistics)",
      "Feature vector",
      "Image (mathematics)",
      "Image quality",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Residual",
      "Support vector machine",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Han",
        "given_name": "Han"
      },
      {
        "surname": "Zhuo",
        "given_name": "Li"
      },
      {
        "surname": "Li",
        "given_name": "Jiafeng"
      },
      {
        "surname": "Zhang",
        "given_name": "Jing"
      },
      {
        "surname": "Wang",
        "given_name": "Meng"
      }
    ]
  },
  {
    "title": "Internal and external spatial–temporal constraints for person reidentification",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103302",
    "abstract": "Spatial–temporal information is easy to achieve in a practical surveillance scene, but it is often neglected in most current person reidentification (ReID) methods. Employing spatial–temporal information as a constrain has been verified as beneficial for ReID. However, there is no effective modeling according to the pedestrian movement law. In this paper, we present a ReID framework with internal and external spatial–temporal constraints, termed as IESC-ReID. A novel residual spatial attention module is proposed to build a spatial–temporal constraint and increase the robustness to partial occlusions or camera viewpoint changes. A Laplace-based spatial–temporal constraint is also introduced to eliminate irrelevant gallery images, which are gathered by the internal learning network. IESC-ReID constrains the attention within the functioning range of the channel space, and utilizes additional spatial–temporal constrains to further constrain results. Intensive experiments show that these constraints consistently improve the performance. Extensive experimental results on numerous publicly available datasets show that the proposed method outperforms several state-of-the-art ReID algorithms. Our code is publicly available at https://github.com/jiaming-wang/IESC.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001991",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Mathematics"
    ],
    "authors": [
      {
        "surname": "Shao",
        "given_name": "Zhenfeng"
      },
      {
        "surname": "Wang",
        "given_name": "Jiaming"
      },
      {
        "surname": "Lu",
        "given_name": "Tao"
      },
      {
        "surname": "Zhang",
        "given_name": "Ruiqian"
      },
      {
        "surname": "Huang",
        "given_name": "Xiao"
      },
      {
        "surname": "Lv",
        "given_name": "Xianwei"
      }
    ]
  },
  {
    "title": "Towards better semantic consistency of 2D medical image segmentation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103311",
    "abstract": "The latest deep neural networks for medical segmentation typically utilize transposed convolutional filters and atrous convolutional filters for spatial restoration and larger receptive fields, leading to dilution and inconsistency of visual semantics. To address such issues, we propose a novel attentional up-concatenation structure to build an auxiliary path for direct access to multi-level features. In addition, we employ a new structural loss to bring better morphological awareness and reduce the segmentation flaws caused by the semantic inconsistencies. Thorough experiments on the challenging optic cup/disc segmentation, cellular segmentation and lung segmentation tasks were performed to evaluate the proposed methods. Further ablation analysis demonstrated the effectiveness of the different components of the model and illustrated its efficiency. The proposed methods achieved the best performance and speed compared to the state-of-the-art models in three tasks on seven public datasets, including DRISHTI-GS, RIM-r3, REFUGE, MESSIDOR, TNBC, GlaS and LUNA.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002054",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Concatenation (mathematics)",
      "Consistency (knowledge bases)",
      "Convolutional neural network",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Programming language",
      "Segmentation",
      "Semantics (computer science)"
    ],
    "authors": [
      {
        "surname": "Wen",
        "given_name": "Yang"
      },
      {
        "surname": "Chen",
        "given_name": "Leiting"
      },
      {
        "surname": "Deng",
        "given_name": "Yu"
      },
      {
        "surname": "Ning",
        "given_name": "Jin"
      },
      {
        "surname": "Zhou",
        "given_name": "Chuan"
      }
    ]
  },
  {
    "title": "Multi-ColorGAN: Few-shot vehicle recoloring via memory-augmented networks",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103317",
    "abstract": "Despite the notable successes of Generative adversarial networks (GANs) achieved to date, applying them to real-world problems still poses significant challenges. In real traffic surveillance scenarios, for the task of generating images of multiple color of truck heads and cars without changing textures and license plates, conditional image generation hardly manipulate the generated images by the color attribute. Image style transfer methods inevitably produce color smearing. Even state-of-the-art methods of disentangled representation learning (e.g. MixNMatch) cannot disentangle colors individually, ensuring that irrelevant factors, such as texture remain the same. To solve this problem, we present an approach called Multi-ColorGAN based on memory-augmented networks for multi-color real vehicle coloring/generation with limited data. In particular, our model could filter out unwanted color changes in specific areas with a simple but effective method called Fusion Module, and generate more natural color images. Experiments on three vehicle image benchmarks and a new truck image dataset are conducted to evaluate the proposed Multi-ColorGAN compared to state-of-the-art.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002091",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Economics",
      "Filter (signal processing)",
      "Generative grammar",
      "Image (mathematics)",
      "Image editing",
      "Law",
      "License",
      "Management",
      "Operating system",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Wei"
      },
      {
        "surname": "Chen",
        "given_name": "Si-Bao"
      },
      {
        "surname": "Xu",
        "given_name": "Li-Xiang"
      },
      {
        "surname": "Luo",
        "given_name": "Bin"
      }
    ]
  },
  {
    "title": "Indoor scene understanding based on manhattan and non-manhattan projection of spatial right-angles",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103307",
    "abstract": "Understanding of indoor scenes has considerable value in computer vision. Most previous methods infer indoor scenes via manhattan assumption. However, attic ceilings do not satisfy manhattan assumption and understanding them remains a big challenge. Non-manhattan ceilings can be seen as compositions of spatial right-angles projections. In this paper, we presented a method to understand indoor scenes including both manhattan structures and non-manhattan attic ceilings from a single image. First, angle projections are detected and assigned to different clusters. Then vanishing points of attic ceilings can be estimated. Third, it is possible to determine the attic ceilings of non-manhattan surfaces. The proposed approach requires no prior training. We compared the estimated attic layout against the ground truth and measured the percentage of pixels that were incorrectly classified. Experimental results showed that the method can understand indoor scenes including both manhattan and non-manhattan attic ceilings, meeting the requirements of robot navigation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002029",
    "keywords": [
      "Algorithm",
      "Archaeology",
      "Artificial intelligence",
      "Attic",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Geography",
      "Ground truth",
      "Pixel",
      "Projection (relational algebra)",
      "Roof"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Luping"
      },
      {
        "surname": "Wei",
        "given_name": "Hui"
      }
    ]
  },
  {
    "title": "Camera style transformation with preserved self-similarity and domain-dissimilarity in unsupervised person re-identification",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103303",
    "abstract": "The inconsistency caused by different factors, such as different camera imaging methods, complex imaging environments, and changes in light, present a huge challenge to person re-identification (re-ID). Unsupervised domain adaptation (UDA) can solve the inconsistency issue to a certain extent, but different datasets may not have any overlapping of people’s identities. Therefore, it is necessary to pay attention to people’s identities in solving domain-dissimilarity. A camera imaging style transformation with preserved self-similarity and domain-dissimilarity (CSPSD) is proposed to solve the cross-domain issue in person re-ID. First, CycleGAN is applied to determine the style conversion between source and target domains. Intra-domain identity constraints are used to maintain identity consistency between source and target domains during the image style transformation process. Maximum mean difference (MMD) is used to reduce the difference in feature distribution between source and target domains. Then, a one-to-n mapping method is proposed to achieve the mapping between positive pairs and distinguish negative pairs. Any sample image from the source domain and its transformed image or a transformed image with the same identity information compose a positive pair. The transformed image and any image from the target domain compose a negative pair. Next, a circle loss function is used to improve the learning speed of positive and negative pairs. Finally, the proposed CSPSD that can effectively reduce the difference between domains and an existing feature learning network work together to learn a person re-ID model. The proposed method is applied to three public datasets, Market-1501, DukeMTMC-reID, and MSMT17. The comparative experimental results confirm the proposed method can achieve highly competitive recognition accuracy in person re-ID.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002005",
    "keywords": [
      "Acoustics",
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Botany",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Domain (mathematical analysis)",
      "Feature (linguistics)",
      "Gene",
      "Identification (biology)",
      "Identity (music)",
      "Image (mathematics)",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Similarity (geometry)",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Zhiqin"
      },
      {
        "surname": "Luo",
        "given_name": "Yaqin"
      },
      {
        "surname": "Chen",
        "given_name": "Sixin"
      },
      {
        "surname": "Qi",
        "given_name": "Guanqiu"
      },
      {
        "surname": "Mazur",
        "given_name": "Neal"
      },
      {
        "surname": "Zhong",
        "given_name": "Chengyan"
      },
      {
        "surname": "Li",
        "given_name": "Qiwang"
      }
    ]
  },
  {
    "title": "Deep feature enhancing and selecting network for weakly supervised temporal action localization",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103276",
    "abstract": "Weakly supervised temporal action localization is a challenging computer vision problem that uses only video-level labels and lacks the supervision of temporal annotations. In this task, the majority of existing methods usually identify the most discriminative snippets and ignore other relevant snippets. To address this problem, we propose a deep feature enhancing and selecting network. It generates multiple masks for both capturing more complete temporal interval of actions and keeping its high classification accuracy. After that, we further propose a novel selection strategy to balance the influence of multiple masks and improve the model performance. In the experiments, we evaluate the proposed method on the THUMOS’14 and ActivityNet datasets, and the results show the effectiveness of our approach for weakly supervised temporal action localization.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001802",
    "keywords": [
      "Action (physics)",
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Discriminative model",
      "Economics",
      "Feature (linguistics)",
      "Feature selection",
      "Interval (graph theory)",
      "Linguistics",
      "Machine learning",
      "Management",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Selection (genetic algorithm)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Jiaruo"
      },
      {
        "surname": "Ge",
        "given_name": "Yongxin"
      },
      {
        "surname": "Qin",
        "given_name": "Xiaolei"
      },
      {
        "surname": "Li",
        "given_name": "Ziqiang"
      },
      {
        "surname": "Huang",
        "given_name": "Sheng"
      },
      {
        "surname": "Chen",
        "given_name": "Feiyu"
      }
    ]
  },
  {
    "title": "Distinguishing between natural and recolored images via lateral chromatic aberration",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103295",
    "abstract": "With the development of image colorization technique, the recolored images (RIs) become more and more authentic, making it very difficult to visually distinguish from natural images (NIs). Recently, researchers have proposed the detection methods towards recolored images. However, the current detection still has limitations such as poor generalization, large-scale training samples, high-dimensional features for training, and high computation cost. To address those issues, this paper proposes a novel method based on the lateral chromatic aberration (LCA) inconsistency and its statistical differences. Generally, RIs have fewer numbers of LCA characteristics than that of NIs, that inspire us to design the classifier for distinguishing two types of images. In particular, we propose to adopt very low 5-dimensional features to feed a classical SVM mechanism. The baseline ImageNet and Oxford datasets are used to verify the effectiveness of the proposed method, in which the performance of our proposed method rivals the prior arts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001954",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Chromatic aberration",
      "Chromatic scale",
      "Classifier (UML)",
      "Combinatorics",
      "Computation",
      "Computer science",
      "Computer vision",
      "Generalization",
      "Image (mathematics)",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Yangxin"
      },
      {
        "surname": "Zheng",
        "given_name": "Ning"
      },
      {
        "surname": "Qiao",
        "given_name": "Tong"
      },
      {
        "surname": "Xu",
        "given_name": "Ming"
      },
      {
        "surname": "Wu",
        "given_name": "Jiasheng"
      }
    ]
  },
  {
    "title": "Sensor-based image manipulation localization with Discriminative Random fields and Graph Cut",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103273",
    "abstract": "We present PRNU-based image manipulation localization as a probabilistic labeling task in a flexible discriminative random field (DRF) setup. Instead of reaching local decisions independent of each other, discriminative random fields incorporate local inter-label dependencies while keeping the formulation general enough to make label assignments depend on both local and non-local image characteristics. With an improved form of association potential combining normalized correlation and the deviation of the measured correlation from the expected correlation and an interaction potential defined as the weighted L2 norm squared between intensities of neighboring pixels, we were able to localize even considerably small manipulations on realistic tampered images. We experimented with different combinations of window sizes to capture features to predict the correlation more accurately than already existing algorithms. Experimental results indicate that our algorithm outperforms recent state of the art methods based on multiscale analysis strategies. We also found that for inspecting manipulated images which are JPEG compressed, it helps to train the predictor with JPEG images rather than with uncompressed images and for all quality factors, it is possible to work with two predictors, one trained for images with lower quality factors and another for higher quality factors.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001784",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Correlation",
      "Cut",
      "Discriminative model",
      "Geometry",
      "Image (mathematics)",
      "Image quality",
      "Image segmentation",
      "JPEG",
      "Mathematics",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Pixel",
      "Probabilistic logic",
      "Random forest",
      "Uncompressed video",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Chakraborty",
        "given_name": "Sujoy"
      },
      {
        "surname": "Kirchner",
        "given_name": "Matthias"
      }
    ]
  },
  {
    "title": "Multiple attention encoded cascade R-CNN for scene text detection",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103261",
    "abstract": "Inspired by instance segmentation algorithms, researchers have proposed quantity of segmentation-based methods for text detection, achieving remarkable results on scene text with arbitrary orientation and large aspect ratios. Following their success, we believe cascade architecture and extracting contextual information in multiple aspects are powerful to boost performance on the basis of segmentation-based methods, especially in decreasing false positive texts in complex natural scene. Based on such consideration, we propose a multiple-context-aware and cascade CNN structure, which appropriately encodes multiple categories of context information into a cascade R-CNN framework. Specifically, the proposed method consists of two stages, i.e., feature generation and cascade detection. During the first stage, we define ISTK (Isolated Selective Text Kernel) module to refine feature map, which sequentially encodes channel-wise and kernel-size attention information by designing multiple branches and different kernel sizes in isolate form. Afterwards, we build long-range spatial dependencies in feature map via non-local operations. Built on contextual feature map, Cascade Mask R-CNN structure progressively refines accurate boundaries of text instances with multi-stage framework. We conduct comparative experiments on ICDAR2015 and 2017-MLT datasets, where the proposed method outperform comparative methods in terms of effectiveness and efficiency measurements.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001711",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Cascade",
      "Chemistry",
      "Chromatography",
      "Combinatorics",
      "Computer science",
      "Context (archaeology)",
      "Feature (linguistics)",
      "Kernel (algebra)",
      "Linguistics",
      "Mathematics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Yirui"
      },
      {
        "surname": "Liu",
        "given_name": "Wenxiang"
      },
      {
        "surname": "Wan",
        "given_name": "Shaohua"
      }
    ]
  },
  {
    "title": "Label projection online hashing for balanced similarity",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103314",
    "abstract": "Since online hashing has the advantages of low storage and fast calculation ,it attracts the attention of many scholars. However, the learning of new data streams separates the similarity between new data and existing data in many online hashing methods, which leads to poor retrieval performance. In addition, the similarity measure ignores the expression of different similarity. In this paper, we propose a novel supervised method, namely Label Projection Online Hashing for Balanced Similarity (LPOH). Compared with existing online hashing methods, LPOH aims to solve the problem of the effective establishment of the projection between the label vector and the binary code, and the successful realization of description of different similarity between the same labeled data. Specifically, LPOH overcomes the problem of similarity deviation caused by data imbalance via establishing a mapping matrix to derive a relationship between the data label vector and the binary code. Furthermore, the error between the binary code and the hash function concerning data streams is described. Extensive experiments on widely-used three benchmark datasets demonstrate that LPOH outperforms the state-of-the-art online hashing methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032100208X",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Binary code",
      "Binary number",
      "Code (set theory)",
      "Computer science",
      "Computer security",
      "Data mining",
      "Double hashing",
      "Feature hashing",
      "Geodesy",
      "Geography",
      "Hash function",
      "Hash table",
      "Image (mathematics)",
      "Mathematics",
      "Nearest neighbor search",
      "Pattern recognition (psychology)",
      "Programming language",
      "Projection (relational algebra)",
      "Set (abstract data type)",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Fang",
        "given_name": "Yuzhi"
      },
      {
        "surname": "Zhang",
        "given_name": "Huaxiang"
      },
      {
        "surname": "Liu",
        "given_name": "Li"
      }
    ]
  },
  {
    "title": "Sign language recognition based on global-local attention",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103280",
    "abstract": "Video-level sign language recognition is still a challenging task due to the influence of sign language-independent factors and timing requirements. This paper constructs a sign language recognition framework based on global-local feature description, and proposes a three-dimensional residual global network model with attention layer and a local network model based on target detection. The global feature description is based on the whole video behavior for time series modeling. The improved timing conversion layer is used to explore the timing information of different periods and learn the video representations of different timings. In the local module the hand is located through the target detection network to highlight its key role in the whole sign language behavior, which strengthens the category differences, and compensates the global network. Experiments on two well-known Chinese sign language datasets (SLR_Dataset and DEVSIGN_D) show that the proposed method can obtain higher recognition accuracy (respectively 89.2%, 91%) and better generalization performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001838",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Attention network",
      "Computer science",
      "Computer security",
      "Feature (linguistics)",
      "Generalization",
      "Key (lock)",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Residual",
      "Sign (mathematics)",
      "Sign language",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Shujun"
      },
      {
        "surname": "Zhang",
        "given_name": "Qun"
      }
    ]
  },
  {
    "title": "Learning complementary Siamese networks for real-time high-performance visual tracking",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103299",
    "abstract": "Recently, Siamese based methods have made a breakthrough in the visual tracking field. However, the existing trackers still cannot take full advantage of the deep features. In this work, we improve the performances of Siamese trackers by complementary learning with different types of matching features. Specifically, a Matching Activation Network (MAN) is firstly designed to highlight the matching regions of the search image given a template. Since only sparse parts of feature maps contribute to the matching result, an important design choice is to emphasize the weak-matching features by erasing the strong-matching ones and learn complementary classifiers from both types of features. Then we propose a novel complementary region proposal network (CoRPN) to take complementary features as inputs and their outputs complement to each other, which are fused to improve the performance. Experiments show that our proposed tracker achieves leading performances on five tracking datasets while retaining real-time speed.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032100198X",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "BitTorrent tracker",
      "Chemistry",
      "Complement (music)",
      "Complementation",
      "Computer science",
      "Computer vision",
      "Eye tracking",
      "Feature (linguistics)",
      "Field (mathematics)",
      "Gene",
      "Linguistics",
      "Matching (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pedagogy",
      "Phenotype",
      "Philosophy",
      "Psychology",
      "Pure mathematics",
      "Statistics",
      "Tracking (education)"
    ],
    "authors": [
      {
        "surname": "Tan",
        "given_name": "Ke"
      },
      {
        "surname": "Xu",
        "given_name": "Ting-Bing"
      },
      {
        "surname": "Wei",
        "given_name": "Zhenzhong"
      }
    ]
  },
  {
    "title": "Adversarial steganography based on sparse cover enhancement",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103325",
    "abstract": "CNN (Convolutional Neural Network) steganalyzers achieve enormous improvements in detecting stego images. However, they are easily deceived by adversarial steganography, which combines adversarial attack and steganography. Currently, there are two kinds of adversarial steganography, function separation and cover enhancement. ADV-EMB (ADVersarial EMBedding) is a typical function separation method. It forces the steganographic modifications along side the gradient directions of the target CNN steganalyzer on partial image elements. It results in relatively low deceiving success rate against the target model. ADS (ADversarial Steganography) is the first adversarial steganography, which is based on cover enhancement. It introduces much distortions, so it can be easily detected by non-target steganalyzers. To overcome such defects of the previous works, in this paper, we propose a novel cover enhancement method, denoted as SPS-ENH (SParSe ENHancement). Through sparse ± 1 adversarial perturbations, we effectively compress the distortions caused in cover enhancement. In addition, a re-trying scheme is introduced to further reduce the distortion scale. Extensive experiments show that the proposed method outperforms the previous works in the average classification error rates under non-target steganalyzers and deceiving success rates against target CNN models. When combining with the min–max strategy, the proposed method converges in less iterations and provides higher security level than ADV-EMB.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321002133",
    "keywords": [
      "Adversarial system",
      "Algorithm",
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Biology",
      "Computer science",
      "Convolutional neural network",
      "Cover (algebra)",
      "Distortion (music)",
      "Embedding",
      "Engineering",
      "Evolutionary biology",
      "Function (biology)",
      "Image (mathematics)",
      "Mechanical engineering",
      "Pattern recognition (psychology)",
      "Steganalysis",
      "Steganography",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Qin",
        "given_name": "Chuan"
      },
      {
        "surname": "Zhang",
        "given_name": "Weiming"
      },
      {
        "surname": "Dong",
        "given_name": "Xiaoyi"
      },
      {
        "surname": "Zha",
        "given_name": "Hongyue"
      },
      {
        "surname": "Yu",
        "given_name": "Nenghai"
      }
    ]
  },
  {
    "title": "Three Degree Binary Graph and Shortest Edge Clustering for re-ranking in multi-feature image retrieval",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103282",
    "abstract": "Graph methods have been widely employed in re-ranking for image retrieval. Although we can effectively find visually similar images through these methods, the ranking lists given by those approaches may contain some candidates which appear to be irrelevant to a query. Most of these candidates fall into two categories: (1) the irrelevant outliers located near to the query images in a graph; and (2) the images from another cluster which close to the query. Therefore, eliminating these two types of images from the ordered retrieval sets is expected to further boost the retrieval precision. In this paper, we build a Three Degree Binary Graph (TDBG) to eliminate the outliers and utilize a set-based greedy algorithm to reduce the influence of adjacent manifolds. Moreover, a multi-feature fusion method is proposed to enhance the retrieval performance further. Experimental results obtained on three public datasets demonstrate the superiority of the proposed approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032100184X",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Binary number",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Feature (linguistics)",
      "Graph",
      "Image (mathematics)",
      "Image retrieval",
      "Linguistics",
      "Mathematics",
      "Outlier",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Ranking (information retrieval)",
      "Set (abstract data type)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Lao",
        "given_name": "Guihong"
      },
      {
        "surname": "Liu",
        "given_name": "Shenglan"
      },
      {
        "surname": "Tan",
        "given_name": "Chenwei"
      },
      {
        "surname": "Wang",
        "given_name": "Yang"
      },
      {
        "surname": "Li",
        "given_name": "Guangzhe"
      },
      {
        "surname": "Xu",
        "given_name": "Li"
      },
      {
        "surname": "Feng",
        "given_name": "Lin"
      },
      {
        "surname": "Wang",
        "given_name": "Feilong"
      }
    ]
  },
  {
    "title": "A survey on spatio-temporal framework for kinematic gait analysis in RGB videos",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103218",
    "abstract": "Human gait recognition from videos is one of the promising research topics for analyzing human walking behavior. Spatio-temporal features and kinematics interesting points (three dimensional skeleton points) are the two key metrics in the gait examination. In general, input to gait recognition methods is categorized into 3 groups namely; two dimensional video-based, depth image-based and three dimensional (3D) skeleton-based methods. This work aims to present a survey on spatio-temporal and kinematic gait characteristics based on visual and 3D skeletal traits in RGB videos. A detailed insight on the various benchmarked gait databases, gait recognition representations based on model-based, model-free approaches and classifiers are presented in this review. Also, this paper investigates the performance metrics, application areas and covariate factors that influence the gait recognition process. Finally, the paper outlines the future perspective of gait recognition system based on kinematic joint points.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001449",
    "keywords": [
      "Artificial intelligence",
      "Classical mechanics",
      "Computer science",
      "Computer vision",
      "Gait",
      "Gait analysis",
      "Kinematics",
      "Machine learning",
      "Medicine",
      "Motion (physics)",
      "Motion capture",
      "Operating system",
      "Pattern recognition (psychology)",
      "Perspective (graphical)",
      "Physical medicine and rehabilitation",
      "Physics",
      "Process (computing)",
      "RGB color model"
    ],
    "authors": [
      {
        "surname": "Amsaprabhaa",
        "given_name": "M."
      },
      {
        "surname": "Nancy Jane",
        "given_name": "Y."
      },
      {
        "surname": "Khanna Nehemiah",
        "given_name": "H."
      }
    ]
  },
  {
    "title": "Adaptive fractional motion and disparity estimation skipping in MV-HEVC",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103223",
    "abstract": "MV-HEVC can efficiently compress multiview video data captured from different viewpoints. To achieve high coding efficiency, it consists of not only inter coding but also interview coding. The inter coding includes a motion estimation (ME) process that reduces temporal redundancies between consecutive frames, and the interview coding performs a disparity estimation (DE) that reduces interview redundancies between neighboring views. As a result, MV-HEVC needs high encoding complexity to perform both ME and DE. In order to reduce the complexity, this paper proposes an adaptive fractional ME and DE skipping method in a partitioned inter prediction unit (PU) mode, based on a result of a 2 N × 2 N inter PU coding. Experimental results show that the proposed method efficiently reduces the encoding complexity with negligible coding loss, compared to conventional methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001486",
    "keywords": [
      "Algorithm",
      "Algorithmic efficiency",
      "Coding (social sciences)",
      "Coding tree unit",
      "Computational complexity theory",
      "Computer science",
      "Context-adaptive binary arithmetic coding",
      "Data compression",
      "Decoding methods",
      "Mathematics",
      "Motion estimation",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Lee",
        "given_name": "Jin Young"
      },
      {
        "surname": "Park",
        "given_name": "Sang-hyo"
      }
    ]
  },
  {
    "title": "Facial micro-expression recognition based on accordion spatio-temporal representation and random forests",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103183",
    "abstract": "Micro-expressions are very brief involuntary facial expressions which appear on the face of humans when they unconsciously conceal an emotion. Creating a solution allowing an automatic recognition of the facial micro-expressions from video sequences has garnered increasing attention from experts across such different disciplines as computer science, security, and psychology. This paper offered a solution to facial micro-expressions recognition, based on accordion spatio-temporal representation and Random Forests. The proposed feature space, called “Uniform Local Binary Patterns on an Accordion 2D representation of sub-regions presented by a Pyramid of levels (LBPAccP u2)”, exploits the effectiveness of uniform LBP patterns applied on an accordion representation of sub-regions at different sizes. Random Forests were used to select the most discriminating features and reduce the classification ambiguity of similar micro-expressions through a new proximity measure. The main objective of our paper was to demonstrate that the use of few features could be more efficient to produce a strong micro-expression recognition classifier that outperforms the approaches that rely on high dimensional features space. The experimental results across six micro-expression datasets show the effectiveness of the proposed solution with an accuracy rate that can reach 81.38% on CasmeII dataset. Compared to some famous competitive state-of-the-art approaches, the proposed solution proved its performance thanks to its accuracy rate as well as the number of features it uses.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001164",
    "keywords": [
      "Accordion",
      "Ambiguity",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Expression (computer science)",
      "Face (sociological concept)",
      "Facial expression",
      "Feature (linguistics)",
      "Feature vector",
      "Geometry",
      "Histogram",
      "Image (mathematics)",
      "Law",
      "Linguistics",
      "Local binary patterns",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Programming language",
      "Pyramid (geometry)",
      "Random forest",
      "Representation (politics)",
      "Social science",
      "Sociology",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Guermazi",
        "given_name": "Radhouane"
      },
      {
        "surname": "Ben Abdallah",
        "given_name": "Taoufik"
      },
      {
        "surname": "Hammami",
        "given_name": "Mohamed"
      }
    ]
  },
  {
    "title": "Video hashing with secondary frames and invariant moments",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103209",
    "abstract": "Video hashing is a useful technique of many multimedia systems, such as video copy detection, video authentication, tampering localization, video retrieval, and anti-privacy search. In this paper, we propose a novel video hashing with secondary frames and invariant moments. An important contribution is the secondary frame construction with 3D discrete wavelet transform, which can reach initial data compression and robustness against noise and compression. In addition, since invariant moments are robust and discriminative features, hash generation based on invariant moments extracted from secondary frames can ensure good classification of the proposed video hashing. Extensive experiments on 8300 videos are conducted to validate efficiency of the proposed video hashing. The results show that the proposed video hashing can resist many digital operations and has good discrimination. Performance comparisons with some state-of-the-art algorithms illustrate that the proposed video hashing outperforms the compared algorithms in classification in terms of receiver operating characteristic results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001358",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Discriminative model",
      "Double hashing",
      "Dynamic perfect hashing",
      "Feature hashing",
      "Hash function",
      "Hash table",
      "Invariant (physics)",
      "Mathematical physics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Universal hashing"
    ],
    "authors": [
      {
        "surname": "Tang",
        "given_name": "Zhenjun"
      },
      {
        "surname": "Zhang",
        "given_name": "Shaopeng"
      },
      {
        "surname": "Zhang",
        "given_name": "Xianquan"
      },
      {
        "surname": "Li",
        "given_name": "Zhixin"
      },
      {
        "surname": "Chen",
        "given_name": "Zhenhai"
      },
      {
        "surname": "Yu",
        "given_name": "Chunqiang"
      }
    ]
  },
  {
    "title": "Performance analysis of VVC intra coding",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103202",
    "abstract": "This article presents a performance analysis of Versatile Video Coding (VVC) intra-frame prediction. VVC is the next generation of video coding standards, which has been developed to supply the demand of upcoming video applications. VVC brings several innovations and enhancements for the intra-frame prediction to improve the encoding efficiency. These improvements comprise larger block sizes, more flexible block partitioning, more angular intra-frame prediction modes, multiple transform selection, non-separable secondary transform, among others. This article provides a detailed description of these tools, discussing how they work together in the intra-frame coding flow to raise the compression performance. Moreover, this article presents encoding complexity, encoding usage distribution, and rate-distortion-complexity analyses of the intra-frame prediction tools over different quantization scenarios. Based on these analyses, this article provides support for future works focusing on VVC intra-frame coding, including complexity reduction, complexity control, and real-time hardware design.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001292",
    "keywords": [
      "Algorithm",
      "Algorithmic efficiency",
      "Artificial intelligence",
      "Coding (social sciences)",
      "Computer engineering",
      "Computer science",
      "Context-adaptive binary arithmetic coding",
      "Data compression",
      "Decoding methods",
      "Frame (networking)",
      "Inter frame",
      "Intra-frame",
      "Mathematics",
      "Multiview Video Coding",
      "Reference frame",
      "Residual frame",
      "Statistics",
      "Telecommunications",
      "Video processing",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Saldanha",
        "given_name": "Mário"
      },
      {
        "surname": "Sanchez",
        "given_name": "Gustavo"
      },
      {
        "surname": "Marcon",
        "given_name": "César"
      },
      {
        "surname": "Agostini",
        "given_name": "Luciano"
      }
    ]
  },
  {
    "title": "A regional distance regression network for monocular object distance estimation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103224",
    "abstract": "Monocular pipelines are convenient and cost-effective solutions for object distance estimation in 3D vision. Current methods for monocular object distance estimation either perform inaccurately or require heavy work on data collection. In this paper, we propose a network with R-CNN based structure to implement object detection and distance estimation simultaneously. We append an efficient branch to integrate the information of camera extrinsic parameters with RGB data in our network. Further, optimized multi-scale feature is utilized to enrich the representation power of deep feature, hence to enhance the estimation accuracy. Finally, several regression methods are explored to improve distance estimation results. We train and validate our network on KITTI object dataset, and compare with other methods to show that our method is accurate and easy to train. To prove the generality of our method under other scenarios, we construct a dataset of surveillance scenes, and conduct similar experiments on this dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001474",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Construct (python library)",
      "Data mining",
      "Economics",
      "Estimation",
      "Feature (linguistics)",
      "Generality",
      "Linguistics",
      "Management",
      "Monocular",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Pose",
      "Programming language",
      "Psychology",
      "Psychotherapist",
      "Quantum mechanics",
      "RGB color model",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Yufeng"
      },
      {
        "surname": "Ding",
        "given_name": "Lianghui"
      },
      {
        "surname": "Li",
        "given_name": "Yuxi"
      },
      {
        "surname": "Lin",
        "given_name": "Weiyao"
      },
      {
        "surname": "Zhao",
        "given_name": "Mingbi"
      },
      {
        "surname": "Yu",
        "given_name": "Xiaoyuan"
      },
      {
        "surname": "Zhan",
        "given_name": "Yunlong"
      }
    ]
  },
  {
    "title": "Region-level bit allocation for rate control of 360-degree videos using cubemap projection",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103242",
    "abstract": "Featuring with more uniform sampling density in the sphere domain and less non-uniform geometric deformations in the planar domain, variants of cubemap projection (CMP) format enable the higher compression ratio in on-going 360-degree video coding standardization. Different from single-view videos, 360-degree CMP videos feature with content discontinuity combined with the abrupt change of motion vectors between some adjacent faces. However, there is few bit allocation scheme designed for rate control of video coding of CMP format. Thus, this paper proposes a region-level bit allocation scheme for rate control of interframe coding of CMP format. The proposed scheme consists of two parts. The first part is machine learning based high HEVC coding cost region detection for individual faces, where the feature descriptor of a CTU consists of the face based texture complexity, motion magnitude, motion density, and temporal coherence of motion vector. The second part is fitting function based region-level bit allocation. Different from previous work, bits are assigned to the high coding cost region and non-high coding cost region in individual faces of CMP format. Experimental results indicate that the proposed scheme achieves higher bitrate accuracy and larger BD-WS-PSNR compared with the original rate control scheme of the reference software of HEVC, HM16.16 with the 360Lib.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001589",
    "keywords": [
      "Algorithm",
      "Algorithmic efficiency",
      "Artificial intelligence",
      "Coding (social sciences)",
      "Coding tree unit",
      "Computer science",
      "Computer vision",
      "Context-adaptive binary arithmetic coding",
      "Data compression",
      "Decoding methods",
      "Frame (networking)",
      "Image (mathematics)",
      "Inter frame",
      "Mathematics",
      "Motion compensation",
      "Motion vector",
      "Quarter-pixel motion",
      "Reference frame",
      "Reference software",
      "Statistics",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Nien",
        "given_name": "Yu-Chieh"
      },
      {
        "surname": "Tang",
        "given_name": "Chih-Wei"
      }
    ]
  },
  {
    "title": "Robust model adaption for colour-based particle filter tracking with contextual information",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103270",
    "abstract": "Color-based particle filters have emerged as an appealing method for targets tracking. As the target may undergo rapid and significant appearance changes, the template (i.e. scale of the target, color distribution histogram) also needs to be updated. Traditional updates without learning contextual information may imply a high risk of distorting the model and losing the target. In this paper, a new algorithm utilizing the environmental information to update both the scale of the tracker and the reference appearance model for the purpose of object tracking in video sequences has been put forward. The proposal makes use of the well-established color-based particle filter tracking while differentiating the foreground and background particles according to their matching score. A roaming phenomenon that yields the estimation to shrink and diverge is investigated. The proposed solution is tested using both simulated and publicly available benchmark datasets where a comparison with six state-of-the-art trackers has been carried out. The results demonstrate the feasibility of the proposal and lie down foundations for further research on tackling complex visual tracking problems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001760",
    "keywords": [
      "Active appearance model",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "BitTorrent tracker",
      "Computer science",
      "Computer vision",
      "Eye tracking",
      "Filter (signal processing)",
      "Geodesy",
      "Geography",
      "Histogram",
      "Image (mathematics)",
      "Object (grammar)",
      "Particle filter",
      "Pattern recognition (psychology)",
      "Pedagogy",
      "Physics",
      "Psychology",
      "Quantum mechanics",
      "Scale (ratio)",
      "Tracking (education)",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Xiao",
        "given_name": "Jingjing"
      },
      {
        "surname": "Oussalah",
        "given_name": "Mourad"
      }
    ]
  },
  {
    "title": "Dynamic Dual-Peak Network: A real-time human detection network in crowded scenes",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103195",
    "abstract": "Human detection in crowded scenes is challenging since the objects occlude and overlap each other. Compared to general pedestrian detection, there is also more variation in human posture. This paper proposes a real-time human detection network, Dynamic Dual-Peak Network (DDPNet), which specifically addresses human object detection in overlapping and crowded scenes. We design a deep cascade fusion module to enhance the feature extraction capability of the anchor-free model for small objects in crowded scenes. In the meantime, the head–body dual-peak activation module is used to improve the prediction score of the central region of the occluded individual through low occlusion components. By this improvement strategy, the network’s ability is enhanced to discriminate individuals in crowded scenes and alleviate the problem caused by individual posture variation. Ultimately, we propose a novel Exhale–Inhale method to adjust the feature mapping ranges for various scale objects dynamically. In the process of ground truth mapping, the overlapping of individual feature information is reduced. Our DDPNet achieves competitive performance on the CrowdHuman dataset and executes real-time inference of almost 3x ∼ 7x faster than competitive methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001243",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Cascade",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Computer vision",
      "Dual (grammatical number)",
      "Engineering",
      "Feature (linguistics)",
      "Ground truth",
      "Inference",
      "Linguistics",
      "Literature",
      "Object detection",
      "Operating system",
      "Pattern recognition (psychology)",
      "Pedestrian",
      "Pedestrian detection",
      "Philosophy",
      "Process (computing)",
      "Transport engineering"
    ],
    "authors": [
      {
        "surname": "Xie",
        "given_name": "Yefan"
      },
      {
        "surname": "Zheng",
        "given_name": "Jiangbin"
      },
      {
        "surname": "Hou",
        "given_name": "Xuan"
      },
      {
        "surname": "Xi",
        "given_name": "Yue"
      },
      {
        "surname": "Tian",
        "given_name": "Fengming"
      }
    ]
  },
  {
    "title": "Development of scale and illumination invariant feature detector with application to UAV attitude estimation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103258",
    "abstract": "Feature detection has great importance in many applications such as vision navigation. Examining the developed detectors, it is found in many recent studies that most of the scale-invariant detectors are sensitive to illumination. In this work, we propose a novel detector that has good robustness to both scale and illumination. Motivated by the good robustness of Log-Gabor kernels toward light changes, we employ these kernels as a basis to construct the scale space. To detect potential features, we develop an effective interest points measure which is motivated by the concept of the autocorrelation and Hessian matrices. To confirm the good performance of our detector, we hold experiments on many datasets and with comparisons to common state-of-the-art methods. Furthermore, we evaluate the saliency of the detected features on a UAV attitude estimation task.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001681",
    "keywords": [
      "Applied mathematics",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Detector",
      "Gene",
      "Hessian matrix",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Robustness (evolution)",
      "Scale invariance",
      "Statistics",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Djerida",
        "given_name": "Achraf"
      },
      {
        "surname": "Zhao",
        "given_name": "Zhonghua"
      },
      {
        "surname": "Zhao",
        "given_name": "Jiankang"
      }
    ]
  },
  {
    "title": "Privacy-preserving reversible data hiding based on quad-tree block encoding and integer wavelet transform",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103203",
    "abstract": "Applications on the cloud server have matured, and protecting the privacy of the content owner has attracted more attention. Privacy-Preserving Reversible data hiding (PP-RDH) is an efficient technique for embedding additional data into an encrypted image. In this paper, we propose a privacy-preserving reversible data hiding scheme using the quad-tree partition and Integer Wavelet Transform (IWT) techniques. Our scheme focuses on improving the embedding rate and quality of the recovered image when a 2 × 2-sized, block-based image encryption method is applied to ensure relative higher security. On this basis, the IWT technique transforms the encrypted image, and coefficients in three high frequency subbands are converted into 8-bit binary system. Then, the quad-tree partition technique encodes each 8 × 8-sized coefficient block, since there are many zeroes in the front bit planes. The experimental results indicated that our proposed scheme significantly improved the embedding rate, and guaranteed lossless image recovery and data extraction.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001309",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Combinatorics",
      "Computer network",
      "Computer science",
      "Data compression",
      "Embedding",
      "Encryption",
      "Geometry",
      "Image (mathematics)",
      "Information hiding",
      "Integer (computer science)",
      "Lossless compression",
      "Mathematical analysis",
      "Mathematics",
      "Partition (number theory)",
      "Programming language",
      "Theoretical computer science",
      "Tree (set theory)",
      "Wavelet",
      "Wavelet transform"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Xu"
      },
      {
        "surname": "Chang",
        "given_name": "Ching-Chun"
      },
      {
        "surname": "Lin",
        "given_name": "Chia-Chen"
      },
      {
        "surname": "Chang",
        "given_name": "Chin-Chen"
      }
    ]
  },
  {
    "title": "Joint model of gradient magnitude and Gabor features via Spatio-Temporal slice",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103204",
    "abstract": "To form a high-performance video quality predictor, we developed a framework for full-reference (FR) video quality assessment that integrates Spatio-temporal slice analysis (STS) to create a high-performance predictor of video quality. However, both gradient and Gabor are spatial–temporal structural capturers used for the simultaneous extraction of both spatial and temporal features. In this paper, we proposed a novel VQA algorithm via a joint model of gradient magnitude and Gabor features (JMG) between the STS images of the reference videos and their distorted counterparts to assess the degradation of video quality effectively. Firstly, gradient magnitude and the Gabor filter were constructed to extract the spatiotemporal features of the video sequence. However, the two-feature model combined to predict the perceptual quality of frames. This new proposed VQA model is known as the horizontal and time STS (HT-JMG) model. To further investigate the influence of spatial dissimilarity, we combined the frame-by-frame spatial T-JMG(S) factor with the HT-JMG and propose another VQA model, called the time, horizontal, and vertical STS (THV-JMG) model. Finally, the results of the experiment showed that the proposed method has a strong correlation with subjective perception and is competitive with state-of-the-art full reference VQA models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001310",
    "keywords": [
      "Architectural engineering",
      "Artificial intelligence",
      "Astronomy",
      "Biology",
      "Computer science",
      "Computer vision",
      "Engineering",
      "Feature (linguistics)",
      "Feature extraction",
      "Filter (signal processing)",
      "Frame (networking)",
      "Gabor filter",
      "Joint (building)",
      "Linguistics",
      "Magnitude (astronomy)",
      "Mathematics",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Perception",
      "Philosophy",
      "Physics",
      "Reference frame",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Oppong Bediako",
        "given_name": "Daniel"
      },
      {
        "surname": "Mou",
        "given_name": "Xuanqin"
      }
    ]
  },
  {
    "title": "Tile caching for scalable VR video streaming over 5G mobile networks",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103210",
    "abstract": "Currently, VR video delivery over 5G systems is still a very complicated endeavor. One of the major challenges for VR video streaming is the expectations for low latency that current mobile networks can hardly meet. Network caching can reduce the content delivery latency efficiently. However, current caching schemes cannot obtain ideal results for VR video since it requests the viewport interactively. In this paper, we propose a tiled scalable VR video caching scheme over 5G networks. VR chunks are first encoded into multi-granularity quality layers, and are then partitioned into tiles to facilitate viewport data access. By accommodating the 5G network infrastructure, the tiles are cooperatively cached in a three-level hierarchal system to reduce delivery latency. Furthermore, a quality-adaptive request routing algorithm is designed to cater for the 5G bandwidth dynamics. Experimental results show that the proposed scheme can reduce the transmission latency over conventional constant bitrate video caching schemes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032100136X",
    "keywords": [
      "Artificial intelligence",
      "Cache",
      "Computer network",
      "Computer science",
      "Latency (audio)",
      "Operating system",
      "Real-time computing",
      "Scalability",
      "Telecommunications",
      "Viewport"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Kedong"
      },
      {
        "surname": "Liu",
        "given_name": "Yanwei"
      },
      {
        "surname": "Liu",
        "given_name": "Jinxia"
      },
      {
        "surname": "Argyriou",
        "given_name": "Antonios"
      }
    ]
  },
  {
    "title": "Image compression optimized for 3D reconstruction by utilizing deep neural networks",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103208",
    "abstract": "Computer vision tasks are often expected to be executed on compressed images. Classical image compression standards like JPEG 2000 are widely used. However, they do not account for the specific end-task at hand. Motivated by works on recurrent neural network (RNN)-based image compression and three-dimensional (3D) reconstruction, we propose unified network architectures to solve both tasks jointly. These joint models provide image compression tailored for the specific task of 3D reconstruction. Images compressed by our proposed models, yield 3D reconstruction performance superior as compared to using JPEG 2000 compression. Our models significantly extend the range of compression rates for which 3D reconstruction is possible. We also show that this can be done highly efficiently at almost no additional cost to obtain compression on top of the computation already required for performing the 3D reconstruction task.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001346",
    "keywords": [
      "3D reconstruction",
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Composite material",
      "Compression (physics)",
      "Computation",
      "Computer science",
      "Computer vision",
      "Data compression",
      "Economics",
      "Image (mathematics)",
      "Image compression",
      "Image processing",
      "Iterative reconstruction",
      "JPEG",
      "JPEG 2000",
      "Lossless JPEG",
      "Management",
      "Materials science",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Golts",
        "given_name": "Alex"
      },
      {
        "surname": "Schechner",
        "given_name": "Yoav Y."
      }
    ]
  },
  {
    "title": "SAF-Nets: Shape-Adaptive Filter Networks for 3D point cloud processing",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103246",
    "abstract": "A deep learning framework for 3D point cloud processing is proposed in this work. In a point cloud, local neighborhoods have various shapes, and the semantic meaning of each point is determined within the local shape context. Thus, we propose shape-adaptive filters (SAFs), which are dynamically generated from the distributions of local points. The proposed SAFs can extract robust features against noise or outliers, by employing local shape contexts to suppress them. Also, we develop the SAF-Nets for classification and segmentation using multiple SAF layers. Extensive experimental results demonstrate that the proposed SAF-Nets significantly outperform the state-of-the-art conventional algorithms on several benchmark datasets. Moreover, it is shown that SAFs can improve scene flow estimation performance as well.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001619",
    "keywords": [
      "Algorithm",
      "Archaeology",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Cartography",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Filter (signal processing)",
      "Geography",
      "Geometry",
      "Image (mathematics)",
      "Mathematics",
      "Noise (video)",
      "Outlier",
      "Pattern recognition (psychology)",
      "Point (geometry)",
      "Point cloud",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Lee",
        "given_name": "Seon-Ho"
      },
      {
        "surname": "Kim",
        "given_name": "Chang-Su"
      }
    ]
  },
  {
    "title": "Tile caching for scalable VR video streaming over 5G mobile networks",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103210",
    "abstract": "Currently, VR video delivery over 5G systems is still a very complicated endeavor. One of the major challenges for VR video streaming is the expectations for low latency that current mobile networks can hardly meet. Network caching can reduce the content delivery latency efficiently. However, current caching schemes cannot obtain ideal results for VR video since it requests the viewport interactively. In this paper, we propose a tiled scalable VR video caching scheme over 5G networks. VR chunks are first encoded into multi-granularity quality layers, and are then partitioned into tiles to facilitate viewport data access. By accommodating the 5G network infrastructure, the tiles are cooperatively cached in a three-level hierarchal system to reduce delivery latency. Furthermore, a quality-adaptive request routing algorithm is designed to cater for the 5G bandwidth dynamics. Experimental results show that the proposed scheme can reduce the transmission latency over conventional constant bitrate video caching schemes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032100136X",
    "keywords": [
      "Artificial intelligence",
      "Cache",
      "Computer network",
      "Computer science",
      "Latency (audio)",
      "Operating system",
      "Real-time computing",
      "Scalability",
      "Telecommunications",
      "Viewport"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Kedong"
      },
      {
        "surname": "Liu",
        "given_name": "Yanwei"
      },
      {
        "surname": "Liu",
        "given_name": "Jinxia"
      },
      {
        "surname": "Argyriou",
        "given_name": "Antonios"
      }
    ]
  },
  {
    "title": "An Interconnected Feature Pyramid Networks for object detection",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103260",
    "abstract": "Although deep learning makes major breakthroughs in object detection, object detection still faces several limitations listed as follows: (1) Many works underplay the feature selection, leading to the resulting key features are not prominent enough and prone to noise; (2) Many works pass back features in a layer-by-layer manner to achieve multi-scale features. However, as the distance of layers from each other increases, the semantics are diluted, and the transfer of information between layers becomes difficult. To overcome these problems, we propose a new Interconnected Feature Pyramid Networks (IFPN) for feature enhancement. It can simultaneously select attentive features through the attention mechanism and realize the free flow of information. On the basis of the improvements, we design a new IFPN Detector. Experiments on COCO dataset and Smart UVM dataset show that our method can bring a significant improvement.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032100170X",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Data mining",
      "Feature (linguistics)",
      "Feature selection",
      "Geometry",
      "Image (mathematics)",
      "Information flow",
      "Key (lock)",
      "Layer (electronics)",
      "Linguistics",
      "Mathematics",
      "Noise (video)",
      "Object (grammar)",
      "Object detection",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Pyramid (geometry)",
      "Semantics (computer science)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Qiang"
      },
      {
        "surname": "Zhou",
        "given_name": "Lukuan"
      },
      {
        "surname": "Yao",
        "given_name": "Yuncong"
      },
      {
        "surname": "Wang",
        "given_name": "Yong"
      },
      {
        "surname": "Li",
        "given_name": "Jun"
      },
      {
        "surname": "Yang",
        "given_name": "Wankou"
      }
    ]
  },
  {
    "title": "GIFMarking: The robust watermarking for animated GIF based deep learning",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103244",
    "abstract": "Animated GIF has become a key communication tool in contemporary social platforms thanks to highly compatible with affective performance, and it is gradually adopted in commercial applications. Therefore, the copyright protection of the animated GIF requires more attention. Digital watermarking is an effective method to embed invisible data into a digital medium that can identify the creator or authorized users. However, few works have been devoted to robust watermarking for the animated GIF. One of the main challenges is that the animated image also contains time frame dimension information compare with still images. This paper proposes a robust blind watermarking framework based 3D convolutional neural networks for the animated GIF image, which achieves watermark image embedding and extraction for the animated GIF. Also, noise simulation is developed in frame-level to ensure robustness for the attack of the temporal dimension in this framework. Furthermore, the invisibility of the watermarked animated image is optimized by adversarial learning. Experimental results provide the effectiveness of the proposed framework and show advantages over existing works.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001590",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Digital watermarking",
      "Dimension (graph theory)",
      "Embedding",
      "Gene",
      "Image (mathematics)",
      "Image editing",
      "Invisibility",
      "Key (lock)",
      "Mathematics",
      "Noise (video)",
      "Pure mathematics",
      "Robustness (evolution)",
      "Watermark"
    ],
    "authors": [
      {
        "surname": "Liao",
        "given_name": "Xin"
      },
      {
        "surname": "Peng",
        "given_name": "Jing"
      },
      {
        "surname": "Cao",
        "given_name": "Yun"
      }
    ]
  },
  {
    "title": "Amplitude based keyless optical encryption system using deep neural network",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103251",
    "abstract": "Double random phase encryption (DRPE) system is a simple and powerful encoding technique that consists of only two lenses and two random phase masks. However, there are many issues for applying to actual security systems such as phase acquisition, vulnerability to phase retrieval techniques, and data throughput. Although various extensions of DRPE have addressed each issue, there is no comprehensive solution. To tackle all the issues of DRPE, we propose a new amplitude-based DRPE (ADRPE) system using deep learning. The encoding is the same as the current ADRPE system, and the decoding is achieved by an inverse ADRPE system using convolution neural networks. Our system can achieve a real-time end-to-end encryption system without any additional optical devices and exposure of the keys. To demonstrate our method, we applied it to simulations with various datasets such as passwords, Quick-Response (QR) codes, and fingerprints.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001632",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Channel (broadcasting)",
      "Chemistry",
      "Computer hardware",
      "Computer science",
      "Computer security",
      "Convolution (computer science)",
      "Decoding methods",
      "Encoding (memory)",
      "Encryption",
      "Interference (communication)",
      "Organic chemistry",
      "Password",
      "Pattern recognition (psychology)",
      "Phase (matter)",
      "Telecommunications",
      "Vulnerability (computing)"
    ],
    "authors": [
      {
        "surname": "Inoue",
        "given_name": "Kotaro"
      },
      {
        "surname": "Cho",
        "given_name": "Myungjin"
      }
    ]
  },
  {
    "title": "S&CNet: A lightweight network for fast and accurate depth completion",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103220",
    "abstract": "Dense depth completion is essential for autonomous driving and robotic navigation. Existing methods focused on attaining higher accuracy of the estimated depth, which comes at the price of increasing complexity and cannot be well applied in a real-time system. In this paper, a coarse-to-fine and lightweight network (S&CNet) is proposed for dense depth completion to reduce the computational complexity with negligible sacrifice on accuracy. A dual-stream attention module (S&C enhancer) is proposed according to a new finding of deep neural network-based depth completion, which can capture both the spatial-wise and channel-wise global-range information of extracted features efficiently. Then it is plugged between the encoder and decoder of the coarse estimation network so as to improve the performance. The experiments on KITTI dataset demonstrate that the proposed approach achieves competitive result with respect to state-of-the-art works but via an almost four times faster speed. The S&C enhancer can also be easily plugged into other existing works to boost their performances significantly with negligible additional computations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001450",
    "keywords": [
      "Algorithm",
      "Art",
      "Artificial intelligence",
      "Artificial neural network",
      "Composite material",
      "Computation",
      "Computational complexity theory",
      "Computer engineering",
      "Computer science",
      "Dual (grammatical number)",
      "Encoder",
      "Literature",
      "Materials science",
      "Operating system",
      "Range (aeronautics)",
      "Real-time computing",
      "State (computer science)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Ziyang"
      },
      {
        "surname": "Zhang",
        "given_name": "Lei"
      },
      {
        "surname": "Chen",
        "given_name": "Weihai"
      },
      {
        "surname": "Wu",
        "given_name": "Xingming"
      },
      {
        "surname": "Li",
        "given_name": "Zhengguo"
      }
    ]
  },
  {
    "title": "Robust multimodal discrete hashing for cross-modal similarity search",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103256",
    "abstract": "Hashing technology improves the search efficiency and reduces the storage space of data. However, building an effective modal with unsupervised cross modal retrieval and generating efficient binary code is still a challenging task, considering of some issues needed to be further discussed and researched for unsupervised multimodal hashing. Most of the existing methods ignore the discrete restriction, and manually or experientially determine the weights of each modality. These limitations may significantly reduce the retrieval accuracy of unsupervised cross-modal hashing methods. To solve these problems, we propose a robust hash modal that can efficiently learn binary code by employing a flexible and noise-resistant ℓ 2 , 1 -loss with nonlinear kernel embedding. In addition, we introduce an intermediate state mapping that facilitate later modal optimization to measure the loss between the hash codes and the intermediate states. Experiments on several public multimedia retrieval datasets validate the superiority of the proposed method from various aspects.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032100167X",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Binary code",
      "Binary number",
      "Chemistry",
      "Code (set theory)",
      "Computer science",
      "Computer security",
      "Data mining",
      "Double hashing",
      "Embedding",
      "Feature hashing",
      "Hash function",
      "Hash table",
      "Machine learning",
      "Mathematics",
      "Modal",
      "Pattern recognition (psychology)",
      "Polymer chemistry",
      "Programming language",
      "Set (abstract data type)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Fang",
        "given_name": "Yuzhi"
      }
    ]
  },
  {
    "title": "Sequence-tracker: Multiple object tracking with sequence features in severe occlusion scene",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103250",
    "abstract": "Multiple object tracking is one of the most fundamental tasks in computer vision, and it is still very challenging for real-world applications due to its severe occlusion and motion blur. Most of the existing methods solve these multiple object tracking issues by performing data association based on the deep features of the detections in consecutive frames, which only contain the spatial information of the detected objects. Therefore, the inaccuracy of data association would easily occur, especially in the severe occlusion scenes. In this paper, a novel multiple object tracking model named sequence-tracker (STracker) has been proposed, which combines both the temporal and spatial features to perform data association. We trained a sequence feature extraction network based on video pedestrian re-identification offline, fused the obtained sequence features with the depth features of the previous frame, and then implemented the Hungarian algorithm for data association. Experiments have been carried out to validate the effectiveness of the proposed algorithm and the corresponding results indicates that it can significantly improve the trajectory quality of our dataset in this paper. Remarkably, for the public detector results from MOT official website, the proposed algorithm can achieve up to 57.2% MOTA and 50.9% IDF1 on the MOT17 dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001644",
    "keywords": [
      "Artificial intelligence",
      "Association (psychology)",
      "Astronomy",
      "Biology",
      "Botany",
      "Computer science",
      "Computer vision",
      "Epistemology",
      "Feature (linguistics)",
      "Frame (networking)",
      "Genetics",
      "Identification (biology)",
      "Linguistics",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Pedagogy",
      "Philosophy",
      "Physics",
      "Psychology",
      "Sequence (biology)",
      "Telecommunications",
      "Tracking (education)",
      "Trajectory",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Tan",
        "given_name": "Xu"
      },
      {
        "surname": "Li",
        "given_name": "Zhengwei"
      },
      {
        "surname": "Liang",
        "given_name": "Qiaokang"
      },
      {
        "surname": "Sun",
        "given_name": "Wei"
      },
      {
        "surname": "Wang",
        "given_name": "Yaonan"
      },
      {
        "surname": "Zhang",
        "given_name": "Dan"
      }
    ]
  },
  {
    "title": "Generalized multiple sparse information fusion for vehicle re-identification",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103207",
    "abstract": "Vehicle re-identification (reID) aims to search the target vehicle in a non-overlapping multi-camera network, which is important for the intelligent analysis in large scale of surveillance videos. Many existing methods have employed various techniques to achieve discriminative information. However, those methods always focus on the description of one view for the same vehicle images. Hence, a generated multiple sparse information fusion method is proposed for exploiting latent features from multi-views, which employs three different deep networks to extract multiple features from coarse to fine. And these features are regarded as multi-view features. Besides, to fuse these features reasonably, the paper transfers various features into a common space for better seeking distinctive features. Especially, besides ResNet, two feature learning networks are proposed to learn different features, respectively. One is designed to learn robust feature by dropping some features randomly when training the reID model. Another is to combine various salient features from different layers, which forms strong features for the reID task. Moreover, comprehensive experimental results have demonstrated that our proposed method can achieve competitive performances on benchmark datasets VehicleID and VeRi-776.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001334",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Botany",
      "Computer science",
      "Discriminative model",
      "Electrical engineering",
      "Engineering",
      "Feature (linguistics)",
      "Feature vector",
      "Focus (optics)",
      "Fuse (electrical)",
      "Geodesy",
      "Geography",
      "Identification (biology)",
      "Linguistics",
      "Machine learning",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Salient"
    ],
    "authors": [
      {
        "surname": "Peng",
        "given_name": "Jinjia"
      },
      {
        "surname": "Jiang",
        "given_name": "Guangqi"
      },
      {
        "surname": "Wang",
        "given_name": "Huibing"
      }
    ]
  },
  {
    "title": "Illumination-based adaptive saliency detection network through fusion of multi-source features",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103192",
    "abstract": "Salient object detection (SOD) tasks aim to outline the most concerned part of human vision, which is widely used in computer vision fields. Due to possibility of the insufficient illumination in the application environment (such as night or dim indoor environment), RGB images from visible channels usually lose most of their performance, while thermal images can improve the detection performance. Therefore, it is in urgent need of a robust saliency detection method, which can handle complex illumination conditions and take use of features from multiple sources intelligently. Accordingly, we propose our ‘illumination based multi-source fused salient object detection network’ (IAN-MF-SOD network). Taking the illumination condition as a quantitative reference, we guide features from two sources to fuse adaptively and intelligently, so that our method can enhance both of their advantages. For different illumination conditions, we distribute different fusion weights for each RGB–thermal image pair. Well fused images are generated as inputs to a trained SOD network to obtain saliency maps. Due to the analysis of our proposed IAN-score, our method performs favorably against traditional RGB-based SOD networks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032100122X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Electrical engineering",
      "Engineering",
      "Fuse (electrical)",
      "Fusion",
      "Image (mathematics)",
      "Linguistics",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Philosophy",
      "RGB color model",
      "Salient"
    ],
    "authors": [
      {
        "surname": "Jiang",
        "given_name": "Chunxu"
      },
      {
        "surname": "Liu",
        "given_name": "Yu"
      },
      {
        "surname": "Sun",
        "given_name": "Jinglin"
      },
      {
        "surname": "Guo",
        "given_name": "Jichang"
      },
      {
        "surname": "Lu",
        "given_name": "Wei"
      }
    ]
  },
  {
    "title": "Variational optimization based single image dehazing",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103241",
    "abstract": "In this paper, we present a new approach for single image dehazing based on the proposed variational optimization. A hazy image captures the information about haze in terms of the transmission map and object details present in it. We propose to estimate the initial transmission map by performing the structure-aware smoothing of the hazy image. Further, we formulated a variational optimization for the estimation of final transmission, which refines the initial transmission of a hazy image. Atmospheric light can be considered to be constant throughout the scene for practical purposes. The uniform atmospheric light is computed from the dark channel of a hazy image. The exhaustive experimentation shows that the performance of the proposed method is comparable or better.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001577",
    "keywords": [
      "Artificial intelligence",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Haze",
      "Image (mathematics)",
      "Image processing",
      "Image restoration",
      "Meteorology",
      "Physics",
      "Smoothing",
      "Telecommunications",
      "Transmission (telecommunications)"
    ],
    "authors": [
      {
        "surname": "Singh",
        "given_name": "Kavinder"
      },
      {
        "surname": "Parihar",
        "given_name": "Anil Singh"
      }
    ]
  },
  {
    "title": "Quality assessment of screen content images based on multi-stage dictionary learning",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103248",
    "abstract": "In this paper, we propose an effective method for quality assessment of screen content images (SCIs) based on multi-stage dictionary learning. To simulate the brain’s layered processing of signals, we proposed a hierarchical feature extraction strategy, which is called multi-stage dictionary learning, to simulate the hierarchical information processing of brain. First, the standard deviation of normalized map obtained from training image is used to select the training data in a certain proportion, which can ensure the learning efficiency and reduce the training burden. Next, the reconstructed map is weighted as the input of the next-stage dictionary learning. Then using the trained dictionary, the sparse representation is applied to extract features. Meanwhile, considering that some important features may be ignored in the process of multi-stage dictionary learning, we use Log Gabor filter to extract feature maps, and then calculate the correlation between feature maps as another kind of compensation features. Final, for the two feature sets, we choose SVR and feature codebook to learn two objective scores, and then use the adaptive weighting strategy to get the final objective quality score. Experimental results show that the proposed method is superior to several mainstream SCIs metrics on two publicly available databases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001620",
    "keywords": [
      "Artificial intelligence",
      "Codebook",
      "Computer science",
      "Computer vision",
      "Epistemology",
      "Feature (linguistics)",
      "Feature extraction",
      "Filter (signal processing)",
      "Gabor filter",
      "Linguistics",
      "Machine learning",
      "Medicine",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Quality (philosophy)",
      "Radiology",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Chang",
        "given_name": "Yongli"
      },
      {
        "surname": "Li",
        "given_name": "Sumei"
      },
      {
        "surname": "Liu",
        "given_name": "Anqi"
      },
      {
        "surname": "Jin",
        "given_name": "Jie"
      }
    ]
  },
  {
    "title": "Atom specific multiple kernel dictionary based Sparse Representation Classifier for medium scale image classification",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103228",
    "abstract": "Kernel based Sparse Representation Classifier (KSRC) can classify images with acceptable performance. In addition, Multiple Kernel Learning based SRC (MKL-SRC) computes the weighted sum of multiple kernels in order to construct a unified kernel while the weight of each kernel is calculated as a fixed value in the training phase. In this paper, an MKL-SRC with non-fixed kernel weights for dictionary atoms is proposed. Kernel weights are embedded as new variables to the main KSRC goal function and the resulted optimization problem is solved to find the sparse coefficients and kernel weights simultaneously. As a result, an atom specific multiple kernel dictionary is computed in the training phase which is used by SRC to classify test images. Also, it is proved that the resulting optimization problem is convex and is solvable via common algorithms. The experimental results demonstrate the effectiveness of the proposed approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001504",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Classifier (UML)",
      "Combinatorics",
      "Computer science",
      "Kernel (algebra)",
      "Kernel embedding of distributions",
      "Kernel method",
      "Mathematics",
      "Multiple kernel learning",
      "Optimization problem",
      "Pattern recognition (psychology)",
      "Radial basis function kernel",
      "Sparse approximation",
      "String kernel",
      "Support vector machine",
      "Tree kernel",
      "Variable kernel density estimation"
    ],
    "authors": [
      {
        "surname": "Zamani",
        "given_name": "Fatemeh"
      },
      {
        "surname": "Jamzad",
        "given_name": "Mansour"
      },
      {
        "surname": "Rabiee",
        "given_name": "Hamid R."
      }
    ]
  },
  {
    "title": "Beyond ITQ: Efficient binary multi-view subspace learning for instance retrieval",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103234",
    "abstract": "The existing hashing methods mainly handle either the feature based nearest-neighbor search or the category-level image retrieval, whereas a few efforts are devoted to instance retrieval problem. In this paper, we propose a binary multi-view fusion framework for directly recovering a latent Hamming subspace from the multi-view features for instance retrieval. More specifically, the multi-view subspace reconstruction and the binary quantization are integrated in a unified framework so as to minimize the discrepancy between the original multi-view high-dimensional Euclidean space and the resulting compact Hamming subspace. Besides, our method is essentially an unsupervised learning scheme without any labeled data involved, and thus can be used in the cases when the supervised information is unavailable or insufficient. Experiments on public benchmark and large-scale datasets reveal that our method achieves competitive retrieval performance comparable to the state-of-the-arts and has excellent scalability in large-scale scenario.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032100153X",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Binary code",
      "Binary number",
      "Block code",
      "Codebook",
      "Computer science",
      "Computer security",
      "Data mining",
      "Database",
      "Decoding methods",
      "Feature vector",
      "Hamming code",
      "Hamming distance",
      "Hamming space",
      "Hash function",
      "Image (mathematics)",
      "Image retrieval",
      "Mathematics",
      "Nearest neighbor search",
      "Pattern recognition (psychology)",
      "Scalability",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Zhijian"
      },
      {
        "surname": "Li",
        "given_name": "Jun"
      },
      {
        "surname": "Xu",
        "given_name": "Jianhua"
      },
      {
        "surname": "Yang",
        "given_name": "Wankou"
      }
    ]
  },
  {
    "title": "A comparative study between single and multi-frame anomaly detection and localization in recorded video streams",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103232",
    "abstract": "Video anomaly detection is usually studied by considering the spatial and temporal contexts. This paper focuses first on spatial context and shows that it can be a fast real-time solution. In the first part of this work there are two main contributions: employing a new deep network for reconstruction and introducing a new regularity scoring function. The new deep architecture is based on pyramid of input images and compared to UNet, the proposed architecture boosts AUC by 15% and the new regularity scoring function is based on SSIM. The second part employs a multiframe approach to distinguish temporal behavior anomalies. The second approach enhances the results by 7% compared to spatial anomaly detection. Comparing the two approaches, if computing power is limited and real time anomaly detection is looked for, single frame detection is preferred while multi frame analysis offers a much wider possibility of anomaly detection.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001528",
    "keywords": [
      "Anomaly (physics)",
      "Anomaly detection",
      "Archaeology",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Condensed matter physics",
      "Context (archaeology)",
      "Data mining",
      "Evolutionary biology",
      "Frame (networking)",
      "Function (biology)",
      "Geography",
      "Geometry",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Pyramid (geometry)",
      "Spatial contextual awareness",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Bahrami",
        "given_name": "Maedeh"
      },
      {
        "surname": "Pourahmadi",
        "given_name": "Majid"
      },
      {
        "surname": "Vafaei",
        "given_name": "Abbas"
      },
      {
        "surname": "Shayesteh",
        "given_name": "Mohammad Reza"
      }
    ]
  },
  {
    "title": "A salient object detection framework using linear quadratic regulator controller",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103259",
    "abstract": "In this paper, a novel salient object detection framework based on Linear Quadratic Regulator (LQR) controller is proposed. The major goal of this research is to take advantage of optimal control theory for improving the performance of detecting salient objects in images. In this regard, for the sake of detection of salient and non-salient regions, two LQR-based control systems are employed. In the proposed framework, for the initialization of the control systems, background and foreground estimations have been done with two different strategies. Doing so, we would ultimately have more effective distinction between those regions. After the initialization step, the control systems refine both estimations in parallel until reaching a steady state for each of them. Within the mentioned process, by using optimal control concept, specifically LQR controller (for the first time in the field), control signals which are in charge of determining saliency values, would be constantly optimized. At the end, the raw saliency map will be generated by combination of background and foreground optimized initial maps. Finally, the integrated saliency map will be refined by using angular embedding method. The experimental evaluations on three benchmark datasets shows that the proposed framework performs well and introduces comparable results with some deep learning based methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001693",
    "keywords": [
      "Agronomy",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Computer science",
      "Control (management)",
      "Control theory (sociology)",
      "Controller (irrigation)",
      "Embedding",
      "Geodesy",
      "Geography",
      "Initialization",
      "Linear-quadratic regulator",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Optimal control",
      "Process (computing)",
      "Programming language",
      "Salient"
    ],
    "authors": [
      {
        "surname": "Moradi",
        "given_name": "Morteza"
      },
      {
        "surname": "Bayat",
        "given_name": "Farhad"
      },
      {
        "surname": "Charmi",
        "given_name": "Mostafa"
      }
    ]
  },
  {
    "title": "Stereoscopic image quality assessment considering visual mechanism and multi-loss constraints",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103255",
    "abstract": "In this paper, a convolutional neural network (CNN) with multi-loss constraints is designed for stereoscopic image quality assessment (SIQA). A stereoscopic image not only contains monocular information, but also provides binocular information which is as identically crucial as the former. So we take the image patches of left-view images, right-view images and the difference images as the inputs of the network to utilize monocular information and binocular information. Moreover, we propose a method to obtain proxy label of each image patch. It preserves the quality difference between different regions and views. In addition, the multiple loss functions with adaptive loss weights are introduced in the network, which consider both local features and global features and constrain the feature learning from multiple perspectives. And the adaptive loss weights also make the multi-loss CNN more flexible. The experimental results on four public SIQA databases show that the proposed method is superior to other existing SIQA methods with state-of-the-art performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001668",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Image quality",
      "Information loss",
      "Linguistics",
      "Monocular",
      "Philosophy",
      "Stereoscopy"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Sumei"
      },
      {
        "surname": "Li",
        "given_name": "Yueyang"
      },
      {
        "surname": "Han",
        "given_name": "Yongtian"
      }
    ]
  },
  {
    "title": "Recognition of user-dependent and independent static hand gestures: Application to sign language",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103193",
    "abstract": "Static hand gesture (HG) recognition for both user-dependent and user-independent is a challenging problem, especially when there are changes in lighting, hand position, and background, the recognition becomes more complex. To solve this problem, this paper proposes a static hand gesture recognition based on a set of image descriptors: Gradient Local Auto-Correlation (GLAC), Gabor Wavelet Transform (GWT), and Fast Discrete Curve Transform (FDCT). Principal Component Analysis (PCA) was used to reduce dimensionality. Tests were performed on three sign language datasets and one hand posture dataset using neural network classifiers, K-Nearest Neighbor (KNN) classifiers, and combined classifiers. The results obtained were compared to the state of the art and show an accuracy of 100% for user-independent and 98.33% for user-dependent gestures, despite the difficult acquisition conditions of the datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001231",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Curse of dimensionality",
      "Economics",
      "Finance",
      "Gesture",
      "Gesture recognition",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Position (finance)",
      "Principal component analysis",
      "Programming language",
      "Set (abstract data type)",
      "Sign language",
      "Speech recognition",
      "k-nearest neighbors algorithm"
    ],
    "authors": [
      {
        "surname": "Sadeddine",
        "given_name": "Khadidja"
      },
      {
        "surname": "Chelali",
        "given_name": "Fatma Zohra"
      },
      {
        "surname": "Djeradi",
        "given_name": "Rachida"
      },
      {
        "surname": "Djeradi",
        "given_name": "Amar"
      },
      {
        "surname": "Benabderrahmane",
        "given_name": "Sidahmed"
      }
    ]
  },
  {
    "title": "Component-based metric learning for fully automatic kinship verification",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103265",
    "abstract": "This paper introduces a fully automatic method for kinship verification from facial images. Recently, a number of methods have been proposed to verify kinship from facial images, however, most of these methods are needed to exactly align face images before feature extraction in a manual manner. Unlike these methods, our method does not depend on face alignment. Firstly, we localize several facial feature points by utilizing a facial feature detector to extract SIFT descriptor around each feature point of a face image. Lastly, two ways, feature combination and distance metric learning, are used to verify the kinship of a pair of face images. For feature combination, three simple strategies of feature combination and support vector machine classifier are used for kinship verification. For metric learning, we propose a component-based metric learning (CML) method to measure the distance of each face pair, which jointly learns multiple local distance metrics, and one specific distance metric for each facial feature point. Experimental results show the effectiveness of our proposed approach on two popular kinship datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001735",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Component (thermodynamics)",
      "Computer science",
      "Computer vision",
      "Economics",
      "Face (sociological concept)",
      "Feature (linguistics)",
      "Feature extraction",
      "Feature vector",
      "Kinship",
      "Law",
      "Linguistics",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Political science",
      "Scale-invariant feature transform",
      "Social science",
      "Sociology",
      "Thermodynamics"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Huishan"
      },
      {
        "surname": "Chen",
        "given_name": "Jiawei"
      },
      {
        "surname": "Liu",
        "given_name": "Xiao"
      },
      {
        "surname": "Hu",
        "given_name": "Junlin"
      }
    ]
  },
  {
    "title": "WavNet — Visual saliency detection using Discrete Wavelet Convolutional Neural Network",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103236",
    "abstract": "In the recent advancements in image and video analysis, the detection of salient regions in the image becomes the initial step. This plays a crucial role in deciding the performance of such algorithms. In this work, a Multi-Resolution Feature Extraction (MRFE) technique that makes use of Discrete Wavelet Convolutional Neural Network (DWCNN) for generating features is employed. An Enhanced Feature Extraction (EFE) module extracts additional features from the high level features of the DWCNN, which are used to frame both channel as well as spatial attention models for yielding contextual attention maps. A new hybrid loss function is also proposed, which is a combination of Balanced Cross Entropy (BCE) loss and Edge based Structural Similarity (ESSIM) loss that effectively identifies and segments the salient regions with clear boundaries. The method is tested exhaustively with five different benchmark datasets and is proved superior to the existing state-of-the-art methods with a minimum Mean Absolute error (MAE) of 0.03 and F-measure of 0.956.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001541",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Convolutional neural network",
      "Entropy (arrow of time)",
      "Feature (linguistics)",
      "Feature extraction",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Salient",
      "Wavelet"
    ],
    "authors": [
      {
        "surname": "Sasibhooshan",
        "given_name": "Reshmi"
      },
      {
        "surname": "Kumaraswamy",
        "given_name": "Suresh"
      },
      {
        "surname": "Sasidharan",
        "given_name": "Santhoshkumar"
      }
    ]
  },
  {
    "title": "Learning discriminative motion feature for enhancing multi-modal action recognition",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103263",
    "abstract": "Video action recognition is an important topic in computer vision tasks. Most of the existing methods use CNN-based models, and multiple modalities of image features are captured from the videos, such as static frames, dynamic images, and optical flow features. However, these mainstream features contain much static information including object and background information, where the motion information of the action itself is not distinguished and strengthened. In this work, a new kind of motion feature is proposed without static information for video action recognition. We propose a quantization of motion network based on the bag-of-feature method to learn significant and discriminative motion features. In the learned feature map, the object and background information is filtered out, even if the background is moving in the video. Therefore, the motion feature is complementary to the static image feature and the static information in the dynamic image and optical flow. A multi-stream classifier is built with the proposed motion feature and other features, and the performance of action recognition is enhanced comparing to other state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001723",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Discriminative model",
      "Feature (linguistics)",
      "Feature extraction",
      "Image (mathematics)",
      "Linguistics",
      "Motion (physics)",
      "Optical flow",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Jianyu"
      },
      {
        "surname": "Huang",
        "given_name": "Yao"
      },
      {
        "surname": "Shao",
        "given_name": "Zhanpeng"
      },
      {
        "surname": "Liu",
        "given_name": "Chunping"
      }
    ]
  },
  {
    "title": "Generative image inpainting with salient prior and relative total variation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103231",
    "abstract": "Image inpainting is an important research direction of image processing. The generative adversarial network (GAN), which can reconstruct new reasonable content in the corrupted region, is the most interesting tool in current inpainting technologies. However, the previous deep methods generally need to be pre-added the binary mask representing the corruption location as the extra input. A novel inpainting algorithm which does not require additional external labels is proposed in this paper. The algorithm consists of two parts: corruption recognition module and content inpainting module, which can recognize and fill random corruption. In the recognizer, the salient object from the uncorrupted region is used as the prior for distinguishing corruption. In the inpainting module, a two-stage network is applied to reconstruct the image from coarse content to texture details. To avoid the misdetection in recognition which has a negative impact on the restoration in inpainting, we perform relative total variational filtering on the corrupted image, and use the salient map as the supervision of detail reconstruction. Qualitative and quantitative experiments on multiple datasets verify the effectiveness of our recognition module, the competitive advantage of our inpainting module, and the enlightening significance of our total algorithm in image inpainting.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001516",
    "keywords": [
      "Artificial intelligence",
      "Binary image",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Gene",
      "Generative grammar",
      "Image (mathematics)",
      "Image processing",
      "Image restoration",
      "Image texture",
      "Inpainting",
      "Pattern recognition (psychology)",
      "Robustness (evolution)",
      "Salient",
      "Texture synthesis"
    ],
    "authors": [
      {
        "surname": "Shao",
        "given_name": "Hang"
      },
      {
        "surname": "Wang",
        "given_name": "Yongxiong"
      }
    ]
  },
  {
    "title": "Deep image compression with multi-stage representation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103226",
    "abstract": "While deep learning-based image compression methods have shown impressive coding performance, most existing methods are still in the mire of two limitations: (1) unpredictable compression efficiency gain when adopting convolutional neural networks with different depths, and (2) lack of an accurate model to estimate the entropy during the training process. To address these two problems, in this paper, a deep multi-stage representation based image compression (MSRIC) method is proposed. Owing to this architecture, the detail information of shallow stages and the compact information of deep stages can be utilized for image reconstruction. Furthermore, a data-dependent channel-wised factorized probability model (DCFPM) is adopted to increase the accuracy of entropy estimation. Experimental results indicate that the proposed method guarantees better perceptual performance at a wide range of bit-rates. Also, ablation studies are carried out to validate the above mentioned technologies.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001498",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Data compression",
      "Deep learning",
      "Entropy (arrow of time)",
      "Image (mathematics)",
      "Image compression",
      "Image processing",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Zixi"
      },
      {
        "surname": "Ding",
        "given_name": "Guiguang"
      },
      {
        "surname": "Han",
        "given_name": "Jungong"
      },
      {
        "surname": "Li",
        "given_name": "Fan"
      }
    ]
  },
  {
    "title": "Superpixel alpha-expansion and normal adjustment for stereo matching",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103238",
    "abstract": "This paper presents a continuous stereo disparity estimation method based on superpixel segmentation and graph-cuts. We re-parameterize the disparity with a 3D tangent plane, and propose two algorithms to optimize the Markov Random Field (MRF) energy. The first algorithm, called superpixel α -expansion, is built on superpixel segmentation to localize the label proposal and the expansion scope. Three levels of superpixels with increasing granularity are generated for acceleration. The second algorithm, called normal adjustment, optimizes the 3D planes for the regions with low texture and/or illumination changes. The normal adjustment is performed along a depth-first similarity path of superpixels. We evaluate our method on the Middlebury 3.0 evaluation benchmark and the Eth3d benchmark. Experimental results show that our method achieves high accuracy on both evaluation benchmarks. (Middlebury 3.0 evaluation benchmark: http://vision.middlebury.edu/stereo/eval3/. Eth3d benchmark: https://www.eth3d.net/low_res_two_view.)",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001565",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Cut",
      "Geodesy",
      "Geography",
      "Image segmentation",
      "Markov random field",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Ji",
        "given_name": "Penglei"
      },
      {
        "surname": "Li",
        "given_name": "Jie"
      },
      {
        "surname": "Li",
        "given_name": "Hanchao"
      },
      {
        "surname": "Liu",
        "given_name": "Xinguo"
      }
    ]
  },
  {
    "title": "Semantic meaning modulates object importance in human fixation prediction",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103206",
    "abstract": "Humans tend to allocate attention to semantic entities. Objects are important in fixation selection, but not all the objects are equally attractive. In this paper, we introduce the concept of attribute bias to characterize the influence of semantic attributes compared with low-level saliency on fixation distribution. Two different ways are adopted to get two sets of semantic attributes. In both cases, most semantic attributes have a positive influence on drawing attention and contribute more than low-level saliency in object areas. We also find that attribute bias is robust to low-level saliency and can consistently reflect the relative attractiveness of objects with different semantic attributes. It is demonstrated that such bias helps make better fixation predictions by distinguishing the importance of objects, although low-level saliency models with better performance are less dramatically improved by attribute bias. These findings indicate the role of conceptual meaning as opposed to features in visual attention.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001322",
    "keywords": [
      "Artificial intelligence",
      "Attractiveness",
      "Cognitive psychology",
      "Computer science",
      "Demography",
      "Fixation (population genetics)",
      "Meaning (existential)",
      "Natural language processing",
      "Object (grammar)",
      "Population",
      "Psychoanalysis",
      "Psychology",
      "Psychotherapist",
      "Selection (genetic algorithm)",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Aoqi"
      },
      {
        "surname": "Chen",
        "given_name": "Zhenzhong"
      }
    ]
  },
  {
    "title": "Dynamic texture analysis for detecting fake faces in video sequences",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103239",
    "abstract": "The creation of manipulated multimedia content involving human characters has reached in the last years unprecedented realism, calling for automated techniques to expose synthetically generated faces in images and videos. This work explores the analysis of spatio-temporal texture dynamics of the video signal, with the goal of characterizing and distinguishing real and fake sequences. We propose to build a binary decision on the joint analysis of multiple temporal segments and, in contrast to previous approaches, to exploit the textural dynamics of both the spatial and temporal dimensions. This is achieved through the use of Local Derivative Patterns on Three Orthogonal Planes (LDP-TOP), a compact feature representation known to be an important asset for the detection of face spoofing attacks. Experimental analyses on state-of-the-art datasets of manipulated videos show the discriminative power of such descriptors in separating real and fake sequences, and also identifying the creation method used. Linear Support Vector Machines (SVMs) are used which, despite the lower complexity, yield comparable performance to previously proposed deep models for fake content detection.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001553",
    "keywords": [
      "Artificial intelligence",
      "Computer network",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Discriminative model",
      "Exploit",
      "Face (sociological concept)",
      "Feature (linguistics)",
      "Histogram",
      "Image (mathematics)",
      "Law",
      "Linguistics",
      "Local binary patterns",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Social science",
      "Sociology",
      "Spoofing attack",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Bonomi",
        "given_name": "Mattia"
      },
      {
        "surname": "Pasquini",
        "given_name": "Cecilia"
      },
      {
        "surname": "Boato",
        "given_name": "Giulia"
      }
    ]
  },
  {
    "title": "Learning discriminative motion feature for enhancing multi-modal action recognition",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103263",
    "abstract": "Video action recognition is an important topic in computer vision tasks. Most of the existing methods use CNN-based models, and multiple modalities of image features are captured from the videos, such as static frames, dynamic images, and optical flow features. However, these mainstream features contain much static information including object and background information, where the motion information of the action itself is not distinguished and strengthened. In this work, a new kind of motion feature is proposed without static information for video action recognition. We propose a quantization of motion network based on the bag-of-feature method to learn significant and discriminative motion features. In the learned feature map, the object and background information is filtered out, even if the background is moving in the video. Therefore, the motion feature is complementary to the static image feature and the static information in the dynamic image and optical flow. A multi-stream classifier is built with the proposed motion feature and other features, and the performance of action recognition is enhanced comparing to other state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001723",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Discriminative model",
      "Feature (linguistics)",
      "Feature extraction",
      "Image (mathematics)",
      "Linguistics",
      "Motion (physics)",
      "Optical flow",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Jianyu"
      },
      {
        "surname": "Huang",
        "given_name": "Yao"
      },
      {
        "surname": "Shao",
        "given_name": "Zhanpeng"
      },
      {
        "surname": "Liu",
        "given_name": "Chunping"
      }
    ]
  },
  {
    "title": "Generative image inpainting with salient prior and relative total variation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103231",
    "abstract": "Image inpainting is an important research direction of image processing. The generative adversarial network (GAN), which can reconstruct new reasonable content in the corrupted region, is the most interesting tool in current inpainting technologies. However, the previous deep methods generally need to be pre-added the binary mask representing the corruption location as the extra input. A novel inpainting algorithm which does not require additional external labels is proposed in this paper. The algorithm consists of two parts: corruption recognition module and content inpainting module, which can recognize and fill random corruption. In the recognizer, the salient object from the uncorrupted region is used as the prior for distinguishing corruption. In the inpainting module, a two-stage network is applied to reconstruct the image from coarse content to texture details. To avoid the misdetection in recognition which has a negative impact on the restoration in inpainting, we perform relative total variational filtering on the corrupted image, and use the salient map as the supervision of detail reconstruction. Qualitative and quantitative experiments on multiple datasets verify the effectiveness of our recognition module, the competitive advantage of our inpainting module, and the enlightening significance of our total algorithm in image inpainting.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001516",
    "keywords": [
      "Artificial intelligence",
      "Binary image",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Gene",
      "Generative grammar",
      "Image (mathematics)",
      "Image processing",
      "Image restoration",
      "Image texture",
      "Inpainting",
      "Pattern recognition (psychology)",
      "Robustness (evolution)",
      "Salient",
      "Texture synthesis"
    ],
    "authors": [
      {
        "surname": "Shao",
        "given_name": "Hang"
      },
      {
        "surname": "Wang",
        "given_name": "Yongxiong"
      }
    ]
  },
  {
    "title": "A CNN model for real time hand pose estimation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103200",
    "abstract": "Recently convolutional neural networks (CNNs) have been employed to address the problem of hand pose estimation. In this work, we introduce an end-to-end deep architecture that can accurately estimate hand pose through the joint use of model-based and fine-tuning methods. In the model-based stage, we make use of the prior information in hand model geometry to ensure the geometric validity of the estimated poses. Next, we introduce a fine-tuning approach that learns to refine the errors between the model and observed hand. Our approach is validated on three challenging public datasets and achieves state-of-the-art performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001279",
    "keywords": [
      "Architectural engineering",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Engineering",
      "Joint (building)",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Pose"
    ],
    "authors": [
      {
        "surname": "Ding",
        "given_name": "Lu"
      },
      {
        "surname": "Wang",
        "given_name": "Yong"
      },
      {
        "surname": "Laganière",
        "given_name": "Robert"
      },
      {
        "surname": "Huang",
        "given_name": "Dan"
      },
      {
        "surname": "Fu",
        "given_name": "Shan"
      }
    ]
  },
  {
    "title": "LRGAN: Visual anomaly detection using GAN with locality-preferred recoding",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103201",
    "abstract": "Deep neural networks, including deep auto-encoder (DAE) and generative adversarial networks (GAN), have been extensively applied for visual anomaly detection. These models generally assume that reconstruction errors should be lower for normal samples but higher for anomalies. However, it has been found that DAE based models can sometimes reconstruct anomalies very well and thus result in false alarms or misdetections. To address this problem, we propose a model using GAN with locality-preferred recoding, named LRGAN. LRGAN is inspired by the observation that both normal and abnormal samples are not completely scattered throughout the latent space but clustered separately at some local regions. Therefore, a locality-preferred recoding (LR) module is designed to compulsively represent the latent vectors of anomalies by normal ones. As a result, reconstructions of anomalies will approximate to normal samples and corresponding residuals can thus be enlarged. To partly avoid latent vectors of normal samples being recoded, we further present an improved model using GAN with an adaptive LR (ALR) module, named LRGAN+. ALR applies the clustering algorithm to generate a more compact codebook; more importantly, it helps LRGAN + automatically skip the LR module for possible normal samples with a threshold strategy. Our proposed method is evaluated on two public datasets (i.e., MNIST and CIFAR-10) and one real-world industrial dataset (i.e., Fasteners), considering both one-class and multi-class anomaly detection protocols. Experimental results demonstrate that LRGAN is comparable with state-of-the-art methods and LRGAN + outperforms these methods on all datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001280",
    "keywords": [
      "Algorithm",
      "Anomaly (physics)",
      "Anomaly detection",
      "Artificial intelligence",
      "Artificial neural network",
      "Autoencoder",
      "Class (philosophy)",
      "Cluster analysis",
      "Codebook",
      "Computer science",
      "Condensed matter physics",
      "Linguistics",
      "Locality",
      "MNIST database",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Jianzhu"
      },
      {
        "surname": "Huang",
        "given_name": "Wei"
      },
      {
        "surname": "Wang",
        "given_name": "Shengchun"
      },
      {
        "surname": "Dai",
        "given_name": "Peng"
      },
      {
        "surname": "Li",
        "given_name": "Qingyong"
      }
    ]
  },
  {
    "title": "Instance search via instance level segmentation and feature representation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103253",
    "abstract": "Instance search is an interesting task as well as a challenging issue due to the lack of effective feature representation. In this paper, an instance level feature representation built upon fully convolutional instance-aware segmentation is proposed. The feature is ROI-pooled from the segmented instance region. So that instances in various sizes and layouts are represented by deep features in uniform length. This representation is further enhanced by the use of deformable ResNeXt blocks. Superior performance is observed in terms of its distinctiveness and scalability on a challenging evaluation dataset built by ourselves. In addition, the proposed enhancement on the network structure also shows superior performance on the instance segmentation task.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001656",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Database",
      "Economics",
      "Feature (linguistics)",
      "Feature learning",
      "Law",
      "Linguistics",
      "Management",
      "Optimal distinctiveness theory",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Psychology",
      "Psychotherapist",
      "Representation (politics)",
      "Scalability",
      "Segmentation",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Zhan",
        "given_name": "Yu"
      },
      {
        "surname": "Zhao",
        "given_name": "Wan-Lei"
      }
    ]
  },
  {
    "title": "Wasserstein distance feature alignment learning for 2D image-based 3D model retrieval",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103197",
    "abstract": "2D image-based 3D model retrieval has become a hotspot topic in recent years. However, the current existing methods are limited by two aspects. Firstly, they are mostly based on the supervised learning, which limits their application because of the high time and cost consuming of manual annotation. Secondly, the mainstream methods narrow the discrepancy between 2D and 3D domains mainly by the image-level alignment, which may bring the additional noise during the image transformation and influence cross-domain effect. Consequently, we propose a Wasserstein distance feature alignment learning (WDFAL) for this retrieval task. First of all, we describe 3D models through a series of virtual views and use CNNs to extract features. Secondly, we design a domain critic network based on the Wasserstein distance to narrow the discrepancy between two domains. Compared to the image-level alignment, we reduce the domain gap by the feature-level distribution alignment to avoid introducing additional noise. Finally, we extract the visual features from 2D and 3D domains, and calculate their similarity by utilizing Euclidean distance. The extensive experiments can validate the superiority of the WDFAL method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001255",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Domain (mathematical analysis)",
      "Euclidean distance",
      "Feature (linguistics)",
      "Feature vector",
      "Image (mathematics)",
      "Image retrieval",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Yaqian"
      },
      {
        "surname": "Liu",
        "given_name": "Yu"
      },
      {
        "surname": "Zhou",
        "given_name": "Heyu"
      },
      {
        "surname": "Li",
        "given_name": "Wenhui"
      }
    ]
  },
  {
    "title": "Learn from the past – sequentially one-to-one video deblurring network",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103159",
    "abstract": "With the growing availability of hand-held cameras in recent years, more and more images and videos are taken at any time and any place. However, they usually suffer from undesirable blur due to camera shake or object motion in the scene. In recent years, a few modern video deblurring methods are proposed and achieve impressive performance. However, they are still not suitable for practical applications as high computational cost or using future information as input. To address the issues, we propose a sequentially one-to-one video deblurring network (SOON) which can deblur effectively without any future information. It transfers both spatial and temporal information to the next frame by utilizing the recurrent architecture. In addition, we design a novel Spatio-Temporal Attention module to nudge the network to focus on the meaningful and essential features in the past. Extensive experiments demonstrate that the proposed method outperforms the state-of-the-art deblurring methods, both quantitatively and qualitatively, on various challenging real-world deblurring datasets. Moreover, as our method deblurs in an online manner and is potentially real-time, it is more suitable for practical applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001012",
    "keywords": [
      "Artificial intelligence",
      "Astronomy",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Deblurring",
      "Focus (optics)",
      "Frame (networking)",
      "Image (mathematics)",
      "Image processing",
      "Image restoration",
      "Motion blur",
      "Network architecture",
      "Object (grammar)",
      "Optics",
      "Physics",
      "Shake",
      "Telecommunications",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Liang",
        "given_name": "Chih-Hung"
      },
      {
        "surname": "Su",
        "given_name": "Hung-Ting"
      },
      {
        "surname": "Hsu",
        "given_name": "Winston H."
      }
    ]
  },
  {
    "title": "EdgeGAN: One-way mapping generative adversarial network based on the edge information for unpaired training set",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103187",
    "abstract": "Image conversion has attracted mounting attention due to its practical applications. This paper proposes a lightweight network structure that can implement unpaired training sets to complete one-way image mapping, based on the generative adversarial network (GAN) and a fixed-parameter edge detection convolution kernel. Compared with the cycle consistent adversarial network (CycleGAN), the proposed network features simpler structure, fewer parameters (only 37.48% of the parameters in CycleGAN), and less training cost (only 35.47% of the GPU memory usage and 17.67% of the single iteration time in CycleGAN). Remarkably, the cyclic consistency becomes not mandatory for ensuring the consistency of the content before and after image mapping. This network has achieved significant processing effects in some image translation tasks, and its effectiveness and validity have been well demonstrated through typical experiments. In the quantitative classification evaluation based on VGG-16, the algorithm proposed in this paper has achieved superior performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032100119X",
    "keywords": [
      "Adversarial system",
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Combinatorics",
      "Computer science",
      "Consistency (knowledge bases)",
      "Convolution (computer science)",
      "Data mining",
      "Enhanced Data Rates for GSM Evolution",
      "Gene",
      "Generative adversarial network",
      "Image (mathematics)",
      "Image translation",
      "Kernel (algebra)",
      "Mathematics",
      "Messenger RNA",
      "Pattern recognition (psychology)",
      "Programming language",
      "Set (abstract data type)",
      "Translation (biology)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Yijie"
      },
      {
        "surname": "Liang",
        "given_name": "Qiaokang"
      },
      {
        "surname": "Li",
        "given_name": "Zhengwei"
      },
      {
        "surname": "Lei",
        "given_name": "Youcheng"
      },
      {
        "surname": "Sun",
        "given_name": "Wei"
      },
      {
        "surname": "Wang",
        "given_name": "Yaonan"
      },
      {
        "surname": "Zhang",
        "given_name": "Dan"
      }
    ]
  },
  {
    "title": "A fast algorithm based on gray level co-occurrence matrix and Gabor feature for HEVC screen content coding",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103128",
    "abstract": "To reduce the computational complexity of screen content video coding (SCC), a fast algorithm based on gray level co-occurrence matrix and Gabor feature model for HEVC-SCC, denoted as GGM, is proposed in this paper. By studying the correlation of non-zero number in gray level co-occurrence matrix with different partitioning depth, the coding unit (CU) size of intra coding can be prejudged, which selectively skips the intra prediction process of CU in other depth. With Gabor filter, the edge information reflecting the features of screen content images to the human visual system (HVS) are extracted. According to Gabor feature, CUs are classified into natural content CUs (NCCUs), smooth screen content CUs (SSCUs) and complex screen content CUs (CSCUs), with which, the calculation and judgment of unnecessary intra prediction modes are skipped. Under all-intra (AI) configuration, experimental results show that the proposed algorithm GGM can achieve encoding time saving by 42.13% compared with SCM-8.3, and with only 1.85% bit-rate increasement.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000821",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Coding (social sciences)",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Gray level",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Jing"
      },
      {
        "surname": "Ou",
        "given_name": "Jianshan"
      },
      {
        "surname": "Zeng",
        "given_name": "Huanqiang"
      },
      {
        "surname": "Cai",
        "given_name": "Canhui"
      }
    ]
  },
  {
    "title": "Single image deblurring with cross-layer feature fusion and consecutive attention",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103149",
    "abstract": "Single image deblurring aims to restore the single blurry image to its sharp counterpart and remains an active topic of enduring interest. Recently, deep Convolutional Neural Network (CNN) based methods have achieved promising performance. However, two primary limitations mainly exist on those CNNs-based image deblurring methods: most of them simply focus on increasing the complexity of the network, and rarely make full use of features extracted by encoder. Meanwhile, most of the methods perform the deblurred image reconstruction immediately after the decoder, and the roles of the decoded features are always underestimated. To address these issues, we propose a single image deblurring method, in which two modules to fuse multiple features learned in encoder (the Cross-layer Feature Fusion (CFF) module) and manipulate the features after decoder (the Consecutive Attention Module (CAM)) are specially designed, respectively. The CFF module is to concatenate different layers of features from encoder to enhance rich structural information to decoder, and the CAM module is able to generate more important and correlated textures to the reconstructed sharp image. Besides, the ranking content loss is employed to further restore more realistic details in the deblurred images. Comprehensive experiments demonstrate that our proposed method can generate less blur and more textures in deblurred image on both synthetic datasets and real-world image examples.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000924",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deblurring",
      "Electrical engineering",
      "Encoder",
      "Engineering",
      "Feature (linguistics)",
      "Focus (optics)",
      "Fuse (electrical)",
      "Image (mathematics)",
      "Image processing",
      "Image restoration",
      "Linguistics",
      "Operating system",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Yaowei"
      },
      {
        "surname": "Luo",
        "given_name": "Ye"
      },
      {
        "surname": "Zhang",
        "given_name": "Guokai"
      },
      {
        "surname": "Lu",
        "given_name": "Jianwei"
      }
    ]
  },
  {
    "title": "Classifier aided training for semantic segmentation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103177",
    "abstract": "Semantic segmentation is a prominent problem in scene understanding expressed as a dense labeling task with deep learning models being one of the main methods to solve it. Traditional training algorithms for semantic segmentation models produce less than satisfactory results when not combined with post-processing techniques such as CRFs. In this paper, we propose a method to train segmentation models using an approach which utilizes classification information in the training process of the segmentation network. Our method employs the use of classification network that detects the presence of classes in the segmented output. These class scores are then used to train the segmentation model. This method is motivated by the fact that by conditioning the training of the segmentation model with these scores, higher order features can be captured. Our experiments show significantly improved performance of the segmentation model on the CamVid and CityScapes datasets with no additional post processing.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001127",
    "keywords": [
      "Artificial intelligence",
      "CRFS",
      "Classifier (UML)",
      "Computer science",
      "Conditional random field",
      "Economics",
      "Image segmentation",
      "Machine learning",
      "Management",
      "Pattern recognition (psychology)",
      "Scale-space segmentation",
      "Segmentation",
      "Segmentation-based object categorization",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Ahmed",
        "given_name": "Ifham Abdul Latheef"
      },
      {
        "surname": "Jaward",
        "given_name": "Mohamed Hisham"
      }
    ]
  },
  {
    "title": "Disocclusion filling for depth-based view synthesis with adaptive utilization of temporal correlations",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103148",
    "abstract": "The depth image-based rendering paves the path to success of 3-D video. However, one issue still remained in 3-D video is how to fill the disocclusion areas. To this end, Gaussian mixture model (GMM) is commonly employed to generate the background, and then to fill the holes. Nevertheless, GMM usually has poor performance for sequences with big foreground reciprocation. In this paper, we aim to enhance the synthesis performance. Firstly, we propose an expectation maximization based GMM background generation method, in which the pixel mixture distribution is derived. Secondly, we propose a refined foreground depth correlation approach, which recovers the background frame-by-frame based on depth information. Finally, we adaptively choose the background pixels from these two methods for filling. Experimental results show that the proposed method outperforms existing non-deep learning based hole filling methods by around 1.1 dB, and significantly surpasses deep learning based alternative in terms of subjective quality.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000912",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Correlation",
      "Deep learning",
      "Frame (networking)",
      "Gaussian",
      "Geometry",
      "Image (mathematics)",
      "Mathematics",
      "Mixture model",
      "Pattern recognition (psychology)",
      "Physics",
      "Pixel",
      "Quantum mechanics",
      "Rendering (computer graphics)",
      "Saliency map",
      "Telecommunications",
      "View synthesis"
    ],
    "authors": [
      {
        "surname": "Gao",
        "given_name": "Pan"
      },
      {
        "surname": "Zhu",
        "given_name": "Tiantian"
      },
      {
        "surname": "Paul",
        "given_name": "Manoranjan"
      }
    ]
  },
  {
    "title": "Who will receive the ball? Predicting pass recipient in soccer videos",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103190",
    "abstract": "The game of soccer involves an act of one team trying to score a goal against the other. During the game, defending players constantly try to predict the pass of the attacking player to prevent a goal. So, pass prediction is an important facet to anticipate the game strategy of participating teams. Here we present a probabilistic framework for pass prediction. Aberrating the state-of-the-art notion of mutually independent decision models, the proposed framework predicts pass recipients by integrating two dependent models, designed from the coordinates of the players in abstract top-view visualization. To evaluate the real time efficacy of the proposed pass prediction framework, a soccer data set has been introduced. The proposed pass prediction algorithm is compared against recent methods and the ground truth available in the soccer data set. The proposed method outperforms the existing approaches by a noticeable margin.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001206",
    "keywords": [
      "Artificial intelligence",
      "Ball (mathematics)",
      "Computer science",
      "Data mining",
      "Machine learning",
      "Margin (machine learning)",
      "Mathematical analysis",
      "Mathematics",
      "Probabilistic logic",
      "Programming language",
      "Set (abstract data type)",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Sanyal",
        "given_name": "Samriddha"
      }
    ]
  },
  {
    "title": "Subpixel rendering for diamond-shaped PenTile displays using patch-based adaptive filters",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103144",
    "abstract": "We propose a novel subpixel rendering algorithm for diamond-shaped PenTile displays, which reduces color distortions while improving apparent resolutions. We develop two types of subpixel rendering filters: main filter and color distortion reduction (CDR) filters. To derive the filters, we formulate a quadratic program to minimize the difference between an original input image and a virtual image that the human visual system perceives. By imposing two constraints for filter size and coefficients, we obtain the main filter, which has a suitable size and is normalized. Then, we design the CDR filters based on the analysis of various patch patterns for image areas. We define the patch patterns to classify local areas with possible color distortions. By imposing additional constraints according to the patch patterns, we derive the CDR filters. Lastly, by matching local areas in the input image into the pre-defined patch patterns, we render the image using the main filter and the CDR filters, which are applied adaptively to the local areas. Experimental results demonstrate that the proposed subpixel rendering algorithm improves apparent resolutions and suppresses color distortions effectively, thereby outperforming conventional algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000900",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Chemistry",
      "Color filter array",
      "Color gel",
      "Computer science",
      "Computer vision",
      "Filter (signal processing)",
      "Layer (electronics)",
      "Organic chemistry",
      "Pixel",
      "Rendering (computer graphics)",
      "Subpixel rendering",
      "Thin-film transistor"
    ],
    "authors": [
      {
        "surname": "Lee",
        "given_name": "Jae-Han"
      },
      {
        "surname": "Kim",
        "given_name": "Kyung-Rae"
      },
      {
        "surname": "Kim",
        "given_name": "Chang-Su"
      }
    ]
  },
  {
    "title": "A ghostfree contrast enhancement method for multiview images without depth information",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103175",
    "abstract": "High dynamic range (HDR) images greatly improve visual content quality, but pose challenges in processing, acquisition, and display. Images captured in real-world scenarios with multiple nonlinear cameras, extremely short unknown exposure time, and a shared light source present the additional challenges of incremental baseline and angle deviation amongst the cameras. The disparity maps in such conditions are not reliable; therefore, we propose a method that relies on the accurate detection and matching of feature points across adjacent viewpoints. We determine the exposure gain among the matched feature points in the involved views and design an image restoration method to restore multiview low dynamic range (MVLDR) images for each viewpoint. Finally, the fusion of these restored MVLDR images produces high-quality images for each viewpoint without capturing a series of bracketed exposure. Extensive experiments are conducted in controlled and uncontrolled conditions, and results prove that the proposed method competes for the state-of-the-arts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001115",
    "keywords": [
      "Artificial intelligence",
      "Composite material",
      "Computer science",
      "Computer vision",
      "Contrast (vision)",
      "Dynamic range",
      "Feature (linguistics)",
      "High dynamic range",
      "High-dynamic-range imaging",
      "Linguistics",
      "Matching (statistics)",
      "Materials science",
      "Mathematics",
      "Philosophy",
      "Range (aeronautics)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Khan",
        "given_name": "Rizwan"
      },
      {
        "surname": "Yang",
        "given_name": "You"
      },
      {
        "surname": "Liu",
        "given_name": "Qiong"
      },
      {
        "surname": "Qaisar",
        "given_name": "Zahid Hussain"
      }
    ]
  },
  {
    "title": "An improved Gamma correction model for image dehazing in a multi-exposure fusion framework",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103122",
    "abstract": "A usual problem encountered during bad weather conditions is the degraded image quality due to haze/fog. In basic Gamma correction method there is always an uncertainty regarding the choice of a particular exponential factor, which improves the quality of the input image because of the nonlinearity involved in the process. This issue has been solved in this study by proposing a modified Gamma correction method, in which the exponential correction factor is varied incrementally to generate images. We also propose the implementation of an automatic image selection criterion for fusion which helps chose images with varied and distinct features. The implementation of the multi-exposure fusion framework is done in the hue-saturation-value color space which has close resemblance with the human vision. The intensity channel of the selected images is fused in the gradient domain which captures minute details and takes an edge as compared to other conventional fusion based methods. The fused saturation channel is obtained by averaging fusion followed by enhancement using a non-linear sigmoid function. The hue channel of the input hazy image is left unprocessed to avoid color distortion. The experimental analysis demonstrates that the proposed method outperforms most of the single image dehazing methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000766",
    "keywords": [
      "Amplifier",
      "Artificial intelligence",
      "Artificial neural network",
      "Bandwidth (computing)",
      "Channel (broadcasting)",
      "Color correction",
      "Color space",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Distortion (music)",
      "Enhanced Data Rates for GSM Evolution",
      "Exponential function",
      "Fusion",
      "Gamma correction",
      "Hue",
      "Image (mathematics)",
      "Image fusion",
      "Image quality",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Philosophy",
      "Pixel",
      "Sigmoid function"
    ],
    "authors": [
      {
        "surname": "Kumar",
        "given_name": "Avishek"
      },
      {
        "surname": "Jha",
        "given_name": "Rajib Kumar"
      },
      {
        "surname": "Nishchal",
        "given_name": "Naveen K."
      }
    ]
  },
  {
    "title": "Perceptual hash-based coarse-to-fine grained image tampering forensics method",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103124",
    "abstract": "As an active forensic technology, perceptual image hash has important application in image content authenticity detection and integrity authentication. In this paper, we propose a hybrid-feature-based perceptual image hash method that can be used for image tampering detection and tampering localization. In the proposed method, we use the color features of image as global features, use point-based features and block-based features as local features, and combine with the structural features to generate intermediate hash code. Then we encrypt and randomize to generate the final hash code. Using this hash code, we present a coarse-to-fine grained forensics method for image tampering detection. The proposed method can realize object-level tampering localization. Abundant experimental results show that the proposed method is sensitive to content changes caused by malicious attacks, and the tampering localization precision achieves pixel level, and it is robust to a wide range of geometric distortions and content-preserving manipulations. Compared with the state-of-the-art schemes, the proposed scheme yields superior performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032100078X",
    "keywords": [
      "Artificial intelligence",
      "Authentication (law)",
      "Block (permutation group theory)",
      "Code (set theory)",
      "Composite material",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Feature (linguistics)",
      "Geometry",
      "Hash function",
      "Image (mathematics)",
      "Linguistics",
      "Materials science",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Programming language",
      "Range (aeronautics)",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Xiaofeng"
      },
      {
        "surname": "Zhang",
        "given_name": "Qian"
      },
      {
        "surname": "Jiang",
        "given_name": "Chuntao"
      },
      {
        "surname": "Xue",
        "given_name": "Jianru"
      }
    ]
  },
  {
    "title": "Harnessing high-level concepts, visual, and auditory features for violence detection in videos",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103174",
    "abstract": "In detecting sensitive media, violence is one of the hardest to define objectively, and thus, a significant challenge to detect automatically. While many studies were conducted in detecting aspects of violence, very few try to approach the general concept. We propose a method that aims to enable machines to understand a high-level concept of violence by first breaking it down into smaller, more objective ones, such as fights, explosions, blood, and gunshots, to combine them later, leading to a better understanding of the scene. For this, we leverage characteristics of each individual sub-concept of violence (relying upon custom-tailored convolutional neural networks) to guide how they should be described. A fight scene should incorporate temporal features that a scene with blood does not need to describe. A scene with explosions or gunshots should weigh more on its audio features. With this multimodal approach, we trained visual and auditory feature detectors and later combined them into a decision neural network to give us a violence detector that considers several different aspects of the problem. This robust and modular approach allows different cultures and users to adapt the detector to their specific needs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001073",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Detector",
      "Feature (linguistics)",
      "Feature extraction",
      "Human–computer interaction",
      "Leverage (statistics)",
      "Linguistics",
      "Machine learning",
      "Modular design",
      "Operating system",
      "Philosophy",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Peixoto",
        "given_name": "Bruno M."
      },
      {
        "surname": "Lavi",
        "given_name": "Bahram"
      },
      {
        "surname": "Dias",
        "given_name": "Zanoni"
      },
      {
        "surname": "Rocha",
        "given_name": "Anderson"
      }
    ]
  },
  {
    "title": "Attention-guided image captioning with adaptive global and local feature fusion",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103138",
    "abstract": "Although attention mechanisms are exploited widely in encoder-decoder neural network-based image captioning framework, the relation between the selection of salient image regions and the supervision of spatial information on local and global representation learning was overlooked, thereby degrading captioning performance. Consequently, we propose an image captioning scheme based on adaptive spatial information attention (ASIA), extracting a sequence of spatial information of salient objects in a local image region or an entire image. Specifically, in the encoding stage, we extract the object-level visual features of salient objects and their spatial bounding-box. We obtain the global feature maps of an entire image, which are fused with local features and the fused features are fed into the LSTM-based language decoder. In the decoding stage, our adaptive attention mechanism dynamically selects the corresponding image regions specified by an image description. Extensive experiments conducted on two datasets demonstrate the effectiveness of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000869",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Closed captioning",
      "Computer science",
      "Computer vision",
      "Decoding methods",
      "Encoder",
      "Encoding (memory)",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Linguistics",
      "Object (grammar)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Salient"
    ],
    "authors": [
      {
        "surname": "Zhong",
        "given_name": "Xian"
      },
      {
        "surname": "Nie",
        "given_name": "Guozhang"
      },
      {
        "surname": "Huang",
        "given_name": "Wenxin"
      },
      {
        "surname": "Liu",
        "given_name": "Wenxuan"
      },
      {
        "surname": "Ma",
        "given_name": "Bo"
      },
      {
        "surname": "Lin",
        "given_name": "Chia-Wen"
      }
    ]
  },
  {
    "title": "Combining Fields of Experts (FoE) and K-SVD methods in pursuing natural image priors",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103142",
    "abstract": "Natural image prior is one of the most efficient ways to represent images for computer vision tasks. In the literature, filter response statistics prior and synthesis-based sparse representation are two dominant prior models, which have been investigated separately and our knowledge of the relation between these two methods remains limited. In this paper, we examine the inherent relationship between the Fields of Experts (FoE) and K-SVD methods in the pursuit of natural image priors. We theoretically analyze and show that these two prior models have a mutually complementary relationship in the pursuit of the structure of natural images space. Based on these findings, a novel joint statistical prior is proposed, in which adaptive filters are obtained by exploring clues from both priors and utilized to characterize the subtle structure of natural images subspace. Qualitative and quantitative experiments demonstrate that the proposed method achieves a more comprehensive and reliable estimation of natural image prior and is competitive to both alternative and state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000882",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Bayesian probability",
      "Computer science",
      "Computer vision",
      "Data mining",
      "History",
      "Image (mathematics)",
      "Law",
      "Machine learning",
      "Mathematics",
      "Natural (archaeology)",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Prior probability",
      "Relation (database)",
      "Representation (politics)",
      "Singular value decomposition",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Jiang",
        "given_name": "Feng"
      },
      {
        "surname": "Chen",
        "given_name": "ZhiYuan"
      },
      {
        "surname": "Nazir",
        "given_name": "Amril"
      },
      {
        "surname": "Shi",
        "given_name": "WuZhen"
      },
      {
        "surname": "Lim",
        "given_name": "WeiXiang"
      },
      {
        "surname": "Liu",
        "given_name": "ShaoHui"
      },
      {
        "surname": "Rho",
        "given_name": "SeungMin"
      }
    ]
  },
  {
    "title": "Siamese CNN-based rank learning for quality assessment of inpainted images",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103176",
    "abstract": "Existing NR-IIQA (no reference-based inpainted image quality assessment) algorithms assess the quality of an inpainted image via artificially designed unnaturalness expression, which often fail to capture inpainted artifacts. This paper presents a new deep rank learning-based method for NR-IIQA. The model adopts a siamese deep architecture, which takes a pair of inpainted images as input and outputs their rank order. Each branch utilizes a CNN structure to capture the global structure coherence and a patch-wise coherence assessment module (PCAM) to depict the local color and texture consistency in an inpainted image. To train the deep model, we construct a new dataset, which contains thousands of pairs of inpainted images with ground-truth quality ranking labels. Rich ablation studies are conducted to verify the key modules of the proposed architecture. Comparative experimental results demonstrate that our method outperforms existing NR-IIQA metrics in evaluating both inpainted images and inpainting algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001139",
    "keywords": [
      "Artificial intelligence",
      "Coherence (philosophical gambling strategy)",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Deep learning",
      "Image (mathematics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Rank (graph theory)",
      "Ranking (information retrieval)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Meng",
        "given_name": "Xiangdong"
      },
      {
        "surname": "Ma",
        "given_name": "Wei"
      },
      {
        "surname": "Li",
        "given_name": "Chunhu"
      },
      {
        "surname": "Mi",
        "given_name": "Qing"
      }
    ]
  },
  {
    "title": "Cross-modal dynamic convolution for multi-modal emotion recognition",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103178",
    "abstract": "Understanding human emotions requires information from different modalities like vocal, visual, and verbal. Since human emotion is time-varying, the related information is usually represented as temporal sequences and we need to identify both emotion-related clues and their cross-modal interactions inside. However, emotion-related clues are sparse and misaligned in temporally unaligned sequences, making it hard for previous multi-modal emotion recognition methods to catch helpful cross-modal interactions. To this end, we present cross-modal dynamic convolution. To deal with sparsity, cross-modal dynamic convolution models the temporal dimension locally to avoid being overwhelmed by unrelated information. Cross-modal dynamic convolution is easy to stack, enabling it to model long-range cross-modal temporal interactions. Besides, models with cross-modal dynamic convolution are more stable during training than with cross-modal attention, bringing more possibilities in multi-modal sequential model designing. Extensive experiments show that our method can achieve competitive performance compared to previous works while being more efficient.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001085",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Chemistry",
      "Computer science",
      "Convolution (computer science)",
      "Dimension (graph theory)",
      "Mathematics",
      "Modal",
      "Modalities",
      "Polymer chemistry",
      "Pure mathematics",
      "Social science",
      "Sociology",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Wen",
        "given_name": "Huanglu"
      },
      {
        "surname": "You",
        "given_name": "Shaodi"
      },
      {
        "surname": "Fu",
        "given_name": "Ying"
      }
    ]
  },
  {
    "title": "Air-writing recognition using reverse time ordered stroke context",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103065",
    "abstract": "Air-writing is a new human and smart device communication approach, permits users to write inputs in a natural and relentless way. This touch-less way can prevent users fromvirus infection such as COVID-19. Compared with othermethods, air writing ismore challenging due to its unique characteristics such as redundant lifting strokes, multiplicity (different writing styles from various users), and confusion (different character types written in air are similar). Without the need of any starting trigger, a novel reverse time-ordered algorithm is proposed in this paper toefficiently filter out unnecessary lifting strokes, and thus simplifies the matching procedure. As to the second and third issues, a tiered arrangement structure is proposed by sampling the air-writing results with various sampling rates to solvethe multiplicity and confusion problems. Analyzed with other recently proposed air writing algorithms, the proposed approach reaches satisfactory recognition accuracy (above 94%) without any starting triggers.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000341",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Confusion",
      "Filter (signal processing)",
      "Psychoanalysis",
      "Psychology",
      "Sampling (signal processing)"
    ],
    "authors": [
      {
        "surname": "Tsai",
        "given_name": "Tsung-Hsien"
      },
      {
        "surname": "Hsieh",
        "given_name": "Jun-Wei"
      },
      {
        "surname": "Chang",
        "given_name": "Chuan-Wang"
      },
      {
        "surname": "Lay",
        "given_name": "Chin-Rong"
      },
      {
        "surname": "Fan",
        "given_name": "Kuo-Chin"
      }
    ]
  },
  {
    "title": "Generative detect for occlusion object based on occlusion generation and feature completing",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103189",
    "abstract": "Detecting the object with external occlusion has always been a hot topic in computer version, while its accuracy is always limited due to the loss of original object information and increase of new occlusion noise. In this paper, we propose a occluded object detection algorithm named GC-FRCN (Generative feature completing Faster RCNN), which consists of the OSGM (Occlusion Sample Generation Module) and OSIM (Occlusion Sample Inpainting Module). Specifically, the OSGM mines and discards the feature points with high category response on the feature map to enhance the richness of occlusion scenes in the training data set. OSIM learns an implicit mapping relationship from occluded feature map to real feature map adversarially, which aims at improving feature quality by repair the noisy object feature. Extensive experiments and ablation studies have been conducted on four different datasets. All the experiments demonstrate the GC-FRCN can effectively detect objects with local external occlusion and has good robustness for occlusion at different scales.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001218",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Cardiology",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Gene",
      "Image (mathematics)",
      "Inpainting",
      "Linguistics",
      "Medicine",
      "Object (grammar)",
      "Occlusion",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Can"
      },
      {
        "surname": "Lang",
        "given_name": "Wenxi"
      },
      {
        "surname": "Xin",
        "given_name": "Rui"
      },
      {
        "surname": "Mao",
        "given_name": "Kaichen"
      },
      {
        "surname": "Jiang",
        "given_name": "Haiyan"
      }
    ]
  },
  {
    "title": "Revisiting the Iterative Ant-tree for color quantization algorithm",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103180",
    "abstract": "The Iterative Ant-tree for color quantization algorithm has recently been proposed to reduce the colors of an image at a low computational cost. It is a clustering-based method that generates good images compared to several well-known color quantization methods. This article proposes the modification of two features of the original algorithm: the value assigned to the parameter associated with the algorithm and the order in which the pixels of the image are processed. As a result, the new variant of the algorithm generates better images than the original and the results are less sensitive to the value selected for the parameter.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001140",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Color image",
      "Color quantization",
      "Computer science",
      "Image (mathematics)",
      "Image processing",
      "Linde–Buzo–Gray algorithm",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pixel",
      "Quantization (signal processing)",
      "Tree (set theory)",
      "Vector quantization"
    ],
    "authors": [
      {
        "surname": "Pérez-Delgado",
        "given_name": "María-Luisa"
      }
    ]
  },
  {
    "title": "Air-writing recognition using reverse time ordered stroke context",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103065",
    "abstract": "Air-writing is a new human and smart device communication approach, permits users to write inputs in a natural and relentless way. This touch-less way can prevent users fromvirus infection such as COVID-19. Compared with othermethods, air writing ismore challenging due to its unique characteristics such as redundant lifting strokes, multiplicity (different writing styles from various users), and confusion (different character types written in air are similar). Without the need of any starting trigger, a novel reverse time-ordered algorithm is proposed in this paper toefficiently filter out unnecessary lifting strokes, and thus simplifies the matching procedure. As to the second and third issues, a tiered arrangement structure is proposed by sampling the air-writing results with various sampling rates to solvethe multiplicity and confusion problems. Analyzed with other recently proposed air writing algorithms, the proposed approach reaches satisfactory recognition accuracy (above 94%) without any starting triggers.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000341",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Confusion",
      "Filter (signal processing)",
      "Psychoanalysis",
      "Psychology",
      "Sampling (signal processing)"
    ],
    "authors": [
      {
        "surname": "Tsai",
        "given_name": "Tsung-Hsien"
      },
      {
        "surname": "Hsieh",
        "given_name": "Jun-Wei"
      },
      {
        "surname": "Chang",
        "given_name": "Chuan-Wang"
      },
      {
        "surname": "Lay",
        "given_name": "Chin-Rong"
      },
      {
        "surname": "Fan",
        "given_name": "Kuo-Chin"
      }
    ]
  },
  {
    "title": "A comprehensive survey on computer vision based concepts, methodologies, analysis and applications for automatic gun/knife detection",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103165",
    "abstract": "The ability to detect gun and gun held in hand or other body parts is a typical human skill. The same problem presents an imperative task for computer vision system. Automatic observer independent detection of hand held gun or gun held in the other body part, whether it is visible or concealed, provides enhance security in vulnerable places and initiates appropriate action there. Compare to the automatic object detection systems, automatic detection of gun has very few successful attempts. In the present scope of this paper, we present an extensive survey on automatic detection of gun and define a taxonomy for this particular detection system. We also describe the inherent difficulties related with this problem. In this survey of published papers, we examine different approaches used in state-of-the-art attempts and compare performances of these approaches. Finally, this paper concludes pointing to the possible research gaps in related fields.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032100105X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Engineering",
      "Environmental health",
      "Gun violence",
      "Human factors and ergonomics",
      "Human–computer interaction",
      "Medicine",
      "Object detection",
      "Observer (physics)",
      "Pattern recognition (psychology)",
      "Physics",
      "Poison control",
      "Programming language",
      "Quantum mechanics",
      "Scope (computer science)",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Debnath",
        "given_name": "Rajib"
      },
      {
        "surname": "Bhowmik",
        "given_name": "Mrinal Kanti"
      }
    ]
  },
  {
    "title": "Dual link distributed source coding scheme for the transmission of satellite hyperspectral imagery",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103117",
    "abstract": "Traditional lossless compression methods for satellite hyperspectral imagery focus on exploiting spatial and/or spectral redundancy. Those methods do not consider the temporal redundancy between images of the same area that are captured at different times. To exploit the temporal redundancy between hyperspectral images and reduce the amount of information to be transmitted from the space-satellite to the ground station via the downlink, this paper introduces a dual link distributed source coding (DLDSC) scheme for hyperspectral space-satellite communication. The proposed scheme employs the space-satellite dual link (i.e., the downlink and the uplink). The satellite onboard uses some side information from the ground station to calculate the hyperspectral image band coset values, and then, without syndrome coding, transmits to the ground station via the downlink. Coset coding is a typical technique used in distributed source coding (DSC), and here the coset values represent the timely hyperspectral image details. Typically, the coset values have lower entropy than that of the original source values. To exploit the temporal redundancy, the side information is computed in the ground station using the image captured at the previous time for the same area and transmitted to the space-satellite via the uplink. Hyperspectral images from the Hyperion satellite are used for the validation of the proposed scheme. The experimental results indicate that the proposed DLDSC scheme can reduce the original signal entropy by approximately 3.2 bits per sample (bps) and can achieve up to 1.0 bps and 1.6 bps gains over the lossless JPEG2000 standard and the state-of-art predictive CCSDS-123 method, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000730",
    "keywords": [
      "Aerospace engineering",
      "Algorithm",
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Coset",
      "Data compression",
      "Decoding methods",
      "Distributed source coding",
      "Engineering",
      "Entropy encoding",
      "Geography",
      "Hyperspectral imaging",
      "Image (mathematics)",
      "Image compression",
      "Image processing",
      "JPEG 2000",
      "Lossless compression",
      "Mathematics",
      "Operating system",
      "Redundancy (engineering)",
      "Remote sensing",
      "Satellite",
      "Telecommunications",
      "Telecommunications link",
      "Variable-length code"
    ],
    "authors": [
      {
        "surname": "Hagag",
        "given_name": "Ahmed"
      },
      {
        "surname": "Omara",
        "given_name": "Ibrahim"
      },
      {
        "surname": "Chaib",
        "given_name": "Souleyman"
      },
      {
        "surname": "Ma",
        "given_name": "Guangzhi"
      },
      {
        "surname": "El-Samie",
        "given_name": "Fathi E. Abd"
      }
    ]
  },
  {
    "title": "Rethinking pre-training on medical imaging",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103145",
    "abstract": "Transfer learning from natural image datasets, such as ImageNet, is common for applying deep learning to medical imaging. However, the modalities of natural and medical images differ considerably, and the reason for the latest medical research preferring ImageNet to medical data is questionable. In this study, we investigated the properties of medical pre-training and its transfer effectiveness on various medical tasks. Through an intuitive convolution-based analysis, we determined the modality characteristics of images. Surprisingly, medical pre-training showed exceptional performance for a classification task but not for a segmentation task since medical data are visually homogeneous and lack morphological information. Using data with diverse modalities helped overcome such drawbacks, resulting in medical pre-training achieving performance comparable to pre-training with ImageNet with considerably fewer samples than ImageNet for both aforementioned tasks. Finally, a study of learned representations and realistic scenarios indicated that while ImageNet is the best choice for medical imaging, medical pre-training has significant potential.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000894",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Deep learning",
      "Engineering",
      "Machine learning",
      "Medical imaging",
      "Meteorology",
      "Modalities",
      "Modality (human–computer interaction)",
      "Physics",
      "Segmentation",
      "Social science",
      "Sociology",
      "Systems engineering",
      "Task (project management)",
      "Training (meteorology)",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Wen",
        "given_name": "Yang"
      },
      {
        "surname": "Chen",
        "given_name": "Leiting"
      },
      {
        "surname": "Deng",
        "given_name": "Yu"
      },
      {
        "surname": "Zhou",
        "given_name": "Chuan"
      }
    ]
  },
  {
    "title": "Multi-view motion modelled deep attention networks (M2DA-Net) for video based sign language recognition",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103161",
    "abstract": "Currently, video-based Sign language recognition (SLR) has been extensively studied using deep learning models such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs). In addition, using multi view attention mechanism along with CNNs could be an appealing solution that can be considered in order to make the machine interpretation process immune to finger self-occlusions. The proposed multi stream CNN mixes spatial and motion modelled video sequences to create a low dimensional feature vector at multiple stages in the CNN pipeline. Hence, we solve the view invariance problem into a video classification problem using attention model CNNs. For superior network performance during training, the signs are learned through a motion attention network thus focusing on the parts that play a major role in generating a view based paired pooling using a trainable view pair pooling network (VPPN). The VPPN, pairs views to produce a maximally distributed discriminating features from all the views for an improved sign recognition. The results showed an increase in recognition accuracies on 2D video sign language datasets. Similar results were obtained on benchmark action datasets such as NTU RGB D, MuHAVi, WEIZMANN and NUMA as there is no multi view sign language dataset except ours.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001024",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Computer science",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Geodesy",
      "Geography",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Motion (physics)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pipeline (software)",
      "Pooling",
      "Programming language",
      "RGB color model",
      "Recurrent neural network",
      "Sign (mathematics)",
      "Sign language",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "M.",
        "given_name": "Suneetha"
      },
      {
        "surname": "M.V.D.",
        "given_name": "Prasad"
      },
      {
        "surname": "P.V.V.",
        "given_name": "Kishore"
      }
    ]
  },
  {
    "title": "A generic, cluster-centred lossless compression framework for joint auroral data",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103185",
    "abstract": "Studying the well-known phenomenon “aurora” plays a pivotal role in investigating the solar–terrestrial coupling mechanism. A special auroral spectrograph in Antarctic Zhongshan Station constitutes a auroral observation joint system with satellite-borne sensors of the Defense Meteorological Satellite Program. Multipoint observation by this system provides more essential information for relevant studies than single observation by each instrument, but also results in a multifold increased volume of data that are difficult to be either stored or transmitted. To address this difficulty, we develop a clustering-based, generic lossless data compression framework that combines the usage of various ultimate compressors with a hierarchical clustering algorithm to exert the strength of all the compressors in data reduction. This framework achieves an always-best compression performance for different-sized datasets with a reasonable time consumption, which promises the design of pipelines using it for real-time data transmission.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001176",
    "keywords": [
      "Aerospace engineering",
      "Algorithm",
      "Architectural engineering",
      "Artificial intelligence",
      "Cluster analysis",
      "Compression (physics)",
      "Computer science",
      "Data compression",
      "Data mining",
      "Engineering",
      "Geometry",
      "Joint (building)",
      "Lossless compression",
      "Lossy compression",
      "Mathematics",
      "Physics",
      "Real-time computing",
      "Reduction (mathematics)",
      "Satellite",
      "Telecommunications",
      "Thermodynamics",
      "Transmission (telecommunications)"
    ],
    "authors": [
      {
        "surname": "Shang",
        "given_name": "Kun"
      },
      {
        "surname": "Kong",
        "given_name": "Wanqiu"
      },
      {
        "surname": "Qu",
        "given_name": "Tan"
      },
      {
        "surname": "Hu",
        "given_name": "Zejun"
      },
      {
        "surname": "Wu",
        "given_name": "Jiaji"
      },
      {
        "surname": "Pedrycz",
        "given_name": "Witold"
      }
    ]
  },
  {
    "title": "HP-VCS: A high-quality and printer-friendly visual cryptography scheme",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103186",
    "abstract": "Visual Cryptography Scheme (VCS) is a secret-sharing scheme which aims to encrypt a secret message into multiple shares and transmit them to participants over an untrusted communication channel. Although human vision can easily reveal the secret message by stacking a sufficient number of shares, this scheme reduces the visual quality of recovered images. This paper presents a novel high-quality and printer-friendly VCS. When providing high-quality recovery, this scheme keeps the size of the shares the same as the secret image. Experimental results show that, compared with previous work, the proposed scheme significantly improves the performance of recovered images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001188",
    "keywords": [
      "Computer graphics (images)",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Cryptography",
      "Mathematical analysis",
      "Mathematics",
      "Scheme (mathematics)",
      "Secret sharing",
      "Visual cryptography"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Denghui"
      },
      {
        "surname": "Zhu",
        "given_name": "Hongbin"
      },
      {
        "surname": "Liu",
        "given_name": "Shenglong"
      },
      {
        "surname": "Wei",
        "given_name": "Xu"
      }
    ]
  },
  {
    "title": "Stable self-attention adversarial learning for semi-supervised semantic image segmentation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103170",
    "abstract": "The application of adversarial learning for semi-supervised semantic image segmentation based on convolutional neural networks can effectively reduce the number of manually generated labels required in the training process. However, the convolution operator of the generator in the generative adversarial network (GAN) has a local receptive field, so that the long-range dependencies between different image regions can only be modeled after passing through multiple convolutional layers. The present work addresses this issue by introducing a self-attention mechanism in the generator of the GAN to effectively account for relationships between widely separated spatial regions of the input image with supervision based on pixel-level ground truth data. In addition, the adjustment of the discriminator has been demonstrated to affect the stability of GAN training performance. This is addressed by applying spectral normalization to the GAN discriminator during the training process. Our method has better performance than existing full/semi-supervised semantic image segmentation techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001103",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Jia"
      },
      {
        "surname": "Li",
        "given_name": "Zhixin"
      },
      {
        "surname": "Zhang",
        "given_name": "Canlong"
      },
      {
        "surname": "Ma",
        "given_name": "Huifang"
      }
    ]
  },
  {
    "title": "Stacking learning with coalesced cost filtering for accurate stereo matching",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103169",
    "abstract": "Deep learning based stereo matching algorithms have produced impressive disparity estimation for recent years; and the success of them has once overshadowed the conventional ones. In this paper, we intend to reverse this inferiority, by leveraging Stacking Learning with Coalesced Cost Filtering to make the conventional algorithms achieve or even surpass the results of deep learning ones. Four classical and Discriminative Dictionary Learning (DDL) algorithms are adopted as base-models for Stacking. For the former ones, four classical stereo matching algorithms are employed and regarded as ‘Coalesced Cost Filtering Module’; for the latter supervised learning one, we utilize the Discriminative Dictionary Learning (DDL) stereo matching algorithm. Then three categories of features are extracted from the predictions of base-models to train the meta-model. For the meta-model (final classifier) of Stacking, the Random Forest (RF) classifier is selected. In addition, we also employ an advanced one-view disparity refinement strategy to compute the final refined results more efficiently. Performance evaluations on Middlebury v.2 and v.3 stereo data sets demonstrate that the proposed algorithm outperforms other four most challenging stereo matching algorithms. Besides, the submitted online results even show better results than deep learning ones.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001097",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Deep learning",
      "Dictionary learning",
      "Discriminative model",
      "Image (mathematics)",
      "Machine learning",
      "Matching (statistics)",
      "Mathematics",
      "Nuclear magnetic resonance",
      "Pattern recognition (psychology)",
      "Physics",
      "Stacking",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Yao",
        "given_name": "Peng"
      },
      {
        "surname": "Feng",
        "given_name": "Jieqing"
      }
    ]
  },
  {
    "title": "Saliency detection via coarse-to-fine diffusion-based compactness with weighted learning affinity matrix",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103151",
    "abstract": "Diffusion-based compactness is an effective method for foreground-based saliency detection, in which one key is the conventional graph construction. However, the conventional graph only displays the local structure but not preserves global relevance information. Therefore, diffusion-based compactness cannot highlight complete salient object which contains multiple areas with different features, and the extracted salient regions with weak homogeneous. Aiming to address these problems, we propose a saliency detection method via coarse-to-fine diffusion-based compactness with a weighted learning affinity matrix. Firstly, we construct multi-view conventional graphs to calculate the rough compactness cue. Secondly, we build a two-stage multi-view weighted graphs using a weighted learning affinity matrix and compute the coarse-to-fine compactness cue. Extensive experiments tested on three benchmark datasets, demonstrating the superior against several state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000936",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Combinatorics",
      "Compact space",
      "Composite material",
      "Computer science",
      "Diffusion",
      "Geodesy",
      "Geography",
      "Graph",
      "Homogeneous",
      "Materials science",
      "Mathematics",
      "Matrix (chemical analysis)",
      "Pattern recognition (psychology)",
      "Physics",
      "Pure mathematics",
      "Salient",
      "Theoretical computer science",
      "Thermodynamics"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Fan"
      },
      {
        "surname": "Peng",
        "given_name": "Guohua"
      }
    ]
  },
  {
    "title": "Facial parts swapping with generative adversarial networks",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103152",
    "abstract": "In this paper, we present a novel deep generative facial parts swapping method: parts-swapping generative adversarial network (PSGAN). PSGAN independently handles facial parts, such as eyes (left eye and right eye), nose, mouth and jaw, which achieves facial parts swapping by replacing the target facial parts with source facial parts and reconstructing the entire face image with these parts. By separately modeling the facial parts in the form of region inpainting, the proposed method can successfully achieve highly photorealistic face swapping results, enabling users to freely manipulate facial parts. In addition, the proposed method is able to perform jaw editing based on sketch guidance information. Experimental results on the CelebA dataset suggest that our method achieves superior performance for facial parts swapping and provides higher user control flexibility.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000948",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Face (sociological concept)",
      "Flexibility (engineering)",
      "Generative adversarial network",
      "Generative grammar",
      "Image (mathematics)",
      "Image editing",
      "Inpainting",
      "Mathematics",
      "Sketch",
      "Social science",
      "Sociology",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Guo",
        "given_name": "Jingtao"
      },
      {
        "surname": "Liu",
        "given_name": "Yi"
      }
    ]
  },
  {
    "title": "Reversible data hiding in encrypted color images using cross-channel correlations",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103166",
    "abstract": "In recent years, the increasing requirements in cloud storage and cloud computing have made it necessary to encrypt digital images for privacy protection. Meanwhile, many reversible data hiding (RDH) algorithms in the encrypted domain have been proposed. However, most of these algorithms are for gray-level images, and the intrinsic cross-channel correlations of color images cannot be utilized to improve the embedding capacity. In this paper, we propose a novel data hiding method for encrypted color images. In the encryption stage, the homomorphic property of encryption is achieved by basic modular addition. During the data hiding process, the cross-channel correlations between R, G and B channels are generated in encrypted domain, and data hiding is performed by the difference histogram shifting. Analysis and experiments demonstrate that the proposed method is secure and the RDH performance is superior.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321001061",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Cryptography",
      "Domain (mathematical analysis)",
      "Embedding",
      "Encryption",
      "Histogram",
      "Homomorphic encryption",
      "Homomorphic secret sharing",
      "Image (mathematics)",
      "Information hiding",
      "Mathematical analysis",
      "Mathematics",
      "Modular design",
      "Operating system",
      "Secret sharing"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Ming"
      },
      {
        "surname": "Ren",
        "given_name": "Hua"
      },
      {
        "surname": "Xiang",
        "given_name": "Yong"
      },
      {
        "surname": "Zhang",
        "given_name": "Yushu"
      }
    ]
  },
  {
    "title": "A joint cumulative distribution function and gradient fusion based method for dehazing of long shot hazy images",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103087",
    "abstract": "Hazy or foggy weather conditions significantly degrade the visual quality of an image in an outdoor environment. It also changes the color and reduces the contrast of an image. This paper introduces a novel single image dehazing technique to restore a hazy image without considering the physical model of haze formation. In order to find haze-free image, the proposed method does not require the transmission map and its costly refinement process. Since haze effect is dependent on the depth, it severely degrades the visibility of the objects located at a far distance. The objects close to the camera are unaffected. In this paper, we propose a fusion-based haze removal method based on the joint cumulative distribution function (JCDF) that treats faraway haze and nearby haze separately. The output images after the JCDF module, fused in the gradient domain to produce a haze-free image. The proposed method not only significantly enhances visibility but also preserves texture details. The proposed method is experimented and evaluated on a large set of challenging hazy images (large scene depth, night time, dense fog, etc.). Both qualitative and quantitative measures show that the performance of the proposed method is better than the state-of-the-art dehazing techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000523",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Fusion",
      "Geography",
      "Haze",
      "Image (mathematics)",
      "Linguistics",
      "Meteorology",
      "Operating system",
      "Optics",
      "Philosophy",
      "Physics",
      "Process (computing)",
      "Visibility"
    ],
    "authors": [
      {
        "surname": "Agrawal",
        "given_name": "Subhash Chand"
      },
      {
        "surname": "Jalal",
        "given_name": "Anand Singh"
      }
    ]
  },
  {
    "title": "Robust online multi-target visual tracking using a HISP filter with discriminative deep appearance learning",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2020.102952",
    "abstract": "We propose a novel online multi-target visual tracker based on the recently developed Hypothesized and Independent Stochastic Population (HISP) filter. The HISP filter combines advantages of traditional tracking approaches like MHT and point-process-based approaches like PHD filter, and it has linear complexity while maintaining track identities. We apply this filter for tracking multiple targets in video sequences acquired under varying environmental conditions and targets density using a tracking-by-detection approach. We also adopt deep CNN appearance representation by training a verification-identification network (VerIdNet) on large-scale person re-identification data sets. We construct an augmented likelihood in a principled manner using this deep CNN appearance features and spatio-temporal information. Furthermore, we solve the problem of two or more targets having identical label considering the weight propagated with each confirmed hypothesis. Extensive experiments on MOT16 and MOT17 benchmark data sets show that our tracker significantly outperforms several state-of-the-art trackers in terms of tracking accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301802",
    "keywords": [
      "Active appearance model",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "BitTorrent tracker",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Discriminative model",
      "Eye tracking",
      "Filter (signal processing)",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Law",
      "Pattern recognition (psychology)",
      "Pedagogy",
      "Political science",
      "Politics",
      "Psychology",
      "Representation (politics)",
      "Tracking (education)",
      "Video processing",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Baisa",
        "given_name": "Nathanael L."
      }
    ]
  },
  {
    "title": "Multi-label image recognition by using semantics consistency, object correlation, and multiple samples",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103067",
    "abstract": "An image can be annotated from the local perspective, based on objects visually present. An image can also be annotated from the global perspective, based on implicit emotion or meanings derived from it. We propose three points relatively little studied before. First, semantics remain the same even if the image is manipulated by some geometric processes. Second, object correlation is important in image labelling. We propose to use a standard recurrent neural network to take object sequences in random orders. Third, we observe that some entity can be represented by multiple image samples, and multiple samples can be jointly considered to improve recognition performance. These three points are implemented in a network that jointly considers global and local information. With comprehensive evaluation studies, we verify that a simple network with these points is effective and is able to achieve competitive performance compared to the state of the arts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000353",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Cognitive neuroscience of visual object recognition",
      "Computer science",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Correlation",
      "Geometry",
      "Image (mathematics)",
      "Mathematics",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Perspective (graphical)",
      "Programming language",
      "Semantics (computer science)"
    ],
    "authors": [
      {
        "surname": "Chu",
        "given_name": "Wei-Ta"
      },
      {
        "surname": "Huang",
        "given_name": "Si-Heng"
      }
    ]
  },
  {
    "title": "Facial Expression Recognition through person-wise regeneration of expressions using Auxiliary Classifier Generative Adversarial Network (AC-GAN) based model",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103110",
    "abstract": "Recently, Facial Expression Recognition (FER) has gained much attention in the research area for its various applications. In the facial expression recognition task, subject-dependent issue is predominant when a small-scale database is used for training the system. The proposed Auxiliary Classifier Generative Adversarial Network (AC-GAN) based model regenerates ten expressions (angry, contempt, disgust, embarrassment, fear, joy, neutral, pride, sad, surprise) from input face image and recognizes its expression. To alleviate the subject dependence issue, we train the model person-wise and generate all the above expressions for a person and allow the discriminator to classify the expressions. The generator of our model uses U-Net Architecture, and the discriminator uses Capsule Networks for improved feature extraction. The model has been evaluated on the ADFES-BIV dataset yielding an overall classification accuracy of 93.4%. We also compared our model with the existing methods by evaluating our model on commonly used datasets like CK+, KDEF.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000675",
    "keywords": [
      "Anger",
      "Artificial intelligence",
      "Classifier (UML)",
      "Communication",
      "Computer science",
      "Deep learning",
      "Detector",
      "Discriminator",
      "Disgust",
      "Expression (computer science)",
      "Facial expression",
      "Generative adversarial network",
      "Generative grammar",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Programming language",
      "Psychiatry",
      "Psychology",
      "Sadness",
      "Speech recognition",
      "Surprise",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "V.",
        "given_name": "Dharanya"
      },
      {
        "surname": "Joseph Raj",
        "given_name": "Alex Noel"
      },
      {
        "surname": "Gopi",
        "given_name": "Varun P."
      }
    ]
  },
  {
    "title": "Person re-identification based on gait via Part View Transformation Model under variable covariate conditions",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103093",
    "abstract": "Human gait represents an attractive biometric modality to re-identify a person as it requires non contact and it is perceivable at a distance. However, the view angle variation and the presence of covariate factors cause significant difficulties for recognizing gaits. In order to deal with such constraints, this paper presents a Part View Transformation Model (PVTM) for gait based applications. Compared with previous methods, the PVTM is applied on selected relevant parts chosen through a semantic classification step. Conducted on the CASIA-B gait database, experimental results show that the proposed method outperforms well known multi-view methods even under covariate factors (i.e. carrying bag, clothing).",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000547",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Astrophysics",
      "Biochemistry",
      "Biology",
      "Biometrics",
      "Botany",
      "Chemistry",
      "Clothing",
      "Computer science",
      "Computer vision",
      "Covariate",
      "Gait",
      "Gene",
      "History",
      "Identification (biology)",
      "Machine learning",
      "Medicine",
      "Modality (human–computer interaction)",
      "Pattern recognition (psychology)",
      "Physical medicine and rehabilitation",
      "Physics",
      "Transformation (genetics)",
      "Variation (astronomy)"
    ],
    "authors": [
      {
        "surname": "Chtourou",
        "given_name": "Imen"
      },
      {
        "surname": "Fendri",
        "given_name": "Emna"
      },
      {
        "surname": "Hammami",
        "given_name": "Mohamed"
      }
    ]
  },
  {
    "title": "Discriminative semantic region selection for fine-grained recognition",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103084",
    "abstract": "Performances of fine-grained recognition have been greatly improved thanks to the fast developments of deep convolutional neural networks (DCNN). DCNN methods often treat each image region equally. Besides, researchers often rely on visual information for classification. To solve these problems, we propose a novel discriminative semantic region selection method for fine-grained recognition (DSRS). We first select a few image regions and then use the pre-trained DCNN models to predict their semantic correlations with corresponding classes. We use both visual and semantic representations to represent image regions. The visual and semantic representations are then linearly combined for joint representation. The combination parameters are determined by considering both semantic distinctiveness and spatial-semantic correlations. We use the joint representations for classifier training. A testing image can be classified by obtaining the visual and semantic representations and encoded for joint representation and classification. Experiments on several publicly available datasets demonstrate the proposed method's superiority.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000493",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Convolutional neural network",
      "Discriminative model",
      "Law",
      "Natural language processing",
      "Optimal distinctiveness theory",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Psychology",
      "Psychotherapist",
      "Representation (politics)",
      "Selection (genetic algorithm)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Chunjie"
      },
      {
        "surname": "Wang",
        "given_name": "Da-Han"
      },
      {
        "surname": "Li",
        "given_name": "Haisheng"
      }
    ]
  },
  {
    "title": "A bottom-up and top-down human visual attention approach for hyperspectral anomaly detection",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103113",
    "abstract": "Hyperspectral anomaly detection (HAD) is a branch of target detection which tries to locate pixels that are spectrally or spatially different from their background. In this paper, a visual attention approach is developed to leverage HAD. Traditional HAD methods often try to locate anomalous pixels based on spectral information. However, the spatial features of hyperspectral datasets provide valuable information. Here, we aim to fuse spatial and spectral anomaly features based on bottom-up (BU) and top-down (TD) visual attention mechanisms. Owe to the BU attention, spatial features are extracted by mimicking the primary visual cortex neurons functionality. Also, spectral information is obtained throughout a deep neural network that imitating the TD visual attention. The BU and TD approaches’ results are then integrated to provide both spectral and spatial information. The key findings of our results demonstrate the proposed method outperforms the six state-of-the-art AD methods based on different evaluation metrics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000699",
    "keywords": [
      "Anomaly detection",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Electrical engineering",
      "Engineering",
      "Fuse (electrical)",
      "Geology",
      "Hyperspectral imaging",
      "Leverage (statistics)",
      "Pattern recognition (psychology)",
      "Pixel",
      "Remote sensing",
      "Software engineering",
      "Spatial analysis",
      "Top-down and bottom-up design",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Taghipour",
        "given_name": "Ashkan"
      },
      {
        "surname": "Ghassemian",
        "given_name": "Hassan"
      }
    ]
  },
  {
    "title": "Statistical image watermarking using local RHFMs magnitudes and beta exponential distribution",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103123",
    "abstract": "The imperceptibility, robustness and data payload are widely considered as the three main properties vital for any image watermarking systems. They are complimentary to each other and hence challenging to attain the right balance between them. The statistical model-based multiplicative watermarking is an effective way to achieve the tradeoff among imperceptibility, robustness and data payload. Radial harmonic Fourier moments (RHFMs) is a strong tool in image processing, which has many advantages, such as lower noise sensitivity, powerful image description ability and geometric invariance feature. In this paper, we propose a new statistical image watermarking scheme using local RHFMs magnitudes and Beta exponential distribution model. Our image watermarking scheme consists of two parts, namely, embedding and extraction. In the embedding process, we divide the host image into no-overlapping blocks and compute the local RHFMs of image blocks, then insert the watermark signal into the robust local RHFMs magnitudes through multiplicative approach. In the extraction phase, robust local RHFMs magnitudes are firstly modeled by employing the Beta exponential distribution, where the statistical properties of local RHFMs magnitudes are captured accurately. Then the modified maximum likelihood parameter estimation (MMLE) approach is introduced to estimate the statistical parameters of Beta exponential distribution model. And finally an image watermark decoder for multiplicative watermarking is developed using Beta exponential distribution and maximum likelihood decision criterion. Experimental results on some test images and comparison with well-known existing methods demonstrate the efficacy and superiority of the proposed statistical image watermarking.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000778",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Digital watermarking",
      "Exponential distribution",
      "Exponential function",
      "Gene",
      "Image (mathematics)",
      "Laplace distribution",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Robustness (evolution)",
      "Statistical model",
      "Statistics",
      "Watermark"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Xiang-yang"
      },
      {
        "surname": "Tian",
        "given_name": "Jing"
      },
      {
        "surname": "Tian",
        "given_name": "Jia-lin"
      },
      {
        "surname": "Niu",
        "given_name": "Pan-pan"
      },
      {
        "surname": "Yang",
        "given_name": "Hong-ying"
      }
    ]
  },
  {
    "title": "Diverse Features Fusion Network for video-based action recognition",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103121",
    "abstract": "The two-stream convolutional network has been proved to be one milestone in the study of video-based action recognition. Lots of recent works modify internal structure of two-stream convolutional network directly and put top-level features into a 2D/3D convolution fusion module or a simpler one. However, these fusion methods cannot fully utilize features and the way fusing only top-level features lacks rich vital details. To tackle these issues, a novel network called Diverse Features Fusion Network (DFFN) is proposed. The fusion stream of DFFN contains two types of uniquely designed modules, the diverse compact bilinear fusion (DCBF) module and the channel-spatial attention (CSA) module, to distill and refine diverse compact spatiotemporal features. The DCBF modules use the diverse compact bilinear algorithm to fuse features extracted from multiple layers of the base network that are called diverse features in this paper. Further, the CSA module leverages channel attention and multi-size spatial attention to boost key information as well as restraining the noise of fusion features. We evaluate our three-stream network DFFN on three public challenging video action benchmarks: UCF101, HMDB51 and Something-Something V1. Experiment results indicate that our method achieves state-of-the-art performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000754",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Bilinear interpolation",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Electrical engineering",
      "Engineering",
      "Fuse (electrical)",
      "Fusion",
      "Key (lock)",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Deng",
        "given_name": "Haoyang"
      },
      {
        "surname": "Kong",
        "given_name": "Jun"
      },
      {
        "surname": "Jiang",
        "given_name": "Min"
      },
      {
        "surname": "Liu",
        "given_name": "Tianshan"
      }
    ]
  },
  {
    "title": "Contextual information enhanced convolutional neural networks for retinal vessel segmentation in color fundus images",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103134",
    "abstract": "Accurate retinal vessel segmentation is a challenging problem in color fundus image analysis. An automatic retinal vessel segmentation system can effectively facilitate clinical diagnosis and ophthalmological research. In general, this problem suffers from various degrees of vessel thickness, perception of details, and contextual feature fusion in technique. For addressing these challenges, a deep learning based method has been proposed and several customized modules have been integrated into the well-known U-net with encoder–decoder architecture, which is widely employed in medical image segmentation. In the network structure, cascaded dilated convolutional modules have been integrated into the intermediate layers, for obtaining larger receptive field and generating denser encoded feature maps. Also, the advantages of the pyramid module with spatial continuity have been taken for multi-thickness perception, detail refinement, and contextual feature fusion. Additionally, the effectiveness of different normalization approaches has been discussed on different datasets with specific properties. Finally, sufficient comparative experiments have been enforced on three retinal vessel segmentation datasets, DRIVE, CHASE_DB1, and the STARE dataset with unhealthy samples. As a result, the proposed method outperforms the work of predecessors and achieves state-of-the-art performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000845",
    "keywords": [
      "Anthropology",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Encoder",
      "Feature (linguistics)",
      "Fundus (uterus)",
      "Geometry",
      "Image segmentation",
      "Linguistics",
      "Mathematics",
      "Medicine",
      "Normalization (sociology)",
      "Operating system",
      "Ophthalmology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pyramid (geometry)",
      "Segmentation",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Muyi"
      },
      {
        "surname": "Li",
        "given_name": "Kaiqi"
      },
      {
        "surname": "Qi",
        "given_name": "Xingqun"
      },
      {
        "surname": "Dang",
        "given_name": "Hao"
      },
      {
        "surname": "Zhang",
        "given_name": "Guanhong"
      }
    ]
  },
  {
    "title": "A review of video surveillance systems",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103116",
    "abstract": "Automated surveillance systems observe the environment utilizing cameras. The observed scenario is then analysed using motion detection, crowd behaviour, individual behaviour, interaction between individuals, crowds and their surrounding environment. These automatic systems accomplish multitude of tasks which include, detection, interpretation, understanding, recording and creating alarms based on the analysis. Till recent, studies have achieved enhanced monitoring performance along with avoiding possible human failures by manipulation of different features of these systems. This paper presents a comprehensive review of such video surveillance systems as well as the components used with them. The description of the architectures used is presented which follows the most required analyses in these systems. For the bigger picture and wholesome view of the system, existing surveillance systems were compared in terms of characteristics, advantages, and difficulties which are tabulated in this paper. Adding to this, future trends are discussed which charts a path into the upcoming research directions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000729",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Crowds",
      "Data science",
      "Epistemology",
      "Human–computer interaction",
      "Motion (physics)",
      "Motion detection",
      "Multitude",
      "Path (computing)",
      "Philosophy",
      "Programming language",
      "Real-time computing"
    ],
    "authors": [
      {
        "surname": "Elharrouss",
        "given_name": "Omar"
      },
      {
        "surname": "Almaadeed",
        "given_name": "Noor"
      },
      {
        "surname": "Al-Maadeed",
        "given_name": "Somaya"
      }
    ]
  },
  {
    "title": "Underwater image super-resolution using multi-stage information distillation networks",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103136",
    "abstract": "Recently, single image super-resolution (SISR) has been widely applied in the fields of underwater robot vision and obtained remarkable performance. However, most current methods generally suffered from the problem of a heavy burden on computational resources with large model sizes, which limited their real-world underwater robotic applications. In this paper, we introduce and tackle the super resolution (SR) problem for underwater robot vision and provide an efficient solution for near real-time applications. We present a novel lightweight multi-stage information distillation network, named MSIDN, for better balancing performance against applicability, which aggregates the local distilled features from different stages for more powerful feature representation. Moreover, a novel recursive residual feature distillation (RRFD) module is constructed to progressively extract useful features with a modest number of parameters in each stage. We also propose a channel interaction & distillation (CI&D) module that employs channel split operation on the preceding features to produce two-part features and utilizes the inter channel-wise interaction information between them to generate the distilled features, which can effectively extract the useful information of current stage without extra parameters. Besides, we present USR-2K dataset, a collection of over 1.6K samples for large-scale underwater image SR training, and a testset with an additional 400 samples for benchmark evaluation. Extensive experiments on several standard benchmark datasets show that the proposed MSIDN can provide state-of-the-art or even better performance in both quantitative and qualitative measurements.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000833",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Channel (broadcasting)",
      "Chemistry",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Distillation",
      "Feature (linguistics)",
      "Feature extraction",
      "Geodesy",
      "Geography",
      "Geology",
      "Image (mathematics)",
      "Law",
      "Linguistics",
      "Oceanography",
      "Organic chemistry",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Robot",
      "Stage (stratigraphy)",
      "Underwater"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Huan"
      },
      {
        "surname": "Wu",
        "given_name": "Hao"
      },
      {
        "surname": "Hu",
        "given_name": "Qian"
      },
      {
        "surname": "Chi",
        "given_name": "Jianning"
      },
      {
        "surname": "Yu",
        "given_name": "Xiaosheng"
      },
      {
        "surname": "Wu",
        "given_name": "Chengdong"
      }
    ]
  },
  {
    "title": "Deep multi-feature learning architecture for water body segmentation from satellite images",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103141",
    "abstract": "Automatic water body extraction from satellite images of various scenes is a classical and challenging task in remote sensing and image interpretation. Convolutional neural network (CNN) has become prominent option for performing image segmentation task in remote sensing applications. However, CNN-based networks have non-trivial issues for segmenting such as: (1) blurring boundary pixels; (2) large number of trainable parameters; and (3) huge number of training samples. In this paper, we propose an end-to-end multi-feature based CNN architecture, called as W-Net, to perform water body segmentation. W-Net consists of contracting/expanding networks and inception layers. W-Net takes advantage of contracting network to capture context information while localization is achieved with expanding network. With these networks, W-Net is able to train on less number of images and extract water pixels accurately. Use of inception layers reduces computational burden within the network by decreasing total number of trainable parameters. W-Net incorporated two refinement modules to enhance predicted results which mitigate blurring effect and to inspect continuity of boundary pixels. Dataset consisting 2671 images with manually annotated ground truths are built to validate performance and effectiveness of our proposed method. In addition, we evaluated our method on crack detection dataset where W-Net achieved competitive performance with Deepcrack. W-Net accomplished excellent performance on the water body dataset ( I ∕ U = 0 . 9434 and F − s c o r e = 0 . 9509 ).",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000870",
    "keywords": [
      "Aerospace engineering",
      "Artificial intelligence",
      "Artificial neural network",
      "Boundary (topology)",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Context (archaeology)",
      "Convolutional neural network",
      "Deep learning",
      "Economics",
      "Engineering",
      "Feature (linguistics)",
      "Geology",
      "Geometry",
      "Image segmentation",
      "Linguistics",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Net (polyhedron)",
      "Network architecture",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Satellite",
      "Segmentation",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Tambe",
        "given_name": "Rishikesh G."
      },
      {
        "surname": "Talbar",
        "given_name": "Sanjay N."
      },
      {
        "surname": "Chavan",
        "given_name": "Satishkumar S."
      }
    ]
  },
  {
    "title": "Underwater image super-resolution using multi-stage information distillation networks",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103136",
    "abstract": "Recently, single image super-resolution (SISR) has been widely applied in the fields of underwater robot vision and obtained remarkable performance. However, most current methods generally suffered from the problem of a heavy burden on computational resources with large model sizes, which limited their real-world underwater robotic applications. In this paper, we introduce and tackle the super resolution (SR) problem for underwater robot vision and provide an efficient solution for near real-time applications. We present a novel lightweight multi-stage information distillation network, named MSIDN, for better balancing performance against applicability, which aggregates the local distilled features from different stages for more powerful feature representation. Moreover, a novel recursive residual feature distillation (RRFD) module is constructed to progressively extract useful features with a modest number of parameters in each stage. We also propose a channel interaction & distillation (CI&D) module that employs channel split operation on the preceding features to produce two-part features and utilizes the inter channel-wise interaction information between them to generate the distilled features, which can effectively extract the useful information of current stage without extra parameters. Besides, we present USR-2K dataset, a collection of over 1.6K samples for large-scale underwater image SR training, and a testset with an additional 400 samples for benchmark evaluation. Extensive experiments on several standard benchmark datasets show that the proposed MSIDN can provide state-of-the-art or even better performance in both quantitative and qualitative measurements.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000833",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Channel (broadcasting)",
      "Chemistry",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Distillation",
      "Feature (linguistics)",
      "Feature extraction",
      "Geodesy",
      "Geography",
      "Geology",
      "Image (mathematics)",
      "Law",
      "Linguistics",
      "Oceanography",
      "Organic chemistry",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Robot",
      "Stage (stratigraphy)",
      "Underwater"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Huan"
      },
      {
        "surname": "Wu",
        "given_name": "Hao"
      },
      {
        "surname": "Hu",
        "given_name": "Qian"
      },
      {
        "surname": "Chi",
        "given_name": "Jianning"
      },
      {
        "surname": "Yu",
        "given_name": "Xiaosheng"
      },
      {
        "surname": "Wu",
        "given_name": "Chengdong"
      }
    ]
  },
  {
    "title": "Image encryption using linear weighted fractional-order transform",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103098",
    "abstract": "As the linear weighted fractional-order Fourier transform (LWFRFT), an extension of the Fourier transform, has been widely studied, many linear weighted fractional-order transforms (LWFRTs) have been proposed consequently. Our research shows that the LWFRT has limitations when applied to image encryption. For example, its application to image encryption leads to the security risks of key invalidation. In this paper, we propose a new reformulation of the LWFRT which establishes the relation between many fractional-order transforms. With the help of the new reformulation, we point out the limitations of the LWFRT and analyze the reasons for key invalidation in image encryption. Finally, numerical simulation verifies our perspective.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000584",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Data mining",
      "Economics",
      "Encryption",
      "Extension (predicate logic)",
      "Finance",
      "Fourier analysis",
      "Fourier transform",
      "Fractional Fourier transform",
      "Image (mathematics)",
      "Key (lock)",
      "Mathematical analysis",
      "Mathematics",
      "Order (exchange)",
      "Programming language",
      "Relation (database)"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Tieyu"
      },
      {
        "surname": "Yuan",
        "given_name": "Lin"
      },
      {
        "surname": "Chi",
        "given_name": "Yingying"
      }
    ]
  },
  {
    "title": "A content-based late fusion approach applied to pedestrian detection",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103091",
    "abstract": "The diversity of pedestrians detectors proposed in recent years has encouraged some works to fuse them to achieve a more accurate detection. The intuition behind it is to combine the detectors based on its spatial consensus. The hypothesis is that a location pointed by multiple detectors has a high probability of actually belonging to a pedestrian, while false positive regions have little consensus among detectors (small support) which allows discarding the false positives in these regions. We proposed a novel method called Content-Based Spatial Consensus (CSBC), which, in addition to relying on spatial consensus, considers the content of the detection windows to learn a weighted-fusion of pedestrian detectors. The result is a reduction in false alarms and an enhancement in the detection. In this work, we also demonstrated that there is small influence of the feature used to learn the contents of the windows of each detector, which enables our method to be efficient even employing simple features. The CSBC overcomes state-of-the-art fusion methods in the ETH dataset and the Caltech dataset. Particularly, our method is also more efficient, since fewer detectors are necessary to achieve expressive results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000559",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Content (measure theory)",
      "Data mining",
      "Detector",
      "Electrical engineering",
      "Engineering",
      "Epistemology",
      "Fuse (electrical)",
      "Intuition",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Object detection",
      "Pattern recognition (psychology)",
      "Pedestrian",
      "Pedestrian detection",
      "Philosophy",
      "Sensor fusion",
      "Telecommunications",
      "Transport engineering"
    ],
    "authors": [
      {
        "surname": "Sena",
        "given_name": "Jessica"
      },
      {
        "surname": "Jordão",
        "given_name": "Artur"
      },
      {
        "surname": "Schwartz",
        "given_name": "William Robson"
      }
    ]
  },
  {
    "title": "Correlation filter via random-projection based CNNs features combination for visual tracking",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103082",
    "abstract": "Object tracking based on the Convolutional Neural Networks (CNNs) with multiple feature correlation filter (CF) has become one of the best object tracking frameworks. In this paper, we propose a novel approach of CNNs based CF, which combines deep features from CNNs into low-dimensional features. To achieve the dimensionality reduction, random-projection is used due to its data-independence and superior computational efficiency over other widely used. In our proposed approach, the spectral graph theory is applied to generate a random projection matrix. This method bypasses the time-consuming Gram–Schmidt orthogonalization, where the dimension of the feature is high. The combined features have very low dimensions, less than one tenth of the dimensions of the original deep features from CNNs, offering an improvement of tracking speed and without loss of performance simultaneously. Extensive experiments are conducted on large-scale benchmark datasets. The results demonstrate that the proposed algorithm outperforms the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032100047X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Correlation",
      "Eye tracking",
      "Filter (signal processing)",
      "Geometry",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pedagogy",
      "Projection (relational algebra)",
      "Psychology",
      "Random projection",
      "Tracking (education)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Mingke"
      },
      {
        "surname": "Xu",
        "given_name": "Long"
      },
      {
        "surname": "Xiong",
        "given_name": "Jing"
      },
      {
        "surname": "Zhang",
        "given_name": "Xuande"
      }
    ]
  },
  {
    "title": "Multi-task learning with deformable convolution",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103109",
    "abstract": "Multi-task learning aims to tackle various tasks with branched feature sharing architectures. Considering its diversity and complexity, discriminative feature representations need to be extracted for each individual task. Fixed geometric structures as a limitation of convolutional neural networks (CNNs) in building models, is also exists and poses a severe challenge in multi-task learning since the geometric variations will augment when we deal with multiple tasks. In this paper, we go beyond these limitations and propose a novel multi-task network by introducing the deformable convolution. Our design, the Deformable Multi-Task Network (DMTN), starts with a single shared network for constructing a shared feature pool. Then, we present task-specific deformable modules to extract discriminative features to be tailored for each task from the shared feature pool. The task-specific deformable modules utilize two new parts, deformable part and alignment part, to extract more discriminative task-specific features while greatly enhancing the transformation modeling capability. Experiments conducted on various multi-task learning types demonstrate the effectiveness of the proposed method. On multiple classification tasks, semantic segmentation and depth estimation tasks, our DMTN exceeds state-of-the-art approaches against strong baselines.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000687",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Discriminative model",
      "Economics",
      "Feature (linguistics)",
      "Feature learning",
      "Gene",
      "Linguistics",
      "Machine learning",
      "Management",
      "Multi-task learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Segmentation",
      "Task (project management)",
      "Task analysis",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Jie"
      },
      {
        "surname": "Huang",
        "given_name": "Lei"
      },
      {
        "surname": "Wei",
        "given_name": "Zhiqiang"
      },
      {
        "surname": "Zhang",
        "given_name": "Wenfeng"
      },
      {
        "surname": "Qin",
        "given_name": "Qibing"
      }
    ]
  },
  {
    "title": "TheiaNet: Towards fast and inexpensive CNN design choices for image dehazing",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103137",
    "abstract": "This work examines inexpensive design choices for dehazing as an end-to-end image-to-image mapping problem without relying on the physical scattering model. The proposed TheiaNet is free from intermediate-computation of transmission map, enabling haze removal in a highly resource constrained environments. The simplicity of the network is augmented by a spatial cleaning bottleneck block, that adds faster feature extraction without adding to trainable parameters. We also analyze the effectiveness of multi-cue color space (RGB, HSV, LAB, YCbCr) over single cue color space (RGB) for end-to-end dehazing. A comprehensive set of experiments were conducted on HazeRD, D-Hazy and the more recent Reside datasets. The proposed TheiaNet significantly outperforms the existing CNN and GAN based state-of-the-art methods in terms of PSNR and SSIM on all these datasets. It also outperforms all existing methods in term of speed, compute and memory efficiency, making it more efficient. This work highlights how judicious application-specific components can augment simple CNNs to denoise faster, and more accurately than deeper heavier networks, which is supported by an ablation analysis as well.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000791",
    "keywords": [
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Bottleneck",
      "Color image",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Embedded system",
      "Feature (linguistics)",
      "Geometry",
      "Image (mathematics)",
      "Image processing",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "RGB color model",
      "YCbCr"
    ],
    "authors": [
      {
        "surname": "Mehra",
        "given_name": "Aryan"
      },
      {
        "surname": "Narang",
        "given_name": "Pratik"
      },
      {
        "surname": "Mandal",
        "given_name": "Murari"
      }
    ]
  },
  {
    "title": "Image deraining with Adversarial Residual Refinement Network",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103133",
    "abstract": "Prior image deraining works mainly have two problems: (1) they do not generalize well to various datasets; (2) too much detail information is lost in the heavy rain area of the rain image. To overcome these two problems, we propose a new two-stage Adversarial Residual Refinement Network (ARRN) to deal with heavy rain images. Specifically, for the first problem, we first introduce a new implicit rain model to model a rain image as a composition of a background image and a residual image. Based on the proposed implicit model, we then propose the ARRN which consists of an image decomposition stage and an image refinement stage. For the second problem, a new attention Wasserstein Generative Adversarial Networks (WGAN) loss in the refinement stage is introduced to force the network to focus on refining heavily degraded areas. Comprehensive experiments demonstrate the effectiveness of the proposed approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032100081X",
    "keywords": [
      "Adversarial system",
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Decomposition",
      "Ecology",
      "Focus (optics)",
      "Generative adversarial network",
      "Generative grammar",
      "Geology",
      "Image (mathematics)",
      "Image processing",
      "Image restoration",
      "Optics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Physics",
      "Residual",
      "Stage (stratigraphy)"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Wei"
      },
      {
        "surname": "Qiu",
        "given_name": "Song"
      },
      {
        "surname": "Huang",
        "given_name": "Kunyao"
      },
      {
        "surname": "Liu",
        "given_name": "Wei"
      },
      {
        "surname": "Zuo",
        "given_name": "Junzhe"
      },
      {
        "surname": "Guo",
        "given_name": "Haoming"
      }
    ]
  },
  {
    "title": "A deep genetic algorithm for human activity recognition leveraging fog computing frameworks",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103132",
    "abstract": "With modern e-healthcare developments, ambulatory healthcare has become a prominent requirement for physical or mental ailed, elderly, childhood people. One of the major challenges in such applications is timing and precision. A potential solution to this problem is the fog-assisted cloud computing architecture. The activity recognition task is performed with the hybrid advantages of deep learning and genetic algorithms. The video frames captured from vision cameras are subjected to the genetic change detection algorithm, which detects changes in activities of subsequent frames. Consequently, the deep learning algorithm recognizes the activity of the changed frame. This hybrid algorithm is run on top of fog-assisted cloud framework, fogbus and the performance measures including latency, execution time, arbitration time and jitter are observed. Empirical evaluations of the proposed model against three activity data sets shows that the proposed deep genetic algorithm exhibits higher accuracy in inferring human activities as compared to the state-of-the-art algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000857",
    "keywords": [
      "Activity recognition",
      "Algorithm",
      "Architecture",
      "Art",
      "Artificial intelligence",
      "Cloud computing",
      "Computer network",
      "Computer science",
      "Deep learning",
      "Frame (networking)",
      "Genetic algorithm",
      "Jitter",
      "Machine learning",
      "Operating system",
      "Real-time computing",
      "Telecommunications",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Subramanian",
        "given_name": "R. Raja"
      },
      {
        "surname": "Vasudevan",
        "given_name": "V."
      }
    ]
  },
  {
    "title": "A lightweight multi-scale aggregated model for detecting aerial images captured by UAVs",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103058",
    "abstract": "Detecting the objects of interesting from aerial images captured by UAVs is one of the core modules in the UAV-based applications. However, it is very difficult to detection objects from aerial images. The reason is that the scale of objects in the aerial images captured by UAVs varies greatly and needs to meet certain real-time performance in detection. To deal with these challenges, we proposed a lightweight model named DSYolov3. We made the following improvements to the Yolov3 model: 1) multiple scale-aware decision discrimination network to detect objects in different scales, 2) a multi-scale fusion-based channel attention model to exploit the channel-wise information complementation, 3) a sparsity-based channel pruning to compress the model. Extensive experimental evaluation has demonstrated the effectiveness and efficiency of our approach. By the proposed approach, we could not only achieve better performance than most existing detectors but also ensure the models practicable on the UAVs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000286",
    "keywords": [
      "Aerial image",
      "Agronomy",
      "Artificial intelligence",
      "Biology",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Detector",
      "Exploit",
      "Image (mathematics)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Physics",
      "Pruning",
      "Quantum mechanics",
      "Scale (ratio)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Zhaokun"
      },
      {
        "surname": "Liu",
        "given_name": "Xueliang"
      },
      {
        "surname": "Zhao",
        "given_name": "Ye"
      },
      {
        "surname": "Liu",
        "given_name": "Bo"
      },
      {
        "surname": "Huang",
        "given_name": "Zhen"
      },
      {
        "surname": "Hong",
        "given_name": "Richang"
      }
    ]
  },
  {
    "title": "Driver activity recognition by learning spatiotemporal features of pose and human object interaction",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103135",
    "abstract": "Detecting hazardous activity during driving can be useful in curbing roadside accidents. Existing techniques utilizing image based features for encoding such activity can sometimes misclassify crucial scenarios. One particular work by Zhao et al. (2013 [1], 2013 [2], 2011 [3]) suggests an image based feature set that encodes the driver’s pose, which is categorized into one of four activities. We bring more clarity in understanding the activity by proposing a richer, video based feature set that adeptly exploits spatiotemporal information of the driver. Our feature set encodes the driver’s pose, crucial variations in pose and interactions with objects within the vehicle. The feature set is tested on our newly created dataset since the ones used in literature are not publicly available. Our proposed feature set captures a larger number of activities and using standard classifiers and benchmarks it has shown significant improvements over the existing ones.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000808",
    "keywords": [
      "Activity recognition",
      "Artificial intelligence",
      "Biochemistry",
      "CLARITY",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Encoding (memory)",
      "Exploit",
      "Feature (linguistics)",
      "Feature extraction",
      "Image (mathematics)",
      "Linguistics",
      "Machine learning",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Naveed",
        "given_name": "Humza"
      },
      {
        "surname": "Jafri",
        "given_name": "Fareed"
      },
      {
        "surname": "Javed",
        "given_name": "Kashif"
      },
      {
        "surname": "Babri",
        "given_name": "Haroon Atique"
      }
    ]
  },
  {
    "title": "Learned Greedy Method (LGM): A novel neural architecture for sparse coding and beyond",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103095",
    "abstract": "The fields of signal and image processing have been deeply influenced by the introduction of deep neural networks. Despite their impressive success, the architectures used in these solutions come with no clear justification, being “black box” machines that lack interpretability. A constructive remedy to this drawback is a systematic design of networks by unfolding well-understood iterative algorithms. A popular representative of this approach is LISTA, evaluating sparse representations of processed signals. In this paper, we revisit this task and propose an unfolded version of a greedy pursuit algorithm for the same goal. More specifically, we concentrate on the well-known OMP algorithm, and introduce its unfolded and learned version. Key features of our Learned Greedy Method (LGM) are the ability to accommodate a dynamic number of unfolded layers, and a stopping mechanism based on representation error. We develop several variants of the proposed LGM architecture and demonstrate their flexibility and efficiency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000560",
    "keywords": [],
    "authors": [
      {
        "surname": "Khatib",
        "given_name": "Rajaei"
      },
      {
        "surname": "Simon",
        "given_name": "Dror"
      },
      {
        "surname": "Elad",
        "given_name": "Michael"
      }
    ]
  },
  {
    "title": "Line-based visual odometry using local gradient fitting",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103071",
    "abstract": "Visual odometry aims to estimate the relative pose between frames, which is a fundamental task for visual SLAM. In this paper, we present a novel line-based visual odometry (VO) algorithm that fully utilizes the characteristic of line to estimate the projected line of adjacent frame by minimizing the local gradient fitness evaluation. In contrast to the current feature-based or line-based visual odometry, we don ′ t need to explicitly match points or lines of two frames, which is non-trivial and inaccurate in challenging scenarios such as texture-less scenes. In our method, the projected line is calculated simultaneously with the local gradient fitting function of pose estimation based on the constraint that the orientation of the projected line should be perpendicular to the gradient orientation of pixels of its local regions. The proposed method is more robust and reliable than other line-based VO since it fully uses the pixel orientations in the local regions to estimate the projected line and relative pose. We evaluate our method on the real-world RGB-D dataset and synthetic benchmark dataset. Experimental results show that our method achieves the state-of-the-art algorithms in indoors scenes, especially in texture-less scenes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000377",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Geodesy",
      "Geography",
      "Geometry",
      "Line (geometry)",
      "Linguistics",
      "Mathematics",
      "Mobile robot",
      "Odometry",
      "Orientation (vector space)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Pose",
      "Robot",
      "Visual odometry"
    ],
    "authors": [
      {
        "surname": "Lu",
        "given_name": "Junxin"
      },
      {
        "surname": "Fang",
        "given_name": "Zhijun"
      },
      {
        "surname": "Gao",
        "given_name": "Yongbin"
      },
      {
        "surname": "Chen",
        "given_name": "Jieyu"
      }
    ]
  },
  {
    "title": "Siamese target estimation network with AIoU loss for real-time visual tracking",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103107",
    "abstract": "The fully convolutional siamese network based trackers achieve great progress recently. Most of these methods focus on improving the capability of siamese network to represent the target. In this paper, we propose our model which focuses on estimating the state of the target with our proposed novel IoU (intersection over union) loss function which is named AIoU. Our model consists of a siamese subnetwork for feature extraction and a target estimation subnetwork for state representation. The target estimation subnetwork contains a classification head for classifying background and foreground and a regression head for estimating target. In order to regress better bounding boxes, we further study the loss function utilized in the regression head and propose a powerful IoU loss function. Our tracker achieves competitive performance on OTB2015, VOT2018, and VOT2019 benchmarks with a speed of 180 FPS, which proves the effectiveness of our method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000651",
    "keywords": [
      "Artificial intelligence",
      "Bounding overwatch",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Convolutional neural network",
      "Feature extraction",
      "Focus (optics)",
      "Law",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Subnetwork"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Zhiyong"
      },
      {
        "surname": "Hu",
        "given_name": "Chenming"
      },
      {
        "surname": "Nai",
        "given_name": "Ke"
      },
      {
        "surname": "Yuan",
        "given_name": "Jin"
      }
    ]
  },
  {
    "title": "Weighted voting of multi-stream convolutional neural networks for video-based action recognition using optical flow rhythms",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103112",
    "abstract": "Two of the most important premises of an ensemble are the diversity of its components and how to combine their votes. In this paper, we propose a multi-stream architecture based on the weighted voting of convolutional neural networks to deal with the problem of recognizing human actions in videos. A major challenge is how to include temporal aspects into this kind of approach. A key step in this direction is the selection of features that characterize the complexity of human actions in time. In this context, we propose a new stream, Optical Flow Rhythm, besides using other streams for diversity. To combine the streams, a voting system based on a new weighted average fusion method is introduced. In this scheme, the weights of classifiers are defined by an optimization process led by a metaheuristic. Experiments conducted on the UCF101 and HMDB51 datasets demonstrate that our method is comparable to state-of-the-art approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000705",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer security",
      "Context (archaeology)",
      "Convolutional neural network",
      "Data mining",
      "Image (mathematics)",
      "Key (lock)",
      "Law",
      "Machine learning",
      "Operating system",
      "Optical flow",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Process (computing)",
      "Voting",
      "Weighted voting"
    ],
    "authors": [
      {
        "surname": "de Souza Brito",
        "given_name": "André"
      },
      {
        "surname": "Bernardes Vieira",
        "given_name": "Marcelo"
      },
      {
        "surname": "Moraes Villela",
        "given_name": "Saulo"
      },
      {
        "surname": "Tacon",
        "given_name": "Hemerson"
      },
      {
        "surname": "de Lima Chaves",
        "given_name": "Hugo"
      },
      {
        "surname": "de Almeida Maia",
        "given_name": "Helena"
      },
      {
        "surname": "Ttito Concha",
        "given_name": "Darwin"
      },
      {
        "surname": "Pedrini",
        "given_name": "Helio"
      }
    ]
  },
  {
    "title": "Gaussian Process-based Feature-Enriched Blind Image Quality Assessment",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103092",
    "abstract": "The objective of blind-image quality assessment (BIQA) research is the prediction of perceptual quality of images, without reference information. The human’s perceptual assessment of quality of an image is the backbone of BIQA research. Therefore, human-provided, mean opinion score (perceptual quality) has been analyzed in detail, and it has been observed to follow the Gaussian distribution and thus can be ideally modeled by the same. In this paper, we have proposed an integrated two-stage Gaussian process-based hybrid-feature selection algorithm for the BIQA problem. Moreover, a new consolidated feature set (obtained from the proposed algorithm), consisting of momentous Natural Scene Statistics (NSS)-based features is used in combination with the Gaussian process regression algorithm for the design of a new blind-image quality evaluator, referred to as GPR-BIQA. The proposed evaluator is tested on eight IQA legacy databases, and it is found that the proposed evaluator proficiently correlate with the human opinion, and outperformed a substantial number of existing approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000535",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Economics",
      "Epistemology",
      "Feature (linguistics)",
      "Feature selection",
      "Feature vector",
      "Gaussian",
      "Gaussian process",
      "Image (mathematics)",
      "Image quality",
      "Kriging",
      "Linguistics",
      "Machine learning",
      "Metric (unit)",
      "Neuroscience",
      "Operating system",
      "Operations management",
      "Pattern recognition (psychology)",
      "Perception",
      "Philosophy",
      "Physics",
      "Process (computing)",
      "Programming language",
      "Quality (philosophy)",
      "Quality Score",
      "Quantum mechanics",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Khalid",
        "given_name": "Hassan"
      },
      {
        "surname": "Ali",
        "given_name": "Dr. Muhammad"
      },
      {
        "surname": "Ahmed",
        "given_name": "Nisar"
      }
    ]
  },
  {
    "title": "A survey of reversible data hiding in encrypted images – The first 12 years",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103085",
    "abstract": "In the last few years, with the increasing popularity of cloud computing and the availability of mobile smart devices as well as ubiquitous network connections, more and more users are uploading their personal data to remote servers. However, this can lead to significant security breaches, where confidentiality, integrity and authentication are constantly threatened. To overcome these multiple problems, multimedia data must be secured, for example by means of encryption before transmission and storage. In this survey, we look into the issues involved in handling encrypted multimedia data, and more specifically we focus on reversible data hiding in encrypted images (RDHEI). The aim of this survey is to present the birth and evolution of RDHEI methods over the last 12 years. We first highlight different classes and characteristics of RDHEI, then describe representative RDHEI methods. A comparison table is presented to summarize the key features and achievements of each representative RDHEI method considered in this survey. Finally, we share the future outlook of emerging applications and open research topics relevant to RDHEI for the next 12 years and beyond.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032100050X",
    "keywords": [
      "Artificial intelligence",
      "Authentication (law)",
      "Cloud computing",
      "Computer science",
      "Computer security",
      "Confidentiality",
      "Data security",
      "Database",
      "Embedding",
      "Encryption",
      "Information hiding",
      "Internet privacy",
      "Key (lock)",
      "Multimedia",
      "Operating system",
      "Popularity",
      "Psychology",
      "Server",
      "Social psychology",
      "Table (database)",
      "Upload",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Puteaux",
        "given_name": "Pauline"
      },
      {
        "surname": "Ong",
        "given_name": "SimYing"
      },
      {
        "surname": "Wong",
        "given_name": "KokSheik"
      },
      {
        "surname": "Puech",
        "given_name": "William"
      }
    ]
  },
  {
    "title": "MRANet: Multi-atrous residual attention Network for stereo image super-resolution",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103115",
    "abstract": "In recent years, stereo cameras have been widely used in various fields. Due to the limited resolution of real equipments, stereo image super-resolution (SR) is a very important and hot topic. Recent studies have shown that deep network structures can directly affect feature expression and extraction and thus influence the final results. In this paper, we propose a multi-atrous residual attention stereo super-resolution network (MRANet) with parallax extraction and strong discriminative ability. Specifically, we propose a multi-scale atrous residual attention (MARA) block to obtain receptive fields of different scales through a multi-scale atrous convolution and then combine them with attention mechanisms to extract more diverse and meaningful information. Moreover, we propose a stereo feature fusion unit for stereo parallax extraction and single viewpoint feature refinement and integration. Experiments on benchmark datasets show that MRANet achieves state-of-the-art performance in terms of quantitative metrics and visual quality compared with several SR methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000717",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Discriminative model",
      "Feature (linguistics)",
      "Feature extraction",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Linguistics",
      "Parallax",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Residual",
      "Stereo image",
      "Superresolution"
    ],
    "authors": [
      {
        "surname": "Ning",
        "given_name": "Luyao"
      },
      {
        "surname": "Wang",
        "given_name": "Anhong"
      },
      {
        "surname": "Zhao",
        "given_name": "Lijun"
      },
      {
        "surname": "Xue",
        "given_name": "Weimin"
      },
      {
        "surname": "Bu",
        "given_name": "Donghan"
      }
    ]
  },
  {
    "title": "Hybrid prediction-based pixel-value-ordering method for reversible data hiding",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103097",
    "abstract": "Pixel-value-ordering (PVO) is an effective and promising method of reversible data hiding (RDH) and has received much attention in recent years. To improve performance, a pixel-based PVO (PPVO) method was recently introduced to predict the pixels to be embedded in a pixel-wise manner instead of the block-wise manner used by PVO. However, for PPVO, the surrounding neighbors of the predicted pixels are underutilized; moreover, its embedding does not adapt to the local complexity of the image to be embedded. To overcome the shortcomings of PPVO, this paper proposes a novel PVO method based on hybrid prediction for RDH. First, the surrounding neighbors of the pixel to be predicted are fully utilized by a hybrid prediction method, which combines rhombus prediction and pixel-wise prediction. Second, a modified embedding scheme based on multiple histograms is presented for adaptive embedding. Experimental results show the superior performance of the proposed method by comparing it with state-of-the-art RDH methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000572",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Computer science",
      "Embedding",
      "Geometry",
      "Histogram",
      "Image (mathematics)",
      "Information hiding",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pixel",
      "Rhombus",
      "Value (mathematics)"
    ],
    "authors": [
      {
        "surname": "Chang",
        "given_name": "Jie"
      },
      {
        "surname": "Ding",
        "given_name": "Feng"
      },
      {
        "surname": "Li",
        "given_name": "Xiaolong"
      },
      {
        "surname": "Zhu",
        "given_name": "Guopu"
      }
    ]
  },
  {
    "title": "Fast and effective Keypoint-based image copy-move forgery detection using complex-valued moment invariants",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103068",
    "abstract": "Copy-move forgery is one of the most common image tampering schemes, with the potential use for misleading the opinion of the general public. Keypoint-based detection methods exhibit remarkable performance in terms of computational cost and robustness. However, these methods are difficult to effectively deal with the cases when 1) forgery only involves small or smooth regions, 2) multiple clones are conducted or 3) duplicated regions undergo geometric transformations or signal corruptions. To overcome such limitations, we propose a fast and accurate copy-move forgery detection algorithm, based on complex-valued invariant features. First, dense and uniform keypoints are extracted from the whole image, even in small and smooth regions. Then, these keypoints are represented by robust and discriminative moment invariants, where a novel fast algorithm is designed especially for the computation of dense keypoint features. Next, an effective magnitude-phase hierarchical matching strategy is proposed for fast matching a massive number of keypoints while maintaining the accuracy. Finally, a reliable post-processing algorithm is developed, which can simultaneously reduce false negative rate and false positive rate. Extensive experimental results demonstrate the superior performance of our proposed scheme compared with existing state-of-the-art algorithms, with average pixel-level F-measure of 94.54% and average CPU-time of 36.25 s on four publicly available datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000365",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Classical mechanics",
      "Computation",
      "Computer science",
      "Computer vision",
      "Discriminative model",
      "Gene",
      "Image (mathematics)",
      "Invariant (physics)",
      "Matching (statistics)",
      "Mathematical physics",
      "Mathematics",
      "Moment (physics)",
      "Pattern recognition (psychology)",
      "Physics",
      "Pixel",
      "Robustness (evolution)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Niu",
        "given_name": "P."
      },
      {
        "surname": "Wang",
        "given_name": "C."
      },
      {
        "surname": "Chen",
        "given_name": "W."
      },
      {
        "surname": "Yang",
        "given_name": "H."
      },
      {
        "surname": "Wang",
        "given_name": "X."
      }
    ]
  },
  {
    "title": "Detecting facial manipulated videos based on set convolutional neural networks",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103119",
    "abstract": "With the boom of artificial intelligence, facial manipulation technology is becoming more simple and more numerous. At the same time, the technology also has a large and profound negative impact on face forensics, such as Deepfakes. In this paper, in order to aggregate multiframe features to detect facial manipulation videos, we solve facial manipulated video detection from set perspective and propose a novel framework based on set, which is called set convolutional neural network (SCNN). Three instances of the proposed framework SCNN are implemented and evaluated on the Deepfake TIMIT dataset, FaceForensics++ dataset and DFDC Preview datset. The results show that the method outperforms previous methods and can achieve state-of-the-art performance on both datasets. As a perspective, the proposed method is a fusion promotion of single-frame digital video forensics network.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000742",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Face (sociological concept)",
      "Frame (networking)",
      "Generalization",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Perspective (graphical)",
      "Programming language",
      "Set (abstract data type)",
      "Social science",
      "Sociology",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Zhaopeng"
      },
      {
        "surname": "Liu",
        "given_name": "Jiarui"
      },
      {
        "surname": "Lu",
        "given_name": "Wei"
      },
      {
        "surname": "Xu",
        "given_name": "Bozhi"
      },
      {
        "surname": "Zhao",
        "given_name": "Xianfeng"
      },
      {
        "surname": "Li",
        "given_name": "Bin"
      },
      {
        "surname": "Huang",
        "given_name": "Jiwu"
      }
    ]
  },
  {
    "title": "Long-term rate control for concurrent multipath real-time video transmission in heterogeneous wireless networks",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2020.102999",
    "abstract": "Concurrent multipath transmission provides an effective solution for streaming high-quality mobile videos in heterogeneous wireless networks. Rate control is commonly adopted in multimedia communication systems to fully utilize the available network bandwidth. This paper proposes a novel rate control for concurrent multipath video transmission. The existing rate control algorithms mainly adapt bit rate in the short-term pattern, i.e., without considering the long-term video transmission quality. We propose a long-term rate control scheme that takes into account the status of both the transmission buffer and video frames. First, a mathematical model is developed to formulate the non-convex problem of long-term quality maximization. Second, we develop a dynamic programming solution for online encoding bit rate control based on buffer status. The performance evaluation is conducted in a real test bed over LTE and Wi-Fi networks. Experimental results demonstrate that the proposed long-term rate control scheme achieves appreciable improvements over the short-term rate control schemes in terms of video quality and delay performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320302157",
    "keywords": [
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Multipath propagation",
      "Physics",
      "Quantum mechanics",
      "Real-time computing",
      "Telecommunications",
      "Term (time)",
      "Transmission (telecommunications)",
      "Wireless",
      "Wireless network"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Feng"
      },
      {
        "surname": "Zhang",
        "given_name": "Jie"
      },
      {
        "surname": "Zheng",
        "given_name": "Mingkui"
      },
      {
        "surname": "Wu",
        "given_name": "Jiyan"
      },
      {
        "surname": "Ling",
        "given_name": "Nam"
      }
    ]
  },
  {
    "title": "Depth-aware blending of smoothed images for Bokeh effect generation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103089",
    "abstract": "Bokeh effect is used in photography to capture images where the closer objects look sharp and everything else stays out-of-focus. Bokeh photos are generally captured using Single Lens Reflex cameras using shallow depth-of-field. Most of the modern smartphones can take bokeh images by leveraging dual rear cameras or a good auto-focus hardware. However, for smartphones with single-rear camera without a good auto-focus hardware, we have to rely on software to generate bokeh images. This kind of system is also useful to generate bokeh effect in already captured images. In this paper, an end-to-end deep learning framework is proposed to generate high-quality bokeh effect from images. The original image and different versions of smoothed images are blended to generate Bokeh effect with the help of a monocular depth estimation network. The model is trained through three phases to generate visually pleasing bokeh effect. The proposed approach is compared against a saliency detection based baseline and a number of approaches proposed in AIM 2019 Challenge on Bokeh Effect Synthesis. Extensive experiments are shown in order to understand different parts of the proposed algorithm. The network is lightweight and can process an HD image in 0.03 s. This approach ranked second in AIM 2019 Bokeh effect challenge-Perceptual Track.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000511",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Depth of field",
      "Engineering",
      "Focus (optics)",
      "Image (mathematics)",
      "Lens (geology)",
      "Monocular",
      "Operating system",
      "Optics",
      "Petroleum engineering",
      "Photography",
      "Physics",
      "Process (computing)",
      "Programming language",
      "Software",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Dutta",
        "given_name": "Saikat"
      }
    ]
  },
  {
    "title": "High-capacity reversible data hiding in encrypted image based on Huffman coding and differences of high nibbles of pixels",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103060",
    "abstract": "In this paper, we propose a new reversible data hiding method in encrypted images. Due to spatial correlation, there is a large probability that the adjacent pixels of the image have small differences, which is especially obvious on the high four most significant bits (high nibbles) of the pixels. If the high nibble of each pixel is regarded as a 4-bit value, the differences between the high nibbles of the adjacent pixels are mostly concentrated in a small range. Based on this fact, Huffman coding was used to encode all the differences between the high nibbles of the adjacent pixels in order to compress the four most significant bit (MSB) planes efficiently and create a large-capacity room. After creating room, a stream cipher is used to encrypt the image, and the room is reserved in the encrypted image for data hiding without losing information. The experimental results showed that the proposed method can achieve a larger embedding rate and better visual quality of the marked decrypted image than other related methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000304",
    "keywords": [
      "Artificial intelligence",
      "Coding (social sciences)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Data compression",
      "Encryption",
      "Huffman coding",
      "Image (mathematics)",
      "Information hiding",
      "Mathematics",
      "Pixel",
      "Statistics",
      "Stream cipher"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Chih-Cheng"
      },
      {
        "surname": "Chang",
        "given_name": "Chin-Chen"
      },
      {
        "surname": "Chen",
        "given_name": "Kaimeng"
      }
    ]
  },
  {
    "title": "Reversible data hiding based on three shadow images using rhombus magic matrix",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103064",
    "abstract": "This paper proposes a multi-image-based reversible data hiding method using a rhombus magic matrix. It takes a pixel pair as a position coordinate of a 256 × 256 modulus function matrix and extracts a 5-order rhombus matrix. It first embeds a 5-ary secret digit by producing three shadow pixel pairs which satisfies the predefined distance condition. Then it embeds a 6-ary secret digit by permuting the three shadow pixel pairs and assigning them to three ordered shadow images. Third, it embeds another 5-ary secret digit by modifying the pixel pair within a 3-order rhombus magic matrix in a shadow image. The receiver can extract the secret data and recover the original cover image when obtaining all shadow images. It also introduces a new application scenario for hierarchical security data transmission. The experimental results and analysis show that the proposed security scheme provides high embedding capacity with good visual quality of shadow images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032100033X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Composite material",
      "Computer science",
      "Computer vision",
      "Geometry",
      "Image (mathematics)",
      "Materials science",
      "Mathematics",
      "Matrix (chemical analysis)",
      "Pixel",
      "Psychology",
      "Psychotherapist",
      "Rhombus",
      "Shadow (psychology)",
      "Steganography",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Sisheng"
      },
      {
        "surname": "Chang",
        "given_name": "Chin-Chen"
      }
    ]
  },
  {
    "title": "Learning to predict the quality of distorted-then-compressed images via a deep neural network",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2020.103004",
    "abstract": "Being captured by amateur photographers, reciprocally propagated through multimedia pipelines, and compressed with different levels, real-world images usually suffer from a wide variety of hybrid distortions. Faced with this scenario, full-reference (FR) image quality assessment (IQA) algorithms can not deliver promising predictions due to the inferior references. Meanwhile, existing no-reference (NR) IQA algorithms remain limited in their efficacy to deal with different distortion types. To address this obstacle, we explore a NR-IQA metric by predicting the perceptual quality of distorted-then-compressed images using a deep neural network (DNN). First, we propose a novel two-stream DNN to handle both authentic distortions and synthetic compressions and adopt effective strategies to pre-train the two branches of the network. Specifically, we transfer the knowledge learned from in-the-wild images to account for authentic distortions by utilizing a pre-trained deep convolutional neural network (CNN) to provide meaningful initializations. Meanwhile, we build a CNN for synthetic compressions and pre-train it on a dataset including synthetic compressed images. Subsequently, we bilinearly pool these two sets of features as the image representation. The overall network is fine-tuned on an elaborately-designed auxiliary dataset, which is annotated by a reliable objective quality metric. Furthermore, we integrate the output of the authentic-distortion-aware branch with that of the overall network following a two-step prediction manner to boost the prediction performance, which can be applied in the distorted-then-compressed scenario when the reference image is available. Extensive experimental results on several databases especially on the LIVE Wild Compressed Picture Quality Database show that the proposed method achieves state-of-the-art performance with good generalizability and moderate computational complexity.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320302194",
    "keywords": [
      "Amplifier",
      "Artificial intelligence",
      "Artificial neural network",
      "Bandwidth (computing)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Distortion (music)",
      "Economics",
      "Epistemology",
      "Image (mathematics)",
      "Image quality",
      "Law",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Quality (philosophy)",
      "Representation (politics)",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Bowen"
      },
      {
        "surname": "Tian",
        "given_name": "Meng"
      },
      {
        "surname": "Zhang",
        "given_name": "Weixia"
      },
      {
        "surname": "Yao",
        "given_name": "Hongtai"
      },
      {
        "surname": "Wang",
        "given_name": "Xianpei"
      }
    ]
  },
  {
    "title": "Semantic loop closure detection based on graph matching in multi-objects scenes",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103072",
    "abstract": "Robust loop-closure detection is essential for visual SLAM. Traditional methods often focus on the geometric and visual features in most scenes but ignore the semantic information provided by objects. Based on this consideration, we present a strategy that models the visual scene as semantic sub-graph by only preserving the semantic and geometric information from object detection. To align two sub-graphs efficiently, we use a sparse Kuhn–Munkres algorithm to speed up the search for correspondence among nodes. The shape similarity and the Euclidean distance between objects in the 3-D space are leveraged unitedly to measure the image similarity through graph matching. Furthermore, the proposed approach has been analyzed and compared with the state-of-the-art algorithms at several datasets as well as two indoor real scenes, where the results indicate that our semantic graph-based representation without extracting visual features is feasible for loop-closure detection at potential and competitive precision.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000389",
    "keywords": [
      "Artificial intelligence",
      "Closure (psychology)",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Economics",
      "Euclidean geometry",
      "Geometry",
      "Graph",
      "Image (mathematics)",
      "Law",
      "Loop (graph theory)",
      "Market economy",
      "Matching (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Semantic matching",
      "Semantic similarity",
      "Similarity (geometry)",
      "Statistics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Qin",
        "given_name": "Cao"
      },
      {
        "surname": "Zhang",
        "given_name": "Yunzhou"
      },
      {
        "surname": "Liu",
        "given_name": "Yingda"
      },
      {
        "surname": "Lv",
        "given_name": "Guanghao"
      }
    ]
  },
  {
    "title": "Human pose estimation and its application to action recognition: A survey",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103055",
    "abstract": "Human pose estimation aims at predicting the poses of human body parts in images or videos. Since pose motions are often driven by some specific human actions, knowing the body pose of a human is critical for action recognition. This survey focuses on recent progress of human pose estimation and its application to action recognition. We attempt to provide a comprehensive review of recent bottom-up and top-down deep human pose estimation models, as well as how pose estimation systems can be used for action recognition. Thanks to the availability of commodity depth sensors like Kinect and its capability for skeletal tracking, there has been a large body of literature on 3D skeleton-based action recognition, and there are already survey papers such as [1] about this topic. In this survey, we focus on 2D skeleton-based action recognition where the human poses are estimated from regular RGB images instead of depth images. We summarize the performance of recent action recognition methods that use pose estimated from color images as input, then show that there is much room for improvements in this direction.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000262",
    "keywords": [
      "3D pose estimation",
      "Action (physics)",
      "Action recognition",
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Engineering",
      "Estimation",
      "Focus (optics)",
      "Human skeleton",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Pose",
      "Quantum mechanics",
      "RGB color model",
      "Systems engineering"
    ],
    "authors": [
      {
        "surname": "Song",
        "given_name": "Liangchen"
      },
      {
        "surname": "Yu",
        "given_name": "Gang"
      },
      {
        "surname": "Yuan",
        "given_name": "Junsong"
      },
      {
        "surname": "Liu",
        "given_name": "Zicheng"
      }
    ]
  },
  {
    "title": "Copy Move Forgery Detection based on double matching",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103057",
    "abstract": "Copy Move is a technique widespreadly used in digital image tampering, meaning Copy Move Forgery Detection (CMFD) is still a significant research. In this paper, a novel CMFD method is proposed, including double matching process and region localizing process. In double matching process, the first matching is conducted on Delaunay triangles consisting of Local Intensity Order Pattern (LIOP) keypoints, to find the approximate location of suspicious regions. In order to find sufficient keypoint pairs, the existing set of matching triangles is expanded by adding their neighbors iteratively, covering the whole tampered regions, and the second matching with a looser threshold is conducted on the vertices. In the region localizing process, considering the case of multiple copies, Density-Based Spatial Clustering of Applications with Noise (DBSCAN) is used to classify the keypoint pairs described in a new model. Experimental results indicate that the proposed method, with good robustness, outperforms some state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000274",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Canopy clustering algorithm",
      "Chemistry",
      "Cluster analysis",
      "Computer science",
      "Computer vision",
      "Correlation clustering",
      "DBSCAN",
      "Delaunay triangulation",
      "Gene",
      "Matching (statistics)",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Process (computing)",
      "Programming language",
      "Robustness (evolution)",
      "Set (abstract data type)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Lyu",
        "given_name": "Qiyue"
      },
      {
        "surname": "Luo",
        "given_name": "Junwei"
      },
      {
        "surname": "Liu",
        "given_name": "Ke"
      },
      {
        "surname": "Yin",
        "given_name": "Xiaolin"
      },
      {
        "surname": "Liu",
        "given_name": "Jiarui"
      },
      {
        "surname": "Lu",
        "given_name": "Wei"
      }
    ]
  },
  {
    "title": "Pain detection using batch normalized discriminant restricted Boltzmann machine layers",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103062",
    "abstract": "A system for automatic pain detection whereby pain-related features are extracted from facial images using a four-layer Convolutional Deep Belief Network (CDBN) is proposed in this study. The CDBN is trained by greedy layer-wise procedure whereby each added layer is trained as a Convolutional Restricted Boltzmann Machine (CRBM) by contrastive divergence. Since conventional CRBM is trained in a purely unsupervised manner, there is no guarantee that learned features are appropriate for the supervised task at hand. A discriminative objective based on between-class and within-class distances is proposed to adapt CRBM to learn task-related features. When discriminative and generative objectives are appropriately combined, a competitive classification performance can be achieved. Moreover, we introduced batch normalization (BN) units in the structure of the CRBM model to smooth optimization landscape and speed up the learning process. BN units come right before sigmoid units. Extracted features are then used to train a linear SVM to classify each frame into pain or no-pain classes. Extensive experiments on UNBC-McMaster Shoulder Pain database demonstrate the effectiveness of the proposed method for automatic pain detection.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000316",
    "keywords": [
      "Anthropology",
      "Artificial intelligence",
      "Computer science",
      "Deep learning",
      "Discriminative model",
      "Machine learning",
      "Normalization (sociology)",
      "Pattern recognition (psychology)",
      "Restricted Boltzmann machine",
      "Sociology",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Kharghanian",
        "given_name": "Reza"
      },
      {
        "surname": "Peiravi",
        "given_name": "Ali"
      },
      {
        "surname": "Moradi",
        "given_name": "Farshad"
      },
      {
        "surname": "Iosifidis",
        "given_name": "Alexandros"
      }
    ]
  },
  {
    "title": "Activity recognition in video sequences over qualitative abstracts of a diagram-based representation schema",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103061",
    "abstract": "Explicit reasoning over a spatial substrate, i.e., space–time information structures underlying a spatial problem, simplifies reasoning. Diagrammatic reasoning makes use of diagrams for exploiting such underlying structures. This paper proposes a novel approach combining diagrammatic reasoning with qualitative spatial and temporal reasoning techniques to visualize and perceive spatio-temporal relations among objects in a video. The hybrid techniques explore information over the spatial substrate for relational extractions. Different relations among objects in transition define short-term activities. Mealy machines are learned over patterns of short-term activities as activity recognizers. The proposed representation and recognition mechanism is validated by conducting experiments for video activity recognition from DARPA Mind’s Eye and J-HMDB dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000298",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Database",
      "Diagram",
      "Information retrieval",
      "Law",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Schema (genetic algorithms)"
    ],
    "authors": [
      {
        "surname": "Nath",
        "given_name": "Chayanika D."
      },
      {
        "surname": "Hazarika",
        "given_name": "Shyamanta M."
      }
    ]
  },
  {
    "title": "Steganalytic feature based adversarial embedding for adaptive JPEG steganography",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103066",
    "abstract": "In this paper, we present a novel adversarial embedding scheme named Steganalytic Feature based Adversarial Embedding (SFAE), which is elaborately designed in a non-data-driven style. Firstly, a novel DCTR based adversary is designed to generate adversarial stego images which can not only resist feature based steganalysis but also deep learning based steganalysis. Specifically, our adversary consists of an end-to-end neural network structure, while its inner weights are set according to DCTR rather than learned from datasets. Secondly, we use the minimum distance to the cover in steganalytic space as the criterion to select the optimal adversarial stego image, rather than fooling the adversary. Last but not least, we present two SFAE implementations to adapt to different cases. One is Iterative SFAE, which needs to calculate gradients multiple times. Iterative SFAE is more secure but has higher complexity. It fits the case that the steganographer has adequate computing resources. Another implementation is Oneshot SFAE, which can calculate gradients once. Oneshot SFAE trades the security for lower complexity. It fits the steganographer that has stricter requirements for running time. Experiments demonstrate that SFAE is effective to improve the security of conventional steganographic schemes against the state-of-the-art steganalysis including both feature based steganalysis and deep learning based steganalysis.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000328",
    "keywords": [
      "Adversarial system",
      "Adversary",
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Cover (algebra)",
      "Deep learning",
      "Embedding",
      "Engineering",
      "Feature (linguistics)",
      "Image (mathematics)",
      "JPEG",
      "Linguistics",
      "MNIST database",
      "Mathematics",
      "Mechanical engineering",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Steganalysis",
      "Steganography"
    ],
    "authors": [
      {
        "surname": "Ma",
        "given_name": "Sai"
      },
      {
        "surname": "Zhao",
        "given_name": "Xianfeng"
      }
    ]
  },
  {
    "title": "High capacity reversible data hiding in encrypted images using SIBRW and GCC",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2020.102932",
    "abstract": "In this paper, a reversible data hiding in encrypted images (RDHEI) method combining GCC (group classification encoding) and SIBRW containing sixteen image-based rearrangement ways is proposed to achieve high-capacity data embedding in encrypted images. Each way of SIBRW aims at bringing strongly-correlated bits of each higher bit-plane together by rearranging each higher bit-plane. For each higher bit-plane, the optimal way achieving the most concentrated aggregation performance is selected from SIBRW to rearrange this bit-plane, and then, GCC compresses the rearranged bit-plane in group-by-group manner. By making full use of strong-correlation between adjacent groups, GCC can compress not only consecutive several groups whose bits are valued 1 (or 0) but also a single group so that a large embedding space is provided. The encryption method including the bit-level XOR-encryption and scrambling operations enhances the security. The experimental results show that the proposed scheme can achieve large embedding capacity and high security.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301632",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Bit (key)",
      "Bit field",
      "Bit plane",
      "Computer network",
      "Computer science",
      "Drilling",
      "Embedding",
      "Encoding (memory)",
      "Encryption",
      "Engineering",
      "Geometry",
      "Group (periodic table)",
      "Image (mathematics)",
      "Information hiding",
      "Mathematical analysis",
      "Mathematics",
      "Mechanical engineering",
      "Physics",
      "Plane (geometry)",
      "Quantum mechanics",
      "Scheme (mathematics)",
      "Scrambling",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Weng",
        "given_name": "Shaowei"
      },
      {
        "surname": "Zhang",
        "given_name": "Caiying"
      },
      {
        "surname": "Zhang",
        "given_name": "Tiancong"
      },
      {
        "surname": "Chen",
        "given_name": "Kaimeng"
      }
    ]
  },
  {
    "title": "Distance based kernels for video tensors on product of Riemannian matrix manifolds",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103045",
    "abstract": "In this paper, we explore the inherent geometry of video tensors by modeling them as points in product of Riemannian matrix manifolds. A video tensor is decomposed into three modes (factors) using matrix unfolding operation and each mode is represented as a point in a product space of Grassmannian and symmetric positive definite (SPD) matrix manifold. Hence a video is represented as a point in the Cartesian product of three such product spaces. Being a manifold valued (non-Euclidean) representation, application of several state-of-the-art Euclidean machine learning algorithms lead to inferior results. To overcome this, we propose positive definite kernels which map the points from product manifold space to Hilbert space. The proposed kernel functions implicitly make use of geodesic distance on product manifold to obtain a similarity measure and generate a kernel-gram matrix. In addition, we generate a discriminative feature representation for each manifold valued point using kernel-gram matrix diagonalization. Classification is performed in a sparse framework. The proposed methodology is tested over three publicly available datasets for hand gesture, traffic signal and sign language recognition. Experimentation performed over these datasets show that the proposed methodology is powerful in terms of classification accuracy in comparison with the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000195",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Engineering",
      "Euclidean space",
      "Geodesic",
      "Kernel (algebra)",
      "Manifold (fluid mechanics)",
      "Mathematical analysis",
      "Mathematics",
      "Mechanical engineering",
      "Pattern recognition (psychology)",
      "Pure mathematics",
      "Riemannian manifold"
    ],
    "authors": [
      {
        "surname": "Sharma",
        "given_name": "Krishan"
      },
      {
        "surname": "Rameshan",
        "given_name": "Renu"
      }
    ]
  },
  {
    "title": "Real-time video super-resolution using lightweight depthwise separable group convolutions with channel shuffling",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103038",
    "abstract": "In recent years, convolutional neural networks (CNNs) have accelerated the developments of video super resolution (SR) for achieving higher image quality. However, the computational cost of existing CNN-based video super-resolution is too heavy for real-time applications. In this paper, we propose a new video super-resolution framework using lightweight frame alignment module and well-designed up-sampling module for real-time processing. Specifically, our framework, which is called as Lightweight Shuffle Video Super-Resolution Network (LSVSR), combines channel shuffling, depthwise convolution and pointwise group convolution to significantly reduce the computational burden during frame alignment and high-resolution frame reconstruction. On the public benchmark datasets, our proposed network outperforms the state-of-the-art lightweight video SR networks in terms of objective (PSNR and SSIM) and subjective evaluations, number of network parameters and floating-point operations. Our network can achieve real-time 540P to 2160P 4 × super-resolution for more than 60fps using desktop GPUs or mobile phones with neural processing unit.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032100016X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Decoding methods",
      "Frame (networking)",
      "Frame rate",
      "Geodesy",
      "Geography",
      "Group of pictures",
      "Mathematical analysis",
      "Mathematics",
      "Pointwise",
      "Programming language",
      "Shuffling"
    ],
    "authors": [
      {
        "surname": "Xiao",
        "given_name": "Zhijiao"
      },
      {
        "surname": "Zhang",
        "given_name": "Zhikai"
      },
      {
        "surname": "Hung",
        "given_name": "Kwok-Wai"
      },
      {
        "surname": "Lui",
        "given_name": "Simon"
      }
    ]
  },
  {
    "title": "Revealing stable and unstable modes of denoisers through nonlinear eigenvalue analysis",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103041",
    "abstract": "In this paper, we propose to analyze stable and unstable modes of black-box image denoisers through nonlinear eigenvalue analysis. We aim to find input images for which the denoiser output is proportional to the input. We treat this as a generalized nonlinear eigenproblem. Potential implications are wide, as most image processing algorithms can be viewed as black-box operators. We introduce a generalized nonlinear power-method to solve eigenproblems for such operators. This allows us to reveal stable modes of the denoiser: optimal inputs, achieving superior PSNR in noise removal. Analogously to the linear case, such stable modes show coarse structures and correspond to large eigenvalues. We also provide a method to generate unstable modes, which the denoiser suppresses strongly, which are textural with small eigenvalues. We validate the method using total-variation (TV) and demonstrate it on the EPLL (Zoran–Weiss) and the Non-local means denoisers. Finally, we suggest an encryption–decryption application.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000171",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Artificial intelligence",
      "Black box",
      "Computer science",
      "Eigenvalues and eigenvectors",
      "Encryption",
      "Image (mathematics)",
      "Machine learning",
      "Mathematics",
      "Noise (video)",
      "Nonlinear system",
      "Operating system",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Stability (learning theory)"
    ],
    "authors": [
      {
        "surname": "Hait-Fraenkel",
        "given_name": "Ester"
      },
      {
        "surname": "Gilboa",
        "given_name": "Guy"
      }
    ]
  },
  {
    "title": "Parallel-fusion LSTM with synchronous semantic and visual information for image captioning",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103044",
    "abstract": "For synchronously combining the dynamic semantic and visual information in the decoder part of image captioning, we propose a novel parallel-fusion LSTM (pLSTM) structure in this paper. Two parallel LSTMs with attributes and visual information of image are fused by the hidden states at every time step, which makes the attributes and visual information complementary or enhanced for generating more accurate captions. According to the different ways of integrating semantic information from attribute LSTM to visual LSTM, we propose two models pLSTM with attention (pLSTM-A) and pLSTM with guiding (pLSTM-G). pLSTM-A can automatically capture the crucial semantic and visual information to generate captions, and pLSTM-G directly adjusts the hidden state of visual LSTM by synchronous semantic information to the critical region. For verifying the effectiveness of our proposed pLSTM, we conduct a series of experiments on MSCOCO and Flickr30K datasets, and the experimental results outperform some state-of-the-art image captioning methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000183",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Closed captioning",
      "Computer science",
      "Image (mathematics)",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Programming language",
      "Semantics (computer science)",
      "Speech recognition",
      "State (computer science)",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Jing"
      },
      {
        "surname": "Li",
        "given_name": "Kangkang"
      },
      {
        "surname": "Wang",
        "given_name": "Zhe"
      }
    ]
  },
  {
    "title": "Interactive information module for person re-identification",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103033",
    "abstract": "In person re-identification (Re-ID) task, multi-branch networks acquire better performance by combining global features and local features. Obviously, local branch can obtain detailed information of person pictures but may work on invalid regions when person pictures have imprecise bounding boxes. On the contrary, global branch can be aware of the position of person but hard to acquire detailed information of person pictures. Meanwhile, lots of multi-branch networks ignore mutual information among different branches. Therefore, it is necessary to enhance interaction of global branch and local branch. For this purpose, we propose Interactive Information Module (IIM). IIM includes two components named Global-map Attention Module (GAM) and Labeled-class Mutual Learning (LML), respectively. GAM leverages heatmaps generated by global branch to guide calculation of local attention and obtains a composite global feature by combining local features. GAM relys more on the performance of global branch which decides the quality of heatmaps. To improve performance of global branch, we propose LML to promote convergent rate of global branch. Extensive experiments implemented on Market-1501, DukeMTMC-ReID, and CUHK03-NP datasets confirm that our method achieves state-of-the-art results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000122",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Bounding overwatch",
      "Class (philosophy)",
      "Computer science",
      "Economics",
      "Feature (linguistics)",
      "Finance",
      "Identification (biology)",
      "Linguistics",
      "Management",
      "Mutual information",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Position (finance)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Xudong"
      },
      {
        "surname": "Kong",
        "given_name": "Jun"
      },
      {
        "surname": "Jiang",
        "given_name": "Min"
      },
      {
        "surname": "Li",
        "given_name": "Sha"
      }
    ]
  },
  {
    "title": "No-reference quality assessment of HEVC video streams based on visual memory modelling",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2020.103011",
    "abstract": "Providing adequate Quality of Experience (QoE) to end-users is crucial for streaming service providers. In this paper, in order to realize automatic quality assessment, a No-Reference (NR) bitstream Human-Vision-System-(HVS)-based video quality assessment (VQA) model is proposed. Inspired by discoveries from the neuroscience community, which suggest there is a considerable overlap between active areas of the brain when engaging in video quality assessment and saliency detection tasks, saliency maps are used in the proposed method to improve the quality assessment accuracy. To this end, saliency maps are first generated from features extracted from the HEVC bitstream. Then, saliency map statistics are employed to create a model of visual memory. Finally, a support vector regression pipeline learns an estimate of the video quality from the visual memory, saliency, and frame features. Evaluations on SJTU dataset indicate that the proposed bitstream based no-reference video quality assessment algorithm achieves a competitive performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320302236",
    "keywords": [
      "Computer network",
      "Computer science",
      "Economics",
      "Epistemology",
      "Metric (unit)",
      "Operations management",
      "Philosophy",
      "Quality (philosophy)",
      "STREAMS",
      "Video quality"
    ],
    "authors": [
      {
        "surname": "Banitalebi-Dehkordi",
        "given_name": "Mehdi"
      },
      {
        "surname": "Ebrahimi-Moghadam",
        "given_name": "Abbas"
      },
      {
        "surname": "Khademi",
        "given_name": "Morteza"
      },
      {
        "surname": "Hadizadeh",
        "given_name": "Hadi"
      }
    ]
  },
  {
    "title": "Exploiting color for graph-based 3D point cloud denoising",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103027",
    "abstract": "A point cloud is a representation of a 3D scene as a discrete collection of geometry plus other attributes such as color, normal, transparency associated with each point. The traditional acquisition process of a 3D point cloud, e.g. using depth information acquired directly by active sensors or indirectly from multi-viewpoint images, suffers from a significant amount of noise. Hence, the problem of point cloud denoising has recently received a lot of attention. However, most existing techniques attempt to denoise only the geometry of each point, based on the geometry information of the neighboring points; there are very few works at all considering the problem of denoising the color attributes of a point cloud. In this paper, we move beyond the state of the art and we propose a novel technique employing graph-based optimization, taking advantage of the correlation between geometry and color, and using it as a powerful tool for several different tasks, i.e. color denoising, geometry denoising, and combined geometry and color denoising. The proposed method is based on the notion that the correct location of a point also depends on the color attribute and not only the geometry of the neighboring points, and the correct color also depends on the geometry of the neighbors. The proposed method constructs a suitable k-NN graph from geometry and color and applies graph-based convex optimization to obtain the denoised point cloud. Extensive simulation results on both real-world and synthetic point clouds show that the proposed denoising technique outperforms state-of-the-art methods using both subjective and objective quality metrics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000092",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Geometry",
      "Graph",
      "Mathematics",
      "Noise reduction",
      "Point (geometry)",
      "Point cloud",
      "Regular polygon",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Irfan",
        "given_name": "Muhammad Abeer"
      },
      {
        "surname": "Magli",
        "given_name": "Enrico"
      }
    ]
  },
  {
    "title": "Saliency detection via cross-scale deep inference",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103031",
    "abstract": "The small, moderate, and large scale saliency patterns in images are valuable to be extracted in saliency detection. By the observation that the probability of small and large saliency patterns appearing in datasets is lower than that of moderate scale saliency patterns. As results, a deep saliency model trained on such datasets would converge to moderate scale saliency patterns, and it is hard to well infer the small and large scale saliency patterns because they are not encoded efficiently in the model for their low probability. Thus a novel but simple saliency detection method using cross-scale deep inference is presented in this paper. Moreover, a new network architecture, in which the attention mechanism is exploited by multiple layers, is proposed to improve the receptive fields of various scale saliency patterns in different scale images. The presented cross-scale deep inference could improve the representation power of small and large scale saliency patterns encoded in multiple scale images efficiently. The quantitative and qualitative evaluation demonstrates our deep model achieves a promising results across a wide of metrics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000110",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Deep learning",
      "Image (mathematics)",
      "Inference",
      "Law",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Politics",
      "Quantum mechanics",
      "Representation (politics)",
      "Saliency map",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "Ren",
        "given_name": "Dakai"
      },
      {
        "surname": "Wen",
        "given_name": "Xiangming"
      },
      {
        "surname": "Jia",
        "given_name": "Tao"
      },
      {
        "surname": "Chen",
        "given_name": "Jiazhong"
      },
      {
        "surname": "Li",
        "given_name": "Zongyi"
      }
    ]
  },
  {
    "title": "Various density light field image coding based on distortion minimization interpolation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103036",
    "abstract": "In recent years, the light field (LF) as a new imaging modality has attracted wide interest. The large data volume of LF images poses great challenge to LF image coding, and the LF images captured by different devices show significant differences in angular domain. In this paper we propose a view prediction framework to handle LF image coding with various sampling density. All LF images are represented as view arrays. We first partition the views into reference view (RV) set and intermediate view (IV) set. The RVs are rearranged into a pseudo sequence and directly compressed by a video encoder. Other views are then predicted by the RVs. To exploit the four dimensional signal structure, we propose the linear approximation prior (LAP) to reveal the correlation among LF views and efficiently remove the LF data redundancy. Based on the LAP, a distortion minimization interpolation (DMI) method is used to predict IVs. To robustly handle the LF images with different sampling density, we propose an Iteratively Updating depth image based rendering (IU-DIBR) method to extend our DMI. Some auxiliary views are generated to cover the target region and then the DMI calculates reconstruction coefficients for the IVs. Different view partition patterns are also explored. Extensive experiments on different types LF images also valid the efficiency of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000158",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Coding (social sciences)",
      "Computer science",
      "Computer vision",
      "Encoder",
      "Image (mathematics)",
      "Interpolation (computer graphics)",
      "Light field",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Redundancy (engineering)",
      "Rendering (computer graphics)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Shengyang"
      },
      {
        "surname": "Chen",
        "given_name": "Zhibo"
      }
    ]
  },
  {
    "title": "AdvCapsNet: To defense adversarial attacks based on Capsule networks",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103037",
    "abstract": "Convolutional neural networks have achieved the state-of-the-art results across numerous applications, but recent work finds that these models can be easily fooled by adversarial perturbations. This is partially due to gradient calculation instability, which may be amplified throughout network layers (Liao et al., 2018). To address this issue, we propose a novel AdvCapsNet derived from Capsule (Sabour et al., 2017), which utilizes a significantly more complicated non-linearity, to defend against adversarial attacks. In this paper, we focus on the transfer-based black-box adversarial attacks, which are more practical than their white-box counterparts. Specifically, we investigate vanilla Capsule’s robustness and boost its performance by introducing an adversarial loss function as regularization. The weight updating between capsule layers is implemented via dynamic routing regularized by the additional adversarial term. Extensive experiments demonstrate that the proposed AdvCapsNet can significantly boost Capsule’s robustness and that AdvCapsNet is far more resistance to adversarial attacks than alternative baselines, including both CNN- and Capsule-based defense models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000134",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Capsule",
      "Computer science",
      "Computer security",
      "Geology",
      "Paleontology"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Yueqiao"
      },
      {
        "surname": "Su",
        "given_name": "Hang"
      },
      {
        "surname": "Zhu",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "A minimal model for classification of rotated objects with prediction of the angle of rotation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103054",
    "abstract": "In classification tasks, the robustness against various image transformations remains a crucial property of CNN models. When acquired using the data augmentation it comes at the price of a considerable increase in training time and the risk of overfitting. Consequently, researching other ways to endow CNNs with invariance to various transformations is an intensive field of study. This paper presents a new reduced, rotation-invariant, classification model composed of two parts: a feature representation mapping and a classifier. We provide an insight into the principle and we show that the proposed model is trainable. The model we obtain is smaller and has angular prediction capabilities. We illustrate the results on the MNIST-rot and CIFAR-10 datasets. We achieve the state-of-the-art classification score on MNIST-rot, and improve by 20% the state of the art score on rotated CIFAR-10. In all cases, we can predict the rotation angle.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000250",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Classifier (UML)",
      "Computer science",
      "Contextual image classification",
      "Deep learning",
      "Gene",
      "Image (mathematics)",
      "Invariant (physics)",
      "MNIST database",
      "Machine learning",
      "Mathematical physics",
      "Mathematics",
      "Overfitting",
      "Pattern recognition (psychology)",
      "Robustness (evolution)",
      "Rotation (mathematics)"
    ],
    "authors": [
      {
        "surname": "Rodriguez Salas",
        "given_name": "Rosemberg"
      },
      {
        "surname": "Dokládal",
        "given_name": "Petr"
      },
      {
        "surname": "Dokladalova",
        "given_name": "Eva"
      }
    ]
  },
  {
    "title": "Lossless chain code compression with an improved Binary Adaptive Sequential Coding of zero-runs",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103050",
    "abstract": "A new method for encoding a sequence of integers, named Binary Adaptive Sequential Coding with Return to Bias, is proposed in this paper. It extends the compressing pipeline for chain codes’ compression consisting of Burrows Wheeler Transform, Move-To-Front Transform, and Adaptive Arithmetic Coding. We also explain when to include the Zero-Run Transform into the above-mentioned pipeline. The Zero-Run Transform generates a sequence of integers corresponding to the number of zero-runs. This sequence is encoded by Golomb coding, Binary Adaptive Sequential Coding, and the new Binary Adaptive Sequential Coding with Return to Bias. Finally, a comparison is performed with the two state-of-the-art methods. The proposed method achieved similar compression efficiency for the Freeman chain code in eight directions. However, for the chain codes with shorter alphabets (Freeman chain code in four directions, Vertex Chain Code, and Three-OrThogonal chain code), the introduced method outperforms the referenced ones.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000225",
    "keywords": [
      "Adaptive coding",
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Astronomy",
      "Binary code",
      "Binary number",
      "Chain (unit)",
      "Code (set theory)",
      "Coding (social sciences)",
      "Compression (physics)",
      "Computer science",
      "Context-adaptive binary arithmetic coding",
      "Data compression",
      "Image (mathematics)",
      "Image compression",
      "Image processing",
      "Linguistics",
      "Lossless JPEG",
      "Lossless compression",
      "Mathematics",
      "Philosophy",
      "Physics",
      "Programming language",
      "Set (abstract data type)",
      "Statistics",
      "Thermodynamics",
      "Zero (linguistics)"
    ],
    "authors": [
      {
        "surname": "Žalik",
        "given_name": "Borut"
      },
      {
        "surname": "Mongus",
        "given_name": "Domen"
      },
      {
        "surname": "Žalik",
        "given_name": "Krista Rizman"
      },
      {
        "surname": "Podgorelec",
        "given_name": "David"
      },
      {
        "surname": "Lukač",
        "given_name": "Niko"
      }
    ]
  },
  {
    "title": "Salient object detection via a boundary-guided graph structure",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103048",
    "abstract": "Graph-based salient object detection methods have gained more and more attention recently. However, existing works fail to separate effectively salient object and background in some challenging scenes. Inspired by this observation, we propose an effective salient object detection method based on a novel boundary-guided graph structure. More specifically, the input image is firstly segmented into a series of superpixels. Then we integrate two prior cues to generate the coarse saliency map, a novel weighting mechanism is proposed to balance the proportion of two prior cues according to their performance. Secondly, we propose a novel boundary-guided graph structure to explore deeply the intrinsic relevance between superpixels. Based on the proposed graph structure, an iterative propagation mechanism is constructed to refine the coarse saliency map. Experimental results on four datasets show adequately the superiority of the proposed method than other state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000213",
    "keywords": [
      "Artificial intelligence",
      "Boundary (topology)",
      "Computer science",
      "Computer vision",
      "Graph",
      "Mathematical analysis",
      "Mathematics",
      "Medicine",
      "Pattern recognition (psychology)",
      "Radiology",
      "Salient",
      "Theoretical computer science",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Yunhe"
      },
      {
        "surname": "Jia",
        "given_name": "Tong"
      },
      {
        "surname": "Pang",
        "given_name": "Yu"
      },
      {
        "surname": "Sun",
        "given_name": "Jiaduo"
      },
      {
        "surname": "Xue",
        "given_name": "Dingyu"
      }
    ]
  },
  {
    "title": "Quality assessment for color correction-based stitched images via bi-directional matching",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103051",
    "abstract": "With the deepening of social information, the panoramic image has drawn a significant interest of viewers and researchers as it can provide a very wide field of view (FoV). Since panoramic images are usually obtained by capturing images with the overlapping regions and then stitching them together, image stitching plays an important role in generating panoramic images. In order to effectively evaluate the quality of stitched images, a novel quality assessment method based on bi-directional matching is proposed for stitched images. Specifically, dense correspondences between the testing and benchmark stitched images are first established by bi-directional SIFT-flow matching. Then, color-aware, geometric-aware and structure-aware features are respectively extracted and fused via support vector regression (SVR) to obtain the final quality score. Experiments on our newly constructed database and ISIQA database demonstrate that the proposed method can achieve comparable performance compared with the conventional blind quality metrics and the quality metrics specially designed for stitched images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000237",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Field (mathematics)",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Image quality",
      "Image stitching",
      "Matching (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pure mathematics",
      "Scale-invariant feature transform",
      "Statistics",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Xuejin"
      },
      {
        "surname": "Chai",
        "given_name": "Xiongli"
      },
      {
        "surname": "Shao",
        "given_name": "Feng"
      }
    ]
  },
  {
    "title": "Gait recognition based on vision systems: A systematic survey",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103052",
    "abstract": "With the growing popularity of biometrics technology in the pattern recognition field, especiallyidentification of human has gained the attention of researchers from both academia and industry. One such type of biometric technique is Gait recognition, which is used to identify a human being based on their walking style. Generally, two types of approaches are adopted by any algorithm designed for gait recognition, namely model based and model free approaches. The key reason behind the popularity of gait recognition is that it can identify a person from a considerable distance while other biometrics has failed to do so. In this paper, the authors have conducted a survey of extant studies on gait recognition in consideration of gait recognition approaches and phases of a gait cycle. Moreover, some aspects like floor sensors, accelerometer based recognition, the influences of environmental factors, which are ignored by exiting surveys, are also covered in our survey study. The information of gait is usually obtained from different parts of silhouettes. This paper also describes different benchmark datasets for gait recognition. This study will provide firsthand knowledge to the researchers working on the gait recognition domain in any real-world field. It has been observed that work done on the gait recognition with sufficiently high accuracy is limited in comparison to research on various other biometric recognition systems and has enough potential for future research.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000249",
    "keywords": [
      "Accelerometer",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Biometrics",
      "Botany",
      "Computer science",
      "Computer vision",
      "Evolutionary biology",
      "Extant taxon",
      "Field (mathematics)",
      "Gait",
      "Gait analysis",
      "Geodesy",
      "Geography",
      "Identification (biology)",
      "Machine learning",
      "Mathematics",
      "Medicine",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physical medicine and rehabilitation",
      "Popularity",
      "Psychology",
      "Pure mathematics",
      "Social psychology"
    ],
    "authors": [
      {
        "surname": "Kumar",
        "given_name": "Munish"
      },
      {
        "surname": "Singh",
        "given_name": "Navdeep"
      },
      {
        "surname": "Kumar",
        "given_name": "Ravinder"
      },
      {
        "surname": "Goel",
        "given_name": "Shubham"
      },
      {
        "surname": "Kumar",
        "given_name": "Krishan"
      }
    ]
  },
  {
    "title": "Adaptive convolution kernel for artificial neural networks",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2020.103015",
    "abstract": "Many deep neural networks are built by using stacked convolutional layers of fixed and single size (often 3 × 3) kernels. This paper describes a method for learning the size of convolutional kernels to provide varying size kernels in a single layer. The method utilizes a differentiable, and therefore backpropagation-trainable Gaussian envelope which can grow or shrink in a base grid. Our experiments compared the proposed adaptive layers to ordinary convolution layers in a simple two-layer network, a deeper residual network, and a U-Net architecture. The results in the popular image classification datasets such as MNIST, MNIST-CLUTTERED, CIFAR-10, Fashion, and “Faces in the Wild” showed that the adaptive kernels can provide statistically significant improvements on ordinary convolution kernels. A segmentation experiment in the Oxford-Pets dataset demonstrated that replacing ordinary convolution layers in a U-shaped network with 7 × 7 adaptive layers can improve its learning performance and ability to generalize.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032030225X",
    "keywords": [],
    "authors": [
      {
        "surname": "Tek",
        "given_name": "F. Boray"
      },
      {
        "surname": "Çam",
        "given_name": "İlker"
      },
      {
        "surname": "Karlı",
        "given_name": "Deniz"
      }
    ]
  },
  {
    "title": "Single image deraining using Context Aggregation Recurrent Network",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103039",
    "abstract": "Single image deraining is a challenging problem due to the presence of non-uniform rain densities and the ill-posedness of the problem. Moreover, over-/under-deraining can directly impact the performance of vision systems. To address these issues, we propose an end-to-end Context Aggregation Recurrent Network, called CARNet, to remove rain streaks from single images. In this paper, we assume that a rainy image is the linear combination of a clean background image with rain streaks and propose to take advantage of the context information and feature reuse to learn the rain streaks. In our proposed network, we first use the dilation technique to effectively aggregate context information without sacrificing the spatial resolution, and then leverage a gated subnetwork to fuse the intermediate features from different levels. To better learn and reuse rain streaks, we integrate a LSTM module to connect different recurrences for passing the information learned from the previous stages about the rain streaks to the following stage. Finally, to further refine the coarsely derained image, we introduce a refinement module to better preserve image details. As for the loss function, the L1-norm perceptual loss and SSIM loss are adopted to reduce the gridding artifacts caused by the dilated convolution. Experiments conducted on synthetic and real rainy images show that our CARNet achieves superior deraining performance both qualitatively and quantitatively over the state-of-the-art approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000146",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Convolution (computer science)",
      "Dilation (metric space)",
      "Ecology",
      "Feature (linguistics)",
      "Geology",
      "Image (mathematics)",
      "Information loss",
      "Leverage (statistics)",
      "Linguistics",
      "Mathematics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Reuse"
    ],
    "authors": [
      {
        "surname": "Tang",
        "given_name": "Qunfang"
      },
      {
        "surname": "Yang",
        "given_name": "Jie"
      },
      {
        "surname": "Liu",
        "given_name": "Haibo"
      },
      {
        "surname": "Guo",
        "given_name": "Zhiqiang"
      },
      {
        "surname": "Jia",
        "given_name": "Wenjing"
      }
    ]
  },
  {
    "title": "Anomaly3D: Video anomaly detection based on 3D-normality clusters",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103047",
    "abstract": "Abnormal behavior detection in surveillance videos is necessary for public monitoring and safety. In human-based surveillance systems, it requires continuous human attention and observation, which is a difficult task. The autonomous detection of such events is of essential significance. However, due to the scarcity of labeled data and the low occurrence probability of these events, abnormal event detection is a challenging vision problem. In this paper, we introduce a novel two-stage architecture for detecting anomalous behavior in videos. In the first stage, we propose a 3D Convolutional Autoencoder (3D-CAE) architecture to extract spatio-temporal features from normal event training videos. In 3D-CAE, the encoder and decoder architectures are based on 3D convolutions, which can learn both appearance and the motion features effectively in an unsupervised manner. In the second stage, we group the 3D spatio-temporal features into different normality clusters, and then remove the sparse clusters to represent a stronger pattern of normality. From these clusters, one-class SVM classifier is used to distinguish between normal and abnormal events based on the normality scores. Experimental results on four different benchmark datasets show significant performance improvement compared to state-of-the-art approaches while providing results in real-time.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000201",
    "keywords": [
      "Anomaly detection",
      "Artificial intelligence",
      "Autoencoder",
      "Benchmark (surveying)",
      "Classifier (UML)",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Event (particle physics)",
      "Geodesy",
      "Geography",
      "Mathematics",
      "Normality",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Asad",
        "given_name": "Mujtaba"
      },
      {
        "surname": "Yang",
        "given_name": "Jie"
      },
      {
        "surname": "Tu",
        "given_name": "Enmei"
      },
      {
        "surname": "Chen",
        "given_name": "Liming"
      },
      {
        "surname": "He",
        "given_name": "Xiangjian"
      }
    ]
  },
  {
    "title": "Image super-resolution based on deep neural network of multiple attention mechanism",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103019",
    "abstract": "At present, the main super-resolution (SR) method based on convolutional neural network (CNN) is to increase the layer number of the network by skip connection so as to improve the nonlinear expression ability of the model. However, the network also becomes difficult to be trained and converge. In order to train a smaller but better performance SR model, this paper constructs a novel image SR network of multiple attention mechanism(MAMSR), which includes channel attention mechanism and spatial attention mechanism. By learning the relationship between the channels of the feature map and the relationship between the pixels in each position of the feature map, the network can enhance the ability of feature expression and make the reconstructed image more close to the real image. Experiments on public datasets show that our network surpasses some current state-of-the-art algorithms in PSNR, SSIM, and visual effects.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000018",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Channel (broadcasting)",
      "Chemistry",
      "Computer science",
      "Convolutional neural network",
      "Economics",
      "Epistemology",
      "Expression (computer science)",
      "Feature (linguistics)",
      "Finance",
      "Image (mathematics)",
      "Layer (electronics)",
      "Linguistics",
      "Mechanism (biology)",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Position (finance)",
      "Programming language",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Xin"
      },
      {
        "surname": "Li",
        "given_name": "Xiaochuan"
      },
      {
        "surname": "Li",
        "given_name": "Zhiqiang"
      },
      {
        "surname": "Zhou",
        "given_name": "Dake"
      }
    ]
  },
  {
    "title": "Person re-identification from appearance cues and deep Siamese features",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2021.103029",
    "abstract": "Automated person re-identification in a multi-camera surveillance setup is very important for effective tracking and monitoring crowd movement. In this paper, we propose an efficient hierarchical re-identification approach in which color histogram-based comparison is employed to find the closest matches in the gallery set, and next deep feature-based comparison is carried out using the Siamese network. Reduction in search space after the first level of matching helps in improving the accuracy as well as efficiency of prediction by the Siamese network by eliminating dissimilar elements. A silhouette part-based feature extraction scheme is adopted in each level of hierarchy to preserve the relative locations of the different body parts and make the appearance descriptors more discriminating. The proposed approach has been evaluated on five public data sets and also a new data set captured in our laboratory. Results reveal that it outperforms most state-of-the-art approaches in terms of overall accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320321000109",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Computer vision",
      "Data set",
      "Economics",
      "Feature (linguistics)",
      "Hierarchy",
      "Histogram",
      "Identification (biology)",
      "Image (mathematics)",
      "Linguistics",
      "Market economy",
      "Matching (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Set (abstract data type)",
      "Silhouette",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Tagore",
        "given_name": "Nirbhay Kumar"
      },
      {
        "surname": "Singh",
        "given_name": "Ayushman"
      },
      {
        "surname": "Manche",
        "given_name": "Sumanth"
      },
      {
        "surname": "Chattopadhyay",
        "given_name": "Pratik"
      }
    ]
  },
  {
    "title": "Facial expression recognition using frequency multiplication network with uniform rectangular features",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2020.103018",
    "abstract": "Facial expression recognition (FER) is a popular research field in cognitive interaction systems and artificial intelligence. Many deep learning methods achieve outstanding performances at the expense of enormous computation workload. Limiting their application in small devices or offline scenarios. To cope with this drawback, this paper proposes the Frequency Multiplication Network (FMN), a deep learning method operating in the frequency domain that significantly reduces network capacity and computation workload. By taking advantage of the frequency domain conversion, this novel deep learning method utilizes multiplication layers for effective feature extraction. In conjunction with the Uniform Rectangular Features (URF), our method further improves the performance and reduces the training effort. On three publicly available datasets (CK+, Oulu, and MMI), our method achieves substantial improvements in comparison to popular approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320302261",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Combinatorics",
      "Computation",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Domain (mathematical analysis)",
      "Engineering",
      "Facial expression recognition",
      "Facial recognition system",
      "Feature (linguistics)",
      "Feature extraction",
      "Field (mathematics)",
      "Frequency domain",
      "Limiting",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Mechanical engineering",
      "Multiplication (music)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pure mathematics",
      "Workload"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Jinzhao"
      },
      {
        "surname": "Zhang",
        "given_name": "Xingming"
      },
      {
        "surname": "Lin",
        "given_name": "Yubei"
      },
      {
        "surname": "Liu",
        "given_name": "Yang"
      }
    ]
  },
  {
    "title": "Strong ghost removal in multi-exposure image fusion using hole-filling with exposure congruency",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2020.103017",
    "abstract": "It is the most crucial problem to remove ghost in the multi-exposure image fusion of dynamic scene. The traditional fusion methods have good effects to remove weak ghosts. However, they cannot effectively remove strong ghosts. This paper proposes a new strong ghost removal method in multi-exposure image fusion using hole-filling with exposure congruency. First, analyzing the characteristics of strong ghosts, a detection scheme for strong ghost regions is designed by combining histogram matching and exposure difference detection. Subsequently, to effectively extract image local features, a multi-scale fusion network for non-strong ghost regions is designed to obtain a pre-fused image. Further, based on the distribution characteristics of strong ghosts, a hole-filling model with exposure congruency is designed to remove the strong ghosts. Experimental results show that compared with the state-of-the-art methods, the proposed method can obtain better performance in both of subjective and objective evaluation, particularly in terms of effectively removing strong ghosts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320302273",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Fusion",
      "Histogram",
      "Image (mathematics)",
      "Image fusion",
      "Linguistics",
      "Matching (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Shao",
        "given_name": "Hua"
      },
      {
        "surname": "Yu",
        "given_name": "Mei"
      },
      {
        "surname": "Jiang",
        "given_name": "Gangyi"
      },
      {
        "surname": "Pan",
        "given_name": "Zhiyong"
      },
      {
        "surname": "Peng",
        "given_name": "Zongju"
      },
      {
        "surname": "Chen",
        "given_name": "Fen"
      }
    ]
  },
  {
    "title": "Integrating saliency with fuzzy thresholding for brain tumor extraction in MR images",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2020.102964",
    "abstract": "The automatic detection and extraction of tumor area in Magnetic Resonance Imaging (MRI) is an important and challenging task. This paper presents a fully automatic and unsupervised method for fast and accurate extraction of brain tumor area from MR images. The proposed method named as Saliency Based Segmentation (SBS) is based on visual saliency. The saliency model detects the pathologically important area and then fuzzy thresholding is used for extraction of the detected region. The performance of SBS is compared with Adaptively Regularized Kernel-Based Fuzzy C-Means Clustering, Mean Shift and Fuzzy C-Means clustering with Level Set Method. The experimental evaluation validated on BRATS database using Jaccard index (0.84 ± 0.04), Dice Index (0.91 ± 0.02), Execution time (2.99 ± 0.29), Precision (0.82 ± 0.16), Recall (0.97 ± 0.03) and F-measure (0.88 ± 0.10) demonstrates that SBS achieves better segmentation results even in the presence of noise and uneven illumination in images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301899",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Computer vision",
      "Fuzzy clustering",
      "Fuzzy logic",
      "Image (mathematics)",
      "Jaccard index",
      "Noise (video)",
      "Pattern recognition (psychology)",
      "Segmentation",
      "Thresholding"
    ],
    "authors": [
      {
        "surname": "Sran",
        "given_name": "Paramveer Kaur"
      },
      {
        "surname": "Gupta",
        "given_name": "Savita"
      },
      {
        "surname": "Singh",
        "given_name": "Sukhwinder"
      }
    ]
  },
  {
    "title": "Hypergraph-regularized sparse representation for single color image super resolution",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2020.102951",
    "abstract": "Sparsity-based single image super resolution method generates the High-Resolution (HR) output via a corresponding dictionary from the Low-Resolution (LR) input. However, most of these existing methods ignore the complementary information from color channels, which causes the loss of a valid prior and the limitation of HR image quality improvement. In this paper, hypergraph regularization is first incorporated with Joint Color Dictionary Training (JCDT) model and HR image reconstruction (HRIR) model. A novel Hypergraph-regularized Sparse coding-based Super Resolution (HG-ScSR) is proposed. This regularization can not only focus on the illuminance information, but also exploit the self-channel and cross-channel information of three color RGB channels from high-resolution image patches. Especially, the complex relationship is explored among every color image patch pixel and the consistency of the similar pixels is enforced. Both simulated and real data experiments verify the higher performance of the proposed HG-ScSR.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301796",
    "keywords": [
      "Artificial intelligence",
      "Channel (broadcasting)",
      "Color image",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Discrete mathematics",
      "Hypergraph",
      "Image (mathematics)",
      "Image processing",
      "Image quality",
      "Image resolution",
      "Mathematics",
      "Neural coding",
      "Pattern recognition (psychology)",
      "Pixel",
      "RGB color model",
      "Regularization (linguistics)",
      "Sparse approximation"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Minghua"
      },
      {
        "surname": "Wang",
        "given_name": "Qiang"
      }
    ]
  },
  {
    "title": "An efficient copy move forgery detection using adaptive watershed segmentation with AGSO and hybrid feature extraction",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2020.102966",
    "abstract": "Copy-move forgery detection (CMFD) is the process of determining the presence of copied areas in an image. CMFD approaches are mainly classified into two groups: keypoint-based and block-based techniques. In this paper, a new CMFD approach is proposed on the basis of both block and keypoint based approaches. Initially, the forged image is partitioned into non overlapped segments utilizing adaptive watershed segmentation, wherein adaptive H-minima transform is used for extracting the markers. Also, an Adaptive Galactic Swarm Optimization (AGSO) algorithm is used to select optimal gap parameter while selecting the markers for reducing the undesired regional minima, which can increase the segmentation performance. After that, the features from every segment are extracted as segment features (SF) using Hybrid Wavelet Hadamard Transform (HWHT). Then, feature matching is performed using adaptive thresholding. The false matches or outliers can be removed with the help of Random Sample Consensus (RANSAC) algorithm. Finally, the Forgery Region Extraction Algorithm (FREA) is utilized for detecting the copied portion from the host image. Experimental results indicate that the proposed scheme find out image forgery region with Precision = 92.45%; Recall = 93.67% and F1 = 92.75% on MICC-F600 dataset and Precision = 94.52%; Recall = 95.32% and F1 = 93.56% on Bench mark dataset at pixel level. Also, it outperforms the existing approaches when the image undergone certain geometrical transformation and image degradation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301917",
    "keywords": [
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Geometry",
      "Image (mathematics)",
      "Image segmentation",
      "Linguistics",
      "Mathematics",
      "Outlier",
      "Pattern recognition (psychology)",
      "Philosophy",
      "RANSAC",
      "Segmentation",
      "Thresholding"
    ],
    "authors": [
      {
        "surname": "Tinnathi",
        "given_name": "Sreenivasu"
      },
      {
        "surname": "Sudhavani",
        "given_name": "G."
      }
    ]
  },
  {
    "title": "A Model-based dehazing scheme for unmanned aerial vehicle system using radiance boundary constraint and graph model",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2020.102993",
    "abstract": "Unmanned aerial vehicle system (UAVs) imaging has become a challenging area of research due to the dynamic atmospheric environment. The images captured by UAVs are often deteriorated by factors such as clouds occlusion, poor atmospheric illumination, and limited capability of the imaging system. To tackle problems, this paper presents a novel visibility restoration scheme for UAVs images by considering the following two assumptions: (1) The actual scene radiance of a UAVs image is bounded. (2) Pixels sharing the same appearance must have the same transmission value in a local neighborhood. Inspired by above assumptions, an image boundary constraint utilizing the median filter has been imposed on the RGB channel for the rough estimation of transmission-map in aerial images. Furthermore, a graph-model based optimization technique has been used for the transmission-map refinement. The experimental results demonstrate the efficiency of the proposed method in terms of metrics correspond to the human-visual-system (HVS).",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320302108",
    "keywords": [
      "Artificial intelligence",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Constraint (computer-aided design)",
      "Geography",
      "Geology",
      "Geometry",
      "Graph",
      "Mathematical analysis",
      "Mathematics",
      "Meteorology",
      "Pixel",
      "RGB color model",
      "Radiance",
      "Remote sensing",
      "Scheme (mathematics)",
      "Telecommunications",
      "Theoretical computer science",
      "Transmission (telecommunications)",
      "Visibility"
    ],
    "authors": [
      {
        "surname": "Gautam",
        "given_name": "Sidharth"
      },
      {
        "surname": "Gandhi",
        "given_name": "Tapan Kumar"
      },
      {
        "surname": "Panigrahi",
        "given_name": "B.K."
      }
    ]
  },
  {
    "title": "Person Re-identification with Global-Local Background_bias Net",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2020.102961",
    "abstract": "Person Re-identification (Re-ID) is an important technique in intelligent video surveillance. Because of the variations on camera viewpoints and body poses, there are some problems such as body misalignment, the diverse background clutters and partial bodies occlusion, etc. To address these problems, we propose the Global-Local Background_bias Net (GLBN), a novel network architecture that consists of Foreground Partial Segmentation Net (FPSN), Global Aligned Supervision Net (GASN) and Background_bias Constraint Net (BCN) modules. Firstly, to enhance the adaptability of foreground features and reduce the interference of the background, FPSN is applied to perform local segmentation on the foreground image. Secondly, global features generated by GASN are purposed to supervise the learning of local features. Finally, BCN constrains the background information to reduce the impact of background information again. Extensive experiments implemented on the mainstream evaluation datasets including Market1501, DukeMTMC-reID and CUHK03 indicate that our method is efficient and robust.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301875",
    "keywords": [
      "Adaptability",
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Ecology",
      "Geometry",
      "Identification (biology)",
      "Interference (communication)",
      "Mathematics",
      "Net (polyhedron)",
      "Pattern recognition (psychology)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Gong",
        "given_name": "Yuxiu"
      },
      {
        "surname": "Wang",
        "given_name": "Ronggui"
      },
      {
        "surname": "Yang",
        "given_name": "Juan"
      },
      {
        "surname": "Xue",
        "given_name": "Lixia"
      },
      {
        "surname": "Hu",
        "given_name": "Min"
      }
    ]
  },
  {
    "title": "Semantic-aware visual attributes learning for zero-shot recognition",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2020.103010",
    "abstract": "Zero-shot learning (ZSL) aims to recognize unseen image classes without requiring any training samples of these specific classes. The ZSL problem is typically achieved by building up a semantic embedding space like attributes to bridge the visual features and class labels of images. Currently, most ZSL approaches focus on learning a visual-semantic alignment from seen classes using only the human-designed attributes, and then ZSL problem is solved by transferring semantic knowledge from seen classes to the unseen classes. However, few works indicate if the human-designed attributes are discriminative enough for image class prediction. To address this issue, we propose a semantic-aware dictionary learning (SADL) framework to explore these discriminative visual attributes across seen and unseen classes. Furthermore, the semantic cues are elegantly integrated into the feature representations via learned visual attributes for recognition task. Experiments conducted on two challenging benchmark datasets show that our approach outweighs other state-of-the-art ZSL methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320302224",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Linguistics",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Shot (pellet)",
      "Zero (linguistics)"
    ],
    "authors": [
      {
        "surname": "Xie",
        "given_name": "Yurui"
      },
      {
        "surname": "Song",
        "given_name": "Tiecheng"
      },
      {
        "surname": "Li",
        "given_name": "Wei"
      }
    ]
  },
  {
    "title": "Noise reduction for sonar images by statistical analysis and fields of experts",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2020.102995",
    "abstract": "Sonar images are usually suffering from speckle noise which results in poor visual quality. In order to improve the sonar imaging quality, removing or reducing these speckle noises is a very important and arduous task. In this paper, the imaging principle and noise characteristics of the side-scan sonar (SSS) are analyzed, and five typical probability distribution functions are used to fit the seabed reverberation. Through experiment comparison, the Gamma distribution is selected to simulate the noise of the SSS image caused by the reverberation. Simultaneously, the fields of experts denoising algorithm based on the Gamma distribution (Gamma FoE) is proposed for SSS image denoising. In order to perceive and measure the denoising effect better, evaluation indexes of Fast Noise Variance Estimation (FNVE, an image noise estimation method) and Blind Referenceless Image Spatial Quality Evaluator (BRISQUE, an image quality evaluation method) are selected for image quality perception. The final results of the SSS image denoise experiment show that the Gamma FoE denoise algorithm has a better effect on SSS image denoise application than other denoise algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032030211X",
    "keywords": [
      "Acoustics",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Image quality",
      "Noise (video)",
      "Noise reduction",
      "Pattern recognition (psychology)",
      "Physics",
      "Reverberation",
      "SSS*",
      "Sonar",
      "Speckle noise",
      "Speckle pattern"
    ],
    "authors": [
      {
        "surname": "Yuan",
        "given_name": "Fei"
      },
      {
        "surname": "Xiao",
        "given_name": "Fengqi"
      },
      {
        "surname": "Zhang",
        "given_name": "Kaihan"
      },
      {
        "surname": "Huang",
        "given_name": "Yifan"
      },
      {
        "surname": "Cheng",
        "given_name": "En"
      }
    ]
  },
  {
    "title": "Low-rank embedded orthogonal subspace learning for zero-shot classification",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2020.102981",
    "abstract": "Zero-shot classification methods have attracted considerable attention in recent years. Existing ZSC methods encounter domain shift, hubness and visual–semantic gap problems. To address these problems, we propose a low-rank embedded orthogonal subspace learning method (LEOSL) for ZSC. Many previous works project visual features to the semantic space. However, they often suffer from the visual–semantic gap problem. To handle this problem, we project the visual representations and semantic representations to the common subspace. To address the domain shift problem, we restrict the mapping functions with a low-rank constraint. To handle the hubness problem, we introduce the class similarity term so that samples of the same class are located near each other, while samples of different classes are located far away. Furthermore, we restrict the shared representations in the subspace with an orthogonal constraint to remove the correlation between samples. The results show the superiority of LEOSL compared to many state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320302029",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Rank (graph theory)",
      "Subspace topology",
      "Zero (linguistics)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Xiao"
      },
      {
        "surname": "Fang",
        "given_name": "Min"
      },
      {
        "surname": "Liu",
        "given_name": "Jichuan"
      }
    ]
  },
  {
    "title": "Reliable fusion of ToF and stereo data based on joint depth filter",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2020.103006",
    "abstract": "To obtain reliable depth images with high resolution, a novel method is proposed in this study that fuses data acquired from time-of-flight (ToF) and stereo cameras, through which the advantages of both active and passive sensing are utilised. Based on the classic error model of the ToF, gradient information is introduced to establish the likelihood distribution for all disparity candidates. The stereo likelihood is estimated in parallel based on a 3D adaptive support-weight approach. The two independent likelihoods are unified using a maximum likelihood estimation, a process also referred to as a joint depth filter herein. Conventional post-processing methods such as a mutual consistency check are also used after applying a joint depth filter. We also propose a novel hole-filling method based on the seed-growing algorithm to retrieve missing disparities. Experiment results show that the proposed fusion method can produce reliable high-resolution depth maps and outperforms other compared methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320302200",
    "keywords": [
      "Architectural engineering",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Engineering",
      "Filter (signal processing)",
      "Fusion",
      "Joint (building)",
      "Linguistics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Process (computing)",
      "Sensor fusion"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Xuanyin"
      },
      {
        "surname": "Lin",
        "given_name": "Tianpei"
      },
      {
        "surname": "Jiang",
        "given_name": "Xuesong"
      },
      {
        "surname": "Xiang",
        "given_name": "Ke"
      },
      {
        "surname": "Pan",
        "given_name": "Feng"
      }
    ]
  },
  {
    "title": "Text to photo-realistic image synthesis via chained deep recurrent generative adversarial network",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2020.102955",
    "abstract": "Despite the promising progress made in recent years, automatically generating high-resolution realistic images from text descriptions remains a challenging task due to semantic gap between human-written descriptions and diversities of visual appearance. Most existing approaches generate the rough images with the given text descriptions, while the relationship between sentence semantics and visual content is not holistically exploited. In this paper, we propose a novel chained deep recurrent generative adversarial network (CDRGAN) for synthesizing images from text descriptions. Our model uses carefully designed chained deep recurrent generators that simultaneously recovers global image structures and local details. Specially, our method not only considers the logic relationships of image pixels, but also removes computational bottlenecks through parameters sharing. We evaluate our method on three public benchmarks: CUB, Oxford-102 and MS COCO datasets. Experimental results show that our method significantly outperforms the state-of-the-art approaches consistently across different evaluation metrics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301838",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Computer science",
      "Deep learning",
      "Economics",
      "Generative adversarial network",
      "Generative grammar",
      "Image (mathematics)",
      "Image synthesis",
      "Management",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Programming language",
      "Semantics (computer science)",
      "Sentence",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Min"
      },
      {
        "surname": "Lang",
        "given_name": "Congyan"
      },
      {
        "surname": "Feng",
        "given_name": "Songhe"
      },
      {
        "surname": "Wang",
        "given_name": "Tao"
      },
      {
        "surname": "Jin",
        "given_name": "Yi"
      },
      {
        "surname": "Li",
        "given_name": "Yidong"
      }
    ]
  },
  {
    "title": "Gradient-based conditional generative adversarial network for non-uniform blind deblurring via DenseResNet",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2020.102921",
    "abstract": "Blind image deblurring aims to recover the sharp image from a blurry image. The problem is seriously ill-conditioned and many existing algorithms based on kernel estimation require heuristic parameter adjustments and high computational cost, and cannot perform well on non-uniform motion blurs. To address this issue, image deblurring is viewed as an image translation problem in this paper. The authors solve it based on a conditional generative adversarial network (GAN), where the sharp image is restored by an end-to-end trainable neural network. Different from the generative network in basic conditional GAN, the proposed generator is based on dense blocks and residual network (DenseResNet), aiming to mitigate the problems of overfitting and vanishing gradient, and strengthen the blur feature propagation. To generate clear structure, the basic conditional GAN formulation is further revised by introducing joint VGG features and L 1 -based gradient loss. Extensive experimental results demonstrate the superior performance of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301565",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Combinatorics",
      "Computer science",
      "Deblurring",
      "Feature (linguistics)",
      "Heuristic",
      "Image (mathematics)",
      "Image processing",
      "Image restoration",
      "Kernel (algebra)",
      "Linguistics",
      "Mathematical optimization",
      "Mathematics",
      "Overfitting",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Residual"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Hongtian"
      },
      {
        "surname": "Wu",
        "given_name": "Di"
      },
      {
        "surname": "Su",
        "given_name": "Hang"
      },
      {
        "surname": "Zheng",
        "given_name": "Shibao"
      },
      {
        "surname": "Chen",
        "given_name": "Jie"
      }
    ]
  },
  {
    "title": "Block-based image matching for image retrieval",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2020.102998",
    "abstract": "Due to the lighting, translation, scaling and rotation, image matching is a challenge task in computer vision area. In the past decades, local descriptors (e.g. SIFT, SURF and HOG, etc.) and global features (e.g. HSV, CNN, etc.) play a vital role for this task. However, most image matching methods are based on the whole image, i.e., matching the entire image directly base on some image representation methods (e.g. BoW, VLAD and deep learning, etc.). In most situations, this idea is simple and effective, but we recognize that a robust image matching can be realized based on sub-images. Thus, a block-based image matching algorithm is proposed in this paper. First, a new local composite descriptor is proposed, which combines the advantages of local gradient and color features with spatial information. Then, VLAD method is used to encode the proposed composite descriptors in one block, and block-CNN feature is extracted at the same time. Second, a block-based similarity metric is proposed for similarity calculation of two images. Finally, the proposed methods are verified on several benchmark datasets. Compared with other methods, experimental results show that our method achieves better performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320302145",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Block (permutation group theory)",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Feature detection (computer vision)",
      "Geodesy",
      "Geography",
      "Geometry",
      "Image (mathematics)",
      "Image processing",
      "Image retrieval",
      "Linguistics",
      "Matching (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Scale-invariant feature transform",
      "Similarity (geometry)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yanhong"
      },
      {
        "surname": "Zhao",
        "given_name": "Ruizhen"
      },
      {
        "surname": "Liang",
        "given_name": "Liequan"
      },
      {
        "surname": "Zheng",
        "given_name": "Xinwei"
      },
      {
        "surname": "Cen",
        "given_name": "Yigang"
      },
      {
        "surname": "Kan",
        "given_name": "Shichao"
      }
    ]
  },
  {
    "title": "Single image dehazing using a new color channel",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2020.103008",
    "abstract": "Images with hazy scene suffer from low-contrast, which reduces the visible quality of the scene, thus making object detection a more challenging task. Low-contrast can result from foggy weather conditions during image acquisition. Dehazing is a process of removal of haze from the photography of a hazy scene. Single-image dehazing based on dark channel priors are well-known techniques in this field. However, the performance of such techniques is limited to priors or constraints. Moreover, this type of method fails when images have sky-region. So, a method is proposed, which can restore the visibility of hazy images. First, a hazy image is divided into blocks of size 32 × 32, then the score of each block is calculated to select a block having the highest score. Atmospheric light is calculated from the selected block. A new color channel is considered to remove atmospheric scattering, obtained channel value and atmospheric light are then used to calculate the transmission map in the second step. Third, radiance is computed using a transmission map and atmospheric light. The illumination scaling factor is adopted to enhance the quality of a dehazed image in the final step. Experiments are performed on six datasets namely, I-HAZE, O-HAZE, BSDS500, FRIDA, RESIDE dataset and natural images from Google. The proposed method is compared against 11 state-of-the-art methods. The performance is analyzed using fourteen quantitative evaluation metrics. All the results demonstrate that the proposed method outperforms 11 state-of-the-art methods in most of the cases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320302212",
    "keywords": [
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Contrast (vision)",
      "Diffuse sky radiation",
      "Geology",
      "Geometry",
      "Haze",
      "Image (mathematics)",
      "Image quality",
      "Mathematics",
      "Meteorology",
      "Optics",
      "Physics",
      "Radiance",
      "Remote sensing",
      "Scattering",
      "Telecommunications",
      "Transmission (telecommunications)",
      "Visibility"
    ],
    "authors": [
      {
        "surname": "Sahu",
        "given_name": "Geet"
      },
      {
        "surname": "Seal",
        "given_name": "Ayan"
      },
      {
        "surname": "Krejcar",
        "given_name": "Ondrej"
      },
      {
        "surname": "Yazidi",
        "given_name": "Anis"
      }
    ]
  },
  {
    "title": "Attention-based contextual interaction asymmetric network for RGB-D saliency prediction",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2020.102997",
    "abstract": "Saliency prediction on RGB-D images is an underexplored and challenging task in computer vision. We propose a channel-wise attention and contextual interaction asymmetric network for RGB-D saliency prediction. In the proposed network, a common feature extractor provides cross-modal complementarity between the RGB image and corresponding depth map. In addition, we introduce a four-stream feature-interaction module that fully leverages multiscale and cross-modal features for extracting contextual information. Moreover, we propose a channel-wise attention module to highlight the feature representation of salient regions. Finally, we refine coarse maps through a corresponding refinement block. Experimental results show that the proposed network achieves a performance comparable with state-of-the-art saliency prediction methods on two representative datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320302133",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Block (permutation group theory)",
      "Channel (broadcasting)",
      "Chemistry",
      "Complementarity (molecular biology)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Engineering",
      "Extractor",
      "Feature (linguistics)",
      "Feature extraction",
      "Genetics",
      "Geometry",
      "Law",
      "Linguistics",
      "Mathematics",
      "Modal",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Polymer chemistry",
      "Process engineering",
      "RGB color model",
      "Representation (politics)",
      "Salient"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Xinyue"
      },
      {
        "surname": "Jin",
        "given_name": "Ting"
      },
      {
        "surname": "Zhou",
        "given_name": "Wujie"
      },
      {
        "surname": "Lei",
        "given_name": "Jingsheng"
      }
    ]
  },
  {
    "title": "Image annotation based on multi-view robust spectral clustering",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2020.103003",
    "abstract": "Nowadays, image annotation has been a hot topic in the semantic retrieval field due to the abundant growth of digital images. The purpose of these methods is to realize the content of images and assign appropriate keywords to them. Extensive efforts have been conducted in this field, which effectiveness is limited between low-level image features and high-level semantic concepts. In this paper, we propose a Multi-View Robust Spectral Clustering (MVRSC) method, which tries to model the relationship between semantic and multi-features of training images based on the Maximum Correntropy Criterion. A Half-Quadratic optimization framework is used to solve the objective function. According to the constructed model, a few tags are suggested based on a novel decision-level fusion distance. The stability condition and bound calculation of MVRSC are analyzed, as well. Experimental results on real-world Flickr and 500PX datasets, and Corel5K confirm the superiority of the proposed method over other competing models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320302182",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Evolutionary biology",
      "Field (mathematics)",
      "Function (biology)",
      "Image (mathematics)",
      "Image retrieval",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pure mathematics",
      "Spectral clustering",
      "Stability (learning theory)"
    ],
    "authors": [
      {
        "surname": "Zamiri",
        "given_name": "Mona"
      },
      {
        "surname": "Sadoghi Yazdi",
        "given_name": "Hadi"
      }
    ]
  },
  {
    "title": "An anisotropic reference matrix for image steganography",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2020.102969",
    "abstract": "A lot of image steganographic techniques (also called data hiding) conceal secret data by utilizing a reference matrix (RM), and some new types of RMs, such as the turtle shell, an octagon-shaped shell, the Sudoku table, and so on, have been proposed in recent years. In this article, we present a novel type of RM called anisotropic RM. By employing a full search strategy, we have found the optimal parameters for constructing the anisotropic RM. To judge the performance of a parameter set quickly, a theoretical peak signal-to-noise ratio (PSNR) evaluation method is proposed. In addition, we extend the RM to three-dimensional (3D) space. Experimental results show that our data hiding scheme, based on the anisotropic RM, has a better quality stego image than previous methods. Moreover, the 3D RM works better than the traditional 2D RM using the same embedding capacity.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301942",
    "keywords": [
      "Artificial intelligence",
      "Composite material",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Materials science",
      "Mathematics",
      "Matrix (chemical analysis)",
      "Steganography"
    ],
    "authors": [
      {
        "surname": "Lin",
        "given_name": "Juan"
      },
      {
        "surname": "Horng",
        "given_name": "Ji-Hwei"
      },
      {
        "surname": "Liu",
        "given_name": "Yanjun"
      },
      {
        "surname": "Chang",
        "given_name": "Chin-Chen"
      }
    ]
  },
  {
    "title": "Circular intra prediction for 360 degree video coding",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2020.103000",
    "abstract": "Different from traditional 2D video, the contents of 360 degree video are deformed due to the projection from 3D sphere to 2D plane. As a result, the traditional Angular Intra Prediction (AIP) with a linear pattern may not be always efficient. To further improve the coding performance of 360 degree video, a novel intra prediction method is presented in this paper, i.e., Circular Intra Prediction (CIP), which takes consideration of the spherical characteristics of 360 degree video. In specific, the proposed CIP is performed in a circular pattern, where the center of circle is located around the to-be-predicted block, and different centers of circle are able to produce different CIP modes. The distance between center of this circle and center of the to-be-predicted block is adaptively determined according to the degree of projection deformation, where stronger projection deformation needs shorter distance, and vice versa. As the increase of the distance, the CIP is more and more close to the traditional AIP. In addition, one additional binary flag is utilized to achieve better coding performance from the competition between AIP and CIP with the rate-distortion optimization. The proposed algorithm is implemented on the platform of Versatile video coding Test Model (VTM) 5.0 + 360Lib 9.1. Extensive experiments show that the proposed method can achieve bit rate reduction on this platform for 360 degree video coding.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320302169",
    "keywords": [
      "Acoustics",
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Binary number",
      "Coding (social sciences)",
      "Computer science",
      "Computer vision",
      "Degree (music)",
      "Mathematics",
      "Physics",
      "Projection (relational algebra)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Linwei"
      },
      {
        "surname": "Zhang",
        "given_name": "Yun"
      },
      {
        "surname": "Li",
        "given_name": "Na"
      },
      {
        "surname": "Pi",
        "given_name": "Jinyong"
      },
      {
        "surname": "Wang",
        "given_name": "Shiqi"
      }
    ]
  },
  {
    "title": "Motion vector modification distortion analysis-based payload allocation for video steganography",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2020.102986",
    "abstract": "Video steganography forms a covert communication channel by data embedding in cover elements. To consider inter-frame mutual embedding impacts, this paper proposes a payload allocation strategy in video steganography based on motion vector modification distortion analysis. Firstly, the motion vector modification distortion caused by data embedding is analyzed. Then, a rate–distortion model reflecting the residue deviation propagation in successive inter-coded frames is derived. According to this model, the residue deviation propagation weight of each inter-coded frame can be computed. Finally, an inter-frame payload allocation strategy is designed in order to restrain the residue deviation propagation. Experimental results demonstrate that the proposed payload allocation strategy can enhance existing motion vector-based video steganographic methods in terms of undetectability and video coding performance. Besides, the lower computational complexity can be achieved.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320302054",
    "keywords": [
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Coding (social sciences)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Distortion (music)",
      "Embedding",
      "Encoder",
      "Image (mathematics)",
      "Mathematics",
      "Motion vector",
      "Network packet",
      "Operating system",
      "Payload (computing)",
      "Statistics",
      "Steganalysis",
      "Steganography",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Yao",
        "given_name": "Yuanzhi"
      },
      {
        "surname": "Yu",
        "given_name": "Nenghai"
      }
    ]
  },
  {
    "title": "Optimal feature selection-based biometric key management for identity management system: Emotion oriented facial biometric system",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2020.103002",
    "abstract": "Identity management systems with biometric key binding make digital transactions secure and reliable. A novel methodology is proposed to develop an intelligent key management system using facial emotions. Key binding with facial emotions makes use of an intrinsic user specific trait facilitating a more natural computer to human interaction. The proposed system utilizes metaheuristic swarm intelligence based optimization techniques to extract optimal features. The work demonstrates key binding by encrypting an image with a secret key bound to optimal features extracted from facial emotions. Efficiency and correctness of proposed key management is validated by successful decryption at receiving end with any one of the enrolled emotions given as input. Deer Hunting Optimization Algorithm and Chicken Swarm Optimization are merged to select optimal features from facial emotions. The derived algorithm is called Fitness Sorted Deer Hunting Optimization Algorithm with Rooster Update. Seven facial emotions — anger, disgust, fear, happiness, sadness, surprise and neutral are used to extract optimal features from Japanese Female Facial Expressions and Yale Facial datasets to train the neural network. Proposed work achieved better performance results over state-of-art optimization algorithms such as whale optimization algorithm, grey wolf optimization, chicken swarm optimization and deer hunting optimization algorithm. Accuracy of proposed model is 2.2% better than deer hunting optimization algorithm and 12.3% better than chicken swarm optimization for a key length 80.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320302170",
    "keywords": [
      "Aesthetics",
      "Artificial intelligence",
      "Biometrics",
      "Computer science",
      "Computer security",
      "Feature (linguistics)",
      "Feature selection",
      "Identity (music)",
      "Key (lock)",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Selection (genetic algorithm)"
    ],
    "authors": [
      {
        "surname": "Padmanabhan",
        "given_name": "Suresh"
      },
      {
        "surname": "K.R.",
        "given_name": "Radhika"
      }
    ]
  },
  {
    "title": "Multiview video summarization using video partitioning and clustering",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2020.102991",
    "abstract": "Multiview video summarization plays a crucial role in abstracting essential information form multiple videos of the same location and time. In this paper, we propose a new approach for the multiview summarization. The proposed approach uses the BIRCH clustering algorithm for the first time on the initial set of frames to get rid of the static and redundant. The work presents a new approach for shot boundary detection using frame similarity measures Jaccard and Dice. The algorithm performs effectively synchronized merging of keyframes from all camera-views to obtain the final summary. Extensive experimentation conducted on various datasets suggests that the proposed approach significantly outperforms most of the existing video summarization approaches. To state a few, a 1.5% improvement on video length reduction, 24.28% improvement in compression ratio, and 6.4% improvement in quality assessment ratio is observed on the lobby dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320302091",
    "keywords": [
      "Artificial intelligence",
      "Automatic summarization",
      "Chemistry",
      "Cluster analysis",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Economics",
      "Frame (networking)",
      "Image (mathematics)",
      "Jaccard index",
      "Metric (unit)",
      "Operations management",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Programming language",
      "Set (abstract data type)",
      "Shot (pellet)",
      "Similarity (geometry)",
      "Telecommunications",
      "Video processing",
      "Video quality",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Singh Parihar",
        "given_name": "Anil"
      },
      {
        "surname": "Pal",
        "given_name": "Joyeeta"
      },
      {
        "surname": "Sharma",
        "given_name": "Ishita"
      }
    ]
  },
  {
    "title": "Non-maximum suppression for object detection based on the chaotic whale optimization algorithm",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2020.102985",
    "abstract": "Non-maximum suppression (NMS) as a post-processing step for object detection is mainly used to remove redundant bounding boxes in the object and plays a vital role in many detectors. Its positioning accuracy mainly depends on the bounding box with the highest score, and this strategy is difficult to eliminate the false positive. In order to solve the problem, this paper regards the post-processing step as a combinatorial optimization problem and combines the chaotic whale optimization algorithm and non-maximum suppression. The chaotic search method is used to generate an initial combinatorial solution, and the whale optimization algorithm is discretized to create an updated combinatorial strategy. Under the guidance of the fitness function, the optimal combination is searched. In addition, the method of difference set area (DSA) is proposed to optimize the final detection result. The experiment uses the current mainstream framework Faster R-CNN as the detector on PASCAL VOC2012, COCO2017 and the Warships datasets. The experimental results show that the proposed method can significantly improve the average precision (AP) of detectors compared with the most advanced methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320302042",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Bounding overwatch",
      "Chaotic",
      "Computer science",
      "Detector",
      "Fishery",
      "Image (mathematics)",
      "Mathematical optimization",
      "Mathematics",
      "Minimum bounding box",
      "Optimization algorithm",
      "Pascal (unit)",
      "Programming language",
      "Set (abstract data type)",
      "Telecommunications",
      "Whale"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Guixian"
      },
      {
        "surname": "Li",
        "given_name": "Yuancheng"
      }
    ]
  },
  {
    "title": "Unsupervised video object segmentation with distractor-aware online adaptation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2020.102953",
    "abstract": "Unsupervised video object segmentation is a crucial application in video analysis when there is no prior information about the objects. It becomes tremendously challenging when multiple objects occur and interact in a video clip. In this paper, a novel unsupervised video object segmentation approach via distractor-aware online adaptation (DOA) is proposed. DOA models spatiotemporal consistency in video sequences by capturing background dependencies from adjacent frames. Instance proposals are generated by the instance segmentation network for each frame and they are grouped by motion information as positives or hard negatives. To adopt high-quality hard negatives, the block matching algorithm is then applied to preceding frames to track the associated hard negatives. General negatives are also introduced when there are no hard negatives in the sequence. The experimental results demonstrate these two kinds of negatives are complementary. Finally, we conduct DOA using positive, negative, and hard negative masks to update the foreground and background segmentation. The proposed approach achieves state-of-the-art results on two benchmark datasets, the DAVIS 2016 and the Freiburg-Berkeley motion segmentation (FBMS)-59.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301814",
    "keywords": [
      "Adaptation (eye)",
      "Artificial intelligence",
      "Cognitive psychology",
      "Computer science",
      "Computer vision",
      "Neuroscience",
      "Object (grammar)",
      "Psychology",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Ye"
      },
      {
        "surname": "Choi",
        "given_name": "Jongmoo"
      },
      {
        "surname": "Chen",
        "given_name": "Yueru"
      },
      {
        "surname": "Li",
        "given_name": "Siyang"
      },
      {
        "surname": "Huang",
        "given_name": "Qin"
      },
      {
        "surname": "Zhang",
        "given_name": "Kaitai"
      },
      {
        "surname": "Lee",
        "given_name": "Ming-Sui"
      },
      {
        "surname": "Kuo",
        "given_name": "C.-C. Jay"
      }
    ]
  },
  {
    "title": "Two in One Image Secret Sharing Scheme (TiOISSS) for extended progressive visual cryptography using simple modular arithmetic operations",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2020.102996",
    "abstract": "Existing Extended Progressive Visual Cryptography scheme (EPVCS) suffers from the problem of pixel expansion, poor quality of reconstructed image and residual trace of cover images in the reconstructed image. Hence in this paper, Two in One Image secret sharing scheme for EPVCS is proposed which decodes the encrypted image in two stages. The proposed scheme provides k , n threshold construction using meaningful shares without pixel expansion and demonstrates that the reconstructed image has improved quality compared to the existing schemes. To reduce the computational complexity, the proposed scheme uses simple Modular Arithmetic operations instead of Galois field. The proposed scheme has the additional advantages of supporting any value of k and n, no overhead in resizing the secret image and no residual trace of cover image. Simulation results and performance analysis show the effectiveness of proposed scheme with improved contrast, 99% Structural Similarity of the reconstructed image and good progressive reconstruction.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320302121",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Cover (algebra)",
      "Cryptography",
      "Encryption",
      "Engineering",
      "Image (mathematics)",
      "Image quality",
      "Image sharing",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Mechanical engineering",
      "Modular design",
      "Operating system",
      "Overhead (engineering)",
      "Philosophy",
      "Pixel",
      "Scheme (mathematics)",
      "Secret sharing",
      "TRACE (psycholinguistics)",
      "Visual cryptography"
    ],
    "authors": [
      {
        "surname": "Sridhar",
        "given_name": "Srividhya"
      },
      {
        "surname": "Sudha",
        "given_name": "Gnanou Florence"
      }
    ]
  },
  {
    "title": "Conditional generative adversarial network for EEG-based emotion fine-grained estimation and visualization",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2020.102982",
    "abstract": "In the field of affective computing (AC), coarse-grained AC has been developed and widely applied in many fields. Electroencephalogram (EEG) signals contain abundant emotional information. However, it is difficult to develop fine-grained AC due to the lack of fine-grained labeling data and suitable visualization methods for EEG data with fine labels. To achieve a fine mapping of EEG data directly to facial images, we propose a conditional generative adversarial network (cGAN) to establish the relationship between EEG data associated with emotions, a coarse label, and a facial expression image in this study. In addition, a corresponding training strategy is also proposed to realize the fine-grained estimation and visualization of EEG-based emotion. The experiments prove the reasonableness of the proposed method for the generation of fine-grained facial expressions. The image entropy of the generated image indicates that the proposed method can provide a satisfactory visualization of fine-grained facial expressions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320302030",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Electroencephalography",
      "Emotion recognition",
      "Entropy (arrow of time)",
      "Facial expression",
      "Generative grammar",
      "Image (mathematics)",
      "Pattern recognition (psychology)",
      "Physics",
      "Psychiatry",
      "Psychology",
      "Quantum mechanics",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Fu",
        "given_name": "Boxun"
      },
      {
        "surname": "Li",
        "given_name": "Fu"
      },
      {
        "surname": "Niu",
        "given_name": "Yi"
      },
      {
        "surname": "Wu",
        "given_name": "Hao"
      },
      {
        "surname": "Li",
        "given_name": "Yang"
      },
      {
        "surname": "Shi",
        "given_name": "Guangming"
      }
    ]
  },
  {
    "title": "Sentiment-based sub-event segmentation and key photo selection",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2020.102973",
    "abstract": "The number of people collecting photos has surged owing to social media and cloud services in recent years. A typical approach to summarize a photo collection is dividing it into events and selecting key photos from each event. Despite the fact that a certain event comprises several sub-events, few studies have proposed sub-event segmentation. We propose the sentiment analysis-based photo summarization (SAPS) method, which automatically summarizes personal photo collections by utilizing metadata and visual sentiment features. For this purpose, we first cluster events using metadata of photos and then calculate the novelty scores to determine the sub-event boundaries. Next, we summarize the photo collections using a ranking algorithm that measures sentiment, emotion, and aesthetics. We evaluate the proposed method by applying it to the photo collections of six participants consisting of 5,480 photos in total. We observe that our sub-event segmentation based on sentiment features outperforms the existing baseline methods. Furthermore, the proposed method is also more effective in finding sub-event boundaries and key photos, because it focuses on detailed sentiment features instead of general content features.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301954",
    "keywords": [
      "Artificial intelligence",
      "Automatic summarization",
      "Cluster analysis",
      "Computer science",
      "Computer security",
      "Event (particle physics)",
      "Information retrieval",
      "Key (lock)",
      "Metadata",
      "Novelty",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Ranking (information retrieval)",
      "Segmentation",
      "Selection (genetic algorithm)",
      "Sentiment analysis",
      "Social media",
      "Tag cloud",
      "Theology",
      "Visualization",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Bum",
        "given_name": "Junghyun"
      },
      {
        "surname": "Whang",
        "given_name": "Joyce Jiyoung"
      },
      {
        "surname": "Choo",
        "given_name": "Hyunseung"
      }
    ]
  },
  {
    "title": "Densely connected network with improved pyramidal bottleneck residual units for super-resolution",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2020.102963",
    "abstract": "Recent studies have shown that super-resolution can be significantly improved by using deep convolution neural network. Although applying a larger number of convolution kernels can extract more features, increasing the number of feature mappings will dramatically increase the training parameters and time complexity. In order to balance the workload among all units and maintain appropriate time complexity, this paper proposes a new network structure for super-resolution. For the sake of making full use of context information, in the structure, the operations of division (S) and fusion (C) are added to the pyramidal bottleneck residual units, and the dense connected methods are used. The proposed network include a preliminary feature extraction net, seven residual units with dense connections, seven convolution layers with the size of 1 × 1 after each residual unit, and a deconvolution layer. The experimental results show that the proposed network has better performance than most existing methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301887",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Bottleneck",
      "Computer science",
      "Embedded system",
      "Mathematics",
      "Residual",
      "Resolution (logic)"
    ],
    "authors": [
      {
        "surname": "Cao",
        "given_name": "Feilong"
      },
      {
        "surname": "Chen",
        "given_name": "Baijie"
      }
    ]
  },
  {
    "title": "A novel ( k 1 , k 2 , n ) -threshold two-in-one secret image sharing scheme for multiple secrets",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2020.102971",
    "abstract": "A two-in-one secret image sharing scheme (TiOSISS) is the combination of two different secret image sharing schemes (SISSs), which has advantages of both schemes, such as the simple stacking-to-see property and precise recovery with computing devices available. Since most of current TiOSISSs depend on steganography techniques, it results in several drawbacks, such as large pixel expansion and poor visual quality with shares stacking. Besides, researchers ignore the independence between two different SISSs, that is, both SISSs should deal with irrelevant secret images with different thresholds. By controlling the randomness of the sharing phase according to constraints from both SISSs, we combine polynomial-based SISS (PSISS) and random-grid-based visual cryptography scheme (RGVCS) together, and propose an ideal ( k 1 , k 2 , n ) -threshold TiOSISS for multiple secrets. The proposed TiOSISS not only overcomes drawbacks above, but also has high scalability to improve its performances by utilizing different RGVCSs. Sufficient analyses and experiments are provided to verify its security and effectiveness.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301930",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Cryptography",
      "Database",
      "Epistemology",
      "Ideal (ethics)",
      "Image (mathematics)",
      "Mathematical analysis",
      "Mathematics",
      "Philosophy",
      "Pixel",
      "Randomness",
      "Scalability",
      "Scheme (mathematics)",
      "Secret sharing",
      "Statistics",
      "Theoretical computer science",
      "Tree (set theory)",
      "Visual cryptography"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Lintao"
      },
      {
        "surname": "Lu",
        "given_name": "Yuliang"
      },
      {
        "surname": "Yan",
        "given_name": "Xuehu"
      }
    ]
  },
  {
    "title": "Single image dehazing using improved cycleGAN",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2021",
    "doi": "10.1016/j.jvcir.2020.103014",
    "abstract": "Haze is an aggregation of very fine, widely dispersed, solid and/or liquid particles suspended in the atmosphere. In this paper, we propose an end-to-end network for single image dehazing, which enhances the CycleGAN model by introducing a transformer architecture within the generator, which is specific for haze removal. The proposed model is trained in an unpaired fashion with clear and hazy images altogether and does not require pairs of hazy and corresponding ground-truth clear images. Furthermore, the proposed model does not depend on estimating the parameters of the atmospheric scattering model. Rather, it uses a K-estimation module as the generator’s transformer for complete end-to-end modeling. The feature transformer introduced in the proposed generator model transforms the encoded features into desired feature space and then feeds them into the CycleGAN decoder to create a clear image. In the proposed model we further modified the cycle consistency loss to include the SSIM loss along with pixel-wise mean loss to produce a new loss function specific for the reconstruction task, which enhances the performance of the proposed model. The model performs well even on the high-resolution images provided in the NTIRE 2019 challenge dataset for single image dehazing. Further, we perform experiments on NYU-Depth and reside beta datasets. Results of our experiments show the efficacy of the proposed approach compared to the state-of-the-art in removing the haze from the input image.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320302248",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Generator (circuit theory)",
      "Ground truth",
      "Haze",
      "Image (mathematics)",
      "Linguistics",
      "Meteorology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Pixel",
      "Power (physics)",
      "Quantum mechanics",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Chaitanya",
        "given_name": "B.S.N.V."
      },
      {
        "surname": "Mukherjee",
        "given_name": "Snehasis"
      }
    ]
  }
]