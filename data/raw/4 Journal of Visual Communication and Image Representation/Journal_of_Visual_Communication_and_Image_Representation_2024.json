[
  {
    "title": "MVP-HOT: A Moderate Visual Prompt for Hyperspectral Object Tracking",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104326",
    "abstract": "The growing attention to hyperspectral object tracking (HOT) can be attributed to the extended spectral information available in hyperspectral images (HSIs), especially in complex scenarios. This potential makes it a promising alternative to traditional RGB-based tracking methods. However, the scarcity of large hyperspectral datasets poses a challenge for training robust hyperspectral trackers using deep learning methods. Prompt learning, a new paradigm emerging in large language models, involves adapting or fine-tuning a pre-trained model for a specific downstream task by providing task-specific inputs. Inspired by the recent success of prompt learning in language and visual tasks, we propose a novel and efficient prompt learning method for HOT tasks, termed Moderate Visual Prompt for HOT (MVP-HOT). Specifically, MVP-HOT freezes the parameters of the pre-trained model and employs HSIs as visual prompts to leverage the knowledge of the underlying RGB model. Additionally, we develop a moderate and effective strategy to incrementally adapt the HSI prompt information. Our proposed method uses only a few (1.7M) learnable parameters and demonstrates its effectiveness through extensive experiments, MVP-HOT can achieve state-of-the-art performance on three hyperspectral datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002827",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Eye tracking",
      "Hyperspectral imaging",
      "Object (grammar)",
      "Pedagogy",
      "Psychology",
      "Tracking (education)"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Lin"
      },
      {
        "surname": "Xie",
        "given_name": "Shaoxiong"
      },
      {
        "surname": "Li",
        "given_name": "Jia"
      },
      {
        "surname": "Tan",
        "given_name": "Ping"
      },
      {
        "surname": "Hu",
        "given_name": "Wenjin"
      }
    ]
  },
  {
    "title": "MT-Net: Single image dehazing based on meta learning, knowledge transfer and contrastive learning",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104325",
    "abstract": "Single image dehazing is becoming increasingly important as its results impact the efficiency of subsequent computer vision tasks. While many methods have been proposed to address this challenge, existing dehazing approaches often exhibit limited adaptability to different types of images and lack future learnability. In light of this, we propose a dehazing network based on meta-learning, knowledge transfer, and contrastive learning, abbreviated as MT-Net. In our approach, we combine knowledge transfer with meta-learning to tackle these challenges, thus enhancing the network’s generalization performance. We refine the structure of knowledge transfer by introducing a two-phases approach to facilitate learning under the guidance of teacher networks and learning committee networks. We also optimize the negative examples of contrastive learning to reduce the contrast space. Extensive experiments conducted on synthetic and real datasets demonstrate the remarkable performance of our method in both quantitative and qualitative comparisons. The code has been released on https://github.com/71717171fan/MT-Net.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002815",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Economics",
      "Geometry",
      "Image (mathematics)",
      "Knowledge management",
      "Knowledge transfer",
      "Management",
      "Mathematics",
      "Meta learning (computer science)",
      "Net (polyhedron)",
      "Parallel computing",
      "Task (project management)",
      "Transfer (computing)",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Jianlei"
      },
      {
        "surname": "Yang",
        "given_name": "Bingqing"
      },
      {
        "surname": "Wang",
        "given_name": "Shilong"
      },
      {
        "surname": "Wang",
        "given_name": "Maoli"
      }
    ]
  },
  {
    "title": "Human gait recognition using joint spatiotemporal modulation in deep convolutional neural networks",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104322",
    "abstract": "Gait, a person’s distinctive walking pattern, offers a promising biometric modality for surveillance applications. Unlike fingerprints or iris scans, gait can be captured from a distance without the subject’s direct cooperation or awareness. This makes it ideal for surveillance and security applications. Traditional convolutional neural networks (CNNs) often struggle with the inherent variations within video data, limiting their effectiveness in gait recognition. The proposed technique in this work introduces a unique joint spatial–temporal modulation network designed to overcome this limitation. By extracting discriminative feature representations across varying frame levels, the network effectively leverages both spatial and temporal variations within video sequences. The proposed architecture integrates attention-based CNNs for spatial feature extraction and a Bidirectional Long Short-Term Memory (Bi-LSTM) network with a temporal attention module to analyse temporal dynamics. The use of attention in spatial and temporal blocks enhances the network’s capability of focusing on the most relevant segments of the video data. This can improve efficiency since the combined approach enhances learning capabilities when processing complex gait videos. We evaluated the effectiveness of the proposed network using two major datasets, namely CASIA-B and OUMVLP. Experimental analysis on CASIA B demonstrates that the proposed network achieves an average rank-1 accuracy of 98.20% for normal walking, 94.50% for walking with a bag and 80.40% for clothing scenarios. The proposed network also achieved an accuracy of 89.10% for OU-MVLP. These results show the proposed method‘s ability to generalize to large-scale data and consistently outperform current state-of-the-art gait recognition techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002785",
    "keywords": [
      "Acoustics",
      "Architectural engineering",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Engineering",
      "Gait",
      "Joint (building)",
      "Medicine",
      "Modulation (music)",
      "Pattern recognition (psychology)",
      "Physical medicine and rehabilitation",
      "Physics",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Junaid",
        "given_name": "Mohammad Iman"
      },
      {
        "surname": "Prakash",
        "given_name": "Allam Jaya"
      },
      {
        "surname": "Ari",
        "given_name": "Samit"
      }
    ]
  },
  {
    "title": "Scene-aware classifier and re-detector for thermal infrared tracking",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104319",
    "abstract": "Compared with common visible light scenes, the target of infrared scenes lacks information such as the color, texture. Infrared images have low contrast, which not only lead to interference between targets, but also interference between the target and the background. In addition, most infrared tracking algorithms lack a redetection mechanism after lost target, resulting in poor tracking effect after occlusion or blurring. To solve these problems, we propose a scene-aware classifier to dynamically adjust low, middle, and high level features, improving the ability to utilize features in different infrared scenes. Besides, we designed an infrared target re-detector based on multi-domain convolutional network to learn from the tracked target samples and background samples, improving the ability to identify the differences between the target and the background. The experimental results on VOT-TIR2015, VOT-TIR2017 and LSOTB-TIR show that the proposed algorithm achieves the most advanced results in the three infrared object tracking benchmark.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032400275X",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Computer vision",
      "Detector",
      "Infrared",
      "Optics",
      "Pattern recognition (psychology)",
      "Pedagogy",
      "Physics",
      "Psychology",
      "Telecommunications",
      "Thermal infrared",
      "Tracking (education)"
    ],
    "authors": [
      {
        "surname": "Ji",
        "given_name": "Qingbo"
      },
      {
        "surname": "Zhang",
        "given_name": "Pengfei"
      },
      {
        "surname": "Chen",
        "given_name": "Kuicheng"
      },
      {
        "surname": "Zhang",
        "given_name": "Lei"
      },
      {
        "surname": "Hou",
        "given_name": "Changbo"
      }
    ]
  },
  {
    "title": "Faster-slow network fused with enhanced fine-grained features for action recognition",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104328",
    "abstract": "Two-stream methods, which separate human actions and backgrounds into temporal and spatial streams visually, have shown promising results in action recognition datasets. However, prior researches emphasize motion modeling but overlook the robust correlation between motion features and spatial information, causing restriction of the model’s ability to recognize behaviors entailing occlusions or rapid changes. Therefore, we introduce Faster-slow, an improved framework for frame-level motion features. It introduces a Behavioural Feature Enhancement (BFE) module based on a novel two-stream network with different temporal resolutions. BFE consists of two components: MM, which incorporates motion-aware attention to capture dependencies between adjacent frames; STC, which enhances spatio-temporal and channel information to generate optimized features. Overall, BFE facilitates the extraction of finer-grained motion information, while ensuring a stable fusion of information across both streams. We evaluate the Faster-slow on the Atomic Visual Actions dataset, and the Faster-AVA dataset constructed in this paper, yielding promising experimental results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002840",
    "keywords": [
      "Action (physics)",
      "Action recognition",
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Physics",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Xuegang"
      },
      {
        "surname": "Zhu",
        "given_name": "Jiawei"
      },
      {
        "surname": "Yang",
        "given_name": "Liu"
      }
    ]
  },
  {
    "title": "Lightweight macro-pixel quality enhancement network for light field images compressed by versatile video coding",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104329",
    "abstract": "Previous research demonstrated that filtering Macro-Pixels (MPs) in a decoded Light Field Image (LFI) sequence can effectively enhances the quality of the corresponding Sub-Aperture Images (SAIs). In this paper, we propose a deep-learning-based quality enhancement model following the MP-wise processing approach tailored to LFIs encoded by the Versatile Video Coding (VVC) standard. The proposed novel Res2Net Quality Enhancement Convolutional Neural Network (R2NQE-CNN) architecture is both lightweight and powerful, in which the Res2Net modules are employed to perform LFI filtering for the first time, and are implemented with a novel improved 3D-feature-processing structure. The proposed method incorporates only 205K model parameters and achieves significant Y-BD-rate reductions over VVC of up to 32%, representing a relative improvement of up to 33% compared to the state-of-the-art method, which has more than three times the number of parameters of our proposed model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002852",
    "keywords": [
      "Artificial intelligence",
      "Coding (social sciences)",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Image quality",
      "Light field",
      "Macro",
      "Mathematics",
      "Pixel",
      "Programming language",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Hongyue"
      },
      {
        "surname": "Cui",
        "given_name": "Chen"
      },
      {
        "surname": "Jia",
        "given_name": "Chuanmin"
      },
      {
        "surname": "Zhang",
        "given_name": "Xinfeng"
      },
      {
        "surname": "Ma",
        "given_name": "Siwei"
      }
    ]
  },
  {
    "title": "TrMLGAN: Transmission MultiLoss Generative Adversarial Network framework for image dehazing",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104324",
    "abstract": "Hazy environments significantly degrade image quality, leading to poor contrast and reduced visibility. Existing dehazing methods often struggle to predict the transmission map, which is crucial for accurate dehazing. This study introduces the Transmission MultiLoss Generative Adversarial Network (TrMLGAN), a novel framework designed to enhance transmission map estimation for improved dehazing. The transmission map is initially computed using a dark channel prior-based approach and refined using the TrMLGAN framework, which leverages Generative Adversarial Networks (GANs). By integrating multiple loss functions, such as adversarial, pixel-wise similarity, perceptual similarity, and SSIM losses, our method focuses on various aspects of image quality. This enables robust dehazing performance without direct dependence on ground-truth images. Evaluations using PSNR, SSIM, FADE, NIQE, BRISQUE, and SSEQ metrics show that TrMLGAN significantly outperforms state-of-the-art methods across datasets including D-HAZY, HSTS, SOTS Outdoor, NH-HAZE, and D-Hazy, validating its potential for real-world applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002803",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Generative adversarial network",
      "Generative grammar",
      "Image (mathematics)",
      "Telecommunications",
      "Transmission (telecommunications)",
      "Transmission network"
    ],
    "authors": [
      {
        "surname": "Dwivedi",
        "given_name": "Pulkit"
      },
      {
        "surname": "Chakraborty",
        "given_name": "Soumendu"
      }
    ]
  },
  {
    "title": "Color image watermarking using vector SNCM-HMT",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104339",
    "abstract": "An image watermarking scheme is typically evaluated using three main conflicting characteristics: imperceptibility, robustness, and capacity. Developing a good image watermarking method is challenging because it requires a trade-off between these three basic characteristics. In this paper, we proposed a statistical color image watermarking based on robust discrete nonseparable Shearlet transform (DNST)-fast quaternion generic polar complex exponential transform (FQGPCET) magnitude and vector skew-normal-Cauchy mixtures (SNCM)-hidden Markov tree (HMT). The proposed watermarking system consists of two main parts: watermark inserting and watermark extraction. In watermark inserting, we first perform DNST on R, G, and B components of color host image, respectively. We then compute block FQGPCET of DNST domain color components, and embed watermark signal in DNST-FQGPCET magnitudes using multiplicative approach. In watermark extraction, we first analyze the robustness and statistical characteristics of local DNST-FQGPCET magnitudes of color image. We then observe that, vector SNCM-HMT model can capture accurately the marginal distribution and multiple strong dependencies of local DNST-FQGPCET magnitudes. Meanwhile, vector SNCM-HMT parameters can be computed effectively using variational expectation–maximization (VEM) parameter estimation. Motivated by our modeling results, we finally develop a new statistical color image watermark decoder based on vector SNCM-HMT and maximum likelihood (ML) decision rule. Experimental results on extensive test images demonstrate that the proposed statistical color image watermarking provides a performance better than that of most of the state-of-the-art statistical methods and some deep learning approaches recently proposed in the literature.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002955",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Digital watermarking",
      "Gene",
      "Genetics",
      "Image (mathematics)",
      "Mathematics",
      "Recombinant DNA",
      "Vector (molecular biology)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Hongxin"
      },
      {
        "surname": "Ma",
        "given_name": "Runtong"
      },
      {
        "surname": "Niu",
        "given_name": "Panpan"
      }
    ]
  },
  {
    "title": "Consistent prototype contrastive learning for weakly supervised person search",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104321",
    "abstract": "Weakly supervised person search simultaneously addresses detection and re-identification tasks without relying on person identity labels. Prototype-based contrastive learning is commonly used to address unsupervised person re-identification. We argue that prototypes suffer from spatial, temporal, and label inconsistencies, which result in their inaccurate representation. In this paper, we propose a novel Consistent Prototype Contrastive Learning (CPCL) framework to address prototype inconsistency. For spatial inconsistency, a greedy update strategy is developed to introduce ground truth proposals in the training process and update the memory bank only with the ground truth features. To improve temporal consistency, CPCL employs a local window strategy to calculate the prototype within a specific temporal domain window. To tackle label inconsistency, CPCL adopts a prototype nearest neighbor consistency method that leverages the intrinsic information of the prototypes to rectify the pseudo-labels. Experimentally, the proposed method exhibits remarkable performance improvements on both the CUHK-SYSU and PRW datasets, achieving an mAP of 90.2% and 29.3% respectively. Moreover, it achieves state-of-the-art performance on the CUHK-SYSU dataset. The code will be available on the project website: https://github.com/JackFlying/cpcl.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002773",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Machine learning",
      "Mathematics",
      "Natural language processing",
      "Psychology"
    ],
    "authors": [
      {
        "surname": "Lin",
        "given_name": "Huadong"
      },
      {
        "surname": "Yu",
        "given_name": "Xiaohan"
      },
      {
        "surname": "Zhang",
        "given_name": "Pengcheng"
      },
      {
        "surname": "Bai",
        "given_name": "Xiao"
      },
      {
        "surname": "Zheng",
        "given_name": "Jin"
      }
    ]
  },
  {
    "title": "HRGUNet: A novel high-resolution generative adversarial network combined with an improved UNet method for brain tumor segmentation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104345",
    "abstract": "Brain tumor segmentation in MRI images is challenging due to variability in tumor characteristics and low contrast. We propose HRGUNet, which combines a high-resolution generative adversarial network with an improved UNet architecture to enhance segmentation accuracy. Our proposed GAN model uses an innovative discriminator design that is able to process complete tumor labels as input. This approach can better ensure that the generator produces realistic tumor labels compared to some existing GAN models that only use local features. Additionally, we introduce a Multi-Scale Pyramid Fusion (MSPF) module to improve fine-grained feature extraction and a Refined Channel Attention (RCA) module to enhance focus on tumor regions. In comparative experiments, our method was verified on the BraTS2020 and BraTS2019 data sets, and the average Dice coefficient increased by 1.5% and 1.2% respectively, and the Hausdorff distance decreased by 23.9% and 15.2% respectively, showing its robustness and generalization for segmenting complex tumor structures.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324003018",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Generative adversarial network",
      "Generative grammar",
      "Geology",
      "High resolution",
      "Image (mathematics)",
      "Pattern recognition (psychology)",
      "Remote sensing",
      "Resolution (logic)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Dongmei"
      },
      {
        "surname": "Luo",
        "given_name": "Hao"
      },
      {
        "surname": "Li",
        "given_name": "Xingyang"
      },
      {
        "surname": "Chen",
        "given_name": "Shengbing"
      }
    ]
  },
  {
    "title": "Crowd counting network based on attention feature fusion and multi-column feature enhancement",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104323",
    "abstract": "Density map estimation is commonly used for crowd counting. However, using it alone may make some individuals difficult to recognize, due to the problems of target occlusions, scale variations, complex background and heterogeneous distribution. To alleviate these problems, we propose a two-stage crowd counting network based on attention feature fusion and multi-column feature enhancement (AFF-MFE-TNet). In the first stage, AFF-MFE-TNet transforms the input image into a probability map. In the second stage, a multi-column feature enhancement module is constructed to enhance features by expanding the receptive fields, a dual attention feature fusion module is designed to adaptively fuse the features of different scales through attention mechanisms, and a triple counting loss is presented for AFF-MFE-TNet, which can fit the ground truth probability maps and density maps better, and improve the counting performance. Experimental results show that AFF-MFE-TNet can effectively improve the accuracy of crowd counting, as compared with the state-of-the-art.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002797",
    "keywords": [
      "Artificial intelligence",
      "Column (typography)",
      "Computer science",
      "Feature (linguistics)",
      "Frame (networking)",
      "Fusion",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Qian"
      },
      {
        "surname": "Zhong",
        "given_name": "Yixiong"
      },
      {
        "surname": "Fang",
        "given_name": "Jiongtao"
      }
    ]
  },
  {
    "title": "Underwater image enhancement method via extreme enhancement and ultimate weakening",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104341",
    "abstract": "The existing histogram-based methods for underwater image enhancement are prone to over-enhancement, which will affect the analysis of enhanced images. However, an idea that achieves contrast balance by enhancing and weakening the contrast of an image can address the problem. Therefore, an underwater image enhancement method based on extreme enhancement and ultimate weakening (EEUW) is proposed in this paper. This approach comprises two main steps. Firstly, an image with extreme contrast can be achieved by applying grey prediction evolution algorithm (GPE), which is the first time that GPE is introduced into dual-histogram thresholding method to find the optimal segmentation threshold for accurate segmentation. Secondly, a pure gray image can be obtained through a fusion strategy based on the grayscale world assumption to achieve the ultimate weakening. Experiments conducted on three standard underwater image benchmark datasets validate that EEUW outperforms the 10 state-of-the-art methods in improving the contrast of underwater images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002979",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Environmental science",
      "Geology",
      "Image (mathematics)",
      "Image enhancement",
      "Materials science",
      "Oceanography",
      "Underwater"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Yang"
      },
      {
        "surname": "Su",
        "given_name": "Qinghua"
      },
      {
        "surname": "Hu",
        "given_name": "Zhongbo"
      },
      {
        "surname": "Jiang",
        "given_name": "Shaojie"
      }
    ]
  },
  {
    "title": "Multi-level similarity transfer and adaptive fusion data augmentation for few-shot object detection",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104340",
    "abstract": "Few-shot object detection method aims to learn novel classes through a small number of annotated novel class samples without having a catastrophic impact on previously learned knowledge, thereby expanding the trained model’s ability to detect novel classes. For existing few-shot object detection methods, there is a prominent false positive issue for the novel class samples due to the similarity in appearance features and feature distribution between the novel classes and the base classes. That is, the following two issues need to be solved: (1) How to detect these false positive samples in large-scale dataset, and (2) How to utilize the correlations between these false positive samples and other samples to improve the accuracy of the detection model. To address the first issue, an adaptive fusion data augmentation strategy is utilized to enhance the diversity of novel class samples and further alleviate the issue of false positive novel class samples. To address the second issue, a similarity transfer strategy is here proposed to effectively utilize the correlations between different categories. Experimental results demonstrate that the proposed method performs well in various settings of PASCAL VOC and MSCOCO datasets, achieving 48.7 and 11.3 on PASCAL VOC and MSCOCO under few-shot settings (shot = 1) in terms of nAP50 respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002967",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Fusion",
      "Image (mathematics)",
      "Linguistics",
      "Materials science",
      "Metallurgy",
      "Object (grammar)",
      "Optics",
      "Parallel computing",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Shot (pellet)",
      "Similarity (geometry)",
      "Single shot",
      "Transfer (computing)"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Songhao"
      },
      {
        "surname": "Wang",
        "given_name": "Yi"
      }
    ]
  },
  {
    "title": "Exploring the rate-distortion-complexity optimization in neural image compression",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104294",
    "abstract": "Despite a short history, neural image codecs have been shown to surpass classical image codecs in terms of rate–distortion performance. However, most of them suffer from significantly longer decoding times, which hinders the practical applications of neural image codecs. This issue is especially pronounced when employing an effective yet time-consuming autoregressive context model since it would increase entropy decoding time by orders of magnitude. In this paper, unlike most previous works that pursue optimal RD performance while temporally overlooking the coding complexity, we make a systematical investigation on the rate–distortion-complexity (RDC) optimization in neural image compression. By quantifying the decoding complexity as a factor in the optimization goal, we are now able to precisely control the RDC trade-off and then demonstrate how the rate–distortion performance of neural image codecs could adapt to various complexity demands. Going beyond the investigation of RDC optimization, a variable-complexity neural codec is designed to leverage the spatial dependencies adaptively according to industrial demands, which supports fine-grained complexity adjustment by balancing the RDC tradeoff. By implementing this scheme in a powerful base model, we demonstrate the feasibility and flexibility of RDC optimization for neural image codecs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002505",
    "keywords": [
      "Algorithm",
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Composite material",
      "Compression (physics)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Data compression",
      "Distortion (music)",
      "Image (mathematics)",
      "Image compression",
      "Image processing",
      "Materials science",
      "Mathematics"
    ],
    "authors": [
      {
        "surname": "Gao",
        "given_name": "Yixin"
      },
      {
        "surname": "Feng",
        "given_name": "Runsen"
      },
      {
        "surname": "Guo",
        "given_name": "Zongyu"
      },
      {
        "surname": "Chen",
        "given_name": "Zhibo"
      }
    ]
  },
  {
    "title": "Aesthetic image cropping meets VLP: Enhancing good while reducing bad",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104316",
    "abstract": "Aesthetic Image Cropping (AIC) enhances the visual appeal of an image by adjusting its composition and aesthetic elements. People make these adjustments based on these elements, aiming to enhance appealing aspects while minimizing detrimental factors. Motivated by these observations, we propose a novel approach called CLIPCropping, which simulates the human decision-making process in AIC. CLIPCropping leverages Contrastive Language–Image Pre-training (CLIP) to align visual perception with textual description. It consists of three branches: composition embedding, aesthetic embedding, and image cropping. The composition embedding branch learns principles based on Composition Knowledge Embedding (CKE), while the aesthetic embedding branch learns principles based on Aesthetic Knowledge Embedding (AKE). The image cropping branch evaluates the quality of candidate crops by aggregating knowledge from CKE and AKE; an MLP produces the best result. Extensive experiments on three benchmark datasets — GAICD-1236, GAICD-3336, and FCDB — show that CLIPCropping outperforms state-of-the-art methods and provides insightful interpretations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002724",
    "keywords": [
      "Aesthetics",
      "Agricultural engineering",
      "Agriculture",
      "Archaeology",
      "Art",
      "Computer science",
      "Computer vision",
      "Cropping",
      "Engineering",
      "Environmental science",
      "History",
      "Image (mathematics)"
    ],
    "authors": [
      {
        "surname": "Yuan",
        "given_name": "Quan"
      },
      {
        "surname": "Li",
        "given_name": "Leida"
      },
      {
        "surname": "Chen",
        "given_name": "Pengfei"
      }
    ]
  },
  {
    "title": "GLIC: Underwater target detection based on global–local information coupling and multi-scale feature fusion",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104330",
    "abstract": "With the rapid development of object detection technology, underwater object detection has attracted widespread attention. Most of the existing underwater target detection methods are built based on convolutional neural networks (CNNs), which still have some limitations in the utilization of global information and cannot fully capture the key information in the images. To overcome the challenge of insufficient global–local feature extraction, an underwater target detector (namely GLIC) based on global–local information coupling and multi-scale feature fusion is proposed in this paper. Our GLIC consists of three main components: spatial pyramid pooling, global–local information coupling, and multi-scale feature fusion. Firstly, we embed spatial pyramid pooling, which improves the robustness of the model while retaining more spatial information. Secondly, we design the feature pyramid network with global–local information coupling. The global context of the transformer branch and the local features of the CNN branch interact with each other to enhance the feature representation. Finally, we construct a Multi-scale Feature Fusion (MFF) module that utilizes balanced semantic features integrated at the same depth for multi-scale feature fusion. In this way, each resolution in the pyramid receives equal information from others, thus balancing the information flow and making the features more discriminative. As demonstrated in comprehensive experiments, our GLIC, respectively, achieves 88.46%, 87.51%, and 74.94% mAP on the URPC2019, URPC2020, and UDD datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002864",
    "keywords": [
      "Artificial intelligence",
      "Cartography",
      "Computer science",
      "Computer vision",
      "Coupling (piping)",
      "Data mining",
      "Feature (linguistics)",
      "Fusion",
      "Geography",
      "Geology",
      "Information fusion",
      "Linguistics",
      "Materials science",
      "Metallurgy",
      "Oceanography",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Remote sensing",
      "Scale (ratio)",
      "Underwater"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Huipu"
      },
      {
        "surname": "Zhang",
        "given_name": "Meixiang"
      },
      {
        "surname": "Li",
        "given_name": "Yongzhi"
      }
    ]
  },
  {
    "title": "3D human model guided pose transfer via progressive flow prediction network",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104327",
    "abstract": "Human pose transfer is to transfer a conditional person image to a new target pose. The difficulty lies in modeling the large-scale spatial deformation from the conditional pose to the target one. However, the commonly used 2D data representations and one-step flow prediction scheme lead to unreliable deformation prediction because of the lack of 3D information guidance and the great changes in the pose transfer. Therefore, to bring the original 3D motion information into human pose transfer, we propose to simulate the generation process of real person image. We drive the 3D human model reconstructed from the conditional person image with the target pose and project it to the 2D plane. The 2D projection thereby inherits the 3D information of the poses which can guide the flow prediction. Furthermore, we propose a progressive flow prediction network consisting of two streams. One stream is to predict the flow by decomposing the complex pose transformation into multiple sub-transformations. The other is to generate the features of the target image according to the predicted flow. Besides, to enhance the reliability of the generated invisible regions, we use the target pose information which contains structural information from the flow prediction stream as the supplementary information to the feature generation. The synthesized images with accurate depth information and sharp details demonstrate the effectiveness of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002839",
    "keywords": [
      "3d model",
      "Artificial intelligence",
      "Computer science",
      "Flow (mathematics)",
      "Geometry",
      "Mathematics"
    ],
    "authors": [
      {
        "surname": "Ma",
        "given_name": "Furong"
      },
      {
        "surname": "Xia",
        "given_name": "Guiyu"
      },
      {
        "surname": "Liu",
        "given_name": "Qingshan"
      }
    ]
  },
  {
    "title": "A memory access number constraint-based string prediction technique for high throughput SCC implemented in AVS3",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104338",
    "abstract": "String prediction (SP) is a highly efficient screen content coding (SCC) tool that has been adopted in international and Chinese video coding standards. SP exhibits a highly flexible and efficient ability to predict repetitive matching patterns. However, SP also suffers from low throughput of decoded display output pixels per memory access, which is synchronized with the decoder clock, due to the high number of memory accesses required to decode an SP coding unit for display. Even in state-of-the-art (SOTA) SP, the worst-case scenario involves two memory accesses for decoding each 4-pixel basic string unit across two memory access units, resulting in a throughput as low as two pixels per memory access (PPMA). To solve this problem, we are the first to propose a technique called memory access number constraint-based string prediction (MANC-SP) to achieve high throughput in SCC. First, a novel MANC-SP framework is proposed, a well-designed memory access number constraint rule is established on the basis of statistical data, and a constrained RDO-based string searching method is presented. Compared with the existing SOTA SP, the experimental results demonstrate that MANC-SP can improve the throughput from 2 to 2.67 PPMA, achieving a throughput improvement of 33.33% while maintaining a negligible impact on coding efficiency and complexity.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002943",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Computer science",
      "Constraint (computer-aided design)",
      "Geometry",
      "Mathematical physics",
      "Mathematics",
      "Operating system",
      "Parallel computing",
      "String (physics)",
      "Throughput",
      "Wireless"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Liping"
      },
      {
        "surname": "Yan",
        "given_name": "Zuge"
      },
      {
        "surname": "Hu",
        "given_name": "Keli"
      },
      {
        "surname": "Feng",
        "given_name": "Sheng"
      },
      {
        "surname": "Wang",
        "given_name": "Jiangda"
      },
      {
        "surname": "Cao",
        "given_name": "Xueyan"
      },
      {
        "surname": "Lin",
        "given_name": "Tao"
      }
    ]
  },
  {
    "title": "Video Question Answering: A survey of the state-of-the-art",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104320",
    "abstract": "Video Question Answering (VideoQA) emerges as a prominent trend in the domain of Artificial Intelligence, Computer Vision, and Natural Language Processing. It involves developing systems capable of understanding, analyzing, and responding to questions about the content of videos. The Proposed survey presents an in-depth overview of the current landscape of Question Answering, shedding light on the challenges, methodologies, datasets, and innovative approaches in the domain. The key components of the Video Question Answering (VideoQA) framework include video feature extraction, question processing, reasoning, and response generation. It underscores the importance of datasets in shaping VideoQA research and the diversity of question types, from factual inquiries to spatial and temporal reasoning. The survey highlights the ongoing research directions and future prospects for VideoQA. Finally, the proposed survey gives a road map for future explorations at the intersection of multiple disciplines, emphasizing the ultimate objective of pushing the boundaries of knowledge and innovation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002761",
    "keywords": [
      "Algorithm",
      "Computer science",
      "Information retrieval",
      "Question answering",
      "State (computer science)"
    ],
    "authors": [
      {
        "surname": "P.J.",
        "given_name": "Jeshmol"
      },
      {
        "surname": "Kovoor",
        "given_name": "Binsu C."
      }
    ]
  },
  {
    "title": "Leveraging occupancy map to accelerate video-based point cloud compression",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104292",
    "abstract": "Video-based Point Cloud Compression enables point cloud streaming over the internet by converting dynamic 3D point clouds to 2D geometry and attribute videos, which are then compressed using 2D video codecs like H.266/VVC. However, the complex encoding process of H.266/VVC, such as the quadtree with nested multi-type tree (QTMT) partition, greatly hinders the practical application of V-PCC. To address this issue, we propose a fast CU partition method dedicated to V-PCC to accelerate the coding process. Specifically, we classify coding units (CUs) of projected images into three categories based on the occupancy map of a point cloud: unoccupied, partially occupied, and fully occupied. Subsequently, we employ either statistic-based rules or machine-learning models to manage the partition of each category. For unoccupied CUs, we terminate the partition directly; for partially occupied CUs with explicit directions, we selectively skip certain partition candidates; for the remaining CUs (partially occupied CUs with complex directions and fully occupied CUs), we train an edge-driven LightGBM model to predict the partition probability of each partition candidate automatically. Only partitions with high probabilities are retained for further Rate–Distortion (R–D) decisions. Comprehensive experiments demonstrate the superior performance of our proposed method: under the V-PCC common test conditions, our method reduces encoding time by 52% and 44% in geometry and attribute, respectively, while incurring only 0.68% (0.66%) BD-Rate loss in D1 (D2) measurements and 0.79% (luma) BD-Rate loss in attribute, significantly surpassing state-of-the-art works.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002487",
    "keywords": [
      "Architectural engineering",
      "Cloud computing",
      "Composite material",
      "Compression (physics)",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Data compression",
      "Engineering",
      "Geometry",
      "Materials science",
      "Mathematics",
      "Occupancy",
      "Operating system",
      "Point (geometry)",
      "Point cloud"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Wenyu"
      },
      {
        "surname": "Ding",
        "given_name": "Gongchun"
      },
      {
        "surname": "Ding",
        "given_name": "Dandan"
      }
    ]
  },
  {
    "title": "Robust text watermarking based on average skeleton mass of characters against cross-media attacks",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104300",
    "abstract": "The wide spread of digital documents makes it essential to protect intellectual property and information security. As a key method of digital copyright protection, robust document watermarking technology has attracted much attention in this context. With the rapid development of current electronic devices, the ways of document theft are no longer limited to copy and transmission. Due to the convenient and fast shooting operation of the camera on paper or screen, current text watermarking methods need to be robust to cope with cross-media transmission. To realize the corresponding robust text watermarking, a text watermarking scheme based on the average skeleton mass of characters is proposed in this paper, and the average skeleton mass of adjacent characters is used to represent the watermark information. In this paper, a watermarking scheme is designed to modify character pixels, which can modify glyphs without loss of transparency and provide high embedding capacity. Compared with the existing manually designed font-based text watermarking schemes, this scheme does not need to accurately segment characters, nor does it rely on stretching characters to the same size for matching, which reduces the need for character segmentation. In addition, the experimental results show that the proposed watermarking scheme can be robust to the information transmission modes including print-scan, print-camera and screen-camera.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002566",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Digital watermarking",
      "Image (mathematics)",
      "Mathematics",
      "Programming language",
      "Skeleton (computer programming)"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Xinyi"
      },
      {
        "surname": "Wang",
        "given_name": "Hongxia"
      }
    ]
  },
  {
    "title": "Exposing video surveillance object forgery by combining TSF features and attention-based deep neural networks",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104267",
    "abstract": "Recently, forensics has encountered a new challenge with video surveillance object forgery. This type of forgery combines the characteristics of popular video copy-move and splicing forgeries, failing most existing video forgery detection schemes. In response to this new forgery challenge, this paper proposes a Video Surveillance Object Forgery Detection (VSOFD) method including three parts components: (i) The proposed method presents a special-combined extraction technique that incorporates Temporal-Spatial-Frequent (TSF) perspectives for TSF feature extraction. Furthermore, TSF features can effectively represent video information and benefit from feature dimension reduction, improving computational efficiency. (ii) The proposed method introduces a universal, extensible attention-based Convolutional Neural Network (CNN) baseline for feature processing. This CNN processing architecture is compatible with various series and parallel feed-forward CNN structures, considering these structures as processing backbones. Therefore, the proposed CNN architecture benefits from various state-of-the-art structures, leading to addressing each independent TSF feature. (iii) The method adopts an encoder-attention-decoder RNN framework for feature classification. By incorporating temporal characteristics, the framework can further identify the correlations between the adjacent frames to classify the forgery frames better. Finally, experimental results show that the proposed network can achieve the best F 1 = 94.69 % score, increasing at least 5–12 % from the existing State-Of-The-Art (SOTA) VSOFD schemes and other video forensics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002232",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Deep neural networks",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Zhong",
        "given_name": "Jun-Liu"
      },
      {
        "surname": "Gan",
        "given_name": "Yan-Fen"
      },
      {
        "surname": "Yang",
        "given_name": "Ji-Xiang"
      },
      {
        "surname": "Chen",
        "given_name": "Yu-Huan"
      },
      {
        "surname": "Zhao",
        "given_name": "Ying-Qi"
      },
      {
        "surname": "Lv",
        "given_name": "Zhi-Sheng"
      }
    ]
  },
  {
    "title": "LG-AKD: Application of a lightweight GCN model based on adversarial knowledge distillation to skeleton action recognition",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104286",
    "abstract": "Human action recognition, a pivotal topic in computer vision, is a highly complex and challenging task. It requires the analysis of not only spatial dependencies of targets but also temporal changes in these targets. In recent decades, the advancement of deep learning has led to the development of numerous action recognition methods based on deep neural networks. Given that the skeleton points of the human body can be treated as a graph structure, graph neural networks (GNNs) have emerged as an effective tool for modeling such data, garnering significant interest from researchers. This paper aims to address the issue of low test speed caused by over-complicated deep graph convolutional models. To achieve this, we compress the network structure using knowledge distillation from a teacher-student architecture, leading to a compact and lightweight student GNN. To enhance the model’s robustness and generalization capabilities, we introduce a data augmentation mechanism that generates diverse action sequences while maintaining consistent behavior labels, thereby providing a more comprehensive learning basis for the model. The proposed model integrates three distinct knowledge learning paths: teacher networks, original datasets, and derived data. The fusion of knowledge distillation and data augmentation enables lightweight student networks to outperform their teacher networks in terms of both performance and efficiency. Experimental results demonstrate the efficacy of our approach in the context of skeleton-based human action recognition, highlighting its potential to simplify state-of-the-art models while enhancing their performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002426",
    "keywords": [
      "Action (physics)",
      "Adversarial system",
      "Artificial intelligence",
      "Biochemical engineering",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Distillation",
      "Engineering",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Skeleton (computer programming)"
    ],
    "authors": [
      {
        "surname": "Cui",
        "given_name": "Ran"
      },
      {
        "surname": "Wu",
        "given_name": "Jingran"
      },
      {
        "surname": "Wang",
        "given_name": "Xiang"
      }
    ]
  },
  {
    "title": "Progressive cross-level fusion network for RGB-D salient object detection",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104268",
    "abstract": "Depth maps can provide supplementary information for salient object detection (SOD) and perform better in handling complex scenes. Most existing RGB-D methods only utilize deep cues at the same level, and few methods focus on the information linkage between cross-level features. In this study, we propose a Progressive Cross-level Fusion Network (PCF-Net). It ensures the cross-flow of cross-level features by gradually exploring deeper features, which promotes the interaction and fusion of information between different-level features. First, we designed a Cross-Level Guide Cross-Modal Fusion Module (CGCF) that utilizes the spatial information of upper-level features to suppress modal feature noise and to guide lower-level features for cross-modal feature fusion. Next, the proposed Semantic Enhancement Module (SEM) and Local Enhancement Module (LEM) are used to further introduce deeper features, enhance the high-level semantic information and low-level structural information of cross-modal features, and use self-modality attention refinement to improve the enhancement effect. Finally, the multi-scale aggregation decoder mines enhanced feature information in multi-scale spaces and effectively integrates cross-scale features. In this study, we conducted numerous experiments to demonstrate that the proposed PCF-Net outperforms 16 of the most advanced methods on six popular RGB-D SOD datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002244",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Fusion",
      "Linguistics",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "RGB color model",
      "Salient"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Jianbao"
      },
      {
        "surname": "Pan",
        "given_name": "Chen"
      },
      {
        "surname": "Zheng",
        "given_name": "Yilin"
      },
      {
        "surname": "Zhang",
        "given_name": "Dongping"
      }
    ]
  },
  {
    "title": "Optimized deep learning enabled lecture audio video summarization",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104309",
    "abstract": "Video summarization plays an important role in multiple applications by compressing lengthy video content into compressed representation. The purpose is to present a fine-tuned deep model for lecture audio video summarization. Initially, the input lecture audio-visual video is taken from the dataset. Then, the video shot segmentation (slide segmentation) is done using the YCbCr space colour model. From each video shot, the audio and video within the video shot are segmented using the Honey Badger-based Bald Eagle Algorithm (HBBEA). The HBBEA is obtained by combining the Bald Eagle Search (BES) and Honey Badger Algorithm (HBA). The DRN training is executed by HBBEA to select the finest DRN weights. The relevant video frames are merged with the audio. The proposed HBBEA-based DRN outperformed with a better F1-Score of 91.9 %, Negative predictive value (NPV) of 89.6 %, Positive predictive value (PPV) of 90.7 %, Accuracy of 91.8 %, precision of 91 %, and recall of 92.8 %.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002657",
    "keywords": [
      "Artificial intelligence",
      "Automatic summarization",
      "Computer science",
      "Deep learning",
      "Multimedia",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Chandan Kaur",
        "given_name": "Preet"
      },
      {
        "surname": "Ragha",
        "given_name": "Dr. Leena"
      }
    ]
  },
  {
    "title": "Lossless medical ultrasound image compression based on frequency domain decomposition",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104306",
    "abstract": "Medical ultrasound imaging is a widely used non-invasive method for diagnosing diseases. However, these images contain significant speckle noise, which differs from the characteristics of natural images. This makes effective lossless compression of medical ultrasound images a challenging task. In this paper, we propose a novel hybrid ultrasound image lossless learning compression framework. Firstly, we use the traditional DCT (discrete cosine transform) to transform the original raw pixels of ultrasound images into the frequency domain. Secondly, to effectively compress the numerical values in the frequency domain, we decompose the DCT coefficients into different groups to reduce local and global information redundancy in the frequency domain. Finally, we use learned and non-learned methods to compress the DCT coefficients of different groups separately. The experimental results show that on the Breast ultrasound image dataset, our proposed method achieves a bit rate reduction of 8.6% to 68.9% compared to learned and non-learned methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002621",
    "keywords": [
      "Artificial intelligence",
      "Biomedical engineering",
      "Chemistry",
      "Composite material",
      "Compression (physics)",
      "Computer science",
      "Computer vision",
      "Data compression",
      "Decomposition",
      "Frequency domain",
      "High frequency ultrasound",
      "Image (mathematics)",
      "Image compression",
      "Image processing",
      "Lossless compression",
      "Lossy compression",
      "Materials science",
      "Medicine",
      "Organic chemistry",
      "Radiology",
      "Ultrasound"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Yaqi"
      },
      {
        "surname": "Li",
        "given_name": "Yue"
      }
    ]
  },
  {
    "title": "Spatio-temporal feature learning for enhancing video quality based on screen content characteristics",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104270",
    "abstract": "With the rising demands for remote desktops and online meetings, screen content videos have drawn significant attention. Different from natural videos, screen content videos often exhibit scene switches where the content abruptly changes from one frame to the next. These scene switches result in obvious distortions in compressed videos. Besides, frame freezing, where the content remains unchanged for a certain duration, is also very common in screen content videos. Existing alignment-based models struggle to effectively enhance scene switch frames and lack efficiency when dealing with frame freezing situations. Therefore, we propose a novel alignment-free method that effectively handles both scene switches and frame freezing. In our approach, we develop a spatial and temporal feature extraction module that compresses and extracts spatio-temporal information from three groups of frame inputs. This enables efficient handling of scene switches. In addition, an edge aware block is proposed for extracting edge information, which guides the model to focus on restoring the high-frequency components in frame freezing situations. The fusion module is then designed to adaptively fuse the features from three groups, considering different positions of video frames, to enhance frames during scene switch and frame freezing scenarios. Experimental results demonstrate the significant advancements achieved by the proposed edge aware with spatio-temporal information fusion network (EAST) in enhancing the quality of compressed videos, surpassing the current state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002268",
    "keywords": [
      "Artificial intelligence",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Content (measure theory)",
      "Epistemology",
      "Feature (linguistics)",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Multimedia",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Quality (philosophy)"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Ziyin"
      },
      {
        "surname": "Chan",
        "given_name": "Yui-Lam"
      },
      {
        "surname": "Tsang",
        "given_name": "Sik-Ho"
      },
      {
        "surname": "Kwong",
        "given_name": "Ngai-Wing"
      },
      {
        "surname": "Lam",
        "given_name": "Kin-Man"
      },
      {
        "surname": "Ling",
        "given_name": "Wing-Kuen"
      }
    ]
  },
  {
    "title": "DiZNet: An end-to-end text detection and recognition algorithm with detail in text zone",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104261",
    "abstract": "This paper proposed an efficient and novel end-to-end text detection and recognition framework called DiZNet. DiZNet is built upon a core representation using text detail maps and employs the classical lightweight ResNet18 as the backbone for the text detection and recognition algorithm model. The redesigned Text Attention Head (TAH) takes multiple shallow backbone features as input, effectively extracting pixel-level information of text in images and global text positional features. The extracted text features are integrated into the stackable Feature Pyramid Enhancement Fusion Module (FPEFM). Supervised with text detail map labels, which include boundary information and texture of important text, the model predicts text detail maps and fuses them into the text detection and recognition heads. Through end-to-end testing on publicly available natural scene text benchmark datasets, our approach demonstrates robust generalization capabilities and real-time detection speeds. Leveraging the advantages of text detail map representation, DiZNet achieves a good balance between precision and efficiency on challenging datasets. For example, DiZNet achieves 91.2% Precision and 85.9% F-measure at a speed of 38.4 FPS on Total-Text and 83.8% F-measure at a speed of 30.0 FPS on ICDAR2015, it attains 83.8% F-measure at 30.0 FPS. The code is publicly available at: https://github.com/DiZ-gogogo/DiZNet",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002177",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "End-to-end principle",
      "Pattern recognition (psychology)",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Di"
      },
      {
        "surname": "Zhang",
        "given_name": "Jianxun"
      },
      {
        "surname": "Li",
        "given_name": "Chao"
      }
    ]
  },
  {
    "title": "SR4KVQA: Video quality assessment database and metric for 4K super-resolution",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104290",
    "abstract": "The quality assessment for 4K super-resolution (SR) videos can be conducive to the optimization of video SR algorithms. To improve the subjective and objective consistency of the SR quality assessment, a 4K video database and a blind metric are proposed in this paper. In the database SR4KVQA, there are 30 4K pristine videos, from which 600 SR 4K distorted videos with mean opinion score (MOS) labels are generated by three classic interpolation methods, six SR algorithms based on the deep neural network (DNN), and two SR algorithms based on the generative adversarial network (GAN). The benchmark experiment of the proposed database indicates that video quality assessment (VQA) of the 4K SR videos is challenging for the existing metrics. Among those metrics, the Video-Swin-Transformer backbone demonstrates tremendous potential in the VQA task. Accordingly, a blind VQA metric based on the Video-Swin-Transformer backbone is established, where the normalized loss function and optimized spatio-temporal sampling strategy are applied. The experiment result manifests that the Pearson linear correlation coefficient (PLCC) and Spearman rank-order correlation coefficient (SROCC) of the proposed metric reach 0.8011 and 0.8275 respectively on the SR4KVQA database, which outperforms or competes with the state-of-the-art VQA metrics. The database and the code proposed in this paper are available in the GitHub repository, https://github.com/AlexReadyNico/SR4KVQA.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002463",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Database",
      "Engineering",
      "Metric (unit)",
      "Operations management",
      "Physics",
      "Quality (philosophy)",
      "Quantum mechanics",
      "Resolution (logic)",
      "Video quality"
    ],
    "authors": [
      {
        "surname": "Zheng",
        "given_name": "Ruidi"
      },
      {
        "surname": "Jiang",
        "given_name": "Xiuhua"
      }
    ]
  },
  {
    "title": "PVT2DNet: Polyp segmentation with vision transformer and dual decoder refinement strategy",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104304",
    "abstract": "Colorectal carcinoma is a prevalent malignancy worldwide. Accurate polyp segmentation, along with endoscopic resection, can significantly reduce its incidence and mortality. Most polyp segmentation neural networks are CNN-based and single decoder strategy architectures, which learn limited robust representations. In this paper, we propose a novel network with the vision transformer and dual decoder refinement strategy called PVT2DNet to overcome some limitations of current networks and achieve more precise automated polyp segmentation. The PVT2DNet adopts a pyramid vision transformer encoder and enhances the multi-level features with the context-enhanced module (CEM). Moreover, instead of directly feeding features into a single decoder, we introduce a dual partial cascaded decoder refinement strategy to excavate more informative polyp cues. Extensive experimentations on five widely adopted datasets demonstrate the proposed network outperforms other state-of-the-art on most metrics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002608",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Dual (grammatical number)",
      "Literature",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Segmentation",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Yibiao"
      },
      {
        "surname": "Jin",
        "given_name": "Yan"
      },
      {
        "surname": "Jiang",
        "given_name": "Zhiwei"
      },
      {
        "surname": "Zheng",
        "given_name": "Qiufu"
      }
    ]
  },
  {
    "title": "VTPL: Visual and text prompt learning for visual-language models",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104280",
    "abstract": "Visual-language (V-L) models have achieved remarkable success in learning combined visual–textual representations from large web datasets. Prompt learning, as a solution for downstream tasks, can address the forgetting of knowledge associated with fine-tuning. However, current methods focus on a single modality and fail to fully use multimodal information. This paper aims to address these limitations by proposing a novel approach called visual and text prompt learning (VTPL) to train the model and enhance both visual and text prompts. Visual prompts align visual features with text features, whereas text prompts enrich the semantic information of the text. Additionally, this paper introduces a poly-1 information noise contrastive estimation (InfoNCE) loss and a center loss to increase the interclass distance and decrease the intraclass distance. Experiments on 11 image datasets show that VTPL outperforms state-of-the-art methods, achieving 1.61%, 1.63%, 1.99%, 2.42%, and 2.87% performance boosts over CoOp for 1, 2, 4, 8, and 16 shots, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002360",
    "keywords": [
      "Artificial intelligence",
      "Cognitive psychology",
      "Computer science",
      "Focus (optics)",
      "Forgetting",
      "Machine learning",
      "Modality (human–computer interaction)",
      "Natural language processing",
      "Optics",
      "Physics",
      "Psychology"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Bo"
      },
      {
        "surname": "Wu",
        "given_name": "Zhichao"
      },
      {
        "surname": "Zhang",
        "given_name": "Hao"
      },
      {
        "surname": "He",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "Iterative decoupling deconvolution network for image restoration",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104288",
    "abstract": "The iterative decoupled deblurring BM3D (IDDBM3D) (Danielyan et al., 2011) combines the analysis representation and the synthesis representation, where deblurring and denoising operations are decoupled, so that both problems can be easily solved. However, the IDDBM3D has some limitations. First, the analysis transformation and the synthesis transformation are analytical, thus have limited representation ability. Second, it is difficult to effectively remove image noise from threshold transformation. Third, there exists hyper-parameters to be tuned manually, which is difficult and time consuming. In this work, we propose an iterative decoupling deconvolution network(IDDNet), by unrolling the iterative decoupling algorithm of the IDDBM3D. In the proposed IDDNet, the analysis/synthesis transformation are implemented by encoder/decoder modules; the denoising is implemented by convolutional neural network based denoiser; the hyper-parameters are estimated by hyper-parameter module. We apply our models for image deblurring and super-resolution. Experimental results show that the IDDNet significantly outperforms the state-of-the-art unfolding networks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032400244X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Control engineering",
      "Deconvolution",
      "Decoupling (probability)",
      "Engineering",
      "Image (mathematics)",
      "Image processing",
      "Image restoration",
      "Mathematics"
    ],
    "authors": [
      {
        "surname": "Ji",
        "given_name": "Yixing"
      },
      {
        "surname": "Kong",
        "given_name": "Shengjiang"
      },
      {
        "surname": "Wang",
        "given_name": "Weiwei"
      },
      {
        "surname": "Jia",
        "given_name": "Xixi"
      },
      {
        "surname": "Feng",
        "given_name": "Xiangchu"
      }
    ]
  },
  {
    "title": "A Transformer-based invertible neural network for robust image watermarking",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104317",
    "abstract": "For the existing encoder-noise-decoder (END) based watermarking models, since the coupling between the encoder and the decoder is weak, the encoder generally embeds certain redundant features into the cover image to enable the decoder to extract watermark completely, which will affect watermarking invisibility. To address this problem, this paper proposes a Transformer-based invertible neural network (INN) for robust image watermarking (IWFormer). In order to effectively reduce redundant features, the INN framework is utilized for the watermark embedding and extracting processes, so that the encoded features are highly consistent with the features required for decoding. For enhancing watermarking robustness, an affine Transformer module is designed by mining the global correlation of the cover image. In addition, considering that the human visual system is sensitive to low-frequency variations, the wavelet low-frequency sub-band loss is deployed to guide watermark to be embedded in middle- and high-frequency components, thus further increasing the quality of the encoded images. Experimental results demonstrate that compared with the existing state-of-the-art watermarking models, the proposed IWFormer owns remarkable advantages in terms of both watermarking invisibility and robustness.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002736",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Digital watermarking",
      "Electrical engineering",
      "Engineering",
      "Image (mathematics)",
      "Invertible matrix",
      "Mathematics",
      "Pure mathematics",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "He",
        "given_name": "Zhouyan"
      },
      {
        "surname": "Hu",
        "given_name": "Renzhi"
      },
      {
        "surname": "Wu",
        "given_name": "Jun"
      },
      {
        "surname": "Luo",
        "given_name": "Ting"
      },
      {
        "surname": "Xu",
        "given_name": "Haiyong"
      }
    ]
  },
  {
    "title": "Improved semantic-guided network for skeleton-based action recognition",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104281",
    "abstract": "A fundamental issue in skeleton-based action recognition is the extraction of useful features from skeleton joints. Unfortunately, the current state-of-the-art models for this task have a tendency to be overly complex and parameterized, which results in low model training and inference time efficiency for large-scale datasets. In this work, we develop a simple but yet an efficient baseline for skeleton-based Human Action Recognition (HAR). The architecture is based on adaptive GCNs (Graph Convolutional Networks) to capture the complex interconnections within skeletal structures automatically without the need of a predefined topology. The GCNs are followed and empowered with an attention mechanism to learn more informative representations. This paper reports interesting accuracy on a large-scale dataset NTU-RGB+D 60, 89.7% and 95.0% on respectively Cross-Subject, and Cross-View benchmarks. On NTU-RGB+D 120, 84.6% and 85.8% over Cross-Subject and Cross-Setup settings, respectively. This work provides an improvement of the existing model SGN (Semantic-Guided Neural Networks) when extracting more discriminant spatial and temporal features.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002372",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Economics",
      "Graph",
      "Inference",
      "Machine learning",
      "Management",
      "Parameterized complexity",
      "Pattern recognition (psychology)",
      "Programming language",
      "RGB color model",
      "Skeleton (computer programming)",
      "Task (project management)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Mansouri",
        "given_name": "Amine"
      },
      {
        "surname": "Bakir",
        "given_name": "Toufik"
      },
      {
        "surname": "Elzaar",
        "given_name": "Abdellah"
      }
    ]
  },
  {
    "title": "Effective image compression using hybrid DCT and hybrid capsule auto encoder for brain MR images",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104296",
    "abstract": "Nowadays, image compression is gaining popularity in various fields because of its storage and transmission capability. This work aims to introduce a medical image (MI) compression model in brain magnetic resonance images (MRI) to mitigate issues in bandwidth and storage. Initially, pre-processing is done to neglect the noises in inputs using the Adaptive Linear Smoothing and Histogram Equalization (ALSHE) method. Then, the Region of Interest (ROI) and Non-ROI parts are separately segmented by the Optimized Fuzzy C-Means (OFCM) approach for reducing high complexity issues. Finally, a novel Hybrid Discrete Cosine Transform-Improved Zero Wavelet (DCT-IZW) is proposed for lossless compression and Hybrid Equilibrium Optimization-Capsule Auto Encoder (EO-CAE) for lossy compression. Then, the compressed ROI and Non-ROI images are added together, and the inverse operation of the compression process is performed to obtain the reconstructed image. This study used BRATS (2015, 2018) datasets for simulation and attained better performance than other existing methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002529",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Capsule",
      "Composite material",
      "Compression (physics)",
      "Computer science",
      "Computer vision",
      "Discrete cosine transform",
      "Encoder",
      "Image (mathematics)",
      "Image compression",
      "Image processing",
      "Materials science",
      "Operating system"
    ],
    "authors": [
      {
        "surname": "Puthentharayil Vikraman",
        "given_name": "Bindu"
      },
      {
        "surname": "Afthab",
        "given_name": "Jabeena"
      }
    ]
  },
  {
    "title": "Data compensation and feature fusion for sketch based person retrieval",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104287",
    "abstract": "Sketch re-identification (Re-ID) aims to retrieve pedestrian photo in the gallery dataset by a query sketch drawn by professionals. The sketch Re-ID task has not been adequately studied because collecting such sketches is difficult and expensive. In addition, the significant modality difference between sketches and images makes extracting the discriminative feature information difficult. To address above issues, we introduce a novel sketch-style pedestrian dataset named Pseudo-Sketch dataset. Our proposed dataset maximizes the utilization of the existing person dataset resources and is freely available, thus effectively reducing the expenses associated with the training and deployment phases. Furthermore, to mitigate the modality gap between sketches and visible images, a cross-modal feature fusion network is proposed that incorporates information from each modality. Experiment results show that the proposed Pseudo-Sketch dataset can effectively complement the real sketch dataset, and the proposed network obtains competitive results than SOTA methods. The dataset will be released later.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002438",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Compensation (psychology)",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Fusion",
      "Information retrieval",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Psychoanalysis",
      "Psychology",
      "Sketch"
    ],
    "authors": [
      {
        "surname": "Ye",
        "given_name": "Yu"
      },
      {
        "surname": "Chen",
        "given_name": "Jun"
      },
      {
        "surname": "Sun",
        "given_name": "Zhihong"
      },
      {
        "surname": "Mukherjee",
        "given_name": "Mithun"
      }
    ]
  },
  {
    "title": "Diving deep into human action recognition in aerial videos: A survey",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104298",
    "abstract": "Human Action Recognition from Unmanned Aerial Vehicles is a dynamic research domain with significant benefits in scale, mobility, deployment, and covert observation. This paper offers a comprehensive review of state-of-the-art algorithms for human action recognition and provides a novel taxonomy that categorizes the reviewed methods into two broad categories: Localization based and Globalization based. These categories are defined by how actions are segmented from visual data and how their spatial and temporal structures are modeled. We examine these techniques, highlighting their strengths and limitations, and provide essential background on human action recognition, including fundamental concepts and challenges in aerial videos. Additionally, we discuss existing datasets, enabling a comparative analysis. This survey identifies gaps and suggests future research directions, serving as a catalyst for advancing human action recognition in aerial videos. To our knowledge, this is the first detailed review of this kind.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002542",
    "keywords": [
      "Action (physics)",
      "Action recognition",
      "Artificial intelligence",
      "Class (philosophy)",
      "Communication",
      "Computer science",
      "Computer vision",
      "Geography",
      "Physics",
      "Psychology",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Kapoor",
        "given_name": "Surbhi"
      },
      {
        "surname": "Sharma",
        "given_name": "Akashdeep"
      },
      {
        "surname": "Verma",
        "given_name": "Amandeep"
      }
    ]
  },
  {
    "title": "CAAM: A calibrated augmented attention module for masked face recognition",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104315",
    "abstract": "Along with other aspects of daily life, the COVID-19 pandemic has a substantial impact on the performance of facial recognition (FR) systems installed in various locations for identity verification. To address this pivotal issue, we propose an attention-guided masked face recognition (MFR) method, named Calibrated Augmented Attention Module (CAAM), which consists of two core components: Recursive Attention Gate (RAG) and an Augmented Feature Calibration Block (AFCB). In the first stage, RAG guides the backbone network to pay attention to non-occluded face regions for feature learning by calibrating multi-layer features while progressively reducing the network’s response to mask-occluded regions in a recursive manner. In the second stage, a dual-branch AFCB first augments the attention map generated by RAG to incorporate cross-dimensional interactions, which are then calibrated to build spatial and inter-channel dependencies across informative spatial locations for MFR. Experiments conducted on various masked face datasets validate the superior performance of CAAM.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002712",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Face (sociological concept)",
      "Facial recognition system",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Psychology"
    ],
    "authors": [
      {
        "surname": "Saad Shakeel",
        "given_name": "M."
      }
    ]
  },
  {
    "title": "A robust coverless image-synthesized video steganography based on asymmetric structure",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104303",
    "abstract": "Due to the ability of hiding secret information without modifying image content, coverless image stegonagraphy has gained higher level of security and become a research hot spot. However, in existing methods, the issue of image order disruption during network transmission is overlooked. In this paper, the image-synthesized video carrier is proposed for the first time. The selected images which represent secret information are synthesized to a video in order, thus the image order will not be disrupted during transmission and the effective capacity is greatly increased. Additionally, an asymmetric structure is designed to improve the robustness, in which only the receiver utilizes a robust image retrieval algorithm to restore secret information. Specifically, certain images are randomly selected from a public image database to create multiple coverless image datasets (MCIDs), with each image in a CID mapped to hash sequence. Images are indexed based on secret segments and synthesized into videos. After that, the synthesized videos are sent to the receiver. The receiver decodes the video into frames, identifies the corresponding CID of each frame, retrieves original image, and restores the secret information with the same mapping rule. Experimental results indicate that the proposed method outperforms existing methods in terms of capacity, robustness, and security.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002591",
    "keywords": [
      "Artificial intelligence",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Steganography"
    ],
    "authors": [
      {
        "surname": "Jiao",
        "given_name": "Yueshuang"
      },
      {
        "surname": "Zhang",
        "given_name": "Zhenzhen"
      },
      {
        "surname": "Li",
        "given_name": "Zhenzhen"
      },
      {
        "surname": "Li",
        "given_name": "Zichen"
      },
      {
        "surname": "Li",
        "given_name": "Xiaolong"
      },
      {
        "surname": "Liu",
        "given_name": "Jiaoyun"
      }
    ]
  },
  {
    "title": "Cross-stage feature fusion and efficient self-attention for salient object detection",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104271",
    "abstract": "Salient Object Detection (SOD) approaches usually aggregate high-level semantics with object details layer by layer through a pyramid fusion structure. However, the progressive feature fusion mechanism may lead to gradually dilution of valuable semantics and prediction accuracy. In this work, we propose a Cross-stage Feature Fusion Network (CFFNet) for salient object detection. CFFNet consists of a Cross-stage Semantic Fusion Module (CSF), a Feature Filtering and Fusion Module (FFM), and a progressive decoder to tackle the above problems. Specifically, to alleviate the semantics dilution problem, CSF concatenates different stage backbone features and extracts multi-scale global semantics using transformer blocks. Global semantics are then distributed to corresponding backbone stages for cross-stage semantic fusion. The FFM module implements efficient self-attention-based feature fusion. Different from regular self-attention which has quadratic computational complexity. Finally, a progressive decoder is adopted to refine saliency maps. Experimental results demonstrate that CFFNet outperforms state-of-the-arts on six SOD datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032400227X",
    "keywords": [
      "Aerospace engineering",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Engineering",
      "Feature (linguistics)",
      "Fusion",
      "Geology",
      "Linguistics",
      "Object (grammar)",
      "Object detection",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Salient",
      "Single stage",
      "Stage (stratigraphy)"
    ],
    "authors": [
      {
        "surname": "Xia",
        "given_name": "Xiaofeng"
      },
      {
        "surname": "Ma",
        "given_name": "Yingdong"
      }
    ]
  },
  {
    "title": "U-TPE: A universal approximate thumbnail-preserving encryption method for lossless recovery",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104318",
    "abstract": "Due to the limited local storage space, more and more people are accustomed to uploading images to the cloud, which has aroused concerns about privacy leaks. The traditional solution is to encrypt the images directly. However, in this way, users cannot easily browse the images stored in the cloud. Obviously, the traditional method has lost the visual usability of cloud images. To solve this problem, the Thumbnail-Preserving Encryption (TPE) method is proposed. Although approximate-TPE is more efficient than ideal TPE, it cannot restore the original image without damage and cannot encrypt some images with texture features. Inspired by the above, we propose a universal approximate thumbnail-preserving encryption method with lossless recovery. This method divides the image into equal-sized chunks, each of which is further divided into an embedding area and an adjustment area. The pixels of the embedding area are recorded by prediction. Then, the auxiliary information necessary to restore the image is encrypted and hidden in the embedding area of the encrypted image. Finally, the pixel values of the adjustment area in each block are adjusted so that the average value is close to the original block. Experimental results show that the proposed method can not only restore images losslessly but also process images with different texture features, achieving good generality. On the BOWS2 dataset, all images can be encrypted by adjusting the block size. In addition, it can resist third-party face recognition and comparison, achieving satisfactory results in balancing privacy and visual usability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002748",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Data compression",
      "Encryption",
      "Image (mathematics)",
      "Lossless compression",
      "Mathematics",
      "Thumbnail"
    ],
    "authors": [
      {
        "surname": "Fan",
        "given_name": "Haiju"
      },
      {
        "surname": "Shi",
        "given_name": "Shaowei"
      },
      {
        "surname": "Li",
        "given_name": "Ming"
      }
    ]
  },
  {
    "title": "UAT:Unsupervised object tracking based on graph attention information embedding",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104283",
    "abstract": "An excellent unsupervised tracker includes a powerful base tracker and an effective unsupervised tracking strategy. However, most base trackers lack internal feature representations for information embedding processes. Most unsupervised trackers are not robust enough in complex environments and lack an effective template update strategy. We propose an unsupervised object tracking based on graph attention information embedding (UAT) to solve the above problems. UAT combines graph attention mechanism with multi-scale features to construct a multi-scale graph attention module (MGA). MGA module dynamically and efficiently completes the information embedding between the template branch and the search area branch. The response map obtained by fusing the feature maps of the two branches is more informative about the location of the target. An attention based information reinforcement update module (RUM) improves the robustness of the tracker. RUM enhances the representation of the feature map in both the spatial dimension and the channel dimension. Template features are also updated indirectly through information transfer between the two branches. RUM suppresses background interference and improves network perception during tracking. Experiments on challenging benchmarks such as VOT2018, VOT2019, TrackingNet, OTB100, LaSOT and UAV123 demonstrate that the proposed UAT achieves state-of-the-art performance in unsupervised trackers.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002396",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Embedding",
      "Graph",
      "Graph embedding",
      "Object (grammar)",
      "Pedagogy",
      "Psychology",
      "Theoretical computer science",
      "Tracking (education)",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Wei",
        "given_name": "Lixin"
      },
      {
        "surname": "Zhu",
        "given_name": "Rongzhe"
      },
      {
        "surname": "Hu",
        "given_name": "Ziyu"
      },
      {
        "surname": "Xi",
        "given_name": "Zeyu"
      }
    ]
  },
  {
    "title": "M-YOLOv8s: An improved small target detection algorithm for UAV aerial photography",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104289",
    "abstract": "The object of UAV target detection usually means small target with complicated backgrounds. In this paper, an object detection model M-YOLOv8s based on UAV aerial photography scene is proposed. Firstly, to solve the problem that the YOLOv8s model cannot adapt to small target detection, a small target detection head (STDH) module is introduced to fuse the location and appearance feature information of the shallow layers of the backbone network. Secondly, Inner-Wise intersection over union (Inner-WIoU) is designed as the boundary box regression loss, and auxiliary boundary calculation is used to accelerate the regression speed of the model. Thirdly, the structure of multi-scale feature pyramid network (MS-FPN) can effectively combine the shallow network information with the deep network information and improve the performance of the detection model. Furthermore, a multi-scale cross-spatial attention (MCSA) module is proposed to expand the feature space through multi-scale branch, and then achieves the aggregation of target features through cross-spatial interaction, which improves the ability of the model to extract target features. Finally, the experimental results show that our model does not only possess fewer parameters, but also the values of mAP0.5 are 6.6% and 5.4% higher than the baseline model on the Visdrone2019 validation dataset and test dataset, respectively. Then, as a conclusion, the M-YOLOv8s model achieves better detection performance than some existing ones, indicating that our proposed method can be more suitable for detecting the small targets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002451",
    "keywords": [
      "Aerial photography",
      "Algorithm",
      "Art",
      "Artificial intelligence",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Geography",
      "Photography",
      "Remote sensing",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Duan",
        "given_name": "Siyao"
      },
      {
        "surname": "Wang",
        "given_name": "Ting"
      },
      {
        "surname": "Li",
        "given_name": "Tao"
      },
      {
        "surname": "Yang",
        "given_name": "Wankou"
      }
    ]
  },
  {
    "title": "OODNet: A deep blind JPEG image compression deblocking network using out-of-distribution detection",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104302",
    "abstract": "JPEG is one of the most popular image compression techniques, with numerous applications ranging from medical imaging to surveillance systems. Since JPEG introduces the blocking artifacts to the decompressed visual signals, enhancing the quality of these images is of paramount importance. Recently, various deep neural networks have been proposed for JPEG image deblocking that can effectively reduce the blocking artifacts produced by the JPEG compression technique. However, most of these schemes could only handle decompressed images generated by a set of specific JPEG quality factor (QF) values employed in the network training process. Therefore, when the images are obtained by the JPEG QF values other than those used in the network training process, the performance of deep learning-based JPEG image deblocking schemes drops significantly. To address this, in this paper, we propose a novel deep learning-based blind JPEG image deblocking method, which employs out-of-distribution detection to perform deblocking efficiently for various quality factor (QF) values. The proposed scheme can distinguish between the decompressed images using the QF values used in the training set and those using the QF values not used in the training set, and then, a suitable deblocking strategy for generating high-quality images is developed. The proposed scheme is shown to outperform the state-of-the-art JPEG image deblocking methods for various QF values.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032400258X",
    "keywords": [
      "Artificial intelligence",
      "Composite material",
      "Compression (physics)",
      "Computer science",
      "Computer vision",
      "Deblocking filter",
      "Distribution (mathematics)",
      "Image (mathematics)",
      "Image compression",
      "Image processing",
      "JPEG",
      "JPEG 2000",
      "Materials science",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Ahsan",
        "given_name": "Syed Safwan"
      },
      {
        "surname": "Esmaeilzehi",
        "given_name": "Alireza"
      },
      {
        "surname": "Ahmad",
        "given_name": "M. Omair"
      }
    ]
  },
  {
    "title": "Local and global mixture network for image inpainting",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104312",
    "abstract": "In general, CNN-based inpainting can recover local patterns effectively using convolutional filters, but it may not exploit global correlation fully. On the other hand, transformer-based inpainting can fill in large holes faithfully based on global correlation, rather than local one. In this paper, we propose a novel image inpainting algorithm, called local and global mixture (LGM), to take advantage of the strengths of both approaches and compensate for their weaknesses. The LGM network comprises the local inpainting network (LIN) and the global inpainting network (GIN) in parallel, which are based on convolutional layers and transformer blocks, respectively, and exchange their intermediate results with each other. Furthermore, we develop an error propagation model with a continuous error mask, updated in LIN but used in both LIN and GIN to provide more reliable inpainting results. Extensive experiments demonstrate that the proposed LGM algorithm provides excellent inpainting performance, which indicates the efficacy of the parallel combination of LIN and GIN and the effectiveness of the error propagation model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002682",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Inpainting",
      "Mathematics"
    ],
    "authors": [
      {
        "surname": "Woo",
        "given_name": "Seunggyun"
      },
      {
        "surname": "Ko",
        "given_name": "Keunsoo"
      },
      {
        "surname": "Kim",
        "given_name": "Chang-Su"
      }
    ]
  },
  {
    "title": "Image quilting heuristic compressed sensing video privacy protection coding for abnormal behavior detection in private scenes",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104307",
    "abstract": "For video intelligence applications in private scenes such as home environments, traditional image processing methods are usually based on clear raw data and are prone to privacy leakage. Therefore, our team proposed multilayer compressed sensing (MCS) encoding to reduce image quality for visual privacy protection (VPP). However, the way in which MCS coding is implemented leads to unavoidable information loss. On this basis, inspired by the image quilting (IQ) algorithm, an image quilting heuristic MCS (IQ-MCS) coding method is proposed in this paper to improve the problem of faster information loss in the MCS coding process, which means that a similar privacy protection effect is achieved at lower coding layers, thus obtaining better application performance. To evaluate the level of VPP, a VPP evaluation algorithm is proposed that is more in line with subjective assessment. Finally, a correlation model between the VPP level and the performance of smart applications is established to balance the relationships between them, taking the detection of abnormal human behavior in private scenes as an example. The model can also provide a reference for the evaluation of other privacy protection methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002633",
    "keywords": [
      "Artificial intelligence",
      "Coding (social sciences)",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Heuristic",
      "Image (mathematics)",
      "Internet privacy",
      "Mathematics",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Jixin"
      },
      {
        "surname": "Hu",
        "given_name": "Shabo"
      },
      {
        "surname": "Yang",
        "given_name": "Haigen"
      },
      {
        "surname": "Sun",
        "given_name": "Ning"
      }
    ]
  },
  {
    "title": "An illumination-guided dual-domain network for image exposure correction",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104313",
    "abstract": "Exposure problems, including underexposure and overexposure, can significantly degrade image quality. Poorly exposed images often suffer from coupled illumination degradation and detail degradation, aggravating the difficulty of recovery. These necessitate a spatial discriminating exposure correction, making achieving uniformly exposed and visually consistent images challenging. To address these issues, we propose an Illumination-guided Dual-domain Network (IDNet), which employs a Dual-Domain Module (DDM) to simultaneously recover illumination and details from the frequency and spatial domains, respectively. The DDM also integrates a structural re-parameterization technique to enhance the detail-aware capabilities with reduced computational cost. An Illumination Mask Predictor (IMP) is introduced to guide exposure correction by estimating the optimal illumination mask. The comparison with 26 methods on three benchmark datasets shows that IDNet achieves superior performance with fewer parameters and lower computational complexity. These results confirm the effectiveness and efficiency of our approach in enhancing image quality across various exposure scenarios.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002694",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Domain (mathematical analysis)",
      "Dual (grammatical number)",
      "Image (mathematics)",
      "Literature",
      "Mathematical analysis",
      "Mathematics"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Jie"
      },
      {
        "surname": "Zhang",
        "given_name": "Yuantong"
      },
      {
        "surname": "Chen",
        "given_name": "Zhenzhong"
      },
      {
        "surname": "Yang",
        "given_name": "Daiqin"
      }
    ]
  },
  {
    "title": "Blind image deblurring with a difference of the mixed anisotropic and mixed isotropic total variation regularization",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104285",
    "abstract": "This paper proposes a simple model for image deblurring with a new total variation regularization. Classically, the L 1-21 regularizer represents a difference of anisotropic (i.e. L 1) and isotropic (i.e. L 21) total variation, so we define a new regularization as L e-2e, which is the weighted difference of the mixed anisotropic (i.e. L 0 + L 1 = L e) and mixed isotropic (i.e. L 0 + L 21 = L 2e), and it is characterized by sparsity-promotingand robustness in image deblurring. Then, we merge the L 0-gradient into the model for edge-preserving and detail-removing. The union of the L e-2e regularization and L 0-gradient improves the performance of image deblurring and yields high-quality blur kernel estimates. Finally, we design a new solution format that alternately iterates the difference of convex algorithm, the split Bregman method, and the approach of half-quadratic splitting to optimize the proposed model. Experimental results on quantitative datasets and real-world images show that the proposed method can obtain results comparable to state-of-the-art works.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002414",
    "keywords": [
      "Anisotropy",
      "Applied mathematics",
      "Artificial intelligence",
      "Astrophysics",
      "Computer science",
      "Deblurring",
      "Image (mathematics)",
      "Image processing",
      "Image restoration",
      "Isotropy",
      "Mathematics",
      "Optics",
      "Physics",
      "Regularization (linguistics)",
      "Total variation denoising",
      "Variation (astronomy)"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Dandan"
      },
      {
        "surname": "Ge",
        "given_name": "Xianyu"
      },
      {
        "surname": "Liu",
        "given_name": "Jing"
      },
      {
        "surname": "Tan",
        "given_name": "Jieqing"
      },
      {
        "surname": "She",
        "given_name": "Xiangrong"
      }
    ]
  },
  {
    "title": "Background adaptive PosMarker based on online generation and detection for locating watermarked regions in photographs",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104269",
    "abstract": "Robust watermarking technology can embed invisible messages in screens to trace the source of unauthorized screen photographs. Locating the four vertices of the embedded region in the photograph is necessary, as existing watermarking methods require geometric correction of the embedded region before revealing the message. Existing localization methods suffer from a performance trade-off: either causing unaesthetic visual quality by embedding visible markers or achieving poor localization precision, leading to message extraction failure. To address this issue, we propose a background adaptive position marker, PosMarker, based on the gray level co-occurrence matrix and the noise visibility function. Besides, we propose an online generation scheme that employs a learnable generator to cooperate with the detector, allowing joint optimization between the two. This simultaneously improves both visual quality and detection precision. Extensive experiments demonstrate the superior localization precision of our PosMarker-based method compared to others.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002256",
    "keywords": [
      "Artificial intelligence",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Chengxin"
      },
      {
        "surname": "Ling",
        "given_name": "Hefei"
      },
      {
        "surname": "Guo",
        "given_name": "Jinlong"
      },
      {
        "surname": "He",
        "given_name": "Zhenghai"
      }
    ]
  },
  {
    "title": "Corrigendum to “Mask-guided discriminative feature network for occluded person re-identification” [J. Vis. Commun. Image Represent. 101 (2024) 104178]",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104301",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002578",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Computer vision",
      "Discriminative model",
      "Feature (linguistics)",
      "Identification (biology)",
      "Image (mathematics)",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Zhong",
        "given_name": "Fujin"
      },
      {
        "surname": "Wang",
        "given_name": "Yunhe"
      },
      {
        "surname": "Yu",
        "given_name": "Hong"
      },
      {
        "surname": "Hu",
        "given_name": "Jun"
      },
      {
        "surname": "Yang",
        "given_name": "Yan"
      }
    ]
  },
  {
    "title": "Corrigendum to “Mask-guided discriminative feature network for occluded person re-identification” [J. Vis. Commun. Image Represent. 101 (2024) 104178]",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104301",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002578",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Computer vision",
      "Discriminative model",
      "Feature (linguistics)",
      "Identification (biology)",
      "Image (mathematics)",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Zhong",
        "given_name": "Fujin"
      },
      {
        "surname": "Wang",
        "given_name": "Yunhe"
      },
      {
        "surname": "Yu",
        "given_name": "Hong"
      },
      {
        "surname": "Hu",
        "given_name": "Jun"
      },
      {
        "surname": "Yang",
        "given_name": "Yan"
      }
    ]
  },
  {
    "title": "Deep video steganography using temporal-attention-based frame selection and spatial sparse adversarial attack",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104311",
    "abstract": "With the development of deep learning-based steganalysis, video steganography is facing with great challenges. To address the insufficient security against steganalysis of existing deep video steganography, given that the video has both spatial and temporal dimensions, this paper proposes a deep video steganography method using temporal frame selection and spatial sparse adversarial attack. In temporal dimension, a stego frame selection module based on temporal attention is designed to calculate the weight of each frame and selects frames with high weights for message and sparse perturbation embedding. In spatial dimension, sparse adversarial perturbations are performed in the selected frames to improve the ability of resisting steganalysis. Moreover, to control the adversarial perturbations’ sparsity flexibly, an intra-frame dynamic sparsity threshold mechanism is designed by using percentile. Experimental results demonstrate that the proposed method effectively enhances the visual quality and security against steganalysis of video steganography and has controllable sparsity of adversarial perturbations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002670",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Embedding",
      "Frame (networking)",
      "Pattern recognition (psychology)",
      "Selection (genetic algorithm)",
      "Steganography"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Beijing"
      },
      {
        "surname": "Hong",
        "given_name": "Yuting"
      },
      {
        "surname": "Nie",
        "given_name": "Yuxin"
      }
    ]
  },
  {
    "title": "DeepFake detection method based on multi-scale interactive dual-stream network",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104263",
    "abstract": "DeepFake face forgery has a serious negative impact on both society and individuals. Therefore, research on DeepFake detection technologies is necessary. At present, DeepFake detection technology based on deep learning has achieved acceptable results on high-quality datasets; however, its detection performance on low-quality datasets and cross-datasets remains poor. To address this problem, this paper presents a multi-scale interactive dual-stream network (MSIDSnet). The network is divided into spatial- and frequency-domain streams and uses a multi-scale fusion module to capture both the facial features of images that have been manipulated in the spatial domain under different circumstances and the fine-grained high-frequency noise information of forged images. The network fully integrates the features of the spatial- and frequency-domain streams through an interactive dual-stream module and uses vision transformer (ViT) to further learn the global information of the forged facial features for classification. Experimental results confirm that the accuracy of this method reached 99.30 % on the high-quality dataset Celeb-DF-v2, and 95.51 % on the low-quality dataset FaceForensics++. Moreover, the results of the cross-dataset experiments were superior to those of the other comparison methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002190",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Cartography",
      "Computer science",
      "Dual (grammatical number)",
      "Geography",
      "Literature",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "Cheng",
        "given_name": "Ziyuan"
      },
      {
        "surname": "Wang",
        "given_name": "Yiyang"
      },
      {
        "surname": "Wan",
        "given_name": "Yongjing"
      },
      {
        "surname": "Jiang",
        "given_name": "Cuiling"
      }
    ]
  },
  {
    "title": "A fast intra CU partition algorithm in Versatile Video Coding for 360-degree video",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104305",
    "abstract": "Due to the high resolution of 360-degree video, it needs more time to compress using the latest Versatile Video Coding (VVC) compared to traditional video. Meanwhile, the introduction of quad-tree plus multi-type tree (QTMT) in VVC enhances compression performance, but the coding complexity also increases significantly. This paper proposes a fast intra coding unit (CU) partition algorithm for 360-degree video. First, we create a large-scale CU partition mode (CPM) dataset. Next, we design an adaptive CU partition decision network (ACPD-Net) based on adaptive QP, which can simultaneously predict the optimal CPM of four 32 × 32 sub-CUs from the global texture information of 64 × 64 CU. Then, based on the partitioning characteristics of the mapped 360-degree video, the partition structure of 32 × 32 CU is simplified from QTMT to QTBT. The experimental findings indicate that the proposed method can decrease the complexity of intra-frame coding by 61.10% and increase BDBR by 1.99% on average.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032400261X",
    "keywords": [
      "Acoustics",
      "Algorithm",
      "Coding (social sciences)",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Degree (music)",
      "Mathematics",
      "Multiview Video Coding",
      "Partition (number theory)",
      "Physics",
      "Statistics",
      "Video processing",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Yan"
      },
      {
        "surname": "Li",
        "given_name": "Tiansong"
      },
      {
        "surname": "Liu",
        "given_name": "Haokun"
      },
      {
        "surname": "Cui",
        "given_name": "Shaoguo"
      },
      {
        "surname": "Yu",
        "given_name": "Li"
      },
      {
        "surname": "Wu",
        "given_name": "Kejun"
      },
      {
        "surname": "Wang",
        "given_name": "Hongkui"
      }
    ]
  },
  {
    "title": "HPIDN: A Hierarchical prior-guided iterative denoising network with global–local fusion for enhancing low-dose CT images",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104297",
    "abstract": "Low-dose computed tomography (LDCT) is an emerging medical diagnostic tool that reduces radiation exposure but suffers from noise retention. Current CNN-based LDCT denoising algorithms struggle to capture comprehensive global representations, impacting diagnostic accuracy. To address this, we propose a novel Hierarchical Prior-guided Iterative Denoising Network (HPIDN) for LDCT images, consisting of two main modules: the Dynamic Feature Extraction and Fusion Module (DFEFM) and the Feature-domain Iterative Denoising Module (FIDM). DFEFM dynamically captures a comprehensive representation, encompassing detailed local features in intra-relationships and complex global features in inter-relationships. It effectively guides the multi-stage iterative denoising process. FIDM hierarchically fuses the prior with image features from DFEFM by using the dual-domain attention fusion sub-network (DAFSN), enhancing denoising robustness and adaptability. This yields higher-quality images with reduced noise artifacts. Extensive experiments on the Mayo and ELCAP Datasets demonstrate the superior performance of our method quantitatively and qualitatively, improving diagnostic accuracy of lung diseases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002530",
    "keywords": [
      "Artificial intelligence",
      "Computed tomography",
      "Computer science",
      "Computer vision",
      "Fusion",
      "Image denoising",
      "Linguistics",
      "Medicine",
      "Noise reduction",
      "Nuclear medicine",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Radiology"
    ],
    "authors": [
      {
        "surname": "Shi",
        "given_name": "Xiuya"
      },
      {
        "surname": "Yang",
        "given_name": "Yi"
      },
      {
        "surname": "Liu",
        "given_name": "Hao"
      },
      {
        "surname": "Ma",
        "given_name": "Litai"
      },
      {
        "surname": "Zhao",
        "given_name": "Zhibo"
      },
      {
        "surname": "Ren",
        "given_name": "Chao"
      }
    ]
  },
  {
    "title": "Zero-CSC: Low-light image enhancement with zero-reference color self-calibration",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104293",
    "abstract": "Zero-Reference Low-Light Image Enhancement (LLIE) techniques mainly focus on grey-scale inhomogeneities, and few methods consider how to explicitly recover a dark scene to achieve enhancements in color and overall illumination. In this paper, we introduce a novel Zero-Reference Color Self-Calibration framework for enhancing low-light images, termed as Zero-CSC. It effectively emphasizes channel-wise representations that contain fine-grained color information, achieving a natural result in a progressive manner. Furthermore, we propose a Light Up (LU) module with large-kernel convolutional blocks to improve overall illumination, which is implemented with a simple U-Net and further simplified with a light-weight structure. Experiments on representative datasets show that our model consistently achieves state-of-the-art performance in image signal-to-noise ratio, structural similarity, and color accuracy, setting new records on the challenging SICE dataset with improvements of 23.7% in image signal-to-noise ratio and 5.3% in structural similarity compared to the most advanced methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002499",
    "keywords": [
      "Artificial intelligence",
      "Calibration",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Image enhancement",
      "Linguistics",
      "Mathematics",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Statistics",
      "Zero (linguistics)",
      "Zero-point energy"
    ],
    "authors": [
      {
        "surname": "Jin",
        "given_name": "Haiyan"
      },
      {
        "surname": "Chen",
        "given_name": "Yujia"
      },
      {
        "surname": "Zuo",
        "given_name": "Fengyuan"
      },
      {
        "surname": "Su",
        "given_name": "Haonan"
      },
      {
        "surname": "Zhang",
        "given_name": "YuanLin"
      }
    ]
  },
  {
    "title": "GINet:Graph interactive network with semantic-guided spatial refinement for salient object detection in optical remote sensing images",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104257",
    "abstract": "There are many challenging scenarios in the task of salient object detection in optical remote sensing images (RSIs), such as various scales and irregular shapes of salient objects, cluttered backgrounds, etc. Therefore, it is difficult to directly apply saliency models targeting natural scene images to optical RSIs. Besides, existing models often do not give sufficient exploration for the potential relationship of different salient objects or different parts of the salient object. In this paper, we propose a graph interaction network (i.e. GINet) with semantic-guided spatial refinement to conduct salient object detection in optical RSIs. The key advantages of GINet lie in two points. Firstly, the graph interactive reasoning (GIR) module conducts information exchange of different-level features via the graph interaction operation, and enhances features along spatial and channel dimensions via the graph reasoning operation. Secondly, we designed the global content-aware refinement (GCR) module, which incorporates the foreground and background feature-based local information and the semantic feature-based global information simultaneously. Experiments results on two public optical RSIs datasets clearly show the effectiveness and superiority of the proposed GINet when compared with the state-of-the-art models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032400213X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Graph",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Salient",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Chenwei"
      },
      {
        "surname": "Zhou",
        "given_name": "Xiaofei"
      },
      {
        "surname": "Bao",
        "given_name": "Liuxin"
      },
      {
        "surname": "Wang",
        "given_name": "Hongkui"
      },
      {
        "surname": "Wang",
        "given_name": "Shuai"
      },
      {
        "surname": "Zhu",
        "given_name": "Zunjie"
      },
      {
        "surname": "Yan",
        "given_name": "Chenggang"
      },
      {
        "surname": "Zhang",
        "given_name": "Jiyong"
      }
    ]
  },
  {
    "title": "A robust watermarking approach for medical image authentication using dual image and quorum function",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104299",
    "abstract": "To safeguard the identity and copyright of a patient’s medical documents, watermarking strategies are widely used. This work provides a new dual image-based watermarking approach using the quorum function (QF) and AD interpolation technique. AD interpolation is used to create the dual images which helps to increase the embedding capacity. Moreover, the rules for using the QF are designed in such a way, that the original bits are least affected after embedding. As a result, it increases the visual quality of the stego images. A shared secret key has been employed to protect the information hidden in the medical image and to maintain the privacy and confidentiality. The experimental result using PSNR, SSIM, NCC, and EC shows that the suggested technique gives an average PSNR of 68.44 dB and SSIM is close to 0.99 after inserting 786432 watermark bits, which demonstrates the superiority of the scheme over other state-of-the-art schemes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002554",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Authentication (law)",
      "Biology",
      "Computer graphics (images)",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Contouring",
      "Digital watermarking",
      "Dual (grammatical number)",
      "Dual function",
      "Evolutionary biology",
      "Function (biology)",
      "Image (mathematics)",
      "Literature"
    ],
    "authors": [
      {
        "surname": "Dey",
        "given_name": "Ashis"
      },
      {
        "surname": "Chowdhuri",
        "given_name": "Partha"
      },
      {
        "surname": "Pal",
        "given_name": "Pabitra"
      },
      {
        "surname": "Nandi",
        "given_name": "Utpal"
      }
    ]
  },
  {
    "title": "A novel approach for long-term secure storage of domain independent videos",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104279",
    "abstract": "Long-term protection of multimedia contents is a complex task, especially when the video has critical elements. It demands sophisticated technology to ensure confidentiality. In this paper, we propose a blended approach which uses proactive visual cryptography scheme along with video summarization techniques to circumvent the aforementioned issues. Proactive visual cryptography is used to protect digital data by updating periodically or renewing the shares, which are stored in different servers. And, video summarization schemes are useful in various scenarios where memory is a major concern. We use a domain independent scheme for summarizing videos and is applicable to both edited and unedited videos. In our scheme, the visual continuity of the raw video is preserved even after summarization. The original video can be reconstructed through the shares using auxiliary data, which was generated during video summarization phase. The mathematical studies and experimental results demonstrate the applicability of our proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002359",
    "keywords": [
      "Artificial intelligence",
      "Automatic summarization",
      "Computer network",
      "Computer science",
      "Computer security",
      "Cryptography",
      "Domain (mathematical analysis)",
      "Economics",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Multimedia",
      "Physics",
      "Quantum mechanics",
      "Scheme (mathematics)",
      "Server",
      "Task (project management)",
      "Term (time)"
    ],
    "authors": [
      {
        "surname": "Varghese",
        "given_name": "Jina"
      },
      {
        "surname": "Praveen",
        "given_name": "K."
      },
      {
        "surname": "Dutta",
        "given_name": "Sabyasachi"
      },
      {
        "surname": "Adhikari",
        "given_name": "Avishek"
      }
    ]
  },
  {
    "title": "Distance distributions and runtime analysis of perceptual hashing algorithms",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104310",
    "abstract": "Perceptual image hashing refers to a class of algorithms that produce content-based image hashes. These systems use specialized perceptual hash algorithms like Phash, Microsoft’s PhotoDNA, or Facebook’s PDQ to generate a compact digest of an image file that can be roughly compared to a database of known illicit-content digests. Time taken by perceptual hashing algorithms to generate hash code has been computed. Then, we evaluated perceptual hashing algorithms on two million dataset of images. The produced nine variants of the original images were computed and then several distances were calculated. There have been several studies in the past, but in the existing literature size of the data is small and there are very few studies with hash code computation time and robustness tradeoff. This work shows that existing perceptual hashing algorithms are robust for most of the content-preserving operations and there is a tradeoff between computation time and robustness.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002669",
    "keywords": [
      "Algorithm",
      "Computer science",
      "Hash function",
      "Neuroscience",
      "Perception",
      "Programming language",
      "Psychology"
    ],
    "authors": [
      {
        "surname": "Sharma",
        "given_name": "Shivdutt"
      }
    ]
  },
  {
    "title": "Efficient license plate recognition in unconstrained scenarios",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104314",
    "abstract": "Automatic license plate recognition (ALPR) is a critical technology for intelligent transportation systems. Most existing ALPR methods are focused on specific application scenarios. Although there are a few methods that focus on unconstrained scenarios, they are very time-consuming. In this work, we propose an efficient ALPR (EALPR) framework, where we can handle distorted license plates (LP) caused by perspective problems with high efficiency. We design a light LPD structure based on efficient object detection methods and use anchor-free strategies for LPD to alleviate the problem of expensive costs. Benefitting from these optimizations and a united framework structure, the proposed EALPR has real-time efficiency. We evaluate our method on five datasets and the results show that our method achieves state-of-the-art accuracy: 98.15% on OpenALPR(EU), 95.61% on OpenALPR(BR), 99.51% on AOLP(RP), 88.81% on SSIG, 79.41% on CD-HARD. Additionally, our method achieves an impressive speed of 74.9 FPS (Frames Per Second), outperforming existing approaches and demonstrating its efficiency. Our source code can be accessed at https://github.com/wechao18/Efficient-alpr-unconstrained.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002700",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "License",
      "Operating system",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Wei",
        "given_name": "Chao"
      },
      {
        "surname": "Han",
        "given_name": "Fei"
      },
      {
        "surname": "Fan",
        "given_name": "Zizhu"
      },
      {
        "surname": "Shi",
        "given_name": "Linrui"
      },
      {
        "surname": "Peng",
        "given_name": "Cheng"
      }
    ]
  },
  {
    "title": "High-capacity reversible data hiding in encrypted images based on adaptive block coding selection",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104291",
    "abstract": "Recently, data hiding techniques have flourished and addressed various challenges. However, reversible data hiding for encrypted images (RDHEI) using vacating room after encryption (VRAE) framework often falls short in terms of data embedding performance. To address this issue, this paper proposes a novel and high-capacity data hiding method based on adaptive block coding selection. Specifically, iterative encryption and block permutation are applied during image encryption to maintain high pixel correlation within blocks. For each block in the encrypted image, both entropy coding and zero-valued high bit-planes compression coding are pre-applied, then the coding method that vacates the most space is selected, leveraging the strengths of both coding techniques to maximize the effective embeddable room of each encrypted block. This adaptive block coding selection mechanism is suitable for images with varying characteristics. Extensive experiments demonstrate that the proposed VRAE-based method outperforms some state-of-the-art RDHEI methods in data embedding capacity. The average embedding rates (ERs) of the proposed method for three publicly-used datasets including BOSSbase, BOWS-2 and UCID, are 4.041 bpp, 3.929 bpp, and 3.181 bpp, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002475",
    "keywords": [
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Coding (social sciences)",
      "Combinatorics",
      "Computer network",
      "Computer science",
      "Encryption",
      "Image (mathematics)",
      "Information hiding",
      "Mathematics",
      "Selection (genetic algorithm)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Xiao",
        "given_name": "Fengjun"
      },
      {
        "surname": "Wang",
        "given_name": "Ke"
      },
      {
        "surname": "Shen",
        "given_name": "Yanzhao"
      },
      {
        "surname": "Yao",
        "given_name": "Ye"
      }
    ]
  },
  {
    "title": "A visible-infrared person re-identification method based on meta-graph isomerization aggregation module",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104265",
    "abstract": "Due to different imaging principles of visible-infrared cameras, there are modal differences between similar person images. For visible-infrared person re-identification (VI-ReID), existing works focus on extracting and aligning cross-modal global descriptors in the shared feature space, while ignoring local variations and graph-structural correlations in cross-modal image pairs. In order to bridge the modal differences with graph-structure between key-parts, a Meta-Graph Isomerization Aggregation Module (MIAM) is proposed, which includes Meta-Graph Node Isomerization Module (MNI) and Dual Aggregation Module (DA). In order to completely describe discriminative in-graph local features, MNI establishes meta-secondary cyclic isomorphism relations of in-graph local features by using a multi-branch embedding generation mechanism. Then, the local features not only contain the limited information of the fixed regions, but also benefit from the neighboring regions. Meanwhile, the secondary node generation process considers similar and different nodes of pedestrian graph structure to reduce the interference of identity differences. In addition, the Dual Aggregation (DA) module combines spatial self-attention and channel self-attention to establish the interdependence of modal heterogeneous graph structure pairs and achieve inter-modal feature aggregation. To match heterogeneous graph structures, a Node Center Joint Mining Loss (NCJM Loss) is proposed to constrain the distance between the node centers of heterogeneous graphs. The experiments performed on the SYSU-MM01, RegDB and LLCM public datasets demonstrate that the proposed method performs excellently in VI-ReID.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002219",
    "keywords": [
      "Biology",
      "Botany",
      "Catalysis",
      "Chemistry",
      "Computer science",
      "Graph",
      "Identification (biology)",
      "Infrared",
      "Isomerization",
      "Optics",
      "Organic chemistry",
      "Photochemistry",
      "Physics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Chongrui",
        "given_name": "Shan"
      },
      {
        "surname": "Baohua",
        "given_name": "Zhang"
      },
      {
        "surname": "Yu",
        "given_name": "Gu"
      },
      {
        "surname": "Jianjun",
        "given_name": "Li"
      },
      {
        "surname": "Ming",
        "given_name": "Zhang"
      },
      {
        "surname": "Jingyu",
        "given_name": "Wang"
      }
    ]
  },
  {
    "title": "Low-complexity content-aware encoding optimization of batch video",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104295",
    "abstract": "With the proliferation of short-form video traffic, video service providers are faced with the challenge of balancing video quality and bandwidth consumption while processing massive volumes of videos. The most straightforward and simplistic approach is to set uniformly encoding parameters to all videos. However, such an approach fails to consider the differences in video content, and there may be alternative encoding parameter configuration approach that can improve global coding efficiency. Finding the optimal combination of encoding parameter configurations for a batch of videos requires an amount of redundant encoding, thereby introducing significant computational costs. To address this issue, we propose a low-complexity encoding parameter prediction model that can adaptively adjust the values of the encoding parameters based on video content. The experiments show that when only changing the value of the encoding parameter CRF, our prediction model can achieve 27.04%, 6.11%, and 15.92% bit saving in terms of PSNR, SSIM, and VMAF respectively, while maintaining an acceptable complexity compared to the approach using the same CRF value.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002517",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Content (measure theory)",
      "Encoding (memory)",
      "Mathematical analysis",
      "Mathematics"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Jiahao"
      },
      {
        "surname": "Deng",
        "given_name": "Dexin"
      },
      {
        "surname": "Li",
        "given_name": "Yilin"
      },
      {
        "surname": "Yu",
        "given_name": "Lu"
      },
      {
        "surname": "Li",
        "given_name": "Kai"
      },
      {
        "surname": "Chen",
        "given_name": "Ying"
      }
    ]
  },
  {
    "title": "SAFLFusionGait: Gait recognition network with separate attention and different granularity feature learnability fusion",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104284",
    "abstract": "Gait recognition, an essential branch of biometric identification, uses walking patterns to identify individuals. Despite its effectiveness, gait recognition faces challenges such as vulnerability to changes in appearance due to factors like angles and clothing conditions. Recent progress in deep learning has greatly enhanced gait recognition, especially through methods like deep convolutional neural networks, which demonstrate impressive performance. However, current approaches often overlook the connection between coarse-grained and fine-grained features, thereby restricting their overall effectiveness. To address this limitation, we propose a new framework for gait recognition framework that combines deep-supervised fine-grained separation with coarse-grained feature learnability. Our framework includes the LFF module, which consists of the SSeg module for fine-grained information extraction and a mechanism for fusing coarse-grained features. Furthermore, we introduce the F-LCM module to extract local disparity features more effectively with learnable weights. Evaluation on CASIA-B and OU-MVLP datasets shows superior performance compared to classical networks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002402",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Biometrics",
      "Botany",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Feature (linguistics)",
      "Feature extraction",
      "Gait",
      "Granularity",
      "Identification (biology)",
      "Learnability",
      "Linguistics",
      "Machine learning",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physiology"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Yuchen"
      },
      {
        "surname": "Chen",
        "given_name": "Zhenxue"
      },
      {
        "surname": "Liu",
        "given_name": "Chengyun"
      },
      {
        "surname": "Liang",
        "given_name": "Tian"
      },
      {
        "surname": "Lu",
        "given_name": "Dan"
      }
    ]
  },
  {
    "title": "Secret image sharing with distinct covers based on improved Cycling-XOR",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104282",
    "abstract": "Secret image sharing (SIS) is a technique used to distribute confidential data by dividing it into multiple image shadows. Most of the existing approaches or algorithms protect confidential data by encryption with secret keys. This paper proposes a novel SIS scheme without using any secret key. The secret images are first quantized and encrypted by self-encryption into noisy ones. Then, the encrypted images are mixed into secret shares by cross-encryption. The image shadows are generated by replacing the lower bit-planes of the cover images with the secret shares. In the extraction phase, the receiver can restore the quantized secret images by combinatorial operations of the extracted secret shares. Experimental results show that our method is able to deliver a large amount of data payload with a satisfactory cover image quality. Besides, the computational load is very low since the whole scheme is mostly based on cycling-XOR operations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002384",
    "keywords": [
      "Archaeology",
      "Computer science",
      "Computer vision",
      "Cycling",
      "Geography",
      "Image (mathematics)"
    ],
    "authors": [
      {
        "surname": "Lin",
        "given_name": "Jiang-Yi"
      },
      {
        "surname": "Horng",
        "given_name": "Ji-Hwei"
      },
      {
        "surname": "Chang",
        "given_name": "Chin-Chen"
      }
    ]
  },
  {
    "title": "A lightweight and continuous dimensional emotion analysis system of facial expression recognition under complex background",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104260",
    "abstract": "Facial expression recognition technology has a brilliant prospect in applying the Internet of Things systems. Concerning the limited hardware computing capability and high real-time processing requirements, this paper proposes a lightweight emotion analysis system based on edge computing, which could be deployed on edge devices. To further improve the accuracy of dimensional emotion analysis, we propose a modified network structure of MobileNetV3 to measure the intensity of dimensional emotion. The optimization scheme includes introducing the improved efficient channel attention mechanism and the feature pyramid network, adjusting the structure of the model, and optimizing the loss function. Furthermore, the system uses Intel’s OpenVINO toolkit to make the model more suitable for stable operation and provides an operable human–computer interaction interface. The experimental results show that the system has the advantages of few parameters, high recognition accuracy, and low latency. The optimized network size is reduced by 67% compared with the original, the root mean square error of potency and activation is 0.413 and 0.389, and the latency is up to 9ms in Myriad. This work meets the requirements of practical applications and has essential significance with the demand for continuous dimensional emotion analysis. The source code is available at https://github.com/SCNU-RISLAB/Lightweight-and-Continuous-Dimensional-Emotion-Analysis-System-of-Facial-Expression-Recognition/.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002165",
    "keywords": [
      "Artificial intelligence",
      "Cognitive psychology",
      "Communication",
      "Computer science",
      "Emotion recognition",
      "Expression (computer science)",
      "Facial expression",
      "Facial expression recognition",
      "Facial recognition system",
      "Pattern recognition (psychology)",
      "Programming language",
      "Psychology",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Tang",
        "given_name": "Xiaoyu"
      },
      {
        "surname": "Feng",
        "given_name": "Jiewen"
      },
      {
        "surname": "Huang",
        "given_name": "Jinbo"
      },
      {
        "surname": "Xiang",
        "given_name": "Qiuchi"
      },
      {
        "surname": "Xue",
        "given_name": "Bohuan"
      }
    ]
  },
  {
    "title": "High-capacity multi-MSB predictive reversible data hiding in encrypted domain for triangular mesh models",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104246",
    "abstract": "Reversible data hiding in encrypted domain (RDH-ED) is widely used in sensitive fields such as privacy protection and copyright authentication. However, the embedding capacity of existing methods is generally low due to the insufficient use of model topology. In order to improve the embedding capacity, this paper proposes a high-capacity multi-MSB predictive reversible data hiding in encrypted domain (MMPRDH-ED). Firstly, the 3D model is subdivided by triangular mesh subdivision (TMS) algorithm, and its vertices are divided into reference set and embedded set. Then, in order to make full use of the redundant space of embedded vertices, Multi-MSB prediction (MMP) and Multi-layer Embedding Strategy (MLES) are used to improve the capacity. Finally, stream encryption technology is used to encrypt the model and data to ensure data security. The experimental results show that compared with the existing methods, the embedding capacity of MMPRDH-ED is increased by 53 %, which has higher advantages.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002025",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Domain (mathematical analysis)",
      "Embedding",
      "Encryption",
      "Information hiding",
      "Mathematical analysis",
      "Mathematics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Guoyou"
      },
      {
        "surname": "Cheng",
        "given_name": "Xiaoxue"
      },
      {
        "surname": "Yang",
        "given_name": "Fan"
      },
      {
        "surname": "Wang",
        "given_name": "Anhong"
      },
      {
        "surname": "Zhang",
        "given_name": "Xuenan"
      },
      {
        "surname": "Liu",
        "given_name": "Li"
      }
    ]
  },
  {
    "title": "Detecting and tracking moving objects in defocus blur scenes",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104259",
    "abstract": "Object tracking stands as a cornerstone challenge within computer vision, with blurriness analysis representing a burgeoning field of interest. Among the various forms of blur encountered in natural scenes, defocus blur remains significantly underexplored. To bridge this gap, this article introduces the Defocus Blur Video Object Tracking (DBVOT) dataset, specifically crafted to facilitate research in visual object tracking under defocus blur conditions. We conduct a comprehensive performance analysis of 18 state-of-the-art object tracking methods on this unique dataset. Additionally, we propose a selective deblurring framework based on Deblurring Auxiliary Learning Net (DID-Anet), innovatively designed to tackle the complexities of defocus blur. This framework integrates a novel defocus blurriness metric for the smart deblurring of video frames, thereby enhancing the efficacy of tracking methods in defocus blur scenarios. Our extensive experimental evaluations underscore the significant advancements in tracking accuracy achieved by incorporating our proposed framework with leading tracking technologies.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002153",
    "keywords": [
      "Artificial intelligence",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Pedagogy",
      "Psychology",
      "Tracking (education)"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Fen"
      },
      {
        "surname": "Yang",
        "given_name": "Peng"
      },
      {
        "surname": "Dou",
        "given_name": "Jie"
      },
      {
        "surname": "Dou",
        "given_name": "Lei"
      }
    ]
  },
  {
    "title": "FISTA acceleration inspired network design for underwater image enhancement",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104224",
    "abstract": "Underwater image enhancement, especially in color restoration and detail reconstruction, remains a significant challenge. Current models focus on improving accuracy and learning efficiency through neural network design, often neglecting traditional optimization algorithms’ benefits. We propose FAIN-UIE, a novel approach for color and fine-texture recovery in underwater imagery. It leverages insights from the Fast Iterative Shrink-Threshold Algorithm (FISTA) to approximate image degradation, enhancing network fitting speed. FAIN-UIE integrates the residual degradation module (RDM) and momentum calculation module (MC) for gradient descent and momentum simulation, addressing feature fusion losses with the Feature Merge Block (FMB). By integrating multi-scale information and inter-stage pathways, our method effectively maps multi-stage image features, advancing color and fine-texture restoration. Experimental results validate its robust performance, positioning FAIN-UIE as a competitive solution for practical underwater imaging applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001809",
    "keywords": [
      "Acceleration",
      "Artificial intelligence",
      "Classical mechanics",
      "Computer science",
      "Computer vision",
      "Geology",
      "Image (mathematics)",
      "Mathematics",
      "Oceanography",
      "Physics",
      "Underwater"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Bing-Yuan"
      },
      {
        "surname": "Su",
        "given_name": "Jian-Nan"
      },
      {
        "surname": "Chen",
        "given_name": "Guang-Yong"
      },
      {
        "surname": "Gan",
        "given_name": "Min"
      }
    ]
  },
  {
    "title": "DDR: A network of image deraining systems for dark environments",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104244",
    "abstract": "In the domain of computer vision, addressing the degradation of image quality under adverse weather conditions remains a significant challenge. To tackle the challenges of image enhancement and deraining in dark settings, we have integrated image enhancement and deraining technologies to develop the DDR (Dark Environment Deraining Network) system. This specialized network is designed to enhance and clarify images in low-light conditions compromised by raindrops. DDR employs a strategic divide-and-conquer approach and an apt network selection to discern patterns of raindrops and background elements within images. It is capable of mitigating noise and blurring induced by raindrops in dark settings, thus enhancing the visual fidelity of images. Through testing on real-world imagery and the Rain LOL dataset, this innovative network offers a robust solution for deraining tasks in dark conditions, inspiring advancements in the performance of computer vision systems under challenging weather scenarios. The research of DDR provides technical and theoretical support for improving image quality in dark environment.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002001",
    "keywords": [
      "Artificial intelligence",
      "Business",
      "Computer science",
      "Image (mathematics)"
    ],
    "authors": [
      {
        "surname": "Ding",
        "given_name": "Zhongning"
      },
      {
        "surname": "Zhu",
        "given_name": "Yun"
      },
      {
        "surname": "Niu",
        "given_name": "Shaoshan"
      },
      {
        "surname": "Wang",
        "given_name": "Jianyu"
      },
      {
        "surname": "Su",
        "given_name": "Yan"
      }
    ]
  },
  {
    "title": "Advancements in low light image enhancement techniques and recent applications",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104223",
    "abstract": "Low-light image enhancement is an effective solution for improving image recognition by both humans and machines. Due to low illuminance, images captured in such conditions possess less color information compared to those taken in daylight, resulting in occluded images characterized by distortion, low contrast, low brightness, a narrow gray range, and noise. Low-light image enhancement techniques play a crucial role in enhancing the effectiveness of object detection. This paper reviews state-of-the-art low-light image enhancement techniques and their developments in recent years. Techniques such as gray transformation, histogram equalization, defogging, Retinex, image fusion, and wavelet transformation are examined, focusing on their working principles and assessing their ability to improve image quality. Further discussion addresses the contributions of deep learning and cognitive approaches, including attention mechanisms and adversarial methods, to image enhancement.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001792",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Image enhancement"
    ],
    "authors": [
      {
        "surname": "Anoop",
        "given_name": "P.P."
      },
      {
        "surname": "Deivanathan",
        "given_name": "R."
      }
    ]
  },
  {
    "title": "A lightweight target tracking algorithm based on online correction for meta-learning",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104228",
    "abstract": "The traditional Siamese network based object tracking algorithms suffer from high computational complexity, making them difficult to run on embedded devices. Moreover, when faced with long-term tracking tasks, their success rates significantly decline. To address these issues, we propose a lightweight long-term object tracking algorithm called Meta-Master-based Ghost Fast Tracking (MGTtracker),which based on meta-learning. This algorithm integrates the Ghost mechanism to create a lightweight backbone network called G-ResNet, which accurately extracts target features while operating quickly. We design a tiny adaptive weighted fusion feature pyramid network (TiFPN) to enhance feature information fusion and mitigate interference from similar objects. We introduce a lightweight region regression network, the Ghost Decouple Net (GDNet) for target position prediction. Finally, we propose a meta-learning-based online template correction mechanism called Meta-Master to overcome error accumulation in long-term tracking tasks and the difficulty of reacquiring targets after loss. We evaluate the algorithm on public datasets OTB100, VOT2020, VOT2018LT, and LaSOT and deploy it for performance testing on Jetson Xavier NX. Experimental results demonstrate the effectiveness and superiority of the algorithm. Compared to existing classic object tracking algorithms, our approach achieves a faster running speed of 25 FPS on NX, and real-time correction enhances the algorithm’s robustness. Although similar in accuracy and EAO metrics, our algorithm outperforms similar algorithms in speed and effectively addresses the issues of significant cumulative errors and easy target loss during tracking. Code is released at https://github.com/ygh96521/MGTtracker.git.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001846",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Multimedia",
      "Online learning",
      "Pedagogy",
      "Psychology",
      "Tracking (education)"
    ],
    "authors": [
      {
        "surname": "Qi",
        "given_name": "Yongsheng"
      },
      {
        "surname": "Yin",
        "given_name": "Guohua"
      },
      {
        "surname": "Li",
        "given_name": "Yongting"
      },
      {
        "surname": "Liu",
        "given_name": "Liqiang"
      },
      {
        "surname": "Jiang",
        "given_name": "Zhengting"
      }
    ]
  },
  {
    "title": "Local-aware global attention network for person re-identification based on body and hand images",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104207",
    "abstract": "Learning representative, robust and discriminative information from images is essential for effective person re-identification (Re-Id). In this paper, we propose a compound approach for end-to-end discriminative deep feature learning for person Re-Id based on both body and hand images. We carefully design the Local-Aware Global Attention Network (LAGA-Net), a multi-branch deep network architecture consisting of one branch for spatial attention, one branch for channel attention, one branch for global feature representations and another branch for local feature representations. The attention branches focus on the relevant features of the image while suppressing the irrelevant backgrounds. The global and local branches intends to capture global context and fine-grained information, respectively. A set of ablation study shows that each component contributes to the increased performance of the LAGA-Net. Extensive evaluations on four popular body-based person Re-Id benchmarks and two publicly available hand datasets demonstrate that our proposed method consistently outperforms existing state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001639",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Computer vision",
      "Identification (biology)"
    ],
    "authors": [
      {
        "surname": "Baisa",
        "given_name": "Nathanael L."
      }
    ]
  },
  {
    "title": "Semantic attention guided low-light image enhancement with multi-scale perception",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104242",
    "abstract": "Low-light environments often lead to complex degradation of captured images. However, most deep learning-based image enhancement methods for low-light conditions only learn a single-channel mapping relationship between the input image in low-light conditions and the desired image in normal light without considering semantic priors. This may cause the network to deviate from the original color of the region. In addition, deep network architectures are not suitable for low-light image recovery due to low pixel values. To address these issues, we propose a novel network called SAGNet. It consists of two branches:the main branch extracts global enhancement features at the level of the original image, and the other branch introduces semantic information through region-based feature learning and learns local enhancement features for semantic regions with multi-level perception to maintain color consistency. The extracted features are merged with the global enhancement features for semantic consistency and visualization. We also propose an unsupervised loss function to improve the network’s adaptability to general scenes and reduce the effect of sparse datasets. Extensive experiments and ablation studies show that SAGNet maintains color accuracy better in all cases and keeps natural luminance consistency across the semantic range.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001986",
    "keywords": [
      "Artificial intelligence",
      "Cartography",
      "Cognitive psychology",
      "Computer science",
      "Computer vision",
      "Geography",
      "Image (mathematics)",
      "Image enhancement",
      "Neuroscience",
      "Perception",
      "Psychology",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "Hou",
        "given_name": "Yongqi"
      },
      {
        "surname": "Yang",
        "given_name": "Bo"
      }
    ]
  },
  {
    "title": "EMCFN: Edge-based Multi-scale Cross Fusion Network for video frame interpolation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104226",
    "abstract": "Video frame interpolation (VFI) is used to synthesize one or more intermediate frames between two frames in a video sequence to improve the temporal resolution of the video. However, many methods still face challenges when dealing with complex scenes involving high-speed motion, occlusions, and other factors. To address these challenges, we propose an Edge-based Multi-scale Cross Fusion Network (EMCFN) for VFI. We integrate a feature enhancement module (FEM) based on edge information into the U-Net architecture, resulting in richer and more complete feature maps, while also enhancing the preservation of image structure and details. This contributes to generating more accurate and realistic interpolated frames. At the same time, we use a multi-scale cross fusion frame synthesis model (MCFM) composed of three GridNet branches to generate high-quality interpolation frames. We have conducted a series of experiments and the results show that our model exhibits satisfactory performance on different datasets compared with the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001822",
    "keywords": [
      "Artificial intelligence",
      "Cartography",
      "Computer graphics (images)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Enhanced Data Rates for GSM Evolution",
      "Frame (networking)",
      "Fusion",
      "Geography",
      "Interpolation (computer graphics)",
      "Linguistics",
      "Philosophy",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Shaowen"
      },
      {
        "surname": "Yang",
        "given_name": "Xiaohui"
      },
      {
        "surname": "Feng",
        "given_name": "Zhiquan"
      },
      {
        "surname": "Sun",
        "given_name": "Jiande"
      },
      {
        "surname": "Liu",
        "given_name": "Ju"
      }
    ]
  },
  {
    "title": "Semantic attention guided low-light image enhancement with multi-scale perception",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104242",
    "abstract": "Low-light environments often lead to complex degradation of captured images. However, most deep learning-based image enhancement methods for low-light conditions only learn a single-channel mapping relationship between the input image in low-light conditions and the desired image in normal light without considering semantic priors. This may cause the network to deviate from the original color of the region. In addition, deep network architectures are not suitable for low-light image recovery due to low pixel values. To address these issues, we propose a novel network called SAGNet. It consists of two branches:the main branch extracts global enhancement features at the level of the original image, and the other branch introduces semantic information through region-based feature learning and learns local enhancement features for semantic regions with multi-level perception to maintain color consistency. The extracted features are merged with the global enhancement features for semantic consistency and visualization. We also propose an unsupervised loss function to improve the network’s adaptability to general scenes and reduce the effect of sparse datasets. Extensive experiments and ablation studies show that SAGNet maintains color accuracy better in all cases and keeps natural luminance consistency across the semantic range.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001986",
    "keywords": [
      "Artificial intelligence",
      "Cartography",
      "Cognitive psychology",
      "Computer science",
      "Computer vision",
      "Geography",
      "Image (mathematics)",
      "Image enhancement",
      "Neuroscience",
      "Perception",
      "Psychology",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "Hou",
        "given_name": "Yongqi"
      },
      {
        "surname": "Yang",
        "given_name": "Bo"
      }
    ]
  },
  {
    "title": "Bilevel learning approach for nonlocal p -Laplacien image deblurring with variable weights parameter w ( x )",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104248",
    "abstract": "This manuscript introduces an innovative bilevel optimization approach designed to improve the deblurring process by incorporating a nonlocal p -Laplacien model with variable weights. The study includes a theoretical analysis to examine the model’s solution, and an effective algorithm is devised for computing the pristine image, incorporating the learning of parameters associated with weights and nonlocal regularization terms. By carefully selecting these parameters, the suggested nonlocal deblurring model demonstrates superior effectiveness and performance when compared to other existing models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002049",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Bilevel optimization",
      "Computer science",
      "Deblurring",
      "Image (mathematics)",
      "Image processing",
      "Image restoration",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Optimization problem",
      "Process (computing)",
      "Regularization (linguistics)",
      "Variable (mathematics)"
    ],
    "authors": [
      {
        "surname": "Malki",
        "given_name": "Imane El"
      },
      {
        "surname": "Jauberteau",
        "given_name": "François"
      },
      {
        "surname": "Laghrib",
        "given_name": "Amine"
      },
      {
        "surname": "Nachaoui",
        "given_name": "Mourad"
      }
    ]
  },
  {
    "title": "EM-Gait: Gait recognition using motion excitation and feature embedding self-attention",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104266",
    "abstract": "Gait recognition, which can realize long-distance and contactless identification, is an important biometric technology. Recent gait recognition methods focus on learning the pattern of human movement or appearance during walking, and construct the corresponding spatio-temporal representations. However, different individuals have their own laws of movement patterns, simple spatial–temporal features are difficult to describe changes in motion of human parts, especially when confounding variables such as clothing and carrying are included, thus distinguishability of features is reduced. To this end, we propose the Embedding and Motion (EM) block and Fine Feature Extractor (FFE) to capture the motion mode of walking and enhance the difference of local motion rules. The EM block consists of a Motion Excitation (ME) module to capture the changes of temporal motion and an Embedding Self-attention (ES) module to enhance the expression of motion rules. Specifically, without introducing additional parameters, ME module learns the difference information between frames and intervals to obtain the dynamic change representation of walking for frame sequences with uncertain length. By contrast, ES module divides the feature map hierarchically based on element values, blurring the difference of elements to highlight the motion track. Furthermore, we present the FFE, which independently learns the spatio-temporal representations of human body according to different horizontal parts of individuals. Benefiting from EM block and our proposed motion branch, our method innovatively combines motion change information, significantly improving the performance of the model under cross appearance conditions. On the popular dataset CASIA-B, our proposed EM-Gait is better than the existing single-modal gait recognition methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002220",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Embedding",
      "Feature (linguistics)",
      "Gait",
      "Linguistics",
      "Medicine",
      "Motion (physics)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physical medicine and rehabilitation"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Zhengyou"
      },
      {
        "surname": "Du",
        "given_name": "Chengyu"
      },
      {
        "surname": "Zhang",
        "given_name": "Yunpeng"
      },
      {
        "surname": "Bai",
        "given_name": "Jing"
      },
      {
        "surname": "Zhuang",
        "given_name": "Shanna"
      }
    ]
  },
  {
    "title": "IML-SSOD: Interconnected and multi-layer threshold learning for semi-supervised detection",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104220",
    "abstract": "While semi-supervised anchored detector of the R-CNN series has achieved remarkable success, semi-supervised anchor-free detector lacks the ability to generate high-quality flexible pseudo labels, resulting in serious inconsistencies in SSOD. In order to make the network learn more reliable and consistent label data to solve the problem of information bias, we propose an interconnected and multi-layer threshold learning for semi-supervised object detection (IML-SSOD). The Joint Guided Estimation (JGE) module uses the Core Zone refinement module to improve the position accuracy score of low semantic information, and combines the classification and the centerness score as evaluation criteria to predict stable labels. The multi-layer threshold filtering method selects more potential label samples for the student network ensuring the information used in training. Extensive experiments on MS COCO and PASCAL VOC datasets demonstrated the effectiveness of IML-SSOD. Compared with existing methods, our method on VOC achieved 81.9% AP 50 and 57.89% AP 50 : 95 , which is highly competitive.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001767",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Detector",
      "Labeled data",
      "Layer (electronics)",
      "Machine learning",
      "Organic chemistry",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Programming language",
      "Supervised learning",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Ge",
        "given_name": "Bin"
      },
      {
        "surname": "Li",
        "given_name": "Yuyang"
      },
      {
        "surname": "Liu",
        "given_name": "Huanhuan"
      },
      {
        "surname": "Xia",
        "given_name": "Chenxing"
      },
      {
        "surname": "Geng",
        "given_name": "Shuaishuai"
      }
    ]
  },
  {
    "title": "Detection of HEVC double compression based on boundary effect of TU and non-zero DCT coefficient distribution",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104255",
    "abstract": "Video content tampering is a growing concern. While tampering, perpetrator needs to decompress and re-compress a video. Thus, detecting whether a video has undergone double compression is an important issue in video forensics. In this paper, a novel method is proposed for the detection of High Efficiency Video Coding (HEVC) double compression. Firstly, theoretical and statistical analysis are stated on the quality degradation during double compression, and the impact of Quantization Parameter (QP) on Transforming Unit (TU) and Discrete Cosine Transform (DCT) coefficients distribution. Then 3 sub-features based on Boundary Effect of TU and Non-Zero DCT Coefficients are calculated. Further, 3 sub-features are combined into a 26-dimension feature as the proposed detection feature, which is finally fed to the Multilayer Perceptron (MLP) classifier. Experiments are conducted on video sets with different settings, and the result proves that our method achieves better performance under several situations compared with two existing methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002116",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Classifier (UML)",
      "Coding (social sciences)",
      "Computer science",
      "Computer vision",
      "Data compression",
      "Discrete cosine transform",
      "Economics",
      "Image (mathematics)",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Quantization (signal processing)",
      "Statistics",
      "Video quality"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Ruochen"
      },
      {
        "surname": "Sun",
        "given_name": "Tanfeng"
      },
      {
        "surname": "Xu",
        "given_name": "Qiang"
      }
    ]
  },
  {
    "title": "Performance evaluation of efficient segmentation and classification based iris recognition using sheaf attention network",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104262",
    "abstract": "Iris recognition, a precise biometric identification technique, relies on the distinct epigenetic patterns within the iris. Existing methods often face challenges related to segmentation accuracy and classification efficiency. To improve the accuracy and efficiency of iris recognition systems, this research proposes an innovative approach for iris recognition, focusing on efficient segmentation and classification using Convolutional neural networks with Sheaf Attention Networks (CSAN). Main objective is to develop an integrated framework that optimizes iris segmentation and classification. Subsequently, dense extreme inception multipath guided up sampling network is employed for accurate segmentation. Finally, classifiers including convolutional neural network with sheaf attention networks are evaluated. The findings indicate that the proposed method achieves superior iris recognition accuracy and robustness, making it suitable for applications such as secure authentication and access control. By comparing with existing approaches CSAN obtains 99.98%, 99.35%, 99.45% and 99.65% accuracy for the four different proposed datasets respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002189",
    "keywords": [
      "Artificial intelligence",
      "Biometrics",
      "Computer science",
      "IRIS (biosensor)",
      "Iris recognition",
      "Pattern recognition (psychology)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Salve",
        "given_name": "Sushilkumar S."
      },
      {
        "surname": "Narote",
        "given_name": "Sandipann P"
      }
    ]
  },
  {
    "title": "Towards robust image watermarking via random distortion assignment based meta-learning",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104238",
    "abstract": "Recently, deep learning-based image watermarking methods have been proposed for copyright protection, which are robust to common post-processing operations. However, they suffer from distinct performance drops to open-set distortions, where distortions applied on testing samples are unseen in the training stage. To address this issue, we propose a random distortion assignment-based meta-learning framework for robust image watermarking, where meta-train and meta-test tasks are constructed to simulate open-set distortion scenarios. The embedding and extraction network of watermark information is constructed based on the invertible neural network and equipped with a multi-stage distortion layer, which can conduct random combinations of basic post-processing operators. Besides, to obtain a better balance between robustness and visual imperceptibility, a hybrid loss function is designed by considering global and local similarities based on wavelet decomposition to capture multi-scale texture information. Extensive experiments are conducted by considering various open-set distortions to verify the superiority of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001949",
    "keywords": [
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Digital watermarking",
      "Distortion (music)",
      "Image (mathematics)",
      "Mathematics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Shenglie"
      },
      {
        "surname": "He",
        "given_name": "Peisong"
      },
      {
        "surname": "Liu",
        "given_name": "Jiayong"
      },
      {
        "surname": "Luo",
        "given_name": "Jie"
      }
    ]
  },
  {
    "title": "Image cropping based on order learning",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104253",
    "abstract": "A novel approach to image cropping, called crop region comparator (CRC), is proposed in this paper, which learns ordering relationships between aesthetic qualities of different crop regions. CRC employs the single-region refinement (SR) module and the inter-region correlation (IC) module. First, we design the SR module to identify essential information in an original image and consider the composition of each crop candidate. Thus, the SR module helps CRC to adaptively find the best crop region according to the essential information. Second, we develop the IC module, which aggregates the information across two crop candidates to analyze their differences effectively and estimate their ordering relationship reliably. Then, we decide the crop region based on the relative aesthetic scores of all crop candidates, computed by comparing them in a pairwise manner. Extensive experimental results demonstrate that the proposed CRC algorithm outperforms existing image cropping techniques on various datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002098",
    "keywords": [
      "Agricultural engineering",
      "Agriculture",
      "Archaeology",
      "Artificial intelligence",
      "Business",
      "Computer science",
      "Computer vision",
      "Cropping",
      "Engineering",
      "Finance",
      "Geography",
      "Image (mathematics)",
      "Mathematics",
      "Order (exchange)"
    ],
    "authors": [
      {
        "surname": "Shin",
        "given_name": "Nyeong-Ho"
      },
      {
        "surname": "Lee",
        "given_name": "Seon-Ho"
      },
      {
        "surname": "Ko",
        "given_name": "Jinwon"
      },
      {
        "surname": "Kim",
        "given_name": "Chang-Su"
      }
    ]
  },
  {
    "title": "Local-aware global attention network for person re-identification based on body and hand images",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104207",
    "abstract": "Learning representative, robust and discriminative information from images is essential for effective person re-identification (Re-Id). In this paper, we propose a compound approach for end-to-end discriminative deep feature learning for person Re-Id based on both body and hand images. We carefully design the Local-Aware Global Attention Network (LAGA-Net), a multi-branch deep network architecture consisting of one branch for spatial attention, one branch for channel attention, one branch for global feature representations and another branch for local feature representations. The attention branches focus on the relevant features of the image while suppressing the irrelevant backgrounds. The global and local branches intends to capture global context and fine-grained information, respectively. A set of ablation study shows that each component contributes to the increased performance of the LAGA-Net. Extensive evaluations on four popular body-based person Re-Id benchmarks and two publicly available hand datasets demonstrate that our proposed method consistently outperforms existing state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001639",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Computer vision",
      "Identification (biology)"
    ],
    "authors": [
      {
        "surname": "Baisa",
        "given_name": "Nathanael L."
      }
    ]
  },
  {
    "title": "Information entropy induced graph convolutional network for semantic segmentation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104217",
    "abstract": "Semantic segmentation is a challenging task in computer vision. Contextual information has been proven to be helpful for this task. But few previous works have ever tied to explicitly utilizing contextual information from easy regions in images to facilitate the recognition of hard regions for semantic segmentation. In this paper, we propose a method to identify easy regions and hard regions in images for the semantic segmentation task and propagate contextual information from easy regions to hard regions to facilitate the recognition of hard regions. Specifically, to identify easy and hard regions in images for the semantic segmentation task, we introduce information entropy to measure the easiness of the prediction of the semantic labels of the image pixels. Then, we establish a graph by treating easy regions and hard regions as nodes in the graph. After that, we use graph convolution to propagate contextual information from easy regions to hard regions to reduce the uncertainty of the hard regions for semantic segmentation prediction. The proposed method is named Information Entropy Induced Graph Convolution Network (IEGNet). It can be used in popular semantic segmentation models in a plug-and-play way. Our methmod yields compelling results over several datasets (eg., PASCAL VOC 2012, Cityscapes, ADE20K). Obtained experiment results demonstrated that incorporating IEGNet into semantic segmentation models can improve the performance of semantic segmentation effectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001731",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Graph",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Segmentation",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Bai",
        "given_name": "Shuang"
      },
      {
        "surname": "Liang",
        "given_name": "Chen"
      },
      {
        "surname": "Wang",
        "given_name": "Zhen"
      },
      {
        "surname": "Pan",
        "given_name": "Wenchao"
      }
    ]
  },
  {
    "title": "Audio–video collaborative JND estimation model for multimedia applications",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104254",
    "abstract": "With the rapid development of the Internet and multimedia technologies, multimedia applications integrating audio and video are becoming increasingly prevalent in both everyday life and professional environments. A critical challenge is to significantly enhance compression efficiency and bandwidth utilization while maintaining high-quality user experiences. To address this challenge, the Just Noticeable Distortion (JND) estimation model, which leverages the perceptual characteristics of the Human Visual System (HVS), is widely used in image and video coding for improved data compression. However, human visual perception is an integrative process that involves both visual and auditory stimuli. Therefore, this paper investigates the influence of audio signals on visual perception and presents a collaborative audio–video JND estimation model tailored for multimedia applications. Specifically, we characterize audio loudness, duration, and energy as temporal perceptual features, while assigning the audio saliency superimposed on the image plane as the spatial perceptual feature. An audio JND adjustment factor is then designed using a segmentation function. Finally, the proposed model combines the video-based JND model with the audio JND adjustment factor to form the audio–video collaborative JND estimation model. Compared with existing JND models, the model presented in this paper achieves the best subjective quality, with an average PSNR value of 26.97 dB. The experimental results confirm that audio significantly impacts human visual perception. The proposed audio–video collaborative JND model effectively enhances the accuracy of JND estimation for multimedia data, thereby improving compression efficiency and maintaining high-quality user experiences.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002104",
    "keywords": [
      "Computer science",
      "Economics",
      "Estimation",
      "Management",
      "Multimedia"
    ],
    "authors": [
      {
        "surname": "Sheng",
        "given_name": "Ning"
      },
      {
        "surname": "Yin",
        "given_name": "Haibing"
      },
      {
        "surname": "Wang",
        "given_name": "Hongkui"
      },
      {
        "surname": "Mo",
        "given_name": "Longbin"
      },
      {
        "surname": "Liu",
        "given_name": "Yichen"
      },
      {
        "surname": "Huang",
        "given_name": "Xiaofeng"
      },
      {
        "surname": "Lin",
        "given_name": "Jucai"
      },
      {
        "surname": "Tang",
        "given_name": "Xianghong"
      }
    ]
  },
  {
    "title": "Tensor-enhanced shock energy-driven active contours: A novel approach for knowledge-based image segmentation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104218",
    "abstract": "In image segmentation, active contour models (ACMs) have demonstrated their proficiency in accurately delineating complex backgrounds with irregular intensity distributions, even in the presence of noise or variable intensities. However, recent ACMs, including variants of the widely used region-scalable fitting (RSF) model, encounter a specific challenge when initializing contours in regions marked by significant intensity irregularities. This challenge arises from their reliance on minimizing a quadratic loss function within the energy framework, which in turn depends on second-order edge characteristics. To overcome this limitation and enhance segmentation accuracy, we introduce a novel ACM, empowered by a tensor-based structured shock energy function. This energy function amplifies image boundaries by aligning with the sharpest gradients, thereby inducing a second-order edge effect along object boundaries. Integrating this derived energy seamlessly into a region-based level-set formulation bolsters the model’s edge detection capabilities. Through extensive experimentation, we validate the effectiveness of this approach, demonstrating significantly improved tolerance for initialization in the presence of noise and intensity variations. Notably, our solution outperforms prominent region-based segmentation models. This research marks a significant advancement in image segmentation, particularly for challenging scenarios, ultimately pushing the boundaries of current state-of-the-art techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001743",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Energy (signal processing)",
      "Image (mathematics)",
      "Image segmentation",
      "Mathematics",
      "Medicine",
      "Pure mathematics",
      "Radiology",
      "Segmentation",
      "Shock (circulatory)",
      "Statistics",
      "Structure tensor",
      "Tensor (intrinsic definition)"
    ],
    "authors": [
      {
        "surname": "Rahman",
        "given_name": "Chowdhury M. Abid"
      },
      {
        "surname": "Nyeem",
        "given_name": "Hussain"
      }
    ]
  },
  {
    "title": "Facial feature point detection under large range of face deformations",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104264",
    "abstract": "Facial Feature Point Detection (FFPD) plays a significant role in several face analysis tasks such as feature extraction and classification. This paper presents a Fully Automatic FFPD system using the application of Random Forest Regression Voting in a Constrained Local Model (RFRV-CLM) framework. A global detector is used to find the approximate positions of the facial region and eye centers. A sequence of local RFRV-CLMs are used to locate a detailed set of points around the facial features. Both global and local models use Random Forest Regression to vote for optimal positions. The system is evaluated in the task of facial expression localization using five different facial expression databases of different characteristics including age, intensity, 6-basic expressions, 22 compound expressions, static and dynamic images, and deliberate and spontaneous expressions. Quantitative results of the evaluation of automatic point localization against manual points (ground truth) demonstrated that the results of the proposed approach are encouraging and outperform the results of alternative techniques tested on the same databases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002207",
    "keywords": [
      "Aerospace engineering",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Engineering",
      "Face (sociological concept)",
      "Feature (linguistics)",
      "Geometry",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Point (geometry)",
      "Range (aeronautics)"
    ],
    "authors": [
      {
        "surname": "Algaraawi",
        "given_name": "Nora"
      },
      {
        "surname": "Morris",
        "given_name": "Tim"
      },
      {
        "surname": "Cootes",
        "given_name": "Timothy F."
      }
    ]
  },
  {
    "title": "Reversible data hiding for color images based on prediction-error value ordering and adaptive embedding",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104239",
    "abstract": "Prediction-error value ordering (PEVO) is an efficient implementation of reversible data hiding (RDH), which is perfect for color images to exploit the inter-channel and intra-channel correlations synchronously. However, the existing PEVO method has a slight shortage in the mapping selection stage, the candidate mappings are selected under conditions inconsistent with actual embedding in advance, and this is not the optimal solution. Therefore, in this paper, a novel RDH method for color images based on PEVO and adaptive embedding is proposed to implement adaptive two-dimensional (2D) modification for PEVO. Firstly, an improved particle swarm optimization (IPSO) algorithm based on PEVO is designed to alleviate the high temporal complexity caused by the determination of parameters and implement adaptive 2D modification for PEVO. Next, to further optimize the mapping used in embedding, an improved adaptive 2D mapping generation strategy is proposed by introducing the position information of points. In addition, a dynamic payload partition strategy is proposed to improve the embedding performance. Finally, the experimental results show that the PSNR of the image Lena is as high as 62.94 dB and the average PSNR of the proposed method is 1.46 dB higher than that of the state-of-the-art methods for embedding capacity of 20,000 bits.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001950",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Embedding",
      "Information hiding",
      "Machine learning",
      "Mathematics",
      "Mean squared prediction error",
      "Pattern recognition (psychology)",
      "Value (mathematics)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Hui"
      },
      {
        "surname": "Wang",
        "given_name": "Detong"
      },
      {
        "surname": "Chu",
        "given_name": "Zhihui"
      },
      {
        "surname": "Rao",
        "given_name": "Zheheng"
      },
      {
        "surname": "Yao",
        "given_name": "Ye"
      }
    ]
  },
  {
    "title": "Corrigendum to “Heterogeneity constrained color ellipsoid prior image dehazing algorithm” [J. Vis. Commun. Image Represent. 101 (2024) 104177]",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104235",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001913",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Ellipsoid",
      "Geodesy",
      "Geology",
      "Image (mathematics)",
      "Mathematics"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yuxi"
      },
      {
        "surname": "Hu",
        "given_name": "Jing"
      },
      {
        "surname": "Zhang",
        "given_name": "Rongguo"
      },
      {
        "surname": "Wang",
        "given_name": "Lifang"
      },
      {
        "surname": "Zhang",
        "given_name": "Rui"
      },
      {
        "surname": "Liu",
        "given_name": "Xiaojun"
      }
    ]
  },
  {
    "title": "Underwater image enhancement via multicolor space-guided curve estimation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104240",
    "abstract": "High-quality images are of prime importance for vision tasks in underwater environment, ocean exploitation and marine eco-environment protection. However, due to attenuation and scattering related to wavelength and distance, underwater images often suffer from severe distortion, like color cast and loss of detail, challenging the capture of high-quality images. Existing methods mostly focus on pixel-wise mapping from input to output images, which disrupts the latent relationships between neighboring pixels and suffers limited flexibility of linear mapping. Additionally, most methods operate solely within RGB color space, which is insensitive to some image properties such as luminance and saturation, may be inconsistent with human visual perception. In this paper, we combine global and local enhancement, formulating underwater image global enhancement as a task of image-specific piecewise curve estimation based on a deep network, introducing three color spaces (RGB, HSV, CIELab). Our network learns piecewise nonlinear curves specific to different channels of multiple color spaces, thereby introducing highly flexible nonlinear mapping and achieving targeted adjustment. Besides, we employ channel-wise attention mechanism to allocate weights for different adjustment results from multiple color spaces, combining their advantages to further enhance image properties such as luminance, saturation, and color, aiming to improve image quality and align it more closely with human visual perception. To assess the performance of the network, extensive experiments are conducted on synthetic and real-world underwater image datasets, with comparisons made against state-of-the-art methods. Both quantitative and qualitative results indicate our network’s remarkable performance in both visual quality and quantitative metrics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001962",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Geology",
      "Image (mathematics)",
      "Image enhancement",
      "Mathematics",
      "Oceanography",
      "Operating system",
      "Space (punctuation)",
      "Underwater"
    ],
    "authors": [
      {
        "surname": "Hao",
        "given_name": "Shuyu"
      },
      {
        "surname": "Guo",
        "given_name": "Jichang"
      },
      {
        "surname": "An",
        "given_name": "Guanhua"
      },
      {
        "surname": "Wang",
        "given_name": "Yudong"
      }
    ]
  },
  {
    "title": "Corrigendum to “Dual-stream mutually adaptive quality assessment for authentic distortion image” [J. Vis. Commun. Image Represent. 102 (2024) 104216]",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104236",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001925",
    "keywords": [
      "Amplifier",
      "Art",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Computer science",
      "Computer vision",
      "Distortion (music)",
      "Dual (grammatical number)",
      "Epistemology",
      "Image (mathematics)",
      "Image quality",
      "Literature",
      "Mathematics",
      "Philosophy",
      "Psychology",
      "Quality (philosophy)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Huizhen",
        "given_name": "Jia"
      },
      {
        "surname": "Huaibo",
        "given_name": "Zhou"
      },
      {
        "surname": "Hongzheng",
        "given_name": "Qin"
      },
      {
        "surname": "Tonghan",
        "given_name": "Wang"
      }
    ]
  },
  {
    "title": "A novel hybrid network model for image steganalysis",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104251",
    "abstract": "Steganalysis attempts to discover hidden signals in suspected carriers or at the least detect which media contain hidden signals. Conventional approaches to steganalysis depend on artificially designed image features. However, these methods are time-consuming and labor-intensive. Additionally, the statistical methods may not produce optimal outcomes. Deep learning-based steganalysis algorithms which use convolutional neural network (CNN) structures, such as ZhuNet, obviate the need for artificially design features while optimizing the feature extraction and classification processes via training and learning. This approach greatly boosts the applicability and effectiveness of steganalysis. Nevertheless, it is important to note that CNN-based steganalysis algorithms do have some limitations. To begin with, the feature extraction of stego images, which relies on deep neural networks, lacks consideration for the interdependence of local features when constructing the overall feature map. Furthermore, CNN-based steganalysis models use all features indiscriminately to classify stego images, which can potentially reduce the models’ accuracy. Based on ZhuNet, we provide a novel hybrid network model known as ZhuNet-ATT-BiLSTM in order to tackle the aforementioned concerns. This model introduces a Bidirectional Long Short-Term Memory (BiLSTM) structure to mutually learn about the relationships between image features to ensure comprehensive utilization of stego image features. In addition, an attention mechanism is integrated for steganalysis to dynamically allocate weights to feature data, amplifying the signal for the vital features while effectively attenuating the less important and irrelevant features. Lastly, the enhanced model is verified with two open datasets: Bossbase 1.01 and COCO. According to experimental findings, the proposed hybrid network model improves the image steganalysis accuracy by comparing with earlier methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002074",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convolutional neural network",
      "Data mining",
      "Deep learning",
      "Feature (linguistics)",
      "Feature extraction",
      "Image (mathematics)",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Steganalysis",
      "Steganography"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Shichen"
      },
      {
        "surname": "Jia",
        "given_name": "Xingxing"
      },
      {
        "surname": "Zou",
        "given_name": "Fuhua"
      },
      {
        "surname": "Zhang",
        "given_name": "Yangshijie"
      },
      {
        "surname": "Yuan",
        "given_name": "Chengsheng"
      }
    ]
  },
  {
    "title": "Modification in spatial, extraction from transform: Keyless side-information steganography for JPEG",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104249",
    "abstract": "The state of the art steganography approaches strictly assume that the receiver has access to a steganographic key. This limitation was mitigated for images in spacial domain, but the approach does not apply to JPEG images. In this paper, we introduce a keyless steganography scheme for JPEG images. Despite the spatial domain counterpart, our approach for the JPEG domain effectively preserves higher-order statistical models that are used in steganalysis. We show that our approach does not degrade image quality either. The proposed approach is a Side-Information (SI) steganography in the sense that its input is a never-compressed image. Another characteristic of the proposed approach is the separation of the embedding modification and data extraction domains, which can initiate further studies of similar approaches in the future.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002050",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Data mining",
      "Domain (mathematical analysis)",
      "Embedding",
      "Image (mathematics)",
      "Information hiding",
      "JPEG",
      "Key (lock)",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Steganalysis",
      "Steganography",
      "Steganography tools"
    ],
    "authors": [
      {
        "surname": "Darvish Morshedi Hosseini",
        "given_name": "Morteza"
      },
      {
        "surname": "Mahdavi",
        "given_name": "Mojtaba"
      },
      {
        "surname": "Borujeni",
        "given_name": "Shahram Etemadi"
      }
    ]
  },
  {
    "title": "Copy-move forgery detection using Regional Density Center clustering",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104221",
    "abstract": "Copy-move forgery detection is a common image tampering detection technology. In this paper, a novel copy-move forgery detection scheme is proposed. The proposed scheme is based on Regional Density Center (RDC) clustering and Refined Length Homogeneity Filtering (RLHF) policy. First, to obtain an adequate number of keypoints in smooth or small areas of the image, the proposed scheme employs scale normalization and adjustment of the contrast threshold of the input image. Subsequently, to speed up the feature matching process, a matching algorithm based on gray value grouping is used to match the keypoints. RLHF policy is applied to filter the mismatched pairs. To guarantee a good estimation of the affine transformation, the RDC clustering algorithm is proposed to group the matched pairs. Finally, the correlation coefficients are computed to precisely locate the tampered regions. The proposed copy-move forgery detection scheme based on RDC and RLHF can effectively identify duplicated regions of digital images. It demonstrates the effectiveness and robustness of the proposed scheme over many state-of-the-art schemes on public datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001779",
    "keywords": [
      "Affine transformation",
      "Anthropology",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Cluster analysis",
      "Computer science",
      "Computer vision",
      "Gene",
      "Mathematics",
      "Normalization (sociology)",
      "Pattern recognition (psychology)",
      "Pure mathematics",
      "Robustness (evolution)",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Lin",
        "given_name": "Cong"
      },
      {
        "surname": "Wu",
        "given_name": "Yufeng"
      },
      {
        "surname": "Huang",
        "given_name": "Ke"
      },
      {
        "surname": "Yang",
        "given_name": "Hai"
      },
      {
        "surname": "Deng",
        "given_name": "Yuqiao"
      },
      {
        "surname": "Wen",
        "given_name": "Yamin"
      }
    ]
  },
  {
    "title": "Hypergraph clustering based multi-label cross-modal retrieval",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104258",
    "abstract": "Most existing cross-modal retrieval methods face challenges in establishing semantic connections between different modalities due to inherent heterogeneity among them. To establish semantic connections between different modalities and align relevant semantic features across modalities, so as to fully capture important information within the same modality, this paper considers the superiority of hypergraph in representing higher-order relationships, and proposes an image-text retrieval method based on hypergraph clustering. Specifically, we construct hypergraphs to capture feature relationships within image and text modalities, as well as between image and text. This allows us to effectively model complex relationships between features of different modalities and explore the semantic connectivity within and across modalities. To compensate for potential semantic feature loss during the construction of the hypergraph neural network, we design a weight-adaptive coarse and fine-grained feature fusion module for semantic supplementation. Comprehensive experimental results on three common datasets demonstrate the effectiveness of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002141",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Cluster analysis",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Hypergraph",
      "Information retrieval",
      "Mathematics",
      "Modal",
      "Pattern recognition (psychology)",
      "Polymer chemistry"
    ],
    "authors": [
      {
        "surname": "Guo",
        "given_name": "Shengtang"
      },
      {
        "surname": "Zhang",
        "given_name": "Huaxiang"
      },
      {
        "surname": "Liu",
        "given_name": "Li"
      },
      {
        "surname": "Liu",
        "given_name": "Dongmei"
      },
      {
        "surname": "Lu",
        "given_name": "Xu"
      },
      {
        "surname": "Li",
        "given_name": "Liujian"
      }
    ]
  },
  {
    "title": "Stacked deformable convolution network with weighted non-local attention and branch residual connection for image quality assessment",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104214",
    "abstract": "Convolutional neural networks are data-driven Image Quality Assessment (IQA) models based on the convolutional operation in a rectangular window. Deformable convolutions with learnable receptive fields can efficiently extract structural features of irregular objects in an image. By superimposing deformable convolutions, a plug-and-play module is designed to obtain information for irregular geometric shapes. To selectively fuse shallow and deep features, we propose a weighted non-local attention (WNLA) module with the input and output of self-attention in a weighted manner. This paper proposes a dual branch residual full-reference IQA network that combines weighted non-local attention and stacked deformable convolution. The proposed network was trained on PIPAL dataset and tested on LIVE and TID2013. The cross-dataset evaluation shows that the network has a competitive generalization ability. Ablation experiments indicate that the proposed modules can effectively improve the performance of the network. Comparative experiments reveal that our network is superior to existing excellent networks. The codes for training, test and visualization are available at: https://github.com/Pengchang-haha/SDCN.git.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001706",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Connection (principal bundle)",
      "Convolution (computer science)",
      "Engineering",
      "Evaluation methods",
      "Geometry",
      "Image (mathematics)",
      "Image quality",
      "Mathematics",
      "Physics",
      "Quality (philosophy)",
      "Quality assessment",
      "Quantum mechanics",
      "Reliability engineering",
      "Residual"
    ],
    "authors": [
      {
        "surname": "Fan",
        "given_name": "Xiaodong"
      },
      {
        "surname": "Peng",
        "given_name": "Chang"
      },
      {
        "surname": "Jiang",
        "given_name": "Xiaoli"
      },
      {
        "surname": "Han",
        "given_name": "Ying"
      },
      {
        "surname": "Hou",
        "given_name": "Limin"
      }
    ]
  },
  {
    "title": "Design and optimization of an aerobics movement recognition system based on high-dimensional biotechnological data using neural networks",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104227",
    "abstract": "This study presents the design and optimization of an aerobics movement recognition system utilizing high-dimensional biotechnological data. Deep learning techniques are employed to achieve accurate classification and recognition of movement actions. Biosensing technology and wearable devices are used to collect real-time, multidimensional physiological signal data from key anatomical regions of athletes. The system is constructed using convolutional neural networks (CNNs) and Long Short-Term Memory. Model performance is optimized through parameter selection and strategies such as Xavier initialization, the cross-entropy loss function, and the Adam optimizer. The results indicate that Model C achieves an accuracy of 0.987, significantly outperforming the standalone CNNs (accuracy 0.975) and the recurrent neural network models (accuracy 0.965). Furthermore, it demonstrates notable efficiency in practical applications, with considerably reduced execution times of 10 s for data processing, 25 s for feature extraction, and 20 s for classification. This aerobics recognition system excels in performance and efficiency, supporting precise movement classification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001834",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Feature extraction",
      "Initialization",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Programming language"
    ],
    "authors": [
      {
        "surname": "Yihan",
        "given_name": "Ma"
      }
    ]
  },
  {
    "title": "Multimodal spatiotemporal aggregation for point cloud accumulation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104243",
    "abstract": "Point cloud accumulation is a crucial technique in point cloud analysis, facilitating various downstream tasks like surface reconstruction. Current methods merely rely on raw LiDAR points, yielding unsatisfactory performance due to the limited geometric information, particularly in complex scenarios characterized by intricate motions, diverse objects, and an increased number of frames. In this paper, we introduce camera modality data, which is usually acquired alongside LiDAR data at minimal expense. To this end, we present the Multimodal Spatiotemporal Aggregation solution (termed MSA) to thoroughly explore and aggregate these two distinct modalities (sparse 3D points and multi-view 2D images). Concretely, we propose a multimodal spatial aggregation module to bridge the data gap between different modalities in the Bird’s-Eye-View (BEV) space and further fuse them by learnable adaptive channel-wise weights. By assembling their respective strengths, this module generates a reliable and consistent scene representation. Subsequently, we design a temporal aggregation module to capture continuous motion information across consecutive sequences, which is beneficial for identifying the motion state of the foreground scene and enabling the model to extend robustly to longer sequences. Experiments demonstrate MSA outperforms state-of-the-art (SoTA) point cloud accumulation methods across all evaluation metrics in the public benchmark, especially with more frames.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001998",
    "keywords": [
      "Chemistry",
      "Chromatography",
      "Cloud computing",
      "Cloud point",
      "Computer science",
      "Computer vision",
      "Environmental science",
      "Extraction (chemistry)",
      "Geometry",
      "Mathematics",
      "Operating system",
      "Point (geometry)",
      "Point cloud"
    ],
    "authors": [
      {
        "surname": "Pan",
        "given_name": "Jie"
      },
      {
        "surname": "Lin",
        "given_name": "Chunyu"
      },
      {
        "surname": "Nie",
        "given_name": "Lang"
      },
      {
        "surname": "Liu",
        "given_name": "Meiqin"
      },
      {
        "surname": "Zhao",
        "given_name": "Yao"
      }
    ]
  },
  {
    "title": "Blind omnidirectional image quality assessment based on semantic information replenishment",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104241",
    "abstract": "Blind Omnidirectional Image Quality Assessment (BOIQA) is of great significance to the development of the immersive media technology. Most BOIQA metrics are achieved by projecting the raw spherical OIs into other plane spaces for better feature representation. For these metrics, the semantic information at the shared boundaries of the adjacent views are prone to be destroyed, hindering the semantic understanding and further performance improvement. To tackle this problem, we propose a lightweight but effective BOIQA metric via replenishing the damaged semantic information. Specifically, a multi-stream semantic information replenishment module is constructed by the multi-scale feature representation, which is designed to restore the destroyed semantic information from two adjacent views. For module learning, more than 20,000 image triplets are further built. Then, the restored features are integrated for the final quality prediction. To testify the effectiveness of the proposed method, extensive experiments are conducted on the public OIQA databases, and the results prove the superior performance of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001974",
    "keywords": [
      "Antenna (radio)",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Epistemology",
      "Image (mathematics)",
      "Image quality",
      "Omnidirectional antenna",
      "Philosophy",
      "Quality (philosophy)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yi"
      },
      {
        "surname": "Zhou",
        "given_name": "Yu"
      },
      {
        "surname": "Li",
        "given_name": "Mengyu"
      },
      {
        "surname": "Sun",
        "given_name": "Yanjing"
      },
      {
        "surname": "Ding",
        "given_name": "Jicun"
      }
    ]
  },
  {
    "title": "Multi-task network with inter-task consistency learning for face parsing and facial expression recognition at real-time speed",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104213",
    "abstract": "In recent years, face parsing and facial expression recognition have attracted increasing interest. Even though there are relevant results about face parsing and face representation, these approaches seek accuracy at the expense of speed. In this paper, we design a novel multi-task learning network for face parsing and facial expression recognition (MPENet). Specifically, MPENet consists of shared encoders and three downstream branches. In the edge perceiving branch, we use category edge and binary edge to extract face boundary information and improve localization of face boundaries. In the segmentation branch, we use graph learning to fuse edge and semantic information of the image, analyze the relations between different feature regions, and capture more contextual relationships. Finally, we design a consistent learning loss function, forcing different branches to learn the same predictions. We have carried out experiments on face datasets, and found that it shows high precision and fast inference speed. Specifically, MPENet achieves F1 scores of 85.9 on the CelebAMask-HQ dataset and 92.9 on the Lapa dataset, with an inference speed of 92.9 FPS. Moreover, MPENet precisely delineates the semantic boundaries of facial regions and, through consistent multi-task learning, effectively facilitates synergy among various tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032400169X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Consistency (knowledge bases)",
      "Engineering",
      "Expression (computer science)",
      "Face (sociological concept)",
      "Facial expression",
      "Facial expression recognition",
      "Facial recognition system",
      "Linguistics",
      "Natural language processing",
      "Parsing",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Speech recognition",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Haoyu"
      },
      {
        "surname": "Song",
        "given_name": "Haiyu"
      },
      {
        "surname": "Li",
        "given_name": "Peihong"
      }
    ]
  },
  {
    "title": "Learning saliency-awareness Siamese network for visual object tracking",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104237",
    "abstract": "Siamese trackers have emerged as the predominant paradigm in visual tracking owing to their robust similarity matching. However, relying on dense regression strategies for predicting the target’s axis-aligned bounding box often leads to excessive background pixels. This limitation can compromise the model’s accuracy, especially when tracking non-rigid targets. To tackle this issue, this paper presents a novel saliency-awareness Siamese network for visual object tracking. Compared to the bounding box regression network, our method achieves accurate pixel-level target tracking. Specifically, a two-level U-structure encode-decode model is tailored to learn the saliency of backbone features. Additionally, a dual-pipeline parallel tracking framework is proposed, allowing top-down multi-stage saliency mask prediction, by integrating the aforementioned model into the Siamese network. Finally, a convolutional head is devised to generate a precise binary mask for tracking. Extensive experiments on four benchmarks, including VOT2016, VOT2019, GOT-10k, and UAV123, demonstrate that our tracker achieves superior tracking performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001937",
    "keywords": [
      "Artificial intelligence",
      "Cognitive psychology",
      "Communication",
      "Computer science",
      "Computer vision",
      "Eye tracking",
      "Object (grammar)",
      "Pedagogy",
      "Psychology",
      "Tracking (education)",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Peng"
      },
      {
        "surname": "Wang",
        "given_name": "Qinghui"
      },
      {
        "surname": "Dou",
        "given_name": "Jie"
      },
      {
        "surname": "Dou",
        "given_name": "Lei"
      }
    ]
  },
  {
    "title": "Fusing structure from motion and simulation-augmented pose regression from optical flow for challenging indoor environments",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104256",
    "abstract": "The localization of objects is essential in many applications, such as robotics, virtual and augmented reality, and warehouse logistics. Recent advancements in deep learning have enabled localization using monocular cameras. Traditionally, structure from motion (SfM) techniques predict an object’s absolute position from a point cloud, while absolute pose regression (APR) methods use neural networks to understand the environment semantically. However, both approaches face challenges from environmental factors like motion blur, lighting changes, repetitive patterns, and featureless areas. This study addresses these challenges by incorporating additional information and refining absolute pose estimates with relative pose regression (RPR) methods. RPR also struggles with issues like motion blur. To overcome this, we compute the optical flow between consecutive images using the Lucas–Kanade algorithm and use a small recurrent convolutional network to predict relative poses. Combining absolute and relative poses is difficult due to differences between global and local coordinate systems. Current methods use pose graph optimization (PGO) to align these poses. In this work, we propose recurrent fusion networks to better integrate absolute and relative pose predictions, enhancing the accuracy of absolute pose estimates. We evaluate eight different recurrent units and create a simulation environment to pre-train the APR and RPR networks for improved generalization. Additionally, we record a large dataset of various scenarios in a challenging indoor environment resembling a warehouse with transportation robots. Through hyperparameter searches and experiments, we demonstrate that our recurrent fusion method outperforms PGO in effectiveness.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002128",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Environmental science",
      "Flow (mathematics)",
      "Geometry",
      "Image (mathematics)",
      "Mathematics",
      "Motion (physics)",
      "Motion capture",
      "Optical flow",
      "Regression",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Ott",
        "given_name": "Felix"
      },
      {
        "surname": "Heublein",
        "given_name": "Lucas"
      },
      {
        "surname": "Rügamer",
        "given_name": "David"
      },
      {
        "surname": "Bischl",
        "given_name": "Bernd"
      },
      {
        "surname": "Mutschler",
        "given_name": "Christopher"
      }
    ]
  },
  {
    "title": "Versatile depth estimator based on common relative depth estimation and camera-specific relative-to-metric depth conversion",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104252",
    "abstract": "A typical monocular depth estimator is trained for a single camera, so its performance drops severely on images taken with different cameras. To address this issue, we propose a versatile depth estimator (VDE), composed of a common relative depth estimator (CRDE) and multiple relative-to-metric converters (R2MCs). The CRDE extracts relative depth information, and each R2MC converts the relative information to predict metric depths for a specific camera. The proposed VDE can cope with diverse scenes, including both indoor and outdoor scenes, with only a 1.12% parameter increase per camera. Experimental results demonstrate that VDE supports multiple cameras effectively and efficiently and also achieves state-of-the-art performance in the conventional single-camera scenario.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002086",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Engineering",
      "Estimation",
      "Estimator",
      "Geology",
      "Geophysics",
      "Mathematics",
      "Measured depth",
      "Metric (unit)",
      "Operations management",
      "Statistics",
      "Systems engineering"
    ],
    "authors": [
      {
        "surname": "Jun",
        "given_name": "Jinyoung"
      },
      {
        "surname": "Lee",
        "given_name": "Jae-Han"
      },
      {
        "surname": "Kim",
        "given_name": "Chang-Su"
      }
    ]
  },
  {
    "title": "CC-SMC: Chain coding-based segmentation map lossless compression",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104222",
    "abstract": "A segmentation map, either static or dynamic, refers to a two-dimensional picture that may vary with time and indicates the segmentation label per pixel. Both the semantic map and the occupancy map in video-based point cloud compression (V-PCC) belong to the segmentation map we referred to. The semantic map can work for many machine vision tasks like tracking and has been used as a layer of image representation in some image compression methods. The occupancy map constitutes a part of the point cloud coding bitstream. Since segmentation maps are widely used, how to efficiently compress them is of interest. We propose a segmentation map lossless compression scheme namely CC-SMC, exploiting the nature of segmentation maps that usually contain limited colors and sharp edges. Specifically, we design a chain coding-based scheme combined with quadtree-based block partitioning. For intraframe coding, one block is partitioned recursively with a quadtree structure, until the block contains only one color, is smaller than a threshold, or satisfies the defined chain coding condition. We revise the three-orthogonal chain coding method to incorporate contextual information and design effective intraframe prediction methods. For interframe coding, one block may find a reference block; the chain difference between the current and the reference blocks is coded. We implement the proposed scheme and test it on several different kinds of segmentation maps. Compared with advanced lossless image compression techniques, our proposed scheme obtains more than 10% bits reduction as well as more than 20% decoding time-saving. The code is available at https://github.com/Yang-Runyu/CC-SMC.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001780",
    "keywords": [
      "Artificial intelligence",
      "Coding (social sciences)",
      "Composite material",
      "Compression (physics)",
      "Computer science",
      "Data compression",
      "Image (mathematics)",
      "Image compression",
      "Image processing",
      "Lossless JPEG",
      "Lossless compression",
      "Materials science",
      "Mathematics",
      "Segmentation",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Runyu"
      },
      {
        "surname": "Liu",
        "given_name": "Dong"
      },
      {
        "surname": "Wu",
        "given_name": "Feng"
      },
      {
        "surname": "Gao",
        "given_name": "Wen"
      }
    ]
  },
  {
    "title": "Efficient SpineUNetX for X-ray: A spine segmentation network based on ConvNeXt and UNet",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104245",
    "abstract": "Accurate localization and delineation of vertebrae are crucial for diagnosing and treating spinal disorders. To achieve this, we propose an efficient X-ray full-spine vertebra instance segmentation method based on an enhanced U-Net architecture. Several key improvements have been made: the ConvNeXt encoder is employed to effectively capture complex features, and IFE feature extraction is introduced in the skip connections to focus on texture-rich and edge-clear clues. The CBAM attention mechanism is used in the bottleneck to integrate coarse and fine-grained semantic information. The decoder employs a residual structure combined with skip connections to achieve multi-scale contextual information and feature fusion. Our method has been validated through experiments on anterior-posterior and lateral spinal segmentation, demonstrating robust feature extraction and precise semantic segmentation capabilities. It effectively handles various spinal disorders, including scoliosis, vertebral wedging, lumbar spondylolisthesis and spondylolysis. This segmentation foundation enables rapid calibration of vertebral parameters and the computation of relevant metrics, providing valuable references and guidance for advancements in medical imaging.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002013",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Medicine",
      "Molecular biology",
      "SPINE (molecular biology)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Deng",
        "given_name": "Shuangcheng"
      },
      {
        "surname": "Yang",
        "given_name": "Yang"
      },
      {
        "surname": "Wang",
        "given_name": "Junyang"
      },
      {
        "surname": "Li",
        "given_name": "Aijing"
      },
      {
        "surname": "Li",
        "given_name": "Zhiwu"
      }
    ]
  },
  {
    "title": "Non-local feature aggregation quaternion network for single image deraining",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104250",
    "abstract": "The existing deraining methods are based on convolutional neural networks (CNN) learning the mapping relationship between rainy and clean images. However, the real-valued CNN processes the color images as three independent channels separately, which fails to fully leverage color information. Additionally, sliding-window-based neural networks cannot effectively model the non-local characteristics of an image. In this work, we proposed a non-local feature aggregation quaternion network (NLAQNet), which is composed of two concurrent sub-networks: the Quaternion Local Detail Repair Network (QLDRNet) and the Multi-Level Feature Aggregation Network (MLFANet). Furthermore, in the subnetwork of QLDRNet, the Local Detail Repair Block (LDRB) is proposed to repair the backdrop of an image that has not been damaged by rain streaks. Finally, within the MLFANet subnetwork, we have introduced two specialized blocks, namely the Non-Local Feature Aggregation Block (NLAB) and the Feature Aggregation Block (Mix), specifically designed to address the restoration of rain-streak-damaged image backgrounds. Extensive experiments demonstrate that the proposed network delivers strong performance in both qualitative and quantitative evaluations on existing datasets. The code is available at https://github.com/xionggonghe/NLAQNet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002062",
    "keywords": [
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Code (set theory)",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Geometry",
      "Image (mathematics)",
      "Leverage (statistics)",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pooling",
      "Programming language",
      "Set (abstract data type)",
      "Subnetwork"
    ],
    "authors": [
      {
        "surname": "Xiong",
        "given_name": "Gonghe"
      },
      {
        "surname": "Gai",
        "given_name": "Shan"
      },
      {
        "surname": "Nie",
        "given_name": "Bofan"
      },
      {
        "surname": "Chen",
        "given_name": "Feilong"
      },
      {
        "surname": "Sun",
        "given_name": "Chengli"
      }
    ]
  },
  {
    "title": "Unknown Sample Selection and Discriminative Classifier Learning for Generalized Category Discovery",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104203",
    "abstract": "Traditional supervised techniques rely on labeled data, which are not available for unknown classes. Generalized Category Discovery (GCD) aims to categorize data into both known and unknown classes. We proposed a method, Unknown Sample Selection and Discriminative Classifier Learning for GCD (USSDCL). Firstly, we map the features to a linear subspace to identify dimensions that best represent the semantic differences among the known classes and introduce a neighborhood relevance score to assess the consistency of labels among neighbors. Using this score, we can efficiently distinguish between known, unknown, and uncertain samples. Next, we leverage both the known samples, pseudo-labeled unknown samples, and the uncertain samples to train a discriminative classifier. This classifier incorporates both a cross-entropy loss and an uncertainty augmentation loss to encourage closer predictions between uncertain samples and their augmentations, enhancing the classifier’s discriminative capability. Our model exhibits enhanced performance in classifying both known and unknown samples.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001585",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Discriminative model",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Selection (genetic algorithm)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Jichuan"
      },
      {
        "surname": "Li",
        "given_name": "Xiao"
      },
      {
        "surname": "Dong",
        "given_name": "Chunxi"
      }
    ]
  },
  {
    "title": "Efficient 2D transform hardware architecture for the versatile video coding standard",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104202",
    "abstract": "As the latest generation of video coding standards, versatile video coding (VVC) introduces several new coding tools in transform coding to concentrate the energy of residual blocks. In this work, we propose a regular multiplier (RM) based hardware architecture for 2D transform that can process different transform sizes and types using a unified architecture. This architecture consists of a 1D row transform, a transpose memory, and a 1D column transform, allowing for processing 16 coefficients per cycle. For the 1D row/column transform, the unified calculation structure consists of 512 regular multipliers supporting various transform sizes and types. For the transpose memory, we design an SRAM-based diagonal storage approach, along with a FIFO for storing the block information. Compared with the state-of-the-art MCM-based design, the proposed RM-based design shows a 30.9% reduction in area while can operate at a higher frequency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001573",
    "keywords": [
      "Architecture",
      "Arithmetic",
      "Art",
      "Coding (social sciences)",
      "Computer architecture",
      "Computer hardware",
      "Computer science",
      "Hardware architecture",
      "Mathematics",
      "Programming language",
      "Software",
      "Statistics",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Sheng",
        "given_name": "Qinghua"
      },
      {
        "surname": "Pan",
        "given_name": "Rui"
      },
      {
        "surname": "Chen",
        "given_name": "Junyu"
      },
      {
        "surname": "Lai",
        "given_name": "Changcai"
      },
      {
        "surname": "Liu",
        "given_name": "Yuanyuan"
      },
      {
        "surname": "Huang",
        "given_name": "Xiaofeng"
      },
      {
        "surname": "Yin",
        "given_name": "Haibing"
      },
      {
        "surname": "Yan",
        "given_name": "Chenggang"
      }
    ]
  },
  {
    "title": "Pixel-wise low-light image enhancement based on metropolis theorem",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104211",
    "abstract": "Images taken in low-light conditions frequently encounter visibility problems, such as severe noise, reduced brightness, and low contrast. This paper introduces an approach to enhance low-light images using the Metropolis Theorem (MT). The method begins by applying a global gamma correction to the input image, followed by transforming the globally corrected image into the HSV (Hue, Saturation, Value - V) domain. To achieve multi-scale decomposition, an application of the MT is proposed, resulting in approximation and detail sub-images of the V component. Subsequently, local gamma correction is employed on both the final approximation and detail images to enhance local contrast. The refined approximation and detail images are then combined to reconstruct the refined V component. The reconstructed image is obtained by weighting each band of the image with the refined V component. Experimental results demonstrate that the proposed method outperforms state-of-the-art methods, providing improved visual quality and more natural colors.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001676",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Mathematics",
      "Pixel"
    ],
    "authors": [
      {
        "surname": "Demir",
        "given_name": "Y."
      },
      {
        "surname": "Kaplan",
        "given_name": "N.H."
      },
      {
        "surname": "Kucuk",
        "given_name": "S."
      },
      {
        "surname": "Severoglu",
        "given_name": "N."
      }
    ]
  },
  {
    "title": "A dual-branch infrared and visible image fusion network using progressive image-wise feature transfer",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104190",
    "abstract": "To achieve a fused image that contains rich texture details and prominent targets, we present a progressive dual-branch infrared and visible image fusion network called PDFusion, which incorporates the Transformer module. Initially, the proposed network is divided into two branches to extract infrared and visible features independently. Subsequently, the image-wise transfer block (ITB) is introduced to fuse the infrared and visible features at different layers, facilitating the exchange of information between features. The fused features are then fed back into both pathways to contribute to the subsequent feature extraction process. Moreover, in addition to conventional pixel-level and structured loss functions, the contrastive language-image pretraining (CLIP) loss is introduced to guide the network training. Experimental results on publicly available datasets demonstrate the promising performance of PDFusion in the task of infrared and visible image fusion. The exceptional fusion performance of the proposed fusion network can be attributed to the following reasons: (1) The ITB block, particularly with the integration of the Transformer, enhances the capability of representation learning. The Transformer module captures long-range dependencies among image features, enabling a global receptive field that integrates contextual information from the entire image. This leads to a more comprehensive fusion of features. (2) The feature loss based on the CLIP image encoder minimizes the discrepancy between the generated and target images. Consequently, it promotes the generation of semantically coherent and visually appealing fused images. The source code of our method can be found at https://github.com/Changfei-Zhou/PDFusion.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001457",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Dual (grammatical number)",
      "Feature (linguistics)",
      "Feature detection (computer vision)",
      "Fusion",
      "Image (mathematics)",
      "Image fusion",
      "Image processing",
      "Infrared",
      "Linguistics",
      "Literature",
      "Optics",
      "Parallel computing",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Transfer (computing)"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Shaoping"
      },
      {
        "surname": "Zhou",
        "given_name": "Changfei"
      },
      {
        "surname": "Xiao",
        "given_name": "Jian"
      },
      {
        "surname": "Tao",
        "given_name": "Wuyong"
      },
      {
        "surname": "Dai",
        "given_name": "Tianyu"
      }
    ]
  },
  {
    "title": "AFINet: Camouflaged object detection via Attention Fusion and Interaction Network",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104208",
    "abstract": "Since the camouflaged objects share very similar colors and textures with the surroundings, there is still a great challenge in accurately locating and segmenting target objects with the varying sizes and shapes in different scenes. In this paper, we propose a novel Attention Fusion and Interaction network (AFINet) to detect the camouflaged objects by exploring the cross-level complementary information. Specifically, we first propose a Multi-Attention Interaction (MAI) module to fuse the cross-level features containing different characteristics by the attention interaction, thereby fully making use of the specific and complementary information from different levels to deal with scale variation. Furthermore, we design a Location and Boundary Guidance (LBG) module to make each side-output feature aware of where to learn, which can avoid the disturbances of the non-camouflaged regions by distinguishing the subtle differences. Comprehensive experiments and comparisons are conducted on four widely used benchmark datasets, demonstrating that the proposed network achieves state-of-the-art performance. The code and prediction maps will be available at https://github.com/ZhangQing0329/AFINet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001640",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Fusion",
      "Linguistics",
      "Object (grammar)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Qing"
      },
      {
        "surname": "Yan",
        "given_name": "Weiqi"
      },
      {
        "surname": "Zhao",
        "given_name": "Yilin"
      },
      {
        "surname": "Jin",
        "given_name": "Qi"
      },
      {
        "surname": "Zhang",
        "given_name": "Yu"
      }
    ]
  },
  {
    "title": "Dual-stream mutually adaptive quality assessment for authentic distortion image",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104216",
    "abstract": "To address semantic misperception caused by distortion and accurately simulate human perceptual processes, this study proposed a method that utilised dual-stream mutually adaptive feature mapping. To extract complementary features, a dual-stream unsupervised method was used during the training phase. One stream was responsible for the extraction of low-level and global features, whereas the other was dedicated to extracting high-level semantic and positional features. Following the freezing of feature extraction network weights, we proposed a structure that used standard deviation labels to predict the quality distribution. The experimental results obtained from 10 published image quality databases demonstrated the superiority of the proposed algorithm. The algorithm outperformed the majority of mainstream methods when evaluated on authentic distortion databases and exhibited competitive performance on synthetic distortion databases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032400172X",
    "keywords": [
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Computer science",
      "Computer vision",
      "Distortion (music)",
      "Dual (grammatical number)",
      "Epistemology",
      "Image (mathematics)",
      "Image quality",
      "Linguistics",
      "Mathematics",
      "Philosophy",
      "Psychology",
      "Quality (philosophy)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Huizhen",
        "given_name": "Jia"
      },
      {
        "surname": "Huaibo",
        "given_name": "Zhou"
      },
      {
        "surname": "Hongzheng",
        "given_name": "Qin"
      },
      {
        "surname": "Tonghan",
        "given_name": "Wang"
      }
    ]
  },
  {
    "title": "PileNet: A high-and-low pass complementary filter with multi-level feature refinement for salient object detection",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104186",
    "abstract": "Multi-head self-attentions (MSAs) in Transformer are low-pass filters, which will tend to reduce high-frequency signals. Convolutional layers (Convs) in Convolutional Neural Network (CNN) are high-pass filters, which will tend to capture high-frequency components of the images. Therefore, CNN and Transformer contain complementary information, and the combination of the two is necessary for satisfactory detection results. In this work, we propose a novel framework PileNet that efficiently combine CNN and Transformer for accurate salient object detection (SOD). Specifically in PileNet, we introduce complementary encoder that extracts multi-level complementary saliency features. Next, we simplify the complementary features by adjusting the number of channels for all features to a fixed value. By introducing the multi-level feature aggregation (MLFA) and multi-level feature refinement (MLFR) units, the low- and high-level features can easily be transmitted to feature blocks at various pyramid levels. Finally, we fuse all the refined saliency features in a Unet-like structure from top to bottom and use multi-point supervision mechanism to produce the final saliency maps. Extensive experimental results over five widely used saliency benchmark datasets clearly demonstrate that our proposed model can accurately locate the entire salient objects with clear object boundaries and outperform sixteen previous state-of-the-art saliency methods in terms of a wide range of metrics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032400141X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Filter (signal processing)",
      "Linguistics",
      "Mathematics",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Salient"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Xiaoqi"
      },
      {
        "surname": "Duan",
        "given_name": "Liangliang"
      },
      {
        "surname": "Zhou",
        "given_name": "Quanqiang"
      }
    ]
  },
  {
    "title": "Learning degradation priors for reliable no-reference image quality assessment",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104189",
    "abstract": "The goal of No-Reference Image Quality Assessment (NR-IQA) is to endow computers with a human-like ability to evaluate an image’s quality without comparison to a reference. Current deep learning-based methods mainly work in the spatial domain to measure the quality, heavily rely on semantic information, and less on the degradation of the image itself, struggling to accurately judge the quality of an image in similar scenes. In this paper, we propose a novel degradation priors learning architecture to address the NR-IQA task by leveraging learnable degradation priors, along with semantic features. The multi-task learning strategy is introduced to ensure our model could obtain accurate degradation priors for the NR-IQA task. Extensive experiments on public benchmarks demonstrate that our approach outperforms state-of-the-art solutions. Besides we also collect an additional dataset namely ReD-1K to illustrate the superiority of our approach to judge the image quality in similar scenes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001445",
    "keywords": [
      "Artificial intelligence",
      "Bayesian probability",
      "Computer science",
      "Computer vision",
      "Degradation (telecommunications)",
      "Epistemology",
      "Image (mathematics)",
      "Image quality",
      "Mathematics",
      "Philosophy",
      "Prior probability",
      "Quality (philosophy)",
      "Statistics",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Hua"
      },
      {
        "surname": "Shen",
        "given_name": "Zhuonan"
      },
      {
        "surname": "Zheng",
        "given_name": "Bolun"
      },
      {
        "surname": "Chen",
        "given_name": "Quan"
      },
      {
        "surname": "Yu",
        "given_name": "Dingguo"
      },
      {
        "surname": "Chen",
        "given_name": "Yiru"
      },
      {
        "surname": "Yan",
        "given_name": "Chenggang"
      }
    ]
  },
  {
    "title": "Unsupervised video summarization with adversarial graph-based attention network",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104200",
    "abstract": "Video summarization aims to select a subset of video segments that best capture the video storyline. Our study seeks to train an encoder to transform the raw frame features extracted from pre-trained CNN models into representations that embody importance and guide the selection of the video segment. Our main idea is to use graph modeling and attention mechanisms to train the encoder adversarially. The graph representation enables the model to learn the relationship among frames, revealing the intrinsic structure of a video. The attention mechanism allows the model to capture the magnitude of these relationships. In the proposed model, an attention-based encoder is trained using a graph-based generator that reconstructs videos using the encoded features and a discriminator that guides the generator, distinguishing the original and reconstructed video. Thus, by leveraging graph attention and refining mechanisms, the proposed model offers distinct advantages over existing methods, including enhanced summarization accuracy, improved preservation of temporal coherence, and the ability to capture complex semantic linkages within video content. These advancements are substantiated through a comprehensive ablation study, which demonstrates the efficacy of our model using various evaluation metrics — Kendall and Spearman coefficients. The proposed model is evaluated on TVSum and SumMe datasets and achieves results on par with supervised models that used similar encoders and achieved state-of-the-art results compared to other unsupervised models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032400155X",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Automatic summarization",
      "Computer science",
      "Graph",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Gunuganti",
        "given_name": "Jeshmitha"
      },
      {
        "surname": "Yeh",
        "given_name": "Zhi-Ting"
      },
      {
        "surname": "Wang",
        "given_name": "Jenq-Haur"
      },
      {
        "surname": "Norouzi",
        "given_name": "Mehdi"
      }
    ]
  },
  {
    "title": "A dual-branch residual network for inhomogeneous dehazing",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104191",
    "abstract": "Image dehazing has now gained the dominant popularity in the field of image processing, particularly in inhomogeneous scene. Recent years have witnessed great progress in handling homogeneous dehazing problems. Due to the unknown haze distribution of the real world, it is extremely intractable to offer a clear view of the observed scene in limited inhomogeneous datasets. Furthermore, it is impossible to completely avoid artifacts of color distortion, over-enhancement, halo, and blur errors in order to provide reliable and stable results. To this end, we propose an end-to-end Dual-branch Residual Network (DBRN) for inhomogeneous dehazing that is composed of Hybrid Feature Subnet (HFS) and Attention Feature Fusion Subnet (AFFS). HFS explores high-quality global and local hazy features with long-range spatial fusion at different scales in an encoder–decoder manner, while AFFS makes artifact removal possible with a stack of Residual Convolution Attention Module (RCAM). Besides, the joint loss function aims to ensure that the recovered image is close to the ground-truth in the aspects of texture, color, structure index, and so on. Through this design, the model exhibits robustness in inhomogeneous hazy scenes, enabling high-quality visual restoration in scenes with varying haze densities. Extensive experimental results demonstrate that the proposed model performs favorably against the state-of-the-art methods on synthetic datasets and real-world hazy images. In addition, ablation studies are carried out to demonstrate the effectiveness of each component. The source code of the proposed method is available at https://github.com/jing-1196/DBRN/.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001469",
    "keywords": [
      "Algorithm",
      "Art",
      "Computer science",
      "Dual (grammatical number)",
      "Geology",
      "Literature",
      "Residual"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Yifei"
      },
      {
        "surname": "Li",
        "given_name": "Jingjing"
      },
      {
        "surname": "Wei",
        "given_name": "Pingping"
      },
      {
        "surname": "Wang",
        "given_name": "Aichen"
      },
      {
        "surname": "Rao",
        "given_name": "Yuan"
      }
    ]
  },
  {
    "title": "Corrigendum to “Dense-sparse representation matters: A point-based method for volumetric medical image segmentation” [J. Visual Commun. Image Represent. 100 (2024) 104115]",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104205",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001615",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Geometry",
      "Image (mathematics)",
      "Law",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Point (geometry)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Segmentation",
      "Sparse approximation"
    ],
    "authors": [
      {
        "surname": "Jiang",
        "given_name": "Yun"
      },
      {
        "surname": "Liu",
        "given_name": "Bingxi"
      },
      {
        "surname": "Zhang",
        "given_name": "Zequn"
      },
      {
        "surname": "Yan",
        "given_name": "Yao"
      },
      {
        "surname": "Guo",
        "given_name": "Huanting"
      },
      {
        "surname": "Li",
        "given_name": "Yuhang"
      }
    ]
  },
  {
    "title": "Transformer-Based adversarial network for semi-supervised face sketch synthesis",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104204",
    "abstract": "Face sketch synthesis is a technique utilized to convert real face images into artistic sketches, which holds vast potential within criminal investigation and entertainment. The existing methods usually train the generation models on the paired face photo-sketch datasets, which are challenging to acquire. Moreover, their results usually produce blurring, artifacts, and structural distortion, leading to inferior visual effects. To solve the above issues, we propose a semi-supervised Transformer-based adversarial network for face sketch synthesis, which can be trained on unpaired datasets. In the network, the Transformer encoder structure is modified with the adaptive window attention (AWA) to better extract local and global facial features while minimizing computational complexity. A Transformer-based feature fusion module is used to fuse the extracted features. In addition, a detail extractor module is designed by Laplacian operators to effectively preserve the detail information of the face photo images to the face sketch images. In the detail extractor module, we introduce a mask operation to remove the textures that do not exist in the original face photo images. Experimental results on the CUHK, AR, XM2VTS, and CUFSF datasets showcase the excellent subjective and objective performance of the proposed face sketch synthesis method compared to current state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001597",
    "keywords": [
      "Adversarial system",
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Electrical engineering",
      "Engineering",
      "Face (sociological concept)",
      "Sketch",
      "Social science",
      "Sociology",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Shi",
        "given_name": "Zhihua"
      },
      {
        "surname": "Wan",
        "given_name": "Weiguo"
      }
    ]
  },
  {
    "title": "Offline writer identification approach using moment features and high-order correlation functions",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104183",
    "abstract": "For text-independent writer identification(WI) tasks, this paper proposes a novel method based on semantic codebooks and two effective feature descriptors. This method requires constructing semantic codebooks consisting of various subimages and their labels. Moment features and high-order correlation functions are used to describe handwriting style, and the standard Euclidean distance function is used for distance measurement. Then, two-way analysis of variance(TW-ANOVA) is used to filter out character factors that affect identification accuracy. Finally, a combined measure obtained from multiple classifiers based on a fuzzy integral rule is used to determine the candidate writers. To verify the effectiveness of the system, experiments are performed using the Firemaker, IAM, CRUG-CN and Uyghur2016 databases. The evaluation results demonstrate better performance than most existing methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032400138X",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Classical mechanics",
      "Computer science",
      "Correlation",
      "Economics",
      "Finance",
      "Geometry",
      "Identification (biology)",
      "Mathematics",
      "Moment (physics)",
      "Order (exchange)",
      "Pattern recognition (psychology)",
      "Physics"
    ],
    "authors": [
      {
        "surname": "Litifu",
        "given_name": "Ayixiamu"
      },
      {
        "surname": "Xiao",
        "given_name": "Jinsheng"
      },
      {
        "surname": "Yan",
        "given_name": "Yuchen"
      },
      {
        "surname": "Jiang",
        "given_name": "Hao"
      }
    ]
  },
  {
    "title": "BSNet: A bilateral real-time semantic segmentation network based on multi-scale receptive fields",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104188",
    "abstract": "The rise of autonomous driving and mobile robots has drawn attention to real-time road scene segmentation. However, category confusion, incomplete segmentation and inaccurate detail contours are common issues encountered in road traffic scene segmentation tasks. To tackle these road segmentation challenges, we proposes a bilateral real-time semantic segmentation network based on multi-scale receptive fields(BSNet). To enhance the segmentation accuracy of detail contours, an auxiliary detail detection module is introduced in the spatial branch. In the semantic branch, strip pooling and channel attention are added to the short-term dense concatenate module to enrich multi-scale receptive fields and strengthen channel features, addressing issues such as incomplete segmentation and category confusion. The final design of the complementary dual-guided fusion module improves feature fusion effectiveness. Experimental results on two benchmark datasets demonstrate that BSNet significantly improves these road segmentation difficulties and achieves competitive results in both speed and accuracy compared to the current state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001433",
    "keywords": [
      "Artificial intelligence",
      "Cartography",
      "Computer science",
      "Computer vision",
      "Geography",
      "Pattern recognition (psychology)",
      "Receptive field",
      "Scale (ratio)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Jin",
        "given_name": "Zhenyi"
      },
      {
        "surname": "Dou",
        "given_name": "Furong"
      },
      {
        "surname": "Feng",
        "given_name": "Ziliang"
      },
      {
        "surname": "Zhang",
        "given_name": "Chengfang"
      }
    ]
  },
  {
    "title": "X-CDNet: A real-time crosswalk detector based on YOLOX",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104206",
    "abstract": "As urban traffic safety becomes increasingly important, real-time crosswalk detection is playing a critical role in the transportation field. However, existing crosswalk detection algorithms must be improved in terms of accuracy and speed. This study proposes a real-time crosswalk detector called X-CDNet based on YOLOX. Based on the ConvNeXt basic module, we designed a new basic module called Reparameterizable Sparse Large-Kernel (RepSLK) convolution that can be used to expand the model’s receptive field without the addition of extra inference time. In addition, we created a new crosswalk dataset called CD9K, which is based on realistic driving scenes augmented by techniques such as synthetic rain and fog. The experimental results demonstrate that X-CDNet outperforms YOLOX in terms of both detection accuracy and speed. X-CDNet achieves a 93.3 AP50 and a real-time detection speed of 123 FPS.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001627",
    "keywords": [
      "Computer science",
      "Detector",
      "Engineering",
      "Pedestrian",
      "Schema crosswalk",
      "Telecommunications",
      "Transport engineering"
    ],
    "authors": [
      {
        "surname": "Lu",
        "given_name": "Xingyuan"
      },
      {
        "surname": "Xue",
        "given_name": "Yanbing"
      },
      {
        "surname": "Wang",
        "given_name": "Zhigang"
      },
      {
        "surname": "Xu",
        "given_name": "Haixia"
      },
      {
        "surname": "Wen",
        "given_name": "Xianbin"
      }
    ]
  },
  {
    "title": "Transferable adversarial attack on image tampering localization",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104210",
    "abstract": "It is significant to evaluate the security of existing digital image tampering localization algorithms in real-world applications. In this paper, we propose an adversarial attack scheme to reveal the reliability of such deep learning-based tampering localizers, which would be fooled and fail to predict altered regions correctly. Specifically, two practical adversarial example methods are presented in a unified attack framework. In the optimization-based adversarial attack, the victim image forgery is treated as the parameter to be optimized via Adam optimizer. In the gradient-based adversarial attack, the invisible perturbation yielded by Fast Gradient Sign Method (FGSM) is added to the tampered image along gradient ascent direction. The black-box attack is achieved by relying on the transferability of such adversarial examples to different localizers. Extensive experiments verify that our attacks can sharply reduce the tampering localization accuracy while preserving high visual quality for attacked images. Source code is available at https://github.com/multimediaFor/AttackITL.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001664",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Image (mathematics)",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Cao",
        "given_name": "Gang"
      },
      {
        "surname": "Wang",
        "given_name": "Yuqi"
      },
      {
        "surname": "Zhu",
        "given_name": "Haochen"
      },
      {
        "surname": "Lou",
        "given_name": "Zijie"
      },
      {
        "surname": "Yu",
        "given_name": "Lifang"
      }
    ]
  },
  {
    "title": "Shift-insensitive perceptual feature of quadratic sum of gradient magnitude and LoG signals for image quality assessment and image classification",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104215",
    "abstract": "Most existing full-reference (FR) Image quality assessment (IQA) models work in the premise of that the two images should be well registered. Shifting an image would lead to an inaccurate evaluation of image quality, because small spatial shifts are far less noticeable than structural distortion for human observers. To this regard, we propose to study an IQA feature that is shift-insensitive to the basic primitive structure of images, i.e., image edge. According to previous studies, the image gradient magnitude (GM) and the Laplacian of Gaussian (LoG) operator that depict the edge profiles of natural images are highly efficient structural features in IQA tasks. In this paper, we find that the Quadratic sum of the normalized GM and the LoG signals (QGL) has excellent shift-insensitive property in representing image edges after theoretically solving the selection problem of a ratio parameter to balance the GM and LoG signals. Based on the proposed QGL feature, two FR-IQA models can be built directly by measuring the similarity map with mean and standard deviation pooling strategies, named mQGL and sQGL, respectively. Experimental results show that the proposed sQGL and mQGL work robustly on four benchmark IQA databases, and QGL-based models show great shift-insensitive property to spatial translation and image rotation while judging the image quality. In addition, we explore the feasibility of combining QGL feature with deep neural networks, and verify that it can help to promote image pattern recognition in texture classification tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001718",
    "keywords": [
      "Artificial intelligence",
      "Astronomy",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Geometry",
      "Image (mathematics)",
      "Image quality",
      "Linguistics",
      "Magnitude (astronomy)",
      "Mathematics",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Perception",
      "Philosophy",
      "Physics",
      "Psychology",
      "Quadratic equation",
      "Quality (philosophy)",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Congmin"
      },
      {
        "surname": "Mou",
        "given_name": "Xuanqin"
      }
    ]
  },
  {
    "title": "Transferable adversarial attack on image tampering localization",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104210",
    "abstract": "It is significant to evaluate the security of existing digital image tampering localization algorithms in real-world applications. In this paper, we propose an adversarial attack scheme to reveal the reliability of such deep learning-based tampering localizers, which would be fooled and fail to predict altered regions correctly. Specifically, two practical adversarial example methods are presented in a unified attack framework. In the optimization-based adversarial attack, the victim image forgery is treated as the parameter to be optimized via Adam optimizer. In the gradient-based adversarial attack, the invisible perturbation yielded by Fast Gradient Sign Method (FGSM) is added to the tampered image along gradient ascent direction. The black-box attack is achieved by relying on the transferability of such adversarial examples to different localizers. Extensive experiments verify that our attacks can sharply reduce the tampering localization accuracy while preserving high visual quality for attacked images. Source code is available at https://github.com/multimediaFor/AttackITL.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001664",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Image (mathematics)",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Cao",
        "given_name": "Gang"
      },
      {
        "surname": "Wang",
        "given_name": "Yuqi"
      },
      {
        "surname": "Zhu",
        "given_name": "Haochen"
      },
      {
        "surname": "Lou",
        "given_name": "Zijie"
      },
      {
        "surname": "Yu",
        "given_name": "Lifang"
      }
    ]
  },
  {
    "title": "Efficient random-access GPU video decoding for light-field rendering",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104201",
    "abstract": "Compression method for GPU streaming of discrete light fields is proposed in this paper. Views on the scene are encoded with video codec to enable streaming in real time. Instead of using a classic scheme, all frames are encoded according to one reference frame. Any frame is decoded directly, in a random-access manner that is suitable for light-field rendering methods, where only few frames are necessary on the GPU. The proposed scheme reaches the best decoding quality/time ratio in comparison to other schemes, where all preceding frames need to be decompressed, and all-key-frame video that supports random access, but is extremely large. The proposed method solves the space-requirements and streaming-bandwidth issues using the GPU accelerated decoding, and enables incorporating light-field assets in real-time 3D simulations. Compared to existing methods, the proposal is easy to implement, does not depend on specific video format or extension and is efficient on consumer GPUs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001561",
    "keywords": [
      "Algorithm",
      "Computer graphics (images)",
      "Computer network",
      "Computer science",
      "Decoding methods",
      "Parallel computing",
      "Random access",
      "Rendering (computer graphics)"
    ],
    "authors": [
      {
        "surname": "Chlubna",
        "given_name": "Tomáš"
      },
      {
        "surname": "Milet",
        "given_name": "Tomáš"
      },
      {
        "surname": "Zemčík",
        "given_name": "Pavel"
      }
    ]
  },
  {
    "title": "Distance-based feature repack algorithm for video coding for machines",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104150",
    "abstract": "Nowadays, the use of video data for machine (VCM) tasks has become increasingly prevalent, with deep learning and computer vision requiring large volumes of video data for object detection, object tracking, and other tasks. However, the features required for machine tasks are different from those used by humans, and a new approach is needed to encode and compress video data for machine consumption. Video coding for machines has received considerable attention, with many approaches focusing on compressing features rather than the video itself. However, a key challenge in this process is repacking the features in an efficient and effective manner. This paper proposes a distance-based patch tiling and intra-block quilting method to repack feature sequences in a manner that is better suited for existing video codecs, based on statistical analysis of feature characteristics in the channel dimension. Experimental results demonstrate that our method achieves an 65.54% BD-rate gain compared to benchmark methods. This research has significant implications for improving the efficiency of video coding for machine applications, and future work could explore the use of feature dimensionality reduction and combination of neural network (NN) codec to optimize the repacking of features for compression.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001056",
    "keywords": [
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Codec",
      "Coding (social sciences)",
      "Computer hardware",
      "Computer science",
      "Computer vision",
      "Data compression",
      "Feature (linguistics)",
      "Geometry",
      "Linguistics",
      "Mathematics",
      "Multiview Video Coding",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Statistics",
      "Video processing",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Yuan"
      },
      {
        "surname": "Gong",
        "given_name": "Xiaoli"
      },
      {
        "surname": "Yu",
        "given_name": "Hualong"
      },
      {
        "surname": "Wu",
        "given_name": "Zijun"
      },
      {
        "surname": "Yu",
        "given_name": "Lu"
      }
    ]
  },
  {
    "title": "P-NOC: Adversarial training of CAM generating networks for robust weakly supervised semantic segmentation priors",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104187",
    "abstract": "Weakly Supervised Semantic Segmentation (WSSS) techniques explore individual regularization strategies to refine Class Activation Maps (CAMs). In this work, we first analyze complementary WSSS techniques in the literature, their segmentation properties, and the conditions in which they are most effective. Based on these findings, we devise two new techniques: P-NOC and C 2 AM-H. In the first, we promote the conjoint training of two adversarial CAM generating networks: the generator, which progressively learns to erase regions containing class-specific features, and a discriminator, which is refined to gradually shift its attention to new class discriminant features. In the latter, we employ the high quality pseudo-segmentation priors produced by P-NOC to guide the learning to saliency information in a weakly supervised fashion. Finally, we employ both pseudo-segmentation priors and pseudo-saliency proposals in the random walk procedure, resulting in higher quality pseudo-semantic segmentation masks, and competitive results with the state of the art.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001421",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Bayesian probability",
      "Computer science",
      "Geography",
      "Machine learning",
      "Meteorology",
      "Pattern recognition (psychology)",
      "Prior probability",
      "Segmentation",
      "Training (meteorology)"
    ],
    "authors": [
      {
        "surname": "David",
        "given_name": "Lucas"
      },
      {
        "surname": "Pedrini",
        "given_name": "Helio"
      },
      {
        "surname": "Dias",
        "given_name": "Zanoni"
      }
    ]
  },
  {
    "title": "Medical image classification: Knowledge transfer via residual U-Net and vision transformer-based teacher-student model with knowledge distillation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104212",
    "abstract": "With the widespread integration of deep learning techniques in the domain of medical image analysis, there is a prevailing consensus regarding their efficacy in handling high-dimensional and intricate medical image data. However, it is imperative to acknowledge that while complex deep models exhibit a remarkable capacity for processing high-dimensional and intricate data, they often necessitate a substantial allocation of computational resources and time. Furthermore, lightweight models, despite their computational efficiency, tend to underperform when compared to their more intricate counterparts in terms of performance. Hence, the prevailing aspiration is to transfer the cognitive prowess of complex models to their lightweight counterparts. Addressing the aforementioned concern, this study proposes a knowledge distillation approach that encompasses joint feature and soft label transfer. It entails the transference of knowledge from the teacher model’s intermediate features and predictive outcomes to the student model. The student model leverages this knowledge to emulate the behavior of the teacher model, thereby enhancing the precision of its own predictions. Building upon this foundation, we introduce a Res-Transformer teacher model based on the U-Net architecture and a student model known as ResU-Net, which is grounded in residual modules. The Res-Transformer teacher model employs multi-layer residual attention during the downsampling process to capture deep-level features of the image. Subsequently, we have incorporated a Multi-layer Perceptual Attention module (MPA) for each skip connection layer, facilitating the integration of hierarchical upsampled information to restore fine-grained details within the feature maps. The ResU-Net student model enhances network stability through the utilization of residual modules and optimizes skip connections to recover any lost image information during convolutional operations. Lastly, we conducted experimental assessments on multiple disease datasets. The results reveal that the ACC of the Res-Transformer model achieves an impressive 96.9%. Furthermore, through the knowledge distillation method, rich knowledge is effectively transferred to the ResU-Net model, resulting in a remarkable ACC improvement of 7.2%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001688",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Distillation",
      "Electrical engineering",
      "Engineering",
      "Residual",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Song",
        "given_name": "Yucheng"
      },
      {
        "surname": "Wang",
        "given_name": "Jincan"
      },
      {
        "surname": "Ge",
        "given_name": "Yifan"
      },
      {
        "surname": "Li",
        "given_name": "Lifeng"
      },
      {
        "surname": "Guo",
        "given_name": "Jia"
      },
      {
        "surname": "Dong",
        "given_name": "Quanxing"
      },
      {
        "surname": "Liao",
        "given_name": "Zhifang"
      }
    ]
  },
  {
    "title": "FSRDiff: A fast diffusion-based super-resolution method using GAN",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104164",
    "abstract": "Single image super-resolution with diffusion probabilistic models (SRDiff) is a successful diffusion model for image super-resolution that produces high-quality images and is stable during training. However, due to the long sampling time, it is slower in the testing phase than other deep learning-based algorithms. Reducing the total number of diffusion steps can accelerate sampling, but it also causes the inverse diffusion process to deviate from the Gaussian distribution and exhibit a multimodal distribution, which violates the diffusion assumption and degrades the results. To overcome this limitation, we propose a fast SRDiff (FSRDiff) algorithm that integrates a generative adversarial network (GAN) with a diffusion model to speed up SRDiff. FSRDiff employs conditional GAN to approximate the multimodal distribution in the inverse diffusion process of the diffusion model, thus enhancing its sampling efficiency when reducing the total number of diffusion steps. The experimental results show that FSRDiff is nearly 20 times faster than SRDiff in reconstruction while maintaining comparable performance on the DIV2K test set.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001196",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Diffusion",
      "Diffusion process",
      "Filter (signal processing)",
      "Gaussian",
      "Innovation diffusion",
      "Knowledge management",
      "Mathematical optimization",
      "Mathematics",
      "Physics",
      "Probabilistic logic",
      "Quantum mechanics",
      "Sampling (signal processing)",
      "Thermodynamics"
    ],
    "authors": [
      {
        "surname": "Tang",
        "given_name": "Ni"
      },
      {
        "surname": "Zhang",
        "given_name": "Dongxiao"
      },
      {
        "surname": "Gao",
        "given_name": "Juhao"
      },
      {
        "surname": "Qu",
        "given_name": "Yanyun"
      }
    ]
  },
  {
    "title": "Maximum open-set entropy optimization via uncertainty measure for universal domain adaptation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104169",
    "abstract": "Universal Domain Adaptation (UniDA) is a technology that enables the intelligent model to transfer knowledge learned from labeled source domains to related but unlabeled target domains without any prior label set relationship. The key to UniDA lies in rejecting target domain-specific “unknown” samples to achieve domain alignment on shared classes. In this paper, we propose the Maximum Open-set Entropy Optimization via uncertainty measure to adaptively reject “unknown” samples. Specifically, MOEO sets a transition region within the most easily confused class in the open-set space. We optimize the maximum open-set entropy of simple samples outside the transition region to further improve their confidence and separate difficult samples within the transition region by aggregating similar neighbors. Accordingly, a secure boundary is formed between shared samples and “unknown” samples, promoting domain alignment within shared classes. Experiments on four benchmarks show that MOEO outperforms the previous state-of-the-art significantly.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032400124X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Boundary (topology)",
      "Class (philosophy)",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Discrete mathematics",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Entropy (arrow of time)",
      "Mathematical analysis",
      "Mathematics",
      "Measure (data warehouse)",
      "Open set",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Ai",
        "given_name": "Weiwei"
      },
      {
        "surname": "Yang",
        "given_name": "Zhao"
      },
      {
        "surname": "Chen",
        "given_name": "Zhiyong"
      },
      {
        "surname": "Hu",
        "given_name": "Xiao"
      }
    ]
  },
  {
    "title": "MCT-VHD: Multi-modal contrastive transformer for video highlight detection",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104162",
    "abstract": "Autonomous highlight detection aims to identify the most captivating moments in a video, which is crucial for enhancing the efficiency of video editing and browsing on social media platforms. However, current efforts primarily focus on visual elements and often overlook other modalities, such as text information that could provide valuable semantic signals. To overcome this limitation, we propose a Multi-modal Contrastive Transformer for Video Highlight Detection (MCT-VHD). This transformer-based network mainly utilizes video and audio modalities, along with auxiliary text features (if exist) for video highlight detection. Specifically, We enhance the temporal connections within the video by integrating a convolution-based local enhancement module into the transformer blocks. Furthermore, we explore three multi-modal fusion strategies to improve highlight inference performance and employ a contrastive objective to facilitate interactions between different modalities. Comprehensive experiments conducted on three benchmark datasets validate the effectiveness of MCT-VHD, and our ablation studies provide valuable insights into its essential components.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001172",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Chemistry",
      "Computer science",
      "Geodesy",
      "Geography",
      "Inference",
      "Modal",
      "Modalities",
      "Physics",
      "Polymer chemistry",
      "Quantum mechanics",
      "Social science",
      "Sociology",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Jiang",
        "given_name": "Yinhui"
      },
      {
        "surname": "Luo",
        "given_name": "Sihui"
      },
      {
        "surname": "Guo",
        "given_name": "Lijun"
      },
      {
        "surname": "Zhang",
        "given_name": "Rong"
      }
    ]
  },
  {
    "title": "Convolution-transformer blend pyramid network for underwater image enhancement",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104163",
    "abstract": "Underwater images suffer from different types of degradation, where color degradation occurs in the spatial domain and edge degradation in the frequency domain. The high-quality underwater image enhancement represents a crucial milestone in advancing computer vision systems tailored for marine environments. This foundational endeavor encompasses a wide array of applications in computer vision tasks, including underwater inspection, underwater archaeology, and environmental monitoring. However, current convolutional neural network (CNN)-based pyramid frameworks primarily focus on capturing local features, often overlooking the significance of global semantic features that play a crucial role in understanding underwater scenes. Moreover, these frameworks handle spatial and frequency features independently, failing to enhance images by exploring correlation among domain-specific attributes for enabling information consistency. Besides, optimizing the model using a loss function with the same domain attributes from the ground truth may not lead to a better generalization ability. To solve these problems, we propose a new Convolution-Transformer Blend Pyramid Network (CTPN), which consists of a spatial branch and several frequency branches. The CTPN has four key components: a Swin transformer encoder, a CNN-Transformer aggregated encoder–decoder (CTED) and a blend pyramid framework. The Swin transformer encoder is employed to capture global semantic features, benefiting from its ability to extract long-range and global dependencies among features. The CTED fuses local features captured by CNN layers and global semantic features captured by the Swin transformer encoder in the spatial branch, with the help of the Cross-Model Fusion Module (CFM) and Skip-Aggregation Module (SAM). Subsequently, a blend pyramid framework is designed which not only progressively expands the transformed information of the previous domain branch to the current domain branch via the CTED-based refining operation, but also utilizes the proposed Domain Affinity Block (DAB) to explore the connection between domain attributes, ensuring information consistency. The experimental results demonstrate that the proposed method outperforms existing underwater image enhancement methods quantitatively and qualitatively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001184",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Electrical engineering",
      "Encoder",
      "Engineering",
      "Geology",
      "Oceanography",
      "Operating system",
      "Pattern recognition (psychology)",
      "Transformer",
      "Underwater",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Ma",
        "given_name": "Lunpeng"
      },
      {
        "surname": "Hong",
        "given_name": "Dongyang"
      },
      {
        "surname": "Yin",
        "given_name": "Shibai"
      },
      {
        "surname": "Deng",
        "given_name": "Wanqiu"
      },
      {
        "surname": "Yang",
        "given_name": "Yang"
      },
      {
        "surname": "Yang",
        "given_name": "Yee-Hong"
      }
    ]
  },
  {
    "title": "iMVS: Integrating multi-view information on multiple scales for 3D object recognition",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104175",
    "abstract": "3D object recognition is a fundamental task in 3D computer vision. View-based methods have received considerable attention due to their high efficiency and superior performance. To better capture the long-range dependencies among multi-view images, Transformer has recently been introduced into view-based 3D object recognition and achieved excellent performance. However, the information among views on multiple scales is not utilized sufficiently in the existing Transformer-based methods. To address this limitation, we proposed a 3D object recognition method named iMVS to integrate Multi-View information on multiple Scales. Specifically, for the single-view image/features at each scale, we adopt a hybrid feature extraction module consisting of CNN and Transformer to jointly capture local and non-local information. For the extracted multi-view image features at each scale, we develop a feature transfer module including a view Transformer block to achieve the information transfer across views. Following a sequential process of the single-view feature extraction and multi-view feature transfer on multiple scales, the multi-view information is sufficiently interacted. Subsequently, the multi-scale features with multi-view information are fed into our designed feature aggregation module to generate a category-specific descriptor, where the adopted channel Transformer block facilitates the descriptor to be more expressive. Coupling with these designs, our method can fully exploit the information embedded within multi-view images. Experimental results on ModelNet40, ModelNet10 and a real-world dataset MVP-N demonstrate the superior performance of our method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001305",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Object (grammar)"
    ],
    "authors": [
      {
        "surname": "Jiang",
        "given_name": "Jiaqin"
      },
      {
        "surname": "Liu",
        "given_name": "Zhao"
      },
      {
        "surname": "Li",
        "given_name": "Jie"
      },
      {
        "surname": "Tu",
        "given_name": "Jingmin"
      },
      {
        "surname": "Li",
        "given_name": "Li"
      },
      {
        "surname": "Yao",
        "given_name": "Jian"
      }
    ]
  },
  {
    "title": "Multi-scale recurrent attention gated fusion network for single image dehazing",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104171",
    "abstract": "The purpose of single image dehazing is to eliminate the bad influence of haze on images, so as to maintain more scene information of images. In recent years, the convolutional neural networks (CNN) have made significant contributions to single image dehazing. However, the visual quality of dehazed images still needs to be further improved. In view of the problems of single-scale shallow image feature extraction and the insufficient use of intermediate layer features in existing dehazing networks, we propose an end-to-end Multi-scale Recurrent Attention Gated Fusion Network (MRAGFN) to address the image dehazing task. We cascade three Dual Attention Fusion (DAF) modules to progressively form three haze-relevant features map, meanwhile, we adopt downsampling operation on the input to produce global feature map, which are used to weight the three feature maps to compensate for the missing of single-scale feature information. We present Feature Enhancement Module (FEM) to enhance the feature representation ability of these weighted feature maps. We design Recurrent Attention Gated Fusion (RAGF) module by adding attention mechanism and gating mechanism to gradually obtain more refined features based on these weighted features while eliminating redundant features. Experimental results on different hazy images demonstrate that the proposed dehazing network can restore the haze-free images and perform better than the state-of-the-art dehazing networks in terms of the objective indicators (such as PSNR, SSIM)and the subjective visual quality.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001263",
    "keywords": [
      "Artificial intelligence",
      "Attention network",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Feature extraction",
      "Haze",
      "Image (mathematics)",
      "Image quality",
      "Layer (electronics)",
      "Linguistics",
      "Meteorology",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Scale (ratio)",
      "Upsampling"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Xiangfen"
      },
      {
        "surname": "Yang",
        "given_name": "Shuo"
      },
      {
        "surname": "Zhang",
        "given_name": "Qingyi"
      },
      {
        "surname": "Yuan",
        "given_name": "Feiniu"
      }
    ]
  },
  {
    "title": "Scientific mapping and bibliometric analysis of research advancements in underwater image enhancement",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104166",
    "abstract": "Underwater Image Enhancement (UIE) addresses challenges in marine resource exploitation such as absorption, noise, and low contrast. This paper employs bibliometric methods and data from Web of Science to analyze UIE literature. The analysis reveals that research on UIE entered a rapid development stage starting from 2017. Three main areas of technical support for research: underwater image enhancement methods, deep learning-based methods, and underwater image restoration and depth estimation. Hotspots in research include enhancement, deep learning, restoration, feature extraction, and quality assessment. Drawing from the bibliometric analysis, this paper proposes a conceptual framework for UIE research and presents five research suggestions: enhancing method robustness and adaptability, improving real-time performance, integrating software and hardware, establishing underwater image benchmark datasets, and refining quality evaluation systems for underwater images. This study offers valuable references and guidance for the future exploration and development of the UIE field.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001214",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Computer science",
      "Data science",
      "Geography",
      "Image (mathematics)",
      "Image enhancement",
      "Underwater"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Yong"
      },
      {
        "surname": "Chen",
        "given_name": "Renzhang"
      }
    ]
  },
  {
    "title": "Structure based transmission estimation in single image dehazing",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104161",
    "abstract": "The single image dehazing (SID) is challenging because of ill-posed characteristic, since there exist multiple possible dehazed images for a single hazy image due to multiple possible values of transmission. The SID is solved using scattering model, which requires computation of two parameters (transmission and atmospheric light) for its solution. The existing methods have presented priors and techniques to estimate transmission with limited focus on structure of the transmission. This paper proposes a lower bound on transmission using structural measure. The proposed lower bound is utilized to estimate the value of transmission, while preserving structural. Further, a quality control parameter is introduced to ensure non-negative value of the transmission for images with brighter objects than atmospheric light. The accuracy and efficiency of the proposed method is established by comparing with renowned SID methods using benchmark datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001160",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Economics",
      "Estimation",
      "Image (mathematics)",
      "Management",
      "Telecommunications",
      "Transmission (telecommunications)"
    ],
    "authors": [
      {
        "surname": "Raikwar",
        "given_name": "Suresh"
      },
      {
        "surname": "Tapaswi",
        "given_name": "Shashikala"
      },
      {
        "surname": "Sharma",
        "given_name": "Rajendra Kumar"
      }
    ]
  },
  {
    "title": "Heterogeneity constrained color ellipsoid prior image dehazing algorithm",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104177",
    "abstract": "To address the issue of inaccurate transmission estimation in areas with sudden depth changes in hazy images, the image dehazing algorithm with Heterogeneity Constrained Color Ellipsoid Prior (HC-CEP) is proposed. Firstly, a local threshold optimization quadtree search method is designed, in order to solve the problem of inaccurate global atmospheric light estimation and avoid the overall darkening of the image caused by white objects.Then, different regions are selected by a local block heterogeneity window to construct diversity prior vectors to estimate the initial transmission. Finally, by constructing neighborhood heterogeneity weighted constraints on each pixel of the initial transmission and improving the high-order differential filtering of the Scharr operator, the optimization of the transmission is achieved, eliminating halo artifacts in the depth mutation area of the image. Both qualitative and quantitative experimental results show that the proposed algorithm comprehensively considers the heterogeneity of local pixel characteristics, and the dehazing image obtained has better color fidelity and edge detail preservation ability, which can effectively eliminate halo artifacts on the edges of hazy images.The proposed method has better dehazing performance and has a 8% improvement in terms of the FADE metric compared to state-of-the-art dehazing methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001329",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Ellipsoid",
      "Geodesy",
      "Geology",
      "Image (mathematics)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yuxi"
      },
      {
        "surname": "Hu",
        "given_name": "Jing"
      },
      {
        "surname": "Zhang",
        "given_name": "Rongguo"
      },
      {
        "surname": "Wang",
        "given_name": "Lifang"
      },
      {
        "surname": "Zhang",
        "given_name": "Rui"
      },
      {
        "surname": "Liu",
        "given_name": "Xiaojun"
      }
    ]
  },
  {
    "title": "6-DoF grasp estimation method that fuses RGB-D data based on external attention",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104173",
    "abstract": "6-DoF grasp estimation methods based on point clouds have long been a challenge in robotics due to the limitations of single data input, which hinder the robot’s perception of real-world scenarios, thus reducing its robustness. In this work, we propose a 6-DoF grasp pose estimation method based on RGB-D data, which leverages ResNet to extract color image features, utilizes the PointNet++ network to extract geometric information features, and employs an external attention mechanism to fuse both features. Our method is an end-to-end design, and we validate its performance through benchmark tests on a large-scale dataset and evaluations in a simulated robot environment. Our method outperforms previous state-of-the-art methods on public datasets, achieving 47.75mAP and 40.08mAP for seen and unseen objects, respectively. We also test our grasp pose estimation method on multiple objects in a simulated robot environment, demonstrating that our approach exhibits higher grasp accuracy and robustness than previous methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001287",
    "keywords": [
      "Artificial intelligence",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Engineering",
      "Estimation",
      "GRASP",
      "Programming language",
      "RGB color model",
      "Systems engineering"
    ],
    "authors": [
      {
        "surname": "Ran",
        "given_name": "Haosong"
      },
      {
        "surname": "Chen",
        "given_name": "Diansheng"
      },
      {
        "surname": "Chen",
        "given_name": "Qinshu"
      },
      {
        "surname": "Li",
        "given_name": "Yifei"
      },
      {
        "surname": "Luo",
        "given_name": "Yazhe"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiaoyu"
      },
      {
        "surname": "Li",
        "given_name": "Jiting"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiaochuan"
      }
    ]
  },
  {
    "title": "Few-shot defect classification via feature aggregation based on graph neural network",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104172",
    "abstract": "The effectiveness of deep learning models is greatly dependent on the availability of a vast amount of labeled data. However, in the realm of surface defect classification, acquiring and annotating defect samples proves to be quite challenging. Consequently, accurately predicting defect types with only a limited number of labeled samples has emerged as a prominent research focus in recent years. Few-shot learning, which leverages a restricted sample set in the support set, can effectively predict the categories of unlabeled samples in the query set. This approach is particularly well-suited for defect classification scenarios. In this article, we propose a transductive few-shot surface defect classification method, which using both the instance-level relations and distribution-level relations in each few-shot learning task. Furthermore, we calculate class center features in transductive manner and incorporate them into the feature aggregation operation to rectify the positioning of edge samples in the mapping space. This adjustment aims to minimize the distance between samples of the same category, thereby mitigating the influence of unlabeled samples at category boundary on classification accuracy. Experimental results on the public dataset show the outstanding performance of our proposed approach compared to the state-of-the-art methods in the few-shot learning settings. Our code is available at https://github.com/Harry10459/CIDnet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001275",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Chemistry",
      "Chromatography",
      "Class (philosophy)",
      "Code (set theory)",
      "Computer science",
      "Decision boundary",
      "Feature (linguistics)",
      "Feature vector",
      "Focus (optics)",
      "Graph",
      "Labeled data",
      "Linguistics",
      "Machine learning",
      "Optics",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Programming language",
      "Sample (material)",
      "Set (abstract data type)",
      "Shot (pellet)",
      "Support vector machine",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Pengcheng"
      },
      {
        "surname": "Zheng",
        "given_name": "Peixiao"
      },
      {
        "surname": "Guo",
        "given_name": "Xin"
      },
      {
        "surname": "Chen",
        "given_name": "Enqing"
      }
    ]
  },
  {
    "title": "Reversible data hiding with automatic contrast enhancement for color images",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104181",
    "abstract": "Automatic contrast enhancement (ACE) is a technique that can automatically enhance the image contrast. Reversible data hiding (RDH) with ACE (ACERDH) can achieve ACE while hiding data. However, some methods with good performance for color images suffer from insufficient enhancement. Therefore, an ACERDH method based on the R, G, B, and V channels enhancement is proposed. First, histogram shifting with contrast control is proposed to enhance the R, G, and B channels. It can prevent contrast degradation and histogram shifting from stopping prematurely. Then, the V channel is enhanced. Since some RDH methods with non-ACE that can well enhance the V channel have a low automation level, histogram shifting with brightness control that can realize ACE very well is proposed. It can effectively avoid over-enhancement by controlling the brightness. Experimental results verify that the proposed method improves the image quality and embedding capability better than some state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001366",
    "keywords": [
      "Adaptive histogram equalization",
      "Artificial intelligence",
      "Brightness",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Contrast (vision)",
      "Contrast enhancement",
      "Embedding",
      "Histogram",
      "Histogram equalization",
      "Image (mathematics)",
      "Information hiding",
      "Magnetic resonance imaging",
      "Medicine",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Radiology"
    ],
    "authors": [
      {
        "surname": "Han",
        "given_name": "Libo"
      },
      {
        "surname": "Ren",
        "given_name": "Yanzhao"
      },
      {
        "surname": "Tao",
        "given_name": "Sha"
      },
      {
        "surname": "Zhang",
        "given_name": "Xinfeng"
      },
      {
        "surname": "Gao",
        "given_name": "Wanlin"
      }
    ]
  },
  {
    "title": "Memory-guided representation matching for unsupervised video anomaly detection",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104185",
    "abstract": "Recent works on Video Anomaly Detection (VAD) have made advancements in the unsupervised setting, known as Unsupervised VAD (UVAD), which brings it closer to practical applications. Unlike the classic VAD task that requires a clean training set with only normal events, UVAD aims to identify abnormal frames without any labeled normal/abnormal training data. Many existing UVAD methods employ handcrafted surrogate tasks, such as frame reconstruction, to address this challenge. However, we argue that these surrogate tasks are sub-optimal solutions, inconsistent with the essence of anomaly detection. In this paper, we propose a novel approach for UVAD that directly detects anomalies based on similarities between events in videos. Our method generates representations for events while simultaneously capturing prototypical normality patterns, and detects anomalies based on whether an event’s representation matches the captured patterns. The proposed model comprises a memory module to capture normality patterns, and a representation learning network to obtain representations matching the memory module for normal events. A pseudo-label generation module as well as an anomalous event generation module for negative learning are further designed to assist the model to work under the strictly unsupervised setting. Experimental results demonstrate that the proposed method outperforms existing UVAD methods and achieves competitive performance compared with classic VAD methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001408",
    "keywords": [
      "Anomaly detection",
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Event (particle physics)",
      "Frame (networking)",
      "Law",
      "Machine learning",
      "Matching (statistics)",
      "Mathematics",
      "Normality",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Politics",
      "Psychiatry",
      "Psychology",
      "Quantum mechanics",
      "Representation (politics)",
      "Statistics",
      "Telecommunications",
      "Unsupervised learning"
    ],
    "authors": [
      {
        "surname": "Tao",
        "given_name": "Yiran"
      },
      {
        "surname": "Hu",
        "given_name": "Yaosi"
      },
      {
        "surname": "Chen",
        "given_name": "Zhenzhong"
      }
    ]
  },
  {
    "title": "On the use of GNN-based structural information to improve CNN-based semantic image segmentation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104167",
    "abstract": "Convolutional neural networks (CNNs) are widely used for semantic image segmentation across various fields (medicine, robotics), capturing local pixel dependencies for good results. Nevertheless, CNNs struggle to grasp global contextual representations, sometimes leading to structural inconsistencies. Recent approaches aim to broaden their scope using attention mechanisms or deep models, resulting in heavy-weight architectures. To boost CNN performance in semantic segmentation, we propose using a graph neural network (GNN) as a post-processing step. The GNN conducts node classification on appropriately coarsened graphs encoding class probabilities and structural information related to regions segmented by the CNN. The proposal, applicable to any CNN producing a segmentation map, is evaluated on several CNN architectures, using two public datasets (FASSEG and IBSR), with four graph convolution operators. Results reveal performance improvements, enhancing on average the Hausdorff distance by 24.3% on FASSEG and by 74.0% on IBSR. Furthermore, our approach demonstrates resilience to small training datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001226",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "GRASP",
      "Graph",
      "Image segmentation",
      "Pattern recognition (psychology)",
      "Pixel",
      "Programming language",
      "Segmentation",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Coupeau",
        "given_name": "Patty"
      },
      {
        "surname": "Fasquel",
        "given_name": "Jean-Baptiste"
      },
      {
        "surname": "Dinomais",
        "given_name": "Mickaël"
      }
    ]
  },
  {
    "title": "Weakly supervised semantic segmentation based on superpixel affinity",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104168",
    "abstract": "Weakly supervised semantic segmentation (WSSS) usually employs the method of modifying and extending class activation map (CAM) seeds to achieve semantic segmentation. However, the down-sampling of the network weakens the edge awareness of CAMs, leading to under- or over-activation problems, which affects the segmentation quality. Considering the excellent contour attachment property of superpixels and the high semantic similarity between pixels within the same superpixel, we propose a superpixel affinity-based method that uses multi-scale features to aggregate superpixels with the same semantics, providing complete localization supervision for the generation of CAMs. In order to improve the accuracy of semantic labels for superpixels, we utilize a method of deep feature reorganization to improve the quality of network-generated CAM seeds. The experimental results indicate that the proposed method has achieved satisfactory performance on PASCAL VOC 2012 and MS COCO 2014 datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001238",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Feature (linguistics)",
      "Linguistics",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Segmentation",
      "Semantic feature",
      "Semantics (computer science)"
    ],
    "authors": [
      {
        "surname": "Fu",
        "given_name": "Yun"
      },
      {
        "surname": "Wang",
        "given_name": "Wenwu"
      },
      {
        "surname": "Zhu",
        "given_name": "Lei"
      },
      {
        "surname": "Ye",
        "given_name": "Xinyue"
      },
      {
        "surname": "Yue",
        "given_name": "Huagang"
      }
    ]
  },
  {
    "title": "Mask-guided discriminative feature network for occluded person re-identification",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104178",
    "abstract": "In recent years, although research on person re-identification (ReID) has made significant progress, occluded person ReID remains a major challenge. In real-world scenes, persons are often occluded by various obstacles such as vehicles, umbrellas, and other persons. This leads to the noisy interference and the loss of visual information, which result in poor ReID performance of occluded persons. To address this issue, we propose an end-to-end Mask-guided Discriminative Feature Network (MDFNet). First, MDFNet adopts a dual-branch architecture with a shared encoder as the Feature Extraction module for paired images. Each pair of images consist of one image from the training set and its corresponding occluded image generated through an occlusion augmentation strategy. Second, MDFNet utilizes a Mask-guided Discriminative Feature Enhancement and Fusion (MDFEF) module to achieve the fusion and enhancement of global and local features for high-quality person representations. MDFEF module effectively suppresses the interference of occlusion, enriches the representation capacity of person features, and enables the model to focus more on the important features in non-occluded regions. Furthermore, MDFNet introduces a sparse pairwise loss for enabling the model to dynamically adapt to intra-class variations and reduce the negative impact of complex occlusions. The experimental results on four challenging person ReID datasets demonstrate the effectiveness of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001330",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Computer vision",
      "Discriminative model",
      "Feature (linguistics)",
      "Identification (biology)",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Zhong",
        "given_name": "Fujin"
      },
      {
        "surname": "Wang",
        "given_name": "Yunhe"
      },
      {
        "surname": "Yu",
        "given_name": "Hong"
      },
      {
        "surname": "Hu",
        "given_name": "Jun"
      },
      {
        "surname": "Yang",
        "given_name": "Yan"
      }
    ]
  },
  {
    "title": "Multi-hop graph transformer network for 3D human pose estimation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104174",
    "abstract": "Accurate 3D human pose estimation is a challenging task due to occlusion and depth ambiguity. In this paper, we introduce a multi-hop graph transformer network designed for 2D-to-3D human pose estimation in videos by leveraging the strengths of multi-head self-attention and multi-hop graph convolutional networks with disentangled neighborhoods to capture spatio-temporal dependencies and handle long-range interactions. The proposed network architecture consists of a graph attention block composed of stacked layers of multi-head self-attention and graph convolution with learnable adjacency matrix, and a multi-hop graph convolutional block comprised of multi-hop convolutional and dilated convolutional layers. The combination of multi-head self-attention and multi-hop graph convolutional layers enables the model to capture both local and global dependencies, while the integration of dilated convolutional layers enhances the model’s ability to handle spatial details required for accurate localization of the human body joints. Extensive experiments demonstrate the effectiveness and generalization ability of our model, achieving competitive performance on benchmark datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001299",
    "keywords": [
      "Adjacency matrix",
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Graph",
      "Pattern recognition (psychology)",
      "Physics",
      "Pose",
      "Quantum mechanics",
      "Theoretical computer science",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Islam",
        "given_name": "Zaedul"
      },
      {
        "surname": "Ben Hamza",
        "given_name": "A."
      }
    ]
  },
  {
    "title": "A self-supervised image aesthetic assessment combining masked image modeling and contrastive learning",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104184",
    "abstract": "Learning more abundant image features helps improve the image aesthetic assessment task performance. Masked Image Modeling (MIM) is implemented based on the Vision Transformer (ViT), which learns pixel-level features while reconstructing images. Contrastive learning pulls in the same image features while pushing away different image features in the feature space to learn high-level semantic features. Since contrastive learning and MIM capture different levels of image features, combining these two methods could learn more rich feature representations and thus promote the performance of aesthetic assessment. Therefore, we propose a pretext task combining contrastive learning and MIM with learning richer image features. In this approach, the original image is randomly masked and reconstructed on the online network. The reconstructed and original images composition the positive example to calculate the contrastive loss on the target network. In the experiment on the AVA dataset, our method obtained better performance than the baseline.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001391",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Psychology"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Shuai"
      },
      {
        "surname": "Wang",
        "given_name": "Zibei"
      },
      {
        "surname": "Wang",
        "given_name": "Guangao"
      },
      {
        "surname": "Ke",
        "given_name": "Yongzhen"
      },
      {
        "surname": "Qin",
        "given_name": "Fan"
      },
      {
        "surname": "Guo",
        "given_name": "Jing"
      },
      {
        "surname": "Chen",
        "given_name": "Liming"
      }
    ]
  },
  {
    "title": "3D hand pose estimation and reconstruction based on multi-feature fusion",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104160",
    "abstract": "3D hand pose estimation and shape reconstruction is to recover the hand joint points and hand mesh vertices coordinates from the image. However, existing methods usually only use the high-level semantic features extracted by the backbone network to represent the hand mesh vertex features, which leads to a single representation of the hand vertices features and cannot fully utilize the feature information extracted by the network. In this paper, we propose a method for real-time 3D reconstruction of hands from a single RGB image, which enriches the 3D semantic information of the mesh vertices through multi-feature fusion. Firstly, we regress the 2D features of mesh vertices through Integral Pose Regression (IPR) and regard them as prior information to 3D features. Then we design a Multi-Scale Sampling(MSS) module to extract multi-scale information. Finally we fuse 2D prior features, multi-scale features, and high-level semantic features extracted by backbone to represent 3D initial feature. Additionally, we propose a Multi-Root(MR) loss function to address the imbalance problem caused by a single root joint. The experimental results indicate that our network achieves competitive performance on the FreiHAND and HO-3D public datasets, achieving fast inference speed with fewer parameters.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001159",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Engineering",
      "Estimation",
      "Feature (linguistics)",
      "Fusion",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pose",
      "Systems engineering"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Jiye"
      },
      {
        "surname": "Xiang",
        "given_name": "Xuezhi"
      },
      {
        "surname": "Ding",
        "given_name": "Shuai"
      },
      {
        "surname": "El Saddik",
        "given_name": "Abdulmotaleb"
      }
    ]
  },
  {
    "title": "Adaptive HEVC video steganograhpy based on PU partition modes",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104176",
    "abstract": "High EfficiencyVideoCoding (HEVC) −based steganography has gained attention as a prominent research focus. Especially, block structure based HEVC video steganography has received increasing attention due to commendable performance. However, current block structure- based steganography algorithms confront with challenges such as reduced coding efficiency and limited capacity. To avoid these problems, an adaptive video steganography algorithm based on Prediction Unit (PU) partition mode in I-frames is proposed. This is done through the analysis of the block division process and the visual distortion resulting from the modification of the PU partition mode in HEVC. The PU block structure is utilized as steganographic covers, and the Rate Distortion Optimization (RDO) technique is introduced to establish an adaptive distortion function for Syndrome-trellis code (STC). Further comparison is performed between the proposed method and the state-of-the-art steganography algorithms, confirming its advantages in embedding capacity, compression efficiency, visual quality, and resistance to video steganalysis.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001317",
    "keywords": [
      "Algorithm",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Mathematics",
      "Partition (number theory)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Shanshan"
      },
      {
        "surname": "Xu",
        "given_name": "Dawen"
      },
      {
        "surname": "He",
        "given_name": "Songhan"
      }
    ]
  },
  {
    "title": "On the use of GNN-based structural information to improve CNN-based semantic image segmentation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104167",
    "abstract": "Convolutional neural networks (CNNs) are widely used for semantic image segmentation across various fields (medicine, robotics), capturing local pixel dependencies for good results. Nevertheless, CNNs struggle to grasp global contextual representations, sometimes leading to structural inconsistencies. Recent approaches aim to broaden their scope using attention mechanisms or deep models, resulting in heavy-weight architectures. To boost CNN performance in semantic segmentation, we propose using a graph neural network (GNN) as a post-processing step. The GNN conducts node classification on appropriately coarsened graphs encoding class probabilities and structural information related to regions segmented by the CNN. The proposal, applicable to any CNN producing a segmentation map, is evaluated on several CNN architectures, using two public datasets (FASSEG and IBSR), with four graph convolution operators. Results reveal performance improvements, enhancing on average the Hausdorff distance by 24.3% on FASSEG and by 74.0% on IBSR. Furthermore, our approach demonstrates resilience to small training datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001226",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "GRASP",
      "Graph",
      "Image segmentation",
      "Pattern recognition (psychology)",
      "Pixel",
      "Programming language",
      "Segmentation",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Coupeau",
        "given_name": "Patty"
      },
      {
        "surname": "Fasquel",
        "given_name": "Jean-Baptiste"
      },
      {
        "surname": "Dinomais",
        "given_name": "Mickaël"
      }
    ]
  },
  {
    "title": "Weakly supervised semantic segmentation based on superpixel affinity",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104168",
    "abstract": "Weakly supervised semantic segmentation (WSSS) usually employs the method of modifying and extending class activation map (CAM) seeds to achieve semantic segmentation. However, the down-sampling of the network weakens the edge awareness of CAMs, leading to under- or over-activation problems, which affects the segmentation quality. Considering the excellent contour attachment property of superpixels and the high semantic similarity between pixels within the same superpixel, we propose a superpixel affinity-based method that uses multi-scale features to aggregate superpixels with the same semantics, providing complete localization supervision for the generation of CAMs. In order to improve the accuracy of semantic labels for superpixels, we utilize a method of deep feature reorganization to improve the quality of network-generated CAM seeds. The experimental results indicate that the proposed method has achieved satisfactory performance on PASCAL VOC 2012 and MS COCO 2014 datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001238",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Feature (linguistics)",
      "Linguistics",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Segmentation",
      "Semantic feature",
      "Semantics (computer science)"
    ],
    "authors": [
      {
        "surname": "Fu",
        "given_name": "Yun"
      },
      {
        "surname": "Wang",
        "given_name": "Wenwu"
      },
      {
        "surname": "Zhu",
        "given_name": "Lei"
      },
      {
        "surname": "Ye",
        "given_name": "Xinyue"
      },
      {
        "surname": "Yue",
        "given_name": "Huagang"
      }
    ]
  },
  {
    "title": "Image dehazing using non-local haze-lines and multi-exposure fusion",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104145",
    "abstract": "Images captured under haze conditions suffer from color distortion and low saturation due to the light propagates through scattering particles, causing light intensity attenuation and direction deflection, which affects the imaging quality of the visual system. To deal with these issues, we propose an image dehazing method based on non-local haze-line and multi-exposure fusion, called NHMF. Specifically, we first equalize the brightness and color of the input image according to a multi-scale fusion strategy. Meanwhile, we use the dark channel prior based on a local window to solve the atmospheric light value. Afterward, we employ the preprocessed image to obtain fog lines with better generalization performance that enhances the estimation of the transmission rate. Furthermore, we introduce weighted least-squares filtering to refine transmittance estimation accuracy further and ultimately employ an atmospheric scattering model to reverse process the haze-free image. Our extensive experiments on three image enhancement datasets demonstrate the effectiveness of our approach in quantitative and qualitative dehazing of images with haze. Moreover, our method exhibits excellent generalization performance in dehazing remote sensing images and enhancing underwater images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001007",
    "keywords": [
      "Artificial intelligence",
      "Attenuation",
      "Brightness",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Diffuse sky radiation",
      "Geology",
      "Haze",
      "Hue",
      "Image (mathematics)",
      "Image fusion",
      "Image processing",
      "Image restoration",
      "Meteorology",
      "Optics",
      "Physics",
      "Remote sensing",
      "Scattering"
    ],
    "authors": [
      {
        "surname": "Jin",
        "given_name": "Kaijie"
      },
      {
        "surname": "Li",
        "given_name": "Guohou"
      },
      {
        "surname": "Zhou",
        "given_name": "Ling"
      },
      {
        "surname": "Fan",
        "given_name": "Yuqian"
      },
      {
        "surname": "Jiang",
        "given_name": "Jiping"
      },
      {
        "surname": "Dai",
        "given_name": "Chenggang"
      },
      {
        "surname": "Zhang",
        "given_name": "Weidong"
      }
    ]
  },
  {
    "title": "Category-based depth incorporation for salient object ranking",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104165",
    "abstract": "Salient object ranking is a visual task that aims to mimic the human visual system’s ability to prioritize various salient objects in complex scenes. It integrates various computer vision tasks, including object detection and instance segmentation, and draws insights from psychology and biology. Despite recent advancements in salient object ranking research, there is still a need for a more in-depth exploration of certain underlying factors that influence the ranking. Previous studies have primarily focused on factors such as the scale and position of objects, as well as interactions between objects and their context. However, they often overlooked the crucial interaction between objects and observers. Unlike other visual tasks, salient object ranking is highly subjective and influenced by the observer. Therefore, establishing a meaningful connection between the observed object and the observer becomes crucial. Our model addresses this gap by placing emphasis on the distance between objects and observers. It investigates the implicit relationship between object categories and distance through the Category-Aware Attention (CAA) module. This innovative approach incorporates depth into salient object ranking, resulting in an improvement in ranking performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001202",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Mathematics",
      "Object (grammar)",
      "Ranking (information retrieval)",
      "Salient"
    ],
    "authors": [
      {
        "surname": "Zhai",
        "given_name": "Hanxiao"
      },
      {
        "surname": "Chen",
        "given_name": "Zhenxue"
      },
      {
        "surname": "Liu",
        "given_name": "Chengyun"
      },
      {
        "surname": "Bai",
        "given_name": "Huibin"
      },
      {
        "surname": "Wu",
        "given_name": "Q.M. Jonathan"
      }
    ]
  },
  {
    "title": "A review on infrared and visible image fusion algorithms based on neural networks",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104179",
    "abstract": "Infrared and visible image fusion represents a significant segment within the image fusion domain. The recent surge in image processing hardware advancements, including GPUs, TPUs, and cloud computing platforms, has facilitated the fusion of extensive datasets from multiple sensors. Given the remarkable proficiency of neural networks in image feature extraction and fusion, their application in infrared and visible image fusion has emerged as a prominent research area in recent years. This article begins by providing an overview of the current mainstream algorithms for infrared and visible image fusion based on neural networks, detailing the principles of various image fusion algorithms, their representative works, and their respective advantages and disadvantages. Subsequently, it introduces domain-relevant datasets, evaluation metrics, and some typical application scenarios. Finally, the article conducts qualitative and quantitative evaluations of the fusion results of various state-of-the-art algorithms and offers future research prospects based on experimental results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001342",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Fusion",
      "Image (mathematics)",
      "Image fusion",
      "Infrared",
      "Linguistics",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Kaixuan"
      },
      {
        "surname": "Xiang",
        "given_name": "Wei"
      },
      {
        "surname": "Chen",
        "given_name": "Zhenshuai"
      },
      {
        "surname": "Zhang",
        "given_name": "Jian"
      },
      {
        "surname": "Liu",
        "given_name": "Yunpeng"
      }
    ]
  },
  {
    "title": "Dual-domain joint optimization for universal JPEG steganography",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104151",
    "abstract": "JPEG steganography hides secret messages by modifying DCT coefficients in cover images to ensure the undetectability. Currently, most JPEG steganography methods rely on the additive distortion model, assigning adaptive costs to individual coefficients. However, these costs are often designed within a single domain, either DCT or decompressed domain, thereby making the hiding behavior detectable by multi-domain steganalysis features. To improve the steganography security, this paper proposes an universal Dual-Domain JPEG Additive Distortion (DDJAD) framework, which minimize additive distortion in both JPEG and decompressed domain concurrently. A measure to reduce the perceptibility of coefficient modifications within pixel blocks is first formulated and then incorporated into the traditional JPEG steganography model. Finally, the new additive distortion model is solved by fine-tuning existing costs, illustrating the universality of the proposed method. Experimental results showcase the superiority of the proposed method over previous works across various quality factors and payloads, employing typical steganalysis features.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001068",
    "keywords": [
      "Algorithm",
      "Architectural engineering",
      "Art",
      "Artificial intelligence",
      "Computer science",
      "Data compression",
      "Domain (mathematical analysis)",
      "Dual (grammatical number)",
      "Embedding",
      "Engineering",
      "JPEG",
      "Joint (building)",
      "Literature",
      "Mathematical analysis",
      "Mathematics",
      "Steganography",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Xiang"
      },
      {
        "surname": "Li",
        "given_name": "Xiaolong"
      },
      {
        "surname": "Zhao",
        "given_name": "Yao"
      },
      {
        "surname": "Cho",
        "given_name": "Hsunfang"
      }
    ]
  },
  {
    "title": "Patch-based tendency camera multi-constraint learning for unsupervised person re-identification",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104180",
    "abstract": "Unsupervised person re-identification (ReID) is a task that aims to retrieve pedestrians across different cameras from unlabeled data. Existing methods rely on clustering to generate pseudo-labels, but they are inevitably noisy. Although pseudo-label refinement approaches have been presented, the essentiality of patch contours is ignored. The tendency analysis of retrieval between global and patch features has not been well investigated. In this paper, we propose a Patch-based Tendency Camera Multi-Constraint Learning (PTCML) model for unsupervised person ReID. First, to explore the tendentious retrieval of global and patch features, we design a Ranking Tendency Similarity (RTS) score by gauging the distribution discrepancy of distance changes. Second, based on RTS score, we propose a Tendency-based Mutual Complementation (TMC) loss to improve the quality of global and patch pseudo-labels. Third, to resist camera variations, we propose an Adaptive Camera Multi-Constraint (ACM) loss to optimize recognition results with camera distribution constraint and instance constraint simultaneously. Finally, numerous experiments on Market-1501 and MSMT17 demonstrate that our method can significantly surpass the state-of-the-art performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001354",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Cluster analysis",
      "Computer science",
      "Constraint (computer-aided design)",
      "Economics",
      "Geometry",
      "Identification (biology)",
      "Image (mathematics)",
      "Machine learning",
      "Management",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Ranking (information retrieval)",
      "Similarity (geometry)",
      "Task (project management)",
      "Unsupervised learning"
    ],
    "authors": [
      {
        "surname": "Tao",
        "given_name": "Xuefeng"
      },
      {
        "surname": "Kong",
        "given_name": "Jun"
      },
      {
        "surname": "Jiang",
        "given_name": "Min"
      },
      {
        "surname": "Luo",
        "given_name": "Xi"
      },
      {
        "surname": "Liu",
        "given_name": "Tianshan"
      }
    ]
  },
  {
    "title": "A novel extended secret image sharing scheme based on sharing matrix",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104149",
    "abstract": "Ensuring the safety of communicating parties while sharing secret information and concealing their communication behavior in everyday activities has become an increasingly important issue. One approach to address this challenge is through a secret image sharing (SIS) scheme that utilizes meaningful shares. However, existing SIS techniques face ongoing challenges in enhancing the visual quality of those shares while ensuring lossless recovery of the secret image. To overcome these challenges, we propose a novel extended secret image sharing scheme (ESIS) based on a sharing matrix. By locating the secret pixel value within a search window centered on the coordinate of the pixel value pair of the cover images, our method achieves lossless recovery of the secret image with resulting shares that are natural images of continuous tone indistinguishable from original cover images by human visual perception. Extensive experimental results demonstrate its effectiveness and advantages, supported by theoretical analysis.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001044",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Cover (algebra)",
      "Cryptography",
      "Data compression",
      "Engineering",
      "Image (mathematics)",
      "Image sharing",
      "Lossless compression",
      "Mathematical analysis",
      "Mathematics",
      "Mechanical engineering",
      "Pixel",
      "Scheme (mathematics)",
      "Secret sharing",
      "Secure multi-party computation",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Xiaopeng"
      },
      {
        "surname": "Fu",
        "given_name": "Zhengxin"
      },
      {
        "surname": "Yu",
        "given_name": "Bin"
      }
    ]
  },
  {
    "title": "Blind cartoon image quality assessment based on local structure and chromatic statistics",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104152",
    "abstract": "With the help of computer-assisted systems, cartoon synthesis has become convenient and efficient by reusing existing cartoon materials. However, quality evaluation of the obtained cartoon image still relies on labor-intensive subjective judgment. This accordingly raises an urgent demand for effective quality evaluation methods to automatically select a cartoon image of high quality from a set of candidates with different parameter settings. In this paper, a new blind image quality assessment metric is developed for evaluating the perceptual quality of cartoon images by considering structure and chromatic distortions. The extracted gradient-based local structure features and multiscale chromatic statistical features are integrated into one representation for an overall perceptual quality prediction. Experimental results on two benchmark cartoon image datasets, i.e., NBU-CIQAD and HFUT-CID, indicate that the proposed metric outperforms both the state-of-the-art blind quality evaluation methods designed for natural images or synthetic images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032400107X",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Chromatic scale",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Economics",
      "Epistemology",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Image quality",
      "Law",
      "Mathematics",
      "Metric (unit)",
      "Neuroscience",
      "Operations management",
      "Pattern recognition (psychology)",
      "Perception",
      "Philosophy",
      "Political science",
      "Politics",
      "Programming language",
      "Quality (philosophy)",
      "Representation (politics)",
      "Scene statistics",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Hangwei"
      },
      {
        "surname": "Wang",
        "given_name": "Xuejin"
      },
      {
        "surname": "Shao",
        "given_name": "Feng"
      }
    ]
  },
  {
    "title": "A multi-view references image super-resolution framework for generating the large-FOV and high-resolution image",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104123",
    "abstract": "This paper investigates the generation of large field-of-view (FOV) and high-resolution (HR) panorama images for smartphones. Existing techniques like image stitching struggle to produce satisfactory results due to geometry misalignment and inconsistent appearance. To circumvent the inherent challenges of image stitching methods and generate high-quality panoramas, we treat the image stitching problem as a multiple-reference-based super-resolution problem. Specifically, one large-FOV low-resolution (LR) image and several overlapped small-FOV HR images are taken as inputs, where the LR image acts as a base and multi-view references provide rich HR information. Building on this foundation, a novel multi-view references image super-resolution framework (MVRefSR) is proposed. Within this framework, to address the residual geometric misalignment between the LR-Ref image pairs after coarse alignment, a flow-based RefSR network (FlowSRNet) is proposed, which super-resolves LR patches with corresponding HR references. To facilitate adaptive feature fusion and minimize distortion in structured regions, a fusion weight estimation module and a gradient branch are introduced in FlowSRNet. Finally, the large-FOV HR image is generated by combining these SR patches together. Furthermore, the lack of real-world RefSR datasets for smartphones is addressed by designing an innovative dataset construction pipeline. Extensive experiments demonstrate the superior performance of FlowSRNet and MVRefSR over the compared SR methods and image stitching software, which proves the effectiveness of generating panoramas from a new perspective.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000786",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Resolution (logic)",
      "Superresolution"
    ],
    "authors": [
      {
        "surname": "Jiang",
        "given_name": "Jiaqin"
      },
      {
        "surname": "Li",
        "given_name": "Li"
      },
      {
        "surname": "Tan",
        "given_name": "Bin"
      },
      {
        "surname": "Duan",
        "given_name": "Lunhao"
      },
      {
        "surname": "Yao",
        "given_name": "Jian"
      }
    ]
  },
  {
    "title": "Fine-grained image classification based on TinyVit object location and graph convolution network",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104120",
    "abstract": "Fine-grained image classification is a branch of image classification. Recently, vision transformer has made excellent progress in the field of image recognition. Its self-attention mechanism can extract very effective image feature information. However, feeding fixed-size image blocks into the network introduces additional noise, which is detrimental to extract discriminative features for fine-grained images. The vision transformer's network model is large, making it difficult to utilize in practice. Moreover, many of today's fine-grained image classification methods focus on mining discriminative features while ignoring the connections within the image. To address these problems, we propose a novel method based on the lightweight TinyVit backbone network. Our approach utilizes the self-attention weight values of TinyVit as a guide to construct an effective object location (OL) module that cuts and enlarges the object area, providing the network with the opportunity to concentrate on the local object. Additionally, we employ the graph convolutional network (GCN) to create a spatial relationship feature learning (SRFL) module that captures spatial context information between image blocks in TinyVit with the help of the transformer's self-attention weights. OL and SRFL collaborate to jointly guide the classification task. The experimental results show that the proposed method achieved competitive performance, with the second-highest classification faccuracy on both the CUB-200–2011 and NABirds datasets. When tested on the Stanford Dogs dataset, our approach outperformed many popular methods. Our code is uploaded on https://github.com/hhhj1999/SRFL_OL.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000750",
    "keywords": [
      "Artificial intelligence",
      "Backbone network",
      "Computer network",
      "Computer science",
      "Contextual image classification",
      "Convolutional neural network",
      "Discriminative model",
      "Feature (linguistics)",
      "Graph",
      "Image (mathematics)",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zheng",
        "given_name": "Shijie"
      },
      {
        "surname": "Wang",
        "given_name": "Gaocai"
      },
      {
        "surname": "Yuan",
        "given_name": "Yujian"
      },
      {
        "surname": "Huang",
        "given_name": "Shuqiang"
      }
    ]
  },
  {
    "title": "3D hand reconstruction via aggregating intra and inter graphs guided by prior knowledge for hand-object interaction scenario",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104129",
    "abstract": "Recently, 3D hand reconstruction has gained more attention in human–computer cooperation, especially for hand-object interaction scenario. However, it still remains challenge to improve estimation accuracy and ensure physical plausibility from a single RGB image. To overcome the challenge, we propose a 3D hand reconstruction network combining the benefits of model-based and model-free approaches to balance accuracy and physical plausibility for hand-object interaction scenario. Firstly, we present a novel topology-aware MANO pose parameters regression module from 2D joints directly. Moreover, we further carefully design a vertex-joint mutual graph-attention module guided by MANO to jointly refine hand meshes and joints, which model the dependencies of vertex-vertex and joint-joint and capture the correlation of vertex-joint for aggregating intra-graph and inter-graph node features respectively. The experimental results demonstrate that our method achieves state-of-the-art performance on recently benchmark datasets HO3DV2 and Dex-YCB, and outperforms all only model-based approaches or model-free approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000841",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Knowledge graph",
      "Mathematics",
      "Object (grammar)"
    ],
    "authors": [
      {
        "surname": "Shuang",
        "given_name": "Feng"
      },
      {
        "surname": "He",
        "given_name": "Wenbo"
      },
      {
        "surname": "Li",
        "given_name": "Shaodong"
      }
    ]
  },
  {
    "title": "AI-assisted deepfake detection using adaptive blind image watermarking",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104094",
    "abstract": "This paper proposes a new adaptive blind watermarking technology for deepfake detection, which can embed deepfake detection information into the image and verify the image's authenticity without requiring additional information. The proposed scheme utilizes mixed modulation combined with partly sign-altered mean value to embed a set of coefficients that enhance robustness against attacks while maintaining high image quality. Additionally, blind adaptive deepfake detection technology with the tamper detection mean value is employed to detect relative positions adaptively, even when face images are slightly modified or deepfaked. To further improve the performance of the proposed scheme, a gray wolf optimizer is introduced to optimize parameters, and a denoising autoencoder is employed to facilitate the identification of extracted watermarks. This technology will adaptively embed watermark information while preserving the original face image, thereby maintaining the authenticity of the face in the image and verifying the owner of the image. The code is available at https://github.com/lyhsu01/AwDD.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032400049X",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Code (set theory)",
      "Computer science",
      "Computer vision",
      "Digital watermarking",
      "Face (sociological concept)",
      "Gene",
      "Image (mathematics)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Programming language",
      "Robustness (evolution)",
      "Set (abstract data type)",
      "Social science",
      "Sociology",
      "Source code",
      "Watermark"
    ],
    "authors": [
      {
        "surname": "Hsu",
        "given_name": "Ling-Yuan"
      }
    ]
  },
  {
    "title": "Unsupervised single image dehazing — A contour approach",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104119",
    "abstract": "Small particles present in the air degrade the visual clarity of images due to light scattering phenomena caused by particles. This degradation of the image is in the context of attenuation of light intensity and poor contrast, which ultimately have an impact on the image’s quality. Thus, image dehazing is a necessity for better visualization and image analysis. The proposed method uses an unsupervised approach to dehaze the image without using any ground truth image or transmission map, which overcomes the necessity of a paired dataset and a true depth map. The majority of current state-of-the-art haze removal approaches observe color shifts in the sky region. By utilizing the contour process and RGB color plane data, the proposed innovative ”ContourDCP” technique can effectively identify the sky area. After applying the Dark Channel Prior (DCP) to obtain the initial transmission map, it is further reformulate through the contour method to achieve an accurate color representation of the sky area during the image dehazing process. We proposed atmospheric light estimation based on the haze concentration present in the Y-plane after conversion from RGB to YCbCr color space. These modified transmission map and atmospheric light are used in the atmospheric scattering model to recover the scene radiance. Performance evaluation of the method shows the robustness of the proposed method compared to existing state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000749",
    "keywords": [
      "Artificial intelligence",
      "Attenuation",
      "Color image",
      "Computer science",
      "Computer vision",
      "Diffuse sky radiation",
      "Geography",
      "Haze",
      "Image (mathematics)",
      "Image plane",
      "Image processing",
      "Meteorology",
      "Optics",
      "Physics",
      "RGB color model",
      "Radiance",
      "Remote sensing",
      "Scattering",
      "Sky",
      "YCbCr"
    ],
    "authors": [
      {
        "surname": "Dave",
        "given_name": "Chintan"
      },
      {
        "surname": "Patel",
        "given_name": "Hetal"
      },
      {
        "surname": "Kumar",
        "given_name": "Ahlad"
      }
    ]
  },
  {
    "title": "TransGANomaly: Transformer based Generative Adversarial Network for Video Anomaly Detection",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104108",
    "abstract": "Video anomaly detection aims to identify a set of abnormal events in videos. Deep reconstruction and prediction-based models have been employed to detect anomalies. Deep reconstruction models sometimes recreate the abnormal events along with the normal ones. However, the prediction-based approaches have demonstrated encouraging results. This paper presents a video vision transformer (ViViT) based generative adversarial network (GAN), TransGANomaly, a novel approach for detecting anomalies. The proposed framework is a video frame predictor and trained only on normal video data adversarially. The generator of the GAN is a ViViT network that receives 3D input tokens from the video snippets. The generator aims to predict the future frame based on past sequences. After that, the predicted and original frames are given to the model’s discriminator for binary classification. Extensive experiments have been performed on UCSD Pedestrian, CUHK Avenue, and ShanghiaTech datasets to validate the efficacy of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000634",
    "keywords": [
      "Adversarial system",
      "Anomaly detection",
      "Arithmetic",
      "Artificial intelligence",
      "Binary number",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Detector",
      "Discriminator",
      "Frame (networking)",
      "Generative adversarial network",
      "Generative grammar",
      "Generator (circuit theory)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Power (physics)",
      "Programming language",
      "Quantum mechanics",
      "Set (abstract data type)",
      "Telecommunications",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Aslam",
        "given_name": "Nazia"
      },
      {
        "surname": "Kolekar",
        "given_name": "Maheshkumar H."
      }
    ]
  },
  {
    "title": "Privacy-preserving face recognition method based on extensible feature extraction",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104140",
    "abstract": "Face recognition (FR) technique has become a pervasive and ubiquitous part of daily lives, from unlocking our smartphones with a glance to being scanned by surveillance cameras in various outdoor locations. When people’s face photos are uploaded to the cloud for face recognition processing, they often have legitimate concerns about the privacy and security of their biometric data. A number of privacy-preserving face recognition (PPFR) frameworks have been proposed to address these issues by enabling the cloud to perform face recognition without revealing the identity or features of the face photos. However, these frameworks suffer from several limitations. They rely on computationally intensive operations that increase the cost and time of face recognition, leading to less applications in the real-world scenario. Many current frameworks support only one face recognition method and cannot be extended to different models. To overcome these challenges, in this paper, we propose a PPFR framework with high recognition accuracy based on extensible feature extraction for different application scenarios. In particular, features are extracted by a selective model, such as MobileFaceNet, ResNet-18 or ResNet-50, and encrypted by a randomness-based encryption algorithm in both face owner and user. Cloud service provider (SP) performs face recognition by comparing the Euclidean distances between features received from the above two entities. Extensive experiments verify that the proposed framework has significant advantages in terms of accuracy and efficiency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000956",
    "keywords": [
      "Artificial intelligence",
      "Biometrics",
      "Cloud computing",
      "Computer science",
      "Computer security",
      "Data mining",
      "Encryption",
      "Face (sociological concept)",
      "Face Recognition Grand Challenge",
      "Face detection",
      "Facial recognition system",
      "Feature (linguistics)",
      "Feature extraction",
      "Linguistics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Social science",
      "Sociology",
      "Three-dimensional face recognition",
      "Upload",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Weitong"
      },
      {
        "surname": "Zhou",
        "given_name": "Di"
      },
      {
        "surname": "Zhu",
        "given_name": "Zhenxin"
      },
      {
        "surname": "Qiao",
        "given_name": "Tong"
      },
      {
        "surname": "Yao",
        "given_name": "Ye"
      },
      {
        "surname": "Hassaballah",
        "given_name": "Mahmoud"
      }
    ]
  },
  {
    "title": "Efficient object tracking on edge devices with MobileTrack",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104126",
    "abstract": "Object tracking has made significant progress in recent years. However, the state-of-the-art trackers are becoming increasingly heavy and expensive, making their deployment challenging in resource-constrained applications. In this study, we introduce MobileTrack, a visual object tracker that strikes a perfect balance between tracking accuracy and inference speed. Utilizing a novel coordinated perception-aware fusion module and a lightweight prediction head, our proposed methodology outperforms most Siamese trackers on various academic benchmarks in terms of both accuracy and efficiency. When deployed on resource-constrained embedded devices such as NVIDIA Jetson TX2, MobileTrack ensures real-time performance at a speed exceeding 33FPS, while LightTrack only operates at 18FPS. Therefore, MobileTrack holds significant potential to unlock a wide range of practical applications across various industries. MobileTrack is released at here.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000816",
    "keywords": [
      "Artificial intelligence",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Enhanced Data Rates for GSM Evolution",
      "Mathematics",
      "Object (grammar)",
      "Pedagogy",
      "Psychology",
      "Tracking (education)"
    ],
    "authors": [
      {
        "surname": "Zhai",
        "given_name": "Jiang"
      },
      {
        "surname": "Cheng",
        "given_name": "Zinan"
      },
      {
        "surname": "Zhang",
        "given_name": "Wenkang"
      },
      {
        "surname": "Zhu",
        "given_name": "Dejun"
      },
      {
        "surname": "Yang",
        "given_name": "Wankou"
      }
    ]
  },
  {
    "title": "Learn to enhance the low-light image via a multi-exposure generation and fusion method",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104127",
    "abstract": "In low-light image enhancement, single-exposure images contain a limited dynamic range, which hinders the restoration of contrast and texture. To address these problems, we propose a multi-exposure generation and fusion method (MEGF), which simulates multi exposure images and performs feature fusion for low light image enhancement. First, we propose a Multi-Exposure Generation (MEG) block, which generates images with different exposure levels based on the input low-light images. The MEG block employs information entropy as an evaluation measure to prevent the underexposed or overexposed image generation. Then, the Perceptual Importance based Multi-Exposure Feature Enhancement (PIMEFE) module has been developed to fuse the multi-exposure features using the Perceptual Importance-based Feature Fusion (PIFF) module. The PIFF module selects the well-exposed features from the multi-exposure features processed by the Multi Scale Recursive Feature Enhancement (MSRFE) block. Finally, the fused features are input to the Curve Adjustment (CA) block for fine-tuning and provide color enhancement to the fused features. Moreover, we propose the Multiple Exposure Recursive Fusion (MERF) module which estimates the adjustment factors for the CA block with the guidance of multi-exposure features. Experimental results demonstrate that our method outperforms other techniques in terms of image signal-to-noise ratio, structural similarity, and color accuracy on both real and synthetic datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000828",
    "keywords": [
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Computer science",
      "Computer vision",
      "Electrical engineering",
      "Engineering",
      "Entropy (arrow of time)",
      "Feature (linguistics)",
      "Fuse (electrical)",
      "Fusion",
      "Geometry",
      "Image (mathematics)",
      "Image fusion",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Jin",
        "given_name": "Haiyan"
      },
      {
        "surname": "Li",
        "given_name": "Long"
      },
      {
        "surname": "Su",
        "given_name": "Haonan"
      },
      {
        "surname": "Zhang",
        "given_name": "YuanLin"
      },
      {
        "surname": "Xiao",
        "given_name": "ZhaoLin"
      },
      {
        "surname": "Wang",
        "given_name": "Bin"
      }
    ]
  },
  {
    "title": "A convolutional neural network based on noise residual for seam carving detection",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104135",
    "abstract": "Seam carving is a method of resizing images based on content awareness. It can realize image retargeting while retaining the main content of the image. However, it may also be maliciously used to tamper with images, such as changing image semantic content by object removal. Therefore, seam carving detection has become important in image forensics. In this paper, a noise residual-based deep learning method is proposed to detect seam carving images. We try to learn the local noise in-consistency of images to recognize them. Firstly, a noise residual extraction segment is used to learn local noise features in the image, and a noise augmentation module is designed to enrich the features, which leverages the noise features extracted from a steganalysis rich model filter to discover the noise in-consistency between authentic and tampered regions. Then, through the feature dimensionality reduction section, the features are further learned and the size of feature maps are reduced. Finally, the output is obtained through global average pooling and a fully-connected layer. A careful testing strategy is further proposed, which greatly improves the detection performance, especially for seam carving with small scaling ratios. The experimental results demonstrate that our method achieves state-of-the-art performance at various scales, and has good robustness and generalization compared with other methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000907",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Feature extraction",
      "Gene",
      "Image (mathematics)",
      "Noise (video)",
      "Noise reduction",
      "Pattern recognition (psychology)",
      "Residual",
      "Retargeting",
      "Robustness (evolution)",
      "Seam carving"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Dengyong"
      },
      {
        "surname": "Lv",
        "given_name": "Zhenyu"
      },
      {
        "surname": "Li",
        "given_name": "Feng"
      },
      {
        "surname": "Ding",
        "given_name": "Xiangling"
      },
      {
        "surname": "Yang",
        "given_name": "Gaobo"
      }
    ]
  },
  {
    "title": "LIIS: Low-light image instance segmentation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104116",
    "abstract": "Image features in low-light scenes become hard to distinguish and full of noise, which makes the performance of current popular instance segmentation models drastically degraded. We propose a two-stage approach for instance segmentation of low-light images with enhancement followed by segmentation. Stage-I corresponds to the Low-Light Image Enhancement (LLIE) process. We propose a post-processing Detail Enhancement Denoising Module (DEDM) to suppress degradation effects caused by the enhancement in the preprocessing stage. Stage-II represents the segmentation process of enhanced images. We construct the W-BCNet instance segmentation network and design a Wavelet Feature Fusion Module (WFFM) in the feature extraction stage to preserve more fine-grained features. We achieve great segmentation results on LIS, detailed comparative experiments and ablation studies show the advantages and excellent generalization ability of our model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000713",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Image segmentation",
      "Pattern recognition (psychology)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Wei"
      },
      {
        "surname": "Huang",
        "given_name": "Ya"
      },
      {
        "surname": "Zhang",
        "given_name": "Xinyuan"
      },
      {
        "surname": "Han",
        "given_name": "Guijin"
      }
    ]
  },
  {
    "title": "Multi-scale features and attention guided for brain tumor segmentation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104141",
    "abstract": "Brain tumor segmentation supplies a credible reference for clinical treatment and pathological research and facilitates practitioners to diagnose more accurately. However, since the randomness and complexity of tumor shape and location, automatic brain tumor segmentation remains an extremely challenging assignment. In this study, we build an end-to-end convolutional neural network with a U-shaped structure to implement the segmentation of three lesion regions. We propose a multi-scale context block and an attention guidance block to focus on the spatial information at different scales and the interdependence between feature channels to enhance network representations and boost the learning capability of the model. Specifically, the multi-scale context block draws rich feature information through 3D dilated convolution. The attention guidance block reduces the impact of learned redundant features and eliminates the interference of irrelevant regions in the overall global information. Our recommended approach is evaluated on the brain tumor segmentation 2020 validation data. The Dice scores of the enhancing tumor (ET), whole tumor (WT), and tumor core (TC) are 78.19%, 90.10%, and 83.98%, respectively. In addition, the practice is also carried out in 2019 online validation data, and the Dice scores of ET, WT, and TC are 77.31%, 89.64%, and 82.55%, respectively. Experimental results reveal that the recommended approach gains favorable performance in comparison with representative brain tumor segmentation approaches. Our present study would accurately and efficaciously segment the three brain lesion regions and has clinical practice value.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000968",
    "keywords": [
      "Artificial intelligence",
      "Brain tumor",
      "Cartography",
      "Computer science",
      "Geography",
      "Medicine",
      "Pathology",
      "Pattern recognition (psychology)",
      "Scale (ratio)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Zekun"
      },
      {
        "surname": "Zou",
        "given_name": "Yanni"
      },
      {
        "surname": "Chen",
        "given_name": "Hongyu"
      },
      {
        "surname": "Liu",
        "given_name": "Peter X."
      },
      {
        "surname": "Chen",
        "given_name": "Junyu"
      }
    ]
  },
  {
    "title": "Dual contrastive attention-guided deformable convolutional network for single image super-resolution",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104097",
    "abstract": "With its powerful ability to model geometric transformations, the deformable convolutional network brings great improvements for single image super-resolution (SISR). Nevertheless, its location-variant sampling method leads to an escalation in spatial variance as the deformable convolutional layers are stacked, consequently resulting in limited performance. Hence, we propose a novel and effective approach called dual contrastive attention-guided deformable convolutional network (DCADCN) for SISR modeling. Specifically, we propose an attention-guided deformable convolutional module with joint inner and external attention mechanisms to fully exploit the correspondences between input and deformation features and preserve spatial characteristics to the extent possible. Additionally, we propose a dual mixed feature extractor consisting of two parallel sub-paths. This design allows for the learning of diverse and complementary spatial features. Furthermore, contrastive learning is applied to further amplify the role of key features and mitigate the interference of noisy features. Extensive experimental results demonstrate that DCADCN is capable of effectively handling classic SISR, SISR with blind noise, and real-world SISR tasks. Moreover, our method achieves comparable or even better performance with lower computational cost compared to state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032400052X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Dual (grammatical number)",
      "Image (mathematics)",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Resolution (logic)"
    ],
    "authors": [
      {
        "surname": "Qiao",
        "given_name": "Fengjuan"
      },
      {
        "surname": "Zhu",
        "given_name": "Yonggui"
      },
      {
        "surname": "Li",
        "given_name": "Guofang"
      },
      {
        "surname": "Li",
        "given_name": "Bin"
      }
    ]
  },
  {
    "title": "Using Mixture of Experts to accelerate dataset distillation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104137",
    "abstract": "Recently, large datasets have become increasingly necessary for most deep learning tasks, however, large datasets may bring some problems, such as disk storage and huge computational expense. Dataset distillation is an emerging field that aims to synthesize a small dataset from the original dataset, then a random model trained on the distillation dataset can achieve comparable performances to the same architecture model trained on the original dataset. Matching Training Trajectories (MTT) achieves a leading performance in this field, but it needs to pre-train 200 expert models before the formal distillation process, which is called buffer process. In this paper, we propose a new method to reduce the consumed time of buffer process. Concretely, we use Mixture of Experts (MoE) to train several expert models parallelly in buffer process. The experiments show our method can achieve a speedup of up to approximately 4 ∼ 8 × in buffer process with getting comparable distillation performances.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000920",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Distillation",
      "Field (mathematics)",
      "Machine learning",
      "Matching (statistics)",
      "Mathematics",
      "Operating system",
      "Organic chemistry",
      "Parallel computing",
      "Process (computing)",
      "Pure mathematics",
      "Speedup",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Zhi"
      },
      {
        "surname": "Fu",
        "given_name": "Zhenyong"
      }
    ]
  },
  {
    "title": "MP2PMatch: A Mask-guided Part-to-Part Matching network based on transformer for occluded person re-identification",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104128",
    "abstract": "Occluded person re-identification (ReID) remains challenging due to the misaligned body parts. Existing works, mainly utilizing extra clues, excel in predicting holistic person images but falter when confronting substantial occlusion. This paper proposes a transformer-based Mask-guided Part-to-Part Matching (MP2PMatch) network for fine-grained matching. Firstly, the Consistency Occlusion Augmentation (COA) processes holistic person images and corresponding body part masks to construct occluded “image-mask” pairs. Next, we introduce learnable part tokens to capture semantic features of various body parts, performing “pull close” and “push apart” operations based on identity labels and part visibility, ensuring the one-to-one correspondence between part features and body parts. Additionally, the proposed Body Region Attention (BRA) utilizes the overall attention on body regions to guide the network to overcome interference from both occlusion and background. Extensive experiments demonstrate that MP2PMatch achieves exceptional occluded ReID performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032400083X",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Computer vision",
      "Electrical engineering",
      "Engineering",
      "Identification (biology)",
      "Matching (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Statistics",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Lv",
        "given_name": "Guilin"
      },
      {
        "surname": "Ding",
        "given_name": "Yanhui"
      },
      {
        "surname": "Chen",
        "given_name": "Xinyuan"
      },
      {
        "surname": "Zheng",
        "given_name": "Yuanjie"
      }
    ]
  },
  {
    "title": "FFLDGA-Net: Image retrieval method based on Feature Fusion Learnable Descriptor Graph Attention Network",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104109",
    "abstract": "Image retrieval aims to retrieve and return the image in the database that is most similar to the query image. However, the performance of image retrieval models is often hindered by the limited dimensionality of images, which lacks depth information about objects. To address this issue, we propose a novel image retrieval model called FFLDGA-Net (Feature Fusion-based Learnable Descriptor Graph Attention Network). This model aims to overcome the absence of depth information in images by fusing feature information from both image data and point cloud data. First, we introduce the LDGA-Net, which effectively improved the model’s ability to mine hard samples and negative samples. Then, we combine the multi-scale route convolution module with a one-dimensional path aggregation network to fuse point clouds and image features at multiple scales, and establish a high-dimensional relationship between features and low-dimensional features. To mitigate training noise, we incorporate a soft label strategy tailored to the dataset’s characteristics. Our experimental results on two benchmark datasets demonstrate the significant improvements achieved by FFLDGA-Net in image retrieval performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000646",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Feature (linguistics)",
      "Fusion",
      "Geometry",
      "Graph",
      "Image (mathematics)",
      "Linguistics",
      "Mathematics",
      "Net (polyhedron)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Xiaoyu"
      },
      {
        "surname": "Wang",
        "given_name": "Xingmei"
      },
      {
        "surname": "Yang",
        "given_name": "Dongmei"
      },
      {
        "surname": "Ren",
        "given_name": "Wei"
      },
      {
        "surname": "Wang",
        "given_name": "Jinli"
      },
      {
        "surname": "Liu",
        "given_name": "Bo"
      }
    ]
  },
  {
    "title": "MP2PMatch: A Mask-guided Part-to-Part Matching network based on transformer for occluded person re-identification",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104128",
    "abstract": "Occluded person re-identification (ReID) remains challenging due to the misaligned body parts. Existing works, mainly utilizing extra clues, excel in predicting holistic person images but falter when confronting substantial occlusion. This paper proposes a transformer-based Mask-guided Part-to-Part Matching (MP2PMatch) network for fine-grained matching. Firstly, the Consistency Occlusion Augmentation (COA) processes holistic person images and corresponding body part masks to construct occluded “image-mask” pairs. Next, we introduce learnable part tokens to capture semantic features of various body parts, performing “pull close” and “push apart” operations based on identity labels and part visibility, ensuring the one-to-one correspondence between part features and body parts. Additionally, the proposed Body Region Attention (BRA) utilizes the overall attention on body regions to guide the network to overcome interference from both occlusion and background. Extensive experiments demonstrate that MP2PMatch achieves exceptional occluded ReID performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032400083X",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Computer vision",
      "Electrical engineering",
      "Engineering",
      "Identification (biology)",
      "Matching (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Statistics",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Lv",
        "given_name": "Guilin"
      },
      {
        "surname": "Ding",
        "given_name": "Yanhui"
      },
      {
        "surname": "Chen",
        "given_name": "Xinyuan"
      },
      {
        "surname": "Zheng",
        "given_name": "Yuanjie"
      }
    ]
  },
  {
    "title": "FFLDGA-Net: Image retrieval method based on Feature Fusion Learnable Descriptor Graph Attention Network",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104109",
    "abstract": "Image retrieval aims to retrieve and return the image in the database that is most similar to the query image. However, the performance of image retrieval models is often hindered by the limited dimensionality of images, which lacks depth information about objects. To address this issue, we propose a novel image retrieval model called FFLDGA-Net (Feature Fusion-based Learnable Descriptor Graph Attention Network). This model aims to overcome the absence of depth information in images by fusing feature information from both image data and point cloud data. First, we introduce the LDGA-Net, which effectively improved the model’s ability to mine hard samples and negative samples. Then, we combine the multi-scale route convolution module with a one-dimensional path aggregation network to fuse point clouds and image features at multiple scales, and establish a high-dimensional relationship between features and low-dimensional features. To mitigate training noise, we incorporate a soft label strategy tailored to the dataset’s characteristics. Our experimental results on two benchmark datasets demonstrate the significant improvements achieved by FFLDGA-Net in image retrieval performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000646",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Feature (linguistics)",
      "Fusion",
      "Geometry",
      "Graph",
      "Image (mathematics)",
      "Linguistics",
      "Mathematics",
      "Net (polyhedron)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Xiaoyu"
      },
      {
        "surname": "Wang",
        "given_name": "Xingmei"
      },
      {
        "surname": "Yang",
        "given_name": "Dongmei"
      },
      {
        "surname": "Ren",
        "given_name": "Wei"
      },
      {
        "surname": "Wang",
        "given_name": "Jinli"
      },
      {
        "surname": "Liu",
        "given_name": "Bo"
      }
    ]
  },
  {
    "title": "GaitGMT: Global feature mapping transformer for gait recognition",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104139",
    "abstract": "Gait recognition is an important biometric technology that allows for the remote collection of stakeholders’ characteristics, without requiring their explicit cooperation. It has gained considerable attention in the fields of criminal investigation and intelligent security. Previous studies have shown that local gait features can enhance gait recognition performance by improving robustness to disturbances. However, global gait features also play a crucial role in gait recognition. Many researchers have utilized convolutional operations to extract global features, but these operations tend to focus on features within the receptive field, neglecting those outside of it. Therefore, the potential of global gait features has not been fully explored. In this paper, we propose a gait recognition framework based on vision transformers, aiming to enhance the extraction of global gait features. We introduce an adaptive multi-frame global feature mapping (AMGM) method to address the challenge of inconsistent feature dimensions caused by variations in the number of gait frames when fusing global and local features. We evaluate our model on the latest datasets, and the experimental results demonstrate a significant breakthrough. Notably, our model achieves state-of-the-art recognition accuracy, particularly in scenarios where subjects are wearing coats. Additionally, our model achieves remarkable improvements in recognition accuracy through training with small sample sets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000944",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Biometrics",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Feature extraction",
      "Gait",
      "Gene",
      "Linguistics",
      "Medicine",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physical medicine and rehabilitation",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Guilong"
      },
      {
        "surname": "Huang",
        "given_name": "Jiayi"
      },
      {
        "surname": "Chen",
        "given_name": "Guanghai"
      },
      {
        "surname": "Chen",
        "given_name": "Xin"
      },
      {
        "surname": "Deng",
        "given_name": "Xiaoling"
      },
      {
        "surname": "Lan",
        "given_name": "Yubin"
      },
      {
        "surname": "Long",
        "given_name": "Yongbing"
      },
      {
        "surname": "Tian",
        "given_name": "Qi"
      }
    ]
  },
  {
    "title": "Multi-level feature enhancement network for object detection in sonar images",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104147",
    "abstract": "The unstable geometric features affect the accuracy of object detection in sonar images. We thus propose a novel multi-level feature enhancement network to enhance useful features for object detection in sonar images. We first introduce a deformable convolution to model variations in geometric features. In addition, spatial and channel attention modules are utilized to aggregate rich semantic information from features, improving the quality of feature extraction. We further use an adaptive multi-scale feature fusion module for feature weighting so as to enhance fine-grained features and minimize information loss during feature fusion. Then, the cascaded detection module corrects the prediction results of the previous detector with a low Intersection-over-Union (IoU) threshold, where each detector employs adaptive feature enhancement blocks to enhance region proposal features and thus improve detection performance. Experimental results on two real-world sonar image datasets show that our proposed model performs better than several mainstream object detection methods by achieving 2% to 19.4% higher accuracy rates.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001020",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Linguistics",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Sonar"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Xin"
      },
      {
        "surname": "Zhou",
        "given_name": "Zihan"
      },
      {
        "surname": "Wang",
        "given_name": "Manying"
      },
      {
        "surname": "Ning",
        "given_name": "Bo"
      },
      {
        "surname": "Wang",
        "given_name": "Yanhao"
      },
      {
        "surname": "Zhu",
        "given_name": "Pengli"
      }
    ]
  },
  {
    "title": "Dual-branch collaborative transformer for effective image deraining",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104136",
    "abstract": "Recently, Transformer-based architecture has been introduced into single image deraining tasks due to its advantage in modeling non-local information. However, existing approaches typically utilize self-attention along single spatial or channel dimension. They neglect feature fusion in different dimensions to explore contextual information fully, which limits the effective receptive field of the network and makes it challenging to learn image degradation relationships. To fully explore potential correlations between different dimensions of degraded images, we develop a Dual-branch Collaborative Transformer, called DCformer. To be specific, we employ parallel multi-head self-attention (PMSA) as the core block to extract long-range contextual relationships across spatial and channel dimensions. Additionally, a local perception block (LPB) is introduced to provide the ability of local information acquisition in the network, which is complementary to the global modeling ability of the parallel hybrid self-attention mechanism. Finally, we design a feature interaction block (FIB) to further enhance the interaction of features at different resolutions. Extensive experiments on benchmark datasets demonstrate the effectiveness of our proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000919",
    "keywords": [
      "Art",
      "Computer science",
      "Dual (grammatical number)",
      "Electrical engineering",
      "Engineering",
      "Literature",
      "Mathematics",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Qi",
        "given_name": "Xuanyu"
      },
      {
        "surname": "Song",
        "given_name": "Tianyu"
      },
      {
        "surname": "Dong",
        "given_name": "Haobo"
      },
      {
        "surname": "Jin",
        "given_name": "Jiyu"
      },
      {
        "surname": "Jin",
        "given_name": "Guiyue"
      },
      {
        "surname": "Li",
        "given_name": "Pengpeng"
      }
    ]
  },
  {
    "title": "Omni-supervised shadow detection with vision foundation model",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104146",
    "abstract": "Weakly supervised shadow detection has been spreading as cheaper labeling costs in recent years. However, these works tend to use a single annotation manner, i.e., scribble, it may not be the best choice for each shadow scene and thereby cannot exploit more valuable cues for better training. To tackle this issue, this work explores a more flexible labeling strategy, containing scribble, box and point. During the training, rather than only using cross-entropy (CE) loss on the available points, we formulate the supervisions between given annotations with the model’s predictions as a bipartite matching problem, and then design efficient weak learning losses from other valuable perspectives of shadow location, quantity and size. Moreover, considering the challenge of poor boundary localization in one-stage weakly supervised image segmentation, we propose a CNN-assisted tuning strategy to inherit boundary knowledge from the vision foundation model (i.e., SAM), and learn task-specific knowledge from our hybrid annotations by introducing a light-weight CNN branch along with the SAM backbone, namely ShadowSAM. This approach has lower requirements on GPU memory and extracts more local information for identifying tiny shadow regions. Extensive experiments demonstrate that our proposed methods can achieve comparable performance with state-of-the-art (SOTA) fully supervised shadow detectors. Our code will be available at https://github.com/wuwen1994/omni-shadow.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001019",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Foundation (evidence)",
      "Geography",
      "Psychoanalysis",
      "Psychology",
      "Shadow (psychology)"
    ],
    "authors": [
      {
        "surname": "Qian",
        "given_name": "Zeheng"
      },
      {
        "surname": "Wu",
        "given_name": "Wen"
      },
      {
        "surname": "Wu",
        "given_name": "Xian-Tao"
      },
      {
        "surname": "Chen",
        "given_name": "Xiao-Diao"
      }
    ]
  },
  {
    "title": "Lightweight Patch-Wise Casformer for dynamic scene deblurring",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104112",
    "abstract": "In dynamic scenes, motion blur can often occur, which is non-uniform and can be difficult to remove. Recently, the Transformer has shown excellent performance in various image-related tasks such as classification, recognition, and segmentation. Using a Transformer-based backbone network has also shown potential advantages in image deblurring. However, the computational complexity of Transformers increases quadratically with spatial resolution, making it difficult to apply to high-resolution images. To address the above issue, we propose a cascade Transformer (Casformer) that consists of two key modules: Deep Separable Attention (DSA) and Double-Flow Gate (DFG). Our approach effectively reduces computational complexity while suppressing blurry information. Additionally, we discovered an inconsistency between training and testing images during the image restoration process. We addressed this issue by experimentally verifying an inference aggregation method (IAM) that independently predicts patches during inference to address the problem of imbalanced information distribution. Experimental results demonstrate that our design performs well on GoPro and other datasets, e.g. 29.20 dB PSNR on RealBlur-J, exceeding the previous state-of-the-art (SOTA) 0.14 dB.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000671",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Deblurring",
      "Image (mathematics)",
      "Image processing",
      "Image restoration",
      "Inference",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Segmentation",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Ziyi"
      },
      {
        "surname": "Cui",
        "given_name": "Guangmang"
      },
      {
        "surname": "Li",
        "given_name": "Zihan"
      },
      {
        "surname": "Zhao",
        "given_name": "Jufeng"
      }
    ]
  },
  {
    "title": "A deep audio-visual model for efficient dynamic video summarization",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104130",
    "abstract": "The adage “a picture is worth a thousand words” resonates in the digital video domain, suggesting that a video could be seen as a composition of millions of these words. Videos are composed of countless frames. Video summarization creates cohesive visual units in scenes by condensing shots from segments. Video summarization gains prominence by condensing lengthy videos while retaining crucial content. Despite effective techniques using keyframes or keyshots in video summarization, integrating audio components is imperative. This paper focuses on integrating deep learning techniques to generate dynamic summaries enriched with audio. To address that gap, an efficient model employs audio-visual features, enriching summarization for more robust and informative video summaries. The model selects keyshots based on their significance scores, safeguarding essential content. Assigning these scores to specific video shots is a pivotal yet demanding task for video summarization. The model’s evaluation occurs on benchmark datasets, TVSum and SumMe. Experimental outcomes reveal its efficacy, showcasing considerable performance enhancements. On the TVSum, SumMe datasets, an F-Score metric of 79.33% and 66.78%, respectively, is achieved, surpassing previous state-of-the-art techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000853",
    "keywords": [
      "Artificial intelligence",
      "Audio visual",
      "Automatic summarization",
      "Benchmark (surveying)",
      "Computer science",
      "Deep learning",
      "Domain (mathematical analysis)",
      "Economics",
      "Geodesy",
      "Geography",
      "Information retrieval",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Metric (unit)",
      "Multimedia",
      "Operations management",
      "Speech recognition",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "El-Nagar",
        "given_name": "Gamal"
      },
      {
        "surname": "El-Sawy",
        "given_name": "Ahmed"
      },
      {
        "surname": "Rashad",
        "given_name": "Metwally"
      }
    ]
  },
  {
    "title": "GoLDFormer: A global–local deformable window transformer for efficient image restoration",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104117",
    "abstract": "Thanks to the powerful modeling capabilities of multi-head self attention (MSA), transformers have shown significant performance gains in vision tasks. However, as transformers require heavy computation, more efficient designs are required. In this paper, we present an efficient transformer architecture named GoLDFormer for image restoration. GoLDFormer extends the capability of window-based self-attention through two core designs. First, We propose a globally-enhanced window-based transformer block (G-WTB), which applies transposed attention to a compressed window representation rather than the spatial features, thus establishing connections between all windows with less computational complexity. Second, since the interactions between image content and window attention weights can be interpreted as spatially varying convolution, we introduce an adaptive filter structure into transformer models and propose a deformable filtering block (DFB) to enable cross-window connections. By adjusting the shape of the generated filters in the DFB, we can balance the computational costs and the degree of adjacent window interaction. Extensive experiments on several image restoration tasks demonstrate that GoLDFormer achieves competitive results against recent methods but with optimal computational costs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000725",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computation",
      "Computational complexity theory",
      "Computer science",
      "Computer vision",
      "Electrical engineering",
      "Engineering",
      "Operating system",
      "Transformer",
      "Voltage",
      "Window (computing)"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Quan"
      },
      {
        "surname": "Zheng",
        "given_name": "Bolun"
      },
      {
        "surname": "Yan",
        "given_name": "Chenggang"
      },
      {
        "surname": "Zhu",
        "given_name": "Zunjie"
      },
      {
        "surname": "Wang",
        "given_name": "Tingyu"
      },
      {
        "surname": "Slabaugh",
        "given_name": "Gregory"
      },
      {
        "surname": "Yuan",
        "given_name": "Shanxin"
      }
    ]
  },
  {
    "title": "COC-UFGAN: Underwater image enhancement based on color opponent compensation and dual-subnet underwater fusion generative adversarial network",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104101",
    "abstract": "Due to the complex underwater environment and the attenuation of light, the underwater image is produced with various distortions such as color loss, low contrast, noise, blur, and haze-like, which bring significant challenges to underwater image applications. To alleviate these problems, a novel color opponent compensation and a dual-subnet underwater fusion generative adversarial network (COC-UFGAN) are proposed by simulating the information processing mechanism in the human visual system (HVS). Specifically, considering the color opponent mechanism of the primary visual cortex and the characteristics of underwater imaging, an opponent domain is constructed consisting of four color opponent channels: green–red, yellow-blue, cyan-red and black-white. Then, a novel local-to-global color opponent compensation method is designed to restore color loss in underwater imaging. Furthermore, considering the complexity of underwater imaging, a dual-subnet underwater fusion generative adversarial network is designed. Finally, experimental results on synthetic and natural underwater images demonstrate the superiority of the proposed COC-UFGAN over the state-of-the-art methods qualitatively and quantitatively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000567",
    "keywords": [
      "Adversarial system",
      "Art",
      "Artificial intelligence",
      "Compensation (psychology)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Dual (grammatical number)",
      "Face (sociological concept)",
      "Generative grammar",
      "Geology",
      "Image (mathematics)",
      "Literature",
      "Oceanography",
      "Psychoanalysis",
      "Psychology",
      "Social science",
      "Sociology",
      "Subnet",
      "Underwater"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Zhenkai"
      },
      {
        "surname": "Fu",
        "given_name": "Xinxiao"
      },
      {
        "surname": "Lin",
        "given_name": "Chi"
      },
      {
        "surname": "Xu",
        "given_name": "Haiyong"
      }
    ]
  },
  {
    "title": "Locality-constraint Representation with Minkowski distance metric for an effective Face Hallucination",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104142",
    "abstract": "Face hallucination (FaceH), the domain specific super resolution technique has significant importance in the field of image processing and computer vision. These methods provide visually convincing high-resolution face images using the low-resolution input images. To do this, all the state-of-the-art methods make use of distance measure algorithms. The performance of distance measure is one of the core components that impact the performance of FaceH. However, there are many k-NN distance measure algorithms, to the best of our knowledge, the most commonly used metric is the Euclidean distance, though it is the best, a little deviation may result in large elevation on estimating the reconstructed weight of the pixel. To overcome this, it is necessary to find an alternative method and this motivated to develop Minkowski Locality-constrained Representation (MinLcR) FaceH that makes use of Minkowski distance and after which it is integrated with Locality-constrained Representation (LcR) that incorporates the contextual information of the neighboring pixel for better hallucination. The main idea of MinLcR is to make a difference in the distance metric and compare it with methods that make use of Euclidean distance. Experimental analysis carried out using FEI database shows that the proposed method has an effective improvement over other methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032400097X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Constraint (computer-aided design)",
      "Data mining",
      "Distance measures",
      "Distance transform",
      "Divergence (linguistics)",
      "Economics",
      "Euclidean distance",
      "Face (sociological concept)",
      "Face detection",
      "Face hallucination",
      "Facial recognition system",
      "Geometry",
      "Image (mathematics)",
      "Law",
      "Linguistics",
      "Locality",
      "Mathematics",
      "Measure (data warehouse)",
      "Metric (unit)",
      "Minkowski distance",
      "Minkowski space",
      "Operations management",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Similarity (geometry)",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Savitha",
        "given_name": "S."
      },
      {
        "surname": "Pounambal",
        "given_name": "M."
      }
    ]
  },
  {
    "title": "Unveiling tampering traces: Enhancing image reconstruction errors for visualization",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104125",
    "abstract": "Contemporary tampering detection models in digital image forensics often rely on established techniques such as SRM and ELA for revealing evidence of manipulation. Despite their widespread use, these methods often yield unsatisfactory visualizations. In order to enhance the visibility of tampering artifacts, we propose an innovative image representation called Tamper Reconstruction Error (TRE). TRE measures the error between an input image and its reconstructed counterpart using a pre-trained mixed generator. We observed that utilizing a model proficient in computational visual tasks to extract reconstruction errors did not clearly reveal tampering traces in manually manipulated images. To emphasize the more pronounced discrepancies in the reconstruction of tampered images, the image representation TRE is fed into two dedicated extractors designed to capture manipulation features in both the frequency and spatial domains. Throughout the learning process, these extractors adaptively express the essential forgery traces back into spatial domain. Furthermore, to validate the importance of the extracted errors in tampering localization, we introduced a localization annotator. This annotator integrates reconstruction errors at different stages during the encoding and decoding of latent features. Experimental results demonstrate that the integration of extracted features significantly improves the performance of tampering localization, outperforming other state-of-the-art localization frameworks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000804",
    "keywords": [
      "Artificial intelligence",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Zeyu"
      },
      {
        "surname": "Zhao",
        "given_name": "Xianfeng"
      },
      {
        "surname": "Cao",
        "given_name": "Yun"
      }
    ]
  },
  {
    "title": "Progressive feature fusion for SNR-aware low-light image enhancement",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104148",
    "abstract": "Restoring image quality in low-light environments is an intriguing topic. While deep learning models have made significant strides in low-light enhancement, most models do not take into account the inherent characteristics of objects themselves. In this paper, we use the characteristics of the image itself to construct a Signal-to-Noise Ratio (SNR) map that guides the signal space variation to dynamically stretch the pixel values. Specifically, we propose a novel signal-to-noise ratio image-guided enhancement framework that uses the feature information of the original image to guide spatial variations in the image. It involves step-wise guidance for image feature fusion, gradually emphasizing high-frequency feature information within the image. Meanwhile, we introduced a texture optimization module that utilizes the feature information extracted by the feature fusion module to address the issues of overexposure and detail loss. We performed qualitative and quantitative evaluations on synthetic and real low-light image datasets to demonstrate the performance of our method. The experimental results show that our model outperforms other state-of-the-art methods (SOTA) in robust low-light enhancement, especially in processing images captured in complex scenes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324001032",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Feature detection (computer vision)",
      "Feature extraction",
      "Fusion",
      "Image (mathematics)",
      "Image fusion",
      "Image processing",
      "Image quality",
      "Linguistics",
      "Noise (video)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Programming language",
      "SIGNAL (programming language)"
    ],
    "authors": [
      {
        "surname": "Qiao",
        "given_name": "Sihai"
      },
      {
        "surname": "Chen",
        "given_name": "Rong"
      }
    ]
  },
  {
    "title": "DAGNet: Depth-aware Glass-like objects segmentation via cross-modal attention",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104121",
    "abstract": "Transparent or specular objects, such as mirrors, glass windows, and glass walls, have a significant impact on computer vision tasks. Glass-like Objects (GLOS) encompass transparent or specular objects that lack distinctive visual appearances and specific external shapes, posing challenges for GLO segmentation. In this study, we propose a novel bidirectional cross-modal fusion framework with a shift-window cross-attention for GLO segmentation. The framework incorporates a Feature Exchange Module (FEM) and a Shifted-window Cross-attention Feature Fusion Module (SW-CAFM) in each transformer block stage to calibrate, exchange, and fuse cross-modal features. The FEM employs coordinate and spatial attention mechanisms to filter out the noise and recalibrate the features from two modalities. The Shifted-Window Cross-Modal Attention Fusion (SW-CAFM) uses cross-attention to fuse RGB and depth features, leveraging the shifted-window self-attention operation to reduce the computational complexity of cross-attention. The experimental results demonstrate the feasibility and high performance of the proposed method, achieving state-of-the-art results on various glass and mirror benchmarks. The method achieves mIoU accuracies of 90.32%, 94.24%, 88.76%, and 87.47% on the GDD, Trans10K, MSD, and RGBD-Mirror datasets, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000762",
    "keywords": [
      "Artificial intelligence",
      "Composite material",
      "Computer science",
      "Computer vision",
      "Materials science",
      "Modal",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Wan",
        "given_name": "Yingcai"
      },
      {
        "surname": "Zhao",
        "given_name": "Qiankun"
      },
      {
        "surname": "Xu",
        "given_name": "Jiqian"
      },
      {
        "surname": "Wang",
        "given_name": "Huaizhen"
      },
      {
        "surname": "Fang",
        "given_name": "Lijin"
      }
    ]
  },
  {
    "title": "A keypoints-motion-based landmark transfer method for face reenactment",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104138",
    "abstract": "Face reenactment is a task to transfer the facial pose and expression from one driving face to the source face. It can be done by adding many condition information, such as the landmark, 3D Morphable Model (3DMM). Because the landmark information is easy to get and contains face structure and expression information, many works rely on it. One of the key points for the landmark-based method is to generate landmarks that keep the face shape of the source image while having the same expression and pose as the driving image. Concentrating on this problem, we propose a novel and effective method to transfer the expression and pose based on the motion of the key points between driving images. Besides, we also try to improve the fit ability of the current popular backbone by adding an enhancement module.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000932",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Face (sociological concept)",
      "Landmark",
      "Motion (physics)",
      "Parallel computing",
      "Social science",
      "Sociology",
      "Transfer (computing)"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Kuiyuan"
      },
      {
        "surname": "Li",
        "given_name": "Xiaolong"
      },
      {
        "surname": "Zhao",
        "given_name": "Yao"
      }
    ]
  },
  {
    "title": "A 4-channelled hazy image input generation and deep learning-based single image dehazing",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104099",
    "abstract": "The images in foggy weather are often degraded which may affect the object detection process. Hence, in recent times, several schemes have been designed to eliminate the haze effect and enhance the performance of computer vision systems. Although these methods reduce the haze effect, they also produce over-saturation and over-degradation on most occasions. These problems occur due to inadequate utilization of the regional haze properties during the dehazing process. Therefore, a new dehazing model is proposed in this paper to address the issues faced by existing dehazing algorithms. The residual convolutional neural network (RCNN) with 14 layers is developed in the present work. In this model, features to indicate the extent of haze are extracted using the regional characteristics of a hazy image and are used as the fourth channel along with the three colour channels of a hazy image. The proposed RCNN is trained with 4-channel hazy images as input and their corresponding haze-free images as output. The results of our model are obtained with several images taken from three different datasets. Through the experimentation, quantitative and qualitative improvements are observed when compared to state-of-the-art dehazing methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000543",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Image (mathematics)"
    ],
    "authors": [
      {
        "surname": "Balla",
        "given_name": "Pavan Kumar"
      },
      {
        "surname": "Kumar",
        "given_name": "Arvind"
      },
      {
        "surname": "Pandey",
        "given_name": "Rajoo"
      }
    ]
  },
  {
    "title": "Adapting projection-based LiDAR semantic segmentation to natural domains",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104111",
    "abstract": "In this paper, an approach to the semantic segmentation of 3D LiDAR point clouds obtained from natural scenes is introduced. Using a state-of-the-art projection-based semantic segmentation model as the core segmentation network, several recent advances in projection-based 3D semantic segmentation methods are aggregated into a single model. These adaptions include: scan unfolding, soft-kNN post-processing, and multi-projection fusion. A novel Naïve Bayesian approach to multi-projection fusion which weights class probabilities based on the outputs of the base classifiers is proposed to further increase robustness. Quantitative and qualitative evaluations on several datasets, including scenes from both urban and natural environments; show that aggregating these adaptions into a single model can further improve the accuracy of state-of-the-art projection-based approaches. Finally, it is demonstrated that the novel Naïve Bayesian approach to multi-projection fusion addresses a number of the challenges inherent to natural data while also improving results on urban data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032400066X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Geology",
      "Lidar",
      "Natural (archaeology)",
      "Natural language processing",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Projection (relational algebra)",
      "Remote sensing",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Massa",
        "given_name": "Kelian J.L."
      },
      {
        "surname": "Grobler",
        "given_name": "Hans"
      }
    ]
  },
  {
    "title": "Improved YOLOv7 model for underwater sonar image object detection",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104124",
    "abstract": "Underwater object detection technology for sonar images is widely employed and in high demand for civil and military purposes. However, due to the inhomogeneity of the seawater medium, which causes attenuation and distortion of the acoustic signal, achieving ideal performance for sonar object detection approaches is challenging. Furthermore, the process of acoustic wave transmission is further complicated by floating objects and particles, resulting in increased multipath effects. This study proposes an object detection method named YOLOv7C, which is based on deep learning to address these challenges. An attention mechanism is incorporated into the backbone network of the model to improve its ability to handle complex backgrounds in sonar images and effectively extract features. In addition, to facilitate high-order interaction between key features, the Neck part of the network integrates Multi-GnBlock blocks. Model redundancy pruning is used to substantially reduce the model size while maintaining high detection accuracy, thereby enhancing real-time performance. The proposed YOLOv7C achieves a 1.9% increase in the mean average precision, reduces the model memory by 47.50% after pruning, and enhances the detection speed by nearly 2.5 times compared with the YOLOv7C. These findings indicate that the success rate of multi-object detection is significantly enhanced by the attention mechanism and the new module. Additionally, the model can be controlled within a reasonable size by employing appropriate pruning methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000798",
    "keywords": [
      "Artificial intelligence",
      "Channel (broadcasting)",
      "Computer science",
      "Computer vision",
      "Geology",
      "Multipath propagation",
      "Object (grammar)",
      "Object detection",
      "Oceanography",
      "Operating system",
      "Pattern recognition (psychology)",
      "Real-time computing",
      "Redundancy (engineering)",
      "Sonar",
      "Telecommunications",
      "Underwater"
    ],
    "authors": [
      {
        "surname": "Qin",
        "given_name": "Ken Sinkou"
      },
      {
        "surname": "Liu",
        "given_name": "Di"
      },
      {
        "surname": "Wang",
        "given_name": "Fei"
      },
      {
        "surname": "Zhou",
        "given_name": "Jingchun"
      },
      {
        "surname": "Yang",
        "given_name": "Jiaxuan"
      },
      {
        "surname": "Zhang",
        "given_name": "Weishi"
      }
    ]
  },
  {
    "title": "Dense-sparse representation matters: A point-based method for volumetric medical image segmentation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104115",
    "abstract": "Deep learning methods utilizing Convolutional Neural Networks (CNNs) and Transformers have achieved remarkable success in volumetric medical image analysis. While successful, the symmetrical structure of numerous networks pays insufficient attention to the encoding phase, and the large amount of memory occupied by voxels leads to unnecessary redundancy in the network. In this paper, we present a novel approach to handle volumetric medical images by converting them into point cloud and introduce a new asymmetrical segmentation architecture. We propose a dual-path encoder that fully captures both dense and sparse representations of the input point cloud sampled from volumes. Moreover, the two obtained representations are subtracted at the skip connection as a complementary feature during the decoding stage. Experimental results on the Brain Tumor Segmentation (BraTS) and the Multi-sequence Cardiac MR Segmentation tasks demonstrate the great potential of our point-based method for volumetric medical image segmentation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000701",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Geometry",
      "Image (mathematics)",
      "Law",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Point (geometry)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Segmentation",
      "Sparse approximation"
    ],
    "authors": [
      {
        "surname": "Jiang",
        "given_name": "Yun"
      },
      {
        "surname": "Liu",
        "given_name": "Bingxi"
      },
      {
        "surname": "Zhang",
        "given_name": "Zequn"
      },
      {
        "surname": "Yan",
        "given_name": "Yao"
      },
      {
        "surname": "Guo",
        "given_name": "Huanting"
      },
      {
        "surname": "Li",
        "given_name": "Yuhang"
      }
    ]
  },
  {
    "title": "SegIns: A simple extension to instance discrimination task for better localization learning",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104122",
    "abstract": "Recent self-supervised learning methods, where instance discrimination task is a fundamental way of pretraining convolutional neural networks (CNN), excel in transfer learning performance. Even though instance discrimination task is a well suited pretraining method for classification with its image-level learning, lack of dense representation learning makes it sub-optimal for localization tasks such as object detection. In this paper, we aim to mitigate this shortcoming of instance discrimination task by extending it to jointly learn dense representations alongside image-level representations. We add a segmentation branch parallel to the image-level learning to predict class-agnostic masks, enhancing location-awareness of the representations. We show the effectiveness of our pretraining approach on localization tasks by transferring the learned representations to object detection and segmentation tasks, providing relative improvements by up to 1.7% AP on PASCAL VOC and 0.8% AP on COCO object detection, 0.8% AP on COCO instance segmentation and 3.6% mIoU on PASCAL VOC semantic segmentation respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000774",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Engineering",
      "Epistemology",
      "Extension (predicate logic)",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Simple (philosophy)",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Baydar",
        "given_name": "Melih"
      },
      {
        "surname": "Akbas",
        "given_name": "Emre"
      }
    ]
  },
  {
    "title": "DU-Net: A new double U-shaped network for single image dehazing",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104132",
    "abstract": "Convolutional neural networks have achieved remarkable success in single image dehazing tasks, and previous studies verified the dehazing performance of the U-shaped framework. However, most existing U-shaped architecture dehazing networks still face challenges in sufficiently dealing with a large area of haze with low visibility. In this paper, we propose a novel dehazing network named Double U-Net(DU-Net). Specifically, to reduce the interference of haze features in the encoder to the recovery stage when skip-connecting to the decoder directly, we develop a new architecture firstly, which is composed of an extended encoder–decoder. Besides, the hierarchical depth-wise convolution block(HDCB) is designed to gradually increase the receptive field by leveraging the depth-wise convolution, enriching the global information. Moreover, we propose a multi-branch interactive fusion(MIF) which achieves efficient cross-branch and cross-channel interaction through parallel multiple 1D convolutions. Extensive experiments on both synthetic and real-world hazy images demonstrate the effectiveness of our proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000877",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Block (permutation group theory)",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Encoder",
      "Geometry",
      "Image (mathematics)",
      "Interference (communication)",
      "Mathematics",
      "Operating system",
      "Optics",
      "Physics",
      "Visibility"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Xiaodong"
      },
      {
        "surname": "Zhang",
        "given_name": "Long"
      },
      {
        "surname": "Chu",
        "given_name": "Menghui"
      },
      {
        "surname": "Wang",
        "given_name": "Shuo"
      }
    ]
  },
  {
    "title": "Occupancy map-based low complexity motion prediction for video-based point cloud compression",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104110",
    "abstract": "This paper proposes an occupancy map-based low complexity motion prediction method for video-based point cloud compression (V-PCC). Firstly, we propose to utilize the occupancy map, direction gradient, and regional dispersion to divide the attribute maps into static, complex, and common blocks. Then, we propose an early termination method for static blocks, an adaptive motion search range method for complex blocks, and an early inter prediction mode decision algorithm for affine motion regions in common blocks. Experiment results show that, in comparison to the test model category2 (TMC2) v15.0, called the anchor method, the average bitrate savings of Y, U, and V components of the proposed method achieve 24.27%, 32.64%, and 31.23% on 8i voxelized full bodies version 2 (8iVFBv2) dataset, respectively. Further, the time savings is 41.97% for attribute maps. Similarly, the proposed method also achieves consistent performance on Microsoft voxelized upper bodies (MVUB) dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000658",
    "keywords": [
      "Architectural engineering",
      "Artificial intelligence",
      "Cloud computing",
      "Composite material",
      "Compression (physics)",
      "Computer science",
      "Computer vision",
      "Data compression",
      "Engineering",
      "Geometry",
      "Materials science",
      "Mathematics",
      "Motion (physics)",
      "Occupancy",
      "Operating system",
      "Point (geometry)",
      "Point cloud"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yihan"
      },
      {
        "surname": "Wang",
        "given_name": "Yongfang"
      },
      {
        "surname": "Cui",
        "given_name": "Tengyao"
      },
      {
        "surname": "Fang",
        "given_name": "Zhijun"
      }
    ]
  },
  {
    "title": "Blind image quality assessment with semi-supervised learning",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104100",
    "abstract": "Blind image quality assessment (BIQA) aims to automatically predict the perceptual quality of an image without requiring access to its pristine reference counterpart. BIQA models are typically developed through supervised learning, optimizing and testing them by comparing their predictions to human ratings, usually expressed as mean opinion scores (MOS), which can be labor-intensive to collect. The performance of these BIQA models is significantly reliant on the amount of labeled training data. When there is a shortage of human-rated data, these BIQA models may perform inadequately. In this study, we investigate the potential of incorporating unlabeled data to mitigate this issue and enhance the performance of BIQA models. To achieve this, we propose a deep ensemble-based BIQA model (referred to as the “target model”) with two heads: one for quality estimation and the other for pseudo-label generation. Initially, we train it on a small set of human-rated images where the supervisory signals are binary labels indicating the pairwise ranking of perceptual quality for image pairs. Then, the head responsible for pseudo-label generation assigns pseudo-binary labels to unlabeled pairs. Subsequently, we re-train the target model using a combination of labeled and pseudo-labeled datasets. This process can be iterated, allowing for the progressive improvement of the target model’s performance. We conduct comprehensive case studies to illustrate the advantages of utilizing unlabeled data for BIQA, particularly in terms of model generalization and identifying cases of model failure.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000555",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Epistemology",
      "Image (mathematics)",
      "Image quality",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Quality (philosophy)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Xiwen"
      },
      {
        "surname": "Wang",
        "given_name": "Zhihua"
      },
      {
        "surname": "Xu",
        "given_name": "Binwei"
      }
    ]
  },
  {
    "title": "Integrating category-related key regions with a dual-stream network for remote sensing scene classification",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104098",
    "abstract": "Remote sensing image scene classification has made great progress with deep learning. Due to complex backgrounds and the large number of objects with inhomogeneous sizes, remote sensing image scene classification still remains challenging. In fact, only a few parts of regions are expected to be representative of the scene. In this paper, we propose a dual-stream framework for remote sensing classification with the help of category-related regions. Swin Transformer-based global feature extractor is used to produce coarse classification results. A category-related key region localization module is developed to extract the representative region from the whole image. The ResNet50 is employed as a local feature extractor to mine critical features from prominent regions. Finally, the classification results is obtained with the shared weighted classify head on global-local features. Extensive experiments show that our method provides a promising approach for key region locations and performs best on three public remote sensing datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000531",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Contextual image classification",
      "Engineering",
      "Extractor",
      "Feature (linguistics)",
      "Geography",
      "Image (mathematics)",
      "Key (lock)",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Process engineering",
      "Remote sensing"
    ],
    "authors": [
      {
        "surname": "Xiao",
        "given_name": "Fen"
      },
      {
        "surname": "Li",
        "given_name": "Xiang"
      },
      {
        "surname": "Li",
        "given_name": "Wei"
      },
      {
        "surname": "Shi",
        "given_name": "Junjie"
      },
      {
        "surname": "Zhang",
        "given_name": "Ningru"
      },
      {
        "surname": "Gao",
        "given_name": "Xieping"
      }
    ]
  },
  {
    "title": "Size-invariant two-in-one image secret sharing scheme based on gray mixing model",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104134",
    "abstract": "TiOISSS (Two-in-One Image Secret Sharing Scheme) can reconstruct the secret through computation and realize secret preview recovery when the power system is destroyed or the network is paralyzed. However, the visual quality of previewing images obtained by the existing TiOISSS is not satisfactory and usually affected by pixel expansion. To resolve the two problems, this paper introduces multiple gray values in the share image generation stage and establishes an optimization model to obtain the grayscale probabilistic matrix needed in the encoding stage. By using the probability matrix, a secret sharing algorithm is designed to obtain size-invariant shares with multiple gray values. Then, the secret information of another image can be shared and embedded into the multi-level gray shares through Lagrange polynomials over finite fields. In the decryption stage, the electronic shares can be directly superimposed based on a gray mixing model, or they can be printed to transparent films by a multi-level gray printer and then superimposed to obtain the preview image. If computational devices are available, a high-resolution gray image can be obtained by Lagrange interpolation over finite fields. Experimental results indicate that the proposed scheme is effective for generating shares without pixel expansion and improving the preview image quality.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000890",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Class (philosophy)",
      "Computation",
      "Computer science",
      "Computer vision",
      "Cryptography",
      "Gray (unit)",
      "Grayscale",
      "Image (mathematics)",
      "Image processing",
      "Image quality",
      "Image scaling",
      "Image sharing",
      "Invariant (physics)",
      "Lagrange polynomial",
      "Mathematical physics",
      "Mathematics",
      "Medicine",
      "Pixel",
      "Radiology",
      "Secret sharing"
    ],
    "authors": [
      {
        "surname": "Fu",
        "given_name": "Zhengxin"
      },
      {
        "surname": "Huang",
        "given_name": "Hangying"
      },
      {
        "surname": "Yu",
        "given_name": "Bin"
      },
      {
        "surname": "Li",
        "given_name": "Xiaopeng"
      }
    ]
  },
  {
    "title": "Non-iterative reversible information hiding in the secret sharing domain",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104096",
    "abstract": "Long running time due to multiple iterations is the main drawback of existing information hiding in the sharing domain (IHSD) algorithm. To address this problem, we propose non-iterative reversible information hiding in the secret sharing domain (NIIHSD). We calculate the coefficients of the polynomial that make the least significant bit of the shadow pixel equal to ‘0’ or ‘1’ and store them in two forms ( A 0 and A 1 ). According to the information to be embedded, coefficients are randomly selected in A 0 and A 1 to generate qualified shares by once calculation. The comparison with IHSD demonstrates our scheme improves the running speed of the algorithm by a factor of 2 to 4 for different ( k , n ) thresholds and secret image with different sizes. The coefficient forms only need to be created once to be reused. Our scheme has a greater advantage when sharing a large number of images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000518",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Cryptography",
      "Domain (mathematical analysis)",
      "Image (mathematics)",
      "Information hiding",
      "Mathematical analysis",
      "Mathematics",
      "Secret sharing",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Feng-Qing"
      },
      {
        "surname": "Liu",
        "given_name": "Tao"
      },
      {
        "surname": "Yan",
        "given_name": "Bin"
      },
      {
        "surname": "Pan",
        "given_name": "Jeng-Shyang"
      },
      {
        "surname": "Yang",
        "given_name": "Hong-Mei"
      }
    ]
  },
  {
    "title": "A simple transformer-based baseline for crowd tracking with Sequential Feature Aggregation and Hybrid Group Training",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104144",
    "abstract": "Tracking pedestrians in crowded scenes is a challenging task. Existing transformer-based tracking methods integrate detection and tracking into a unified model, which simplifies the tracking process. However, these methods also introduce complicated attention mechanisms that increase the model complexity and cost. To address this issue, we propose SOTTrack, a simple online transformer-based method for crowd tracking. Our method enhances feature learning and training strategies without sacrificing simplicity and efficiency. Specifically, we introduce the Sequential Feature Aggregation (SFA) module and the Hybrid Group Training (HGT) approach. The SFA module fuses features from sequential images to improve the temporal consistency of visual features within short time intervals. The HGT approach assigns different queries to multiple guided tasks, such as label assignment and de-noising, which are only used during training and do not incur any inference cost. We evaluate our method on the MOT17 and MOT20 datasets and demonstrate its competitive performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000993",
    "keywords": [
      "Artificial intelligence",
      "Baseline (sea)",
      "Biology",
      "Chemistry",
      "Computer science",
      "Electrical engineering",
      "Engineering",
      "Epistemology",
      "Feature (linguistics)",
      "Feature tracking",
      "Fishery",
      "Geography",
      "Group (periodic table)",
      "Linguistics",
      "Machine learning",
      "Meteorology",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Pedagogy",
      "Philosophy",
      "Psychology",
      "Simple (philosophy)",
      "Tracking (education)",
      "Training (meteorology)",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Cui"
      },
      {
        "surname": "Wu",
        "given_name": "Zewei"
      },
      {
        "surname": "Ke",
        "given_name": "Wei"
      },
      {
        "surname": "Xiong",
        "given_name": "Zhang"
      }
    ]
  },
  {
    "title": "Multi-Frequency Field Perception and Sparse Progressive Network for low-light image enhancement",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104133",
    "abstract": "Images taken in low-light conditions often suffer from various types of degradation. While most current methods primarily focus on spatial domain information to address these degradation issues, they often overlook the importance of frequency domain information. In order to better solve the degradation problems of low-light images, the Multi-Frequency Field Perception and Sparse Progressive Network (MFSPNet) for low-light image enhancement is proposed through Leveraging the complementary strengths of the frequency domain and the spatial domain, aiming to tackle the challenges of degraded images in intricate scenarios. Specifically, we propose the frequency domain field feature filtering (FDFF) module, which utilizes image frequency distribution information to address issues such as noise and artifacts in low-light images while complementing the spatial domain. Subsequently, we embed different scales of FDFF into four heterogeneous branches to flexibly handle features at various levels of complexity. Furthermore, we design a sparse wing-shaped transformer block (SWTB) that enhances the focus on critical information and complex multi-scale details through adaptive sparse attention unit (ASAU) and illumination multi-scale fusion feedforward network (IMF-FN). In addition, we propose a progressive enhancement strategy for self-knowledge sublimation to gradually improve image quality. At last, we comprehensively assess the proposed network across multiple datasets. Compared to other methods, our approach achieved the highest PSNR scores, with improvements of 3.014 dB and 0.215 dB, respectively, over the next best results. Additionally, our method exhibited the highest SSIM gain. Abundant experimental outcomes demonstrate that our approach outperforms numerous present low-light image enhancement approaches in both objective evaluation metrics and subjective visual effects, showcasing outstanding performance and significant potential.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000889",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Field (mathematics)",
      "Image (mathematics)",
      "Light field",
      "Mathematics",
      "Neuroscience",
      "Perception",
      "Psychology",
      "Pure mathematics"
    ],
    "authors": [
      {
        "surname": "Yuan",
        "given_name": "Shuang"
      },
      {
        "surname": "Li",
        "given_name": "Jinjiang"
      },
      {
        "surname": "Ren",
        "given_name": "Lu"
      },
      {
        "surname": "Chen",
        "given_name": "Zheng"
      }
    ]
  },
  {
    "title": "Audio-visual saliency prediction for movie viewing in immersive environments: Dataset and benchmarks",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104095",
    "abstract": "In this paper, an eye-tracking dataset of movie viewing in the immersive environment is developed, which contains 256 movie clips with 2K QHD resolution and corresponding movie genre labels from IMDb (Internet Movie Database). The dataset provides the audio-visual clues for studying the human visual attention when watching movie using a VR headset, by recording the eye movements using integrated eye tracker. To provide benchmarks for a saliency prediction for movie viewing in the immersive environment, fifteen computational models are evaluated on the dataset, including a newly developed multi-stream audio-visual saliency prediction model based on deep neural networks, named as MSAV. Detailed quantitative and qualitative comparisons and analyses are also provided. The developed dataset and benchmarks could help to facilitate the studies of visual saliency prediction for movie viewing in the immersive environments.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000506",
    "keywords": [
      "Artificial intelligence",
      "Audio visual",
      "Computer science",
      "Computer vision",
      "Eye tracking",
      "Gaze",
      "Headset",
      "Multimedia",
      "Telecommunications",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Zhao"
      },
      {
        "surname": "Zhang",
        "given_name": "Kao"
      },
      {
        "surname": "Cai",
        "given_name": "Hao"
      },
      {
        "surname": "Ding",
        "given_name": "Xiaoying"
      },
      {
        "surname": "Jiang",
        "given_name": "Chenxi"
      },
      {
        "surname": "Chen",
        "given_name": "Zhenzhong"
      }
    ]
  },
  {
    "title": "Edge fusion back projection GAN for large scale face super resolution",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104143",
    "abstract": "Face super-resolution is an important low-level vision task that has wide applications. Existing deep-learning-based face super-resolution (SR) methods often optimize the image super-resolution network by directly minimizing the pixel or feature level distance between the synthetic low-resolution face and the ground truth face image. These methods usually generate blur or over smooth results and lack of high-frequency face details. These are especially true for making high super-resolution of faces. Say for example, a super-resolution of 16x, only 0.4 % of the reference data points are available. To address the problem, we propose a novel network with edge fusion, back projection, and GAN prior (EFBPGAN) which can significantly improve the visual quality and generate realistic faces. To further make use of the spatial information and keep the structural consistency, we have developed new edge fusion and spatial fusion modules. We also propose a back projection extensive based coarse to fine SR pipeline to suppress the distortion and artifacts caused by GAN. Much experimental work has been done, results of which show that our proposed EFBPGAN can outperform the state-of-the-art approaches not only on numerical metrics but also on subjective visual evaluations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000981",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Enhanced Data Rates for GSM Evolution",
      "Face (sociological concept)",
      "Fusion",
      "Image (mathematics)",
      "Linguistics",
      "Materials science",
      "Philosophy",
      "Physics",
      "Projection (relational algebra)",
      "Quantum mechanics",
      "Resolution (logic)",
      "Scale (ratio)",
      "Social science",
      "Sociology",
      "Superresolution"
    ],
    "authors": [
      {
        "surname": "Cheng",
        "given_name": "Xi"
      },
      {
        "surname": "Siu",
        "given_name": "Wan-Chi"
      }
    ]
  },
  {
    "title": "Correlation-attention guided regression network for efficient crowd counting",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104078",
    "abstract": "As a valuable component of intelligent video surveillance, crowd counting has received lots of attention. In practice, however, crowd counting always suffers from the problem of the scale change of pedestrians. To mitigate this limitation, we propose a novel correlation-attention guided regression network to estimate the number of people, termed CGR-Net. To make the generation process of spatial attention and channel attention independent of each other, we design a parallel channel/spatial-wise attention module (PCSAM) to avoid error accumulation. A pixel-wise assisted attention module (PAAM) is developed for learning crowd uneven distribution on the different image pixels to further enhance the ability of the CGR-Net. Furthermore, we present a new loss function to ensure the effectiveness and performance of the proposed method. Comprehensive experimental results demonstrate that our model delivers enhanced representation and attains state-of-the-art performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000336",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Channel (broadcasting)",
      "Component (thermodynamics)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Correlation",
      "Data mining",
      "Evolutionary biology",
      "Function (biology)",
      "Geometry",
      "Image (mathematics)",
      "Law",
      "Machine learning",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics",
      "Pixel",
      "Political science",
      "Politics",
      "Process (computing)",
      "Regression",
      "Relation (database)",
      "Representation (politics)",
      "Spatial correlation",
      "Statistics",
      "Telecommunications",
      "Thermodynamics"
    ],
    "authors": [
      {
        "surname": "Zeng",
        "given_name": "Xin"
      },
      {
        "surname": "Wang",
        "given_name": "Huake"
      },
      {
        "surname": "Guo",
        "given_name": "Qiang"
      },
      {
        "surname": "Wu",
        "given_name": "Yunpeng"
      }
    ]
  },
  {
    "title": "Learning dual attention enhancement feature for visible–infrared person re-identification",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104076",
    "abstract": "Most previous visible–infrared person re-identification methods emphasized learning modality-shared features to narrow the modality differences, while neglecting the benefits of modality-specific features for feature embedding and narrowing the modality gap. To tackle this issue, our paper designs a method based on dual attention enhancement features to use shallow and deep features simultaneously. We first convert visible images into gray images to alleviate the visual difference. Then, to close the difference between modalities by learning the modality-specific features, we design a shallow feature measurement module, in which we use a class-specific maximum mean discrepancy loss to measure the distribution difference of specific features between two modalities. Finally, we design a dual attention feature enhancement module, which aims to mine more useful context information from modality-shared features to shorter the distance between classes within modalities. Specifically, our model exceeds the current SOTAs on SYSU-MM01, with 66.61% Rank-1 accuracy and 62.86% mAP.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000312",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Dual (grammatical number)",
      "Feature (linguistics)",
      "Identification (biology)",
      "Infrared",
      "Linguistics",
      "Literature",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Guoqing"
      },
      {
        "surname": "Zhang",
        "given_name": "Yinyin"
      },
      {
        "surname": "Zhang",
        "given_name": "Hongwei"
      },
      {
        "surname": "Chen",
        "given_name": "Yuhao"
      },
      {
        "surname": "Zheng",
        "given_name": "Yuhui"
      }
    ]
  },
  {
    "title": "UnifiedTT: Visual tracking with unified transformer",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104067",
    "abstract": "Target tracking is an important research task in computer vision. Existing tracking algorithms based on Siamese networks often suffer from the problem of information redundancy between adjacent frames and a lack of ability to capture global dependencies. When similar backgrounds appear around the target, the tracking performance usually significantly decreases. Although target tracking algorithms based on deep convolution and the Transformer have partially addressed these issues, achieving a good balance between the two remains a challenge. In this work, we propose a unified convolution and self-attention Siamese network for target tracking. By utilizing a feature extraction backbone network based on integrated convolution and self-attention styles, we are able to capture globally important regions and key frames while greatly reducing local redundant computations, thereby improving tracking performance. We apply this approach to the task of target tracking and enhance the feature extraction capabilities of both the target template and search template. Experimental results show that our proposed tracking algorithm outperforms some recent classical tracking algorithms, especially achieving improvements of 10.7 % on the high-diversity dataset GOT-10K and 24.7 % on the large-scale and high-quality LaSOT dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000221",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computation",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Eye tracking",
      "Feature extraction",
      "Operating system",
      "Pattern recognition (psychology)",
      "Pedagogy",
      "Psychology",
      "Redundancy (engineering)",
      "Tracking (education)"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Peng"
      },
      {
        "surname": "Duan",
        "given_name": "Zhuolei"
      },
      {
        "surname": "Guan",
        "given_name": "Sujie"
      },
      {
        "surname": "Li",
        "given_name": "Min"
      },
      {
        "surname": "Deng",
        "given_name": "Shaobo"
      }
    ]
  },
  {
    "title": "MRN-LOD: Multi-exposure Refinement Network for Low-light Object Detection",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104079",
    "abstract": "Low-light conditions present a myriad of intricacies for object detection, with many existing methods relying primarily on image enhancement before detection. Sometimes, the enhancement methods are unable to improve the detection performance in low-light conditions. In this paper, we present a new Multi-exposure refinement network for low-light object detection (MRN-LOD) to avoid the need for enhancement before detection. The MRN-LOD contains: multi-exposure feature extractor, adaptive refinement network, and detection head. The developed multi-exposure feature extractor extracts features from the multi-exposure images generated by the low-light image. We introduced the notion of feature extraction from multi-exposure images for object detection in low light. In addition, we proposed an adaptive refinement network to refine the features of low-light images for better detection performance. The detection head uses the refined features to perform object detection. Extensive experimentation on real-world and synthetic datasets shows the superiority of the proposed MRN-LOD.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000348",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Engineering",
      "Extractor",
      "Feature (linguistics)",
      "Feature extraction",
      "Linguistics",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Process engineering"
    ],
    "authors": [
      {
        "surname": "Singh",
        "given_name": "Kavinder"
      },
      {
        "surname": "Parihar",
        "given_name": "Anil Singh"
      }
    ]
  },
  {
    "title": "AMP-BCS: AMP-based image block compressed sensing with permutation of sparsified DCT coefficients",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104092",
    "abstract": "Block compressive sensing (BCS), an emerging approach for signal acquisition and reconstruction, combines high-speed sampling and compression, making it widely applicable in various imaging tasks. However, image BCS generally face the issues: challenges in accurate sampling rate allocation (SRA) and block artifact removal, and poor reconstruction algorithms. In this paper, we propose an approximate message passing (AMP)-based BCS (AMP-BCS) method. Specifically, within the sampling module, a sparsified DCT coefficient-based permutation strategy is proposed to achieve uniform energy distribution among blocks, effectively addressing the issue of SRA. Within the reconstruction module, by reweighting shallow and deep multi-scale features using several attention mechanisms, the multi-scale deep attention network (MDANet) is proposed to improve the denoising capabilities of the AMP reconstruction. Through independent sampling and joint iterative denoising, block artifacts are substantially removed. Extensive experiments demonstrate that the AMP-BCS method significantly outperforms current state-of-the-art BCS algorithms in both visual perception and objective metrics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000476",
    "keywords": [
      "Acoustics",
      "Algorithm",
      "Artifact (error)",
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Compressed sensing",
      "Computer science",
      "Computer vision",
      "Discrete cosine transform",
      "Filter (signal processing)",
      "Geometry",
      "Image (mathematics)",
      "Iterative reconstruction",
      "Mathematics",
      "Noise reduction",
      "Permutation (music)",
      "Physics",
      "Quantum mechanics",
      "Sampling (signal processing)",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Junhui"
      },
      {
        "surname": "Hou",
        "given_name": "Xingsong"
      },
      {
        "surname": "Wang",
        "given_name": "Huake"
      },
      {
        "surname": "Bi",
        "given_name": "Shuhao"
      },
      {
        "surname": "Qian",
        "given_name": "Xueming"
      }
    ]
  },
  {
    "title": "Multiple correlation filters with gaussian constraint for fast online tracking",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104089",
    "abstract": "The correlation filter based online tracking methods usually can achieve high real-time performance due to the leverage of the well-known FFT. However, they are also apt to generate the “corrupted training samples” in scenarios with complex background, which will trigger the model drift and deteriorate the tracking accuracy rapidly. The existing methods usually consider this problem from certain aspect and none of them has mined the potential of combining multiple formulas. In this paper, we propose the Output Constraint Transfer - Discriminative Scale Space Tracking (OCT-DSST) algorithm, which has taken full consideration of multiple channel feature, multiple filters, kernel trick, memory with incremental learning, and the self-supervision mechanism. We re-formulate the online tracking process by combining all formulas above in a unified framework. The so obtained adaptive learning rate can better exploit the feedback information coming from the intermediate tracking results, and effectively mitigate the corrupted sample problem. The experimental results on the OTB-50/100 and the VOT2016 datasets reveal that the proposed method is comparative to most state-of-the-arts algorithms, and can increase the accuracy by 2% and the success rate by 1.7%, compared to the traditional DSST method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000440",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Constraint (computer-aided design)",
      "Discriminative model",
      "Fast Fourier transform",
      "Filter (signal processing)",
      "Geometry",
      "Kernel (algebra)",
      "Leverage (statistics)",
      "Mathematics",
      "Pedagogy",
      "Psychology",
      "Tracking (education)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Jianyi"
      },
      {
        "surname": "Huang",
        "given_name": "Xingxing"
      },
      {
        "surname": "Shu",
        "given_name": "Xinyu"
      },
      {
        "surname": "Dong",
        "given_name": "Xudong"
      }
    ]
  },
  {
    "title": "Self-supervised learning monocular depth estimation from internet photos",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104063",
    "abstract": "Monocular depth estimation (MDE) is a fundamental problem in computer vision. Recently, self-supervised learning (SSL) approaches have attracted significant attention due to the ability to train an MDE network without ground-truth depth data. However, the performance of most existing SSL-MDE methods is yet limited by the available real training dataset, which are either binocular stereo pairs or monocular video sequences. In this paper, we propose a simple but effective generalization of SSL framework such that collections of multiple view Internet photos, a virtually unlimited source of real data, are enabled to train an MDE network. Combining the depth consistency and the mask that alleviates the interference such as moving objects, the network benefits from the real correspondences in adjacent views, thus achieving the improvement. Experiments show that the generalization of Monodepth2 via the proposed method not only leads to superior performance than itself and some data-driven MDE methods, but also stably boosts the performance of multiple state-of-the-art SSL-MDE methods. Besides, experiments on SeasonDepth, a dataset with various environmental conditions, show the good generalization capability of our proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032400018X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Deep learning",
      "Generalization",
      "Ground truth",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Monocular",
      "Pattern recognition (psychology)",
      "The Internet",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Lin",
        "given_name": "Xiaocan"
      },
      {
        "surname": "Li",
        "given_name": "Nan"
      }
    ]
  },
  {
    "title": "Blind quality assessment of light field image based on view and focus stacks",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104074",
    "abstract": "Light field imaging can capture rich information of real scenes, but various distortions will inevitably be introduced in the process of light field image processing. Therefore, it is very important to effectively evaluate the quality of light field images. Due to the lack of commercial light field displays, this paper proposes a blind quality assessment method of light field image based on view and focus stack, based on the visualization technology used in the subjective evaluation of light field images and the human brain's perception process of visual information in the study of visual physiology. First, spatiotemporal Gabor filtering is performed on distorted sub-aperture images (SAIs) to extract the spatial and angular information. On the other hand, the SAIs are refocused to obtain the refocused images with different focusing depths, and then the focus stack is filtered by spatiotemporal Gabor filtering to extract the angular information. In addition, the focus stack is detected to form in-focus area and out-of-focus area, and the features related to spatial structure, depth and semantic information of the refocused images are extracted to evaluate the spatial quality. Finally, support vector regression (SVR) is performed to predict the objective scores by establishing the relationship between the features and subjective scores. Experimental results show that our method can well predict the human eye's perception quality of light field images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000294",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Depth of field",
      "Field (mathematics)",
      "Focus (optics)",
      "Image (mathematics)",
      "Image quality",
      "Light field",
      "Mathematics",
      "Optics",
      "Physics",
      "Pure mathematics",
      "Quality (philosophy)",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Fucui"
      },
      {
        "surname": "Ye",
        "given_name": "Mengmeng"
      },
      {
        "surname": "Shao",
        "given_name": "Feng"
      }
    ]
  },
  {
    "title": "Contextual recovery network for low-light image enhancement with texture recovery",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104050",
    "abstract": "Low-light image enhancement has been a challenging topic in computer vision. In order to recover colors and detailed textures in images, several data-driven enhancement based methods have been developed and obtained encouraging results. However, the network generalization ability is not satisfactory due to the uncertainty of the collected data. In order to address this issue, we propose a network with texture enhancement (TEENet), which synergizes the image brightness recovery and recovers the texture information lost during the process. To recover image brightness, we propose a low-light image enhancement network with a squeeze-excitation operation and construct a texture-optimized network that combines contextual aggregation information to recover texture loss during the enhancement process. We conducted network performance tests using different data and ablation experiments to verify the performance of different components.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000051",
    "keywords": [
      "Artificial intelligence",
      "Brightness",
      "Computer science",
      "Computer vision",
      "Construct (python library)",
      "Generalization",
      "Image (mathematics)",
      "Image processing",
      "Image texture",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Optics",
      "Physics",
      "Process (computing)",
      "Programming language",
      "Texture (cosmology)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Zhen"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiaohuan"
      }
    ]
  },
  {
    "title": "CTHD-Net: CNN-Transformer hybrid dehazing network via residual global attention and gated boosting strategy",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104066",
    "abstract": "Single image dehazing is one of crucial tasks in the field of computer vision. However, existing methods are challenged on how to handle unevenly distributed haze, capture global contextual information, and filter noise while preserving details. To overcome these limitations, a novel dehazing network with residual global attention and gated boosting strategy based on a CNN-Transformer hybrid architecture (CTHD-Net) is proposed. Firstly, a feature encoder with a residual global attention (RGA) module is presented to improve the representation capability of the entire network by adaptively assigning different weights to feature maps. Subsequently, a CNN-Transformer hybrid architecture is designed to enhance the features encoding via the improved Swin-transformer and to capture the long-range dependencies among features by shifted-window Multi-head Self-attention. Finally, an effective Gated Strength-Operation-Subtract (GSOS) Boosting decoder is developed to reuse the key information required for image reconstruction in the shallow features, while effectively preventing haze noise. Extensive evaluation demonstrates that our proposed CTHD-Net significantly outperforms the previous state-of-the-arts in terms of quantity and quality. The source code has been made available at https://github.com/RC-Qiao/CTHD-Net.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032400021X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Boosting (machine learning)",
      "Computer engineering",
      "Computer science",
      "Computer vision",
      "Electrical engineering",
      "Encoder",
      "Engineering",
      "Operating system",
      "Pattern recognition (psychology)",
      "Real-time computing",
      "Residual",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Haiyan"
      },
      {
        "surname": "Qiao",
        "given_name": "Renchao"
      },
      {
        "surname": "Yu",
        "given_name": "Pengfei"
      },
      {
        "surname": "Li",
        "given_name": "Haijiang"
      },
      {
        "surname": "Tan",
        "given_name": "Mingchuan"
      }
    ]
  },
  {
    "title": "A channel-wise contextual module for learned intra video compression",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104070",
    "abstract": "In the multimedia era, exploding image and video data highlight the importance of video compression for storage and transmission. The All-Intra structure is a coding mode in HEVC and VVC, in which each frame is encoded using intra coding, and in this paper learned All-Intra coding is explored on the basis of the research of the learned image compression. A channel-wise contextual module based on channel segmentation is introduced to fully exploit non-local information. Then, two distinct attention mechanisms are designed for different feature layers to enhance the effectiveness of the transform network. Additionally, a post-processing module is employed to enhance the quality of decoded frames. Experimental results on the Kodak and Tecnick datasets demonstrate that the proposed method performs better than the majority of the recent learning-based methods and traditional image codecs (BPG, JPEG2000 and JPEG), and also perform better than traditional video codecs in terms of PSNR.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000257",
    "keywords": [
      "Artificial intelligence",
      "Channel (broadcasting)",
      "Codec",
      "Coding (social sciences)",
      "Computer hardware",
      "Computer network",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Data compression",
      "Exploit",
      "Image (mathematics)",
      "Image compression",
      "Image processing",
      "JPEG",
      "JPEG 2000",
      "Mathematics",
      "Multiview Video Coding",
      "Segmentation",
      "Statistics",
      "Video processing",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Zhan",
        "given_name": "Yanrui"
      },
      {
        "surname": "Xiong",
        "given_name": "Shuhua"
      },
      {
        "surname": "He",
        "given_name": "Xiaohai"
      },
      {
        "surname": "Tang",
        "given_name": "Bowen"
      },
      {
        "surname": "Chen",
        "given_name": "Honggang"
      }
    ]
  },
  {
    "title": "Low-complexity ℓ ∞ -compression of light field images with a deep-decompression stage",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104072",
    "abstract": "To enrich the functionalities of traditional cameras, light field cameras record both the intensity and direction of light rays, so that images can be rendered with user-defined camera parameters via computations. The added capability and flexibility are gained at the cost of gathering typically more than 100 perspectives of the same scene, resulting in large data volume. To cope with this issue, several light field compression schemes have been introduced. However, their ways of exploiting correlations of multidimensional light field data are complex and are hence not suited for cost-effective light field cameras. On the other hand, existing simpler compression schemes do not offer good compression performance. In this work, we propose a novel ℓ ∞ -constrained light-field image compression system that has a very low-complexity DPCM encoder and a CNN-based deep decoder enhancement. Targeting high-fidelity soft-decoding (restoration), the CNN decoding capitalizes on the ℓ ∞ -constraint, i.e. the maximum absolute error bound, and light field properties to remove the compression artifacts. Two different architectures for CNN decoder enhancement are proposed, one is based on 2D CNNs and optimized for fast inference, and another is based on 3D CNNs to maximize ℓ 2 restoration quality. The proposed networks achieve superior performance both in inference speed and restoration quality in comparison to state-of-the-art light field network architectures. In conjunction with ℓ ∞ -EPIC, the proposed architecture, while satisfying a well-defined ℓ ∞ constraint, outperforms existing state-of-the-art ℓ 2 -based light field compression methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000270",
    "keywords": [
      "Computer graphics (images)",
      "Computer science",
      "Scalable Vector Graphics",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Mukati",
        "given_name": "M. Umair"
      },
      {
        "surname": "Zhang",
        "given_name": "Xi"
      },
      {
        "surname": "Wu",
        "given_name": "Xiaolin"
      },
      {
        "surname": "Forchhammer",
        "given_name": "Søren"
      }
    ]
  },
  {
    "title": "Double branch synergies with modal reinforcement for weakly supervised temporal action detection",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104090",
    "abstract": "Weakly supervised Temporal Action localization (WTAL) aims to locate the action instances and identify their corresponding labels. Most current methods rely on a Multi-Instance Learning (MIL) framework to predict start and end boundaries of each action in a video. However, they have shortcomings of incomplete positioning and context confusion. Therefore, we propose an algorithm of Double Branch Synergies with Modal Reinforcement (DBSMR), which utilizes long-short temporal attention to model contextual relationships and refines segmental features to encourage more distinguishable segment classification. In terms of blur boundaries between actions and camouflage background in complex scenes and easily resulting in wrong positioning, we construct a sparse graph focusing on the effective representation of context motion by optical flow modal learning, further enhancing the representation of the active region to be examined, and suppressing the interference from the background. Finally, with the idea of ”All roads lead to Rome”, we design motion-guided loss constraints to balance the long-short temporal module and graph reinforcement module, by which the two branches can converge to almost the same detection goal, thus to achieve an agreement of near ground truth localization result. The algorithm achieves mAP of 69.1% and 42.0% detection performances on the THUMOS14 and ActivityNet1.2 datasets respectively. We also verify its effectiveness by comparing it with the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000452",
    "keywords": [
      "Action (physics)",
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Engineering",
      "Mathematics",
      "Modal",
      "Pattern recognition (psychology)",
      "Physics",
      "Polymer chemistry",
      "Quantum mechanics",
      "Reinforcement",
      "Structural engineering"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Chuanxu"
      },
      {
        "surname": "Wang",
        "given_name": "Jing"
      },
      {
        "surname": "Xu",
        "given_name": "Wenting"
      }
    ]
  },
  {
    "title": "Multi frame multi-head attention learning on deep features for recognizing Indian classical dance poses",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104091",
    "abstract": "The aim of this work is to develop a classifier for the Indian classical dance (ICD) online Bharatanatyam videos using deep learning techniques. Bharatanatyam is the most ancient and popular of all eight types of Indian classical dance forms recognized by the government of India. Many ICD enthusiasts struggle to synchronize their minds to the lyrics and complex dance poses during live performances. Therefore, they record performances on their mobile devices and review them later to improve their performance. The goal of this work is to help dance lovers enjoy live performances by using an AI-based dance pose recognizer. Previous ICD learning models have demonstrated that global feature representations of complex dance poses in video sequences with unpredictable background changes have unreliable performance metrics. Therefore, these models developed their own small dance datasets with few dancers posing under controlled lighting and backgrounds. However, these models have strained performance on online video data due to background, viewpoint, costume, lighting, blurring, and pose-to-background ratios. To overcome these challenges, this work proposes a multi-frame multi-head attention network that operates on multiple frames averaging on attention scores from multiple heads to improve the strained performance of deep learning models on online video data. Specifically, the multi-frame multi-head layer attention (MFMHLA) improves the random pixel distributions of the dancer in the ICD online videos by learning object relationships across frames. The MFMHLA layer works on a set of layered features across three consecutive frames per head to learn attention locations in the central frame. Similarly, multiple heads are employed to extract attention maps from overlapping consecutive frames which are further averaged to generate a semantic relational feature map. Subsequently, these averaged maps are generated at multiple feature resolutions across the ResNet-50 architecture. This results in a chronological enhancement of the dancer’s pose at multiple resolutions across the depth of features in ResNet-50 architecture along with Inception, VGG, and CapsuleNet. The experiments were conducted on our online Bharatanatyam ICD(BOICDVD22) with 10 songs. The results conclude that the presence of MFMHLA has improved pose feature representations of online dance videos burdened with deformations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000464",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Cognitive psychology",
      "Computer science",
      "Computer vision",
      "Dance",
      "Frame (networking)",
      "Geology",
      "Head (geology)",
      "Paleontology",
      "Psychology",
      "Telecommunications",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "D.",
        "given_name": "Anil Kumar"
      },
      {
        "surname": "P.V.V.",
        "given_name": "Kishore"
      },
      {
        "surname": "T.R.",
        "given_name": "Chaithanya"
      },
      {
        "surname": "K.",
        "given_name": "Sravani"
      }
    ]
  },
  {
    "title": "Methods for countering attacks on image watermarking schemes: Overview",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104073",
    "abstract": "Image watermarking is an effective and promising technology. Robust watermarks that are resistant to various attacks allow authors and owners of digital images to protect their rights to digital content, control its distribution and confirm its authenticity. Most of the modern algorithms for robust image watermarking aim to achieve resistance to a large number of different attacks. However, some authors develop algorithms designed to counter targeted attacks. The study of such schemes allows developers of watermarking algorithms to evaluate special means of counteracting various attacks, and then use them to create new robust schemes, both targeted and universal ones. In this paper, we present an overview of robust image watermarking schemes in terms of countering targeted attacks. We review the state-of-the-art in the field of attacking robust watermarks and propose a four-level classification of attacks that includes different levels of attack implementation, including an attacker’s intent, characteristics of actions, the main target and an attack type. The proposed classification considers a watermark as an object of attack and summarizes various characteristics of attacks in a hierarchical manner. We analyze the means of countering common attacks such as image processing attacks, geometric attacks, print-scan and screen capture attacks, collusion attacks, and ambiguity attacks. Based on the results of our review, we highlight the most common methods of countering attacks and formulate promising areas of research in the field of methods for improving security of embedding schemes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000282",
    "keywords": [
      "Ambiguity",
      "Artificial intelligence",
      "Collusion",
      "Computer science",
      "Computer security",
      "Cryptography",
      "Digital Watermarking Alliance",
      "Digital watermarking",
      "Economics",
      "Field (mathematics)",
      "Image (mathematics)",
      "Link encryption",
      "Mathematics",
      "Microeconomics",
      "Multiple encryption",
      "Programming language",
      "Pure mathematics",
      "Watermark",
      "Watermarking attack"
    ],
    "authors": [
      {
        "surname": "Melman",
        "given_name": "Anna"
      },
      {
        "surname": "Evsutin",
        "given_name": "Oleg"
      }
    ]
  },
  {
    "title": "Multi-image super-resolution based low complexity deep network for image compressive sensing reconstruction",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104071",
    "abstract": "Deep learning (DL) has been widely utilized in image compressive sensing (CS) to enhance the quality and speed of reconstruction. The typical deep network for CS reconstruction comprises an initial reconstruction subnetwork, followed by a cascaded deep refinement reconstruction subnetworks. This paper introduces a new low-complexity image CS deep reconstruction framework, GSRCS, which leverages multi-image based deep super-resolution technology to better address the cost constraints of practical applications. The proposed initial reconstruction module generates multiple low-resolution images in parallel by grouping the input measurements, while a high-quality, high-resolution reconstructed image is produced through a multi-image deep super-resolution network. The theoretical derivation and experimental results demonstrate that this method significantly reduces system complexity in terms of parameters and floating-point arithmetic operations, while achieving competitive reconstruction performance compared to the state-of-the-arts. Specifically, the average number of parameters is reduced by over 63%, and the computational complexity is decreased by more than 88%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000269",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Compressed sensing",
      "Computational complexity theory",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Deep learning",
      "Geometry",
      "Image (mathematics)",
      "Image quality",
      "Iterative reconstruction",
      "Mathematics",
      "Reconstruction algorithm",
      "Reduction (mathematics)",
      "Subnetwork"
    ],
    "authors": [
      {
        "surname": "Xiong",
        "given_name": "Qiming"
      },
      {
        "surname": "Gao",
        "given_name": "Zhirong"
      },
      {
        "surname": "Ma",
        "given_name": "Jiayi"
      },
      {
        "surname": "Ma",
        "given_name": "Yong"
      }
    ]
  },
  {
    "title": "Multiple object tracking with segmentation and interactive multiple model",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104064",
    "abstract": "Multiple object tracking (MOT) is a sophisticated computer vision task that aims to detect and track the trajectories of all objects within a given scene. MOT necessitates the establishment of unique identifiers for each object in the scene and currently, the majority of MOT works adopt tracking-by-detection, using re-identification techniques to associate objects based on appearance or motion features. However, traditional MOT methods may yield suboptimal results when appearance features are unreliable or geometric features are confounded by irregular motions. Consequently, this paper proposes a cascaded matching tracker called IMMSegTrack, which replaces detection boxes with segmentation contours for IoU matching and generates convincing prediction outcomes by blending multiple adaptive Kalman filter models. With the guide of interactive multiple model and pixel-level matching, better performance could be achieved through well-designed cascaded association. Our experimental innovations mainly focus on the prediction and matching aspects within the MOT framework, while tests on DanceTrack and SoccerNet datasets have been conducted using the original models of YOLOv8 to obtain desired results. The relevant codes for the experiments are available at https://github.com/xwh129/IMMSegTrack.git. In addition, it is considerable to retrain model weights and expand the experiments to more datasets for better performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000191",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Computer vision",
      "Focus (optics)",
      "Identification (biology)",
      "Identifier",
      "Kalman filter",
      "Matching (statistics)",
      "Mathematics",
      "Object (grammar)",
      "Object detection",
      "Optics",
      "Pattern recognition (psychology)",
      "Pedagogy",
      "Physics",
      "Programming language",
      "Psychology",
      "Segmentation",
      "Statistics",
      "Tracking (education)",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Qi",
        "given_name": "Ke"
      },
      {
        "surname": "Xu",
        "given_name": "Wenhao"
      },
      {
        "surname": "Chen",
        "given_name": "Wenbin"
      },
      {
        "surname": "Tao",
        "given_name": "Xi"
      },
      {
        "surname": "Chen",
        "given_name": "Peijia"
      }
    ]
  },
  {
    "title": "An active contour model based on Jeffreys divergence and clustering technology for image segmentation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104069",
    "abstract": "Classic active contour models (ACMs) generally implement Euclidean distance to measure the gap between true image and fitted one, which may cause issues such as edge leakage and falling into false boundary. In addition, some existing ACMs are sensitive to noise and different initial contours. To resolve these problems, this study raises an ACM based on Jeffreys divergence (KJD) and clustering technique for image segmentation. Firstly, the K-medoids clustering algorithm is deployed to cluster the foreground and background pixels into two sets, which forms original data-driven term and is further embedded into the theory of Jeffreys divergence to formulate a KJD energy. Next, a regularization function regularizes the ranges of optimized data-driven term and level set function respectively, which essentially produces a more stable and robust evolution environment and improves segmentation precision. In contrast with fitting function-based and recently developed ACMs on three types of images, KJD model not only raises segmentation precision, but also reduces computation expense. Experimental outcomes also verify that this model can resist different noise within limits and adapts to various initial contours.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000245",
    "keywords": [
      "Active contour model",
      "Artificial intelligence",
      "Boundary (topology)",
      "Cluster analysis",
      "Computer science",
      "Computer vision",
      "Divergence (linguistics)",
      "Euclidean distance",
      "Image segmentation",
      "Level set (data structures)",
      "Level set method",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Regularization (linguistics)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Ge",
        "given_name": "Pengqiang"
      },
      {
        "surname": "Chen",
        "given_name": "Yiyang"
      },
      {
        "surname": "Wang",
        "given_name": "Guina"
      },
      {
        "surname": "Weng",
        "given_name": "Guirong"
      }
    ]
  },
  {
    "title": "Context-aided unicity matching for person re-identification",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104077",
    "abstract": "Most existing person re-identification methods compute the matching relations between person images based on the similarity ranking. It lacks the global viewpoint and context consideration, inevitably leading to ambiguous matching results and sub-optimal performance. Based on a natural assumption that images belonging to the same identity should not match with images belonging to different identities across views, called the unicity of person matching on the identity level, we propose an end-to-end person unicity matching architecture to learn and refine the person matching relations. We first employ the image samples’ context in feature space to generate the initial soft matchings using graph neural networks, and then utilize the samples’ global context to refine the soft matchings and reach the matching unicity by bipartite graph matching. Considering real-world applications, we further develop a fast algorithm without losing performance. Experimental results on five public benchmarks show the superiority of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000324",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Context (archaeology)",
      "Geography",
      "Identification (biology)",
      "Matching (statistics)",
      "Mathematics",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Cao",
        "given_name": "Min"
      },
      {
        "surname": "Ding",
        "given_name": "Cong"
      },
      {
        "surname": "Chen",
        "given_name": "Chen"
      },
      {
        "surname": "Peng",
        "given_name": "Silong"
      }
    ]
  },
  {
    "title": "LFSimCC: Spatial fusion lightweight network for human pose estimation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104093",
    "abstract": "To address the limitations of existing 2D human pose estimation methods in terms of speed and lightweight, we propose a method called Lightweight Fusion SimCC (LFSimCC). LFSimCC incorporates two modules: LiteFNet, which enhances multi-scale spatial information fusion, and LKC-GAU, which improves the modeling capability of spatial information. Specifically, LiteFNet utilizes a combination of self-attention mechanism and novel spatial convolution to enable feature maps to capture richer multi-level global feature representations within the network. On the other hand, LKC-GAU enhances SimCC’s ability to capture spatial relationships between joints by incorporating a large kernel of convolution and a self-attention mechanism. Furthermore, we design a keypoint information fusion loss (IFL) that enhances the model’s sensitivity to information between keypoints in the human body. Experimental results demonstrate that our method is capable of extracting more decisive information and suppressing redundant feature representations, leading to high recognition accuracy and low inference latency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000488",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Engineering",
      "Estimation",
      "Fusion",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pose",
      "Systems engineering"
    ],
    "authors": [
      {
        "surname": "Zheng",
        "given_name": "Qian"
      },
      {
        "surname": "Guo",
        "given_name": "Hualing"
      },
      {
        "surname": "Yin",
        "given_name": "Yunhua"
      },
      {
        "surname": "Zheng",
        "given_name": "Bin"
      },
      {
        "surname": "Jiang",
        "given_name": "Hongxu"
      }
    ]
  },
  {
    "title": "Coarse-to-fine underwater image enhancement with lightweight CNN and attention-based refinement",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104068",
    "abstract": "This paper presents a deep learning-based underwater image enhancement method supported by a classical image processing technique. The proposed method includes an end-to-end three-module structure. The first module is a lightweight two-branch network that retrieves lost colors and to some extent overall appearance through the global and local image enhancement. The second module is the modified histogram equalization to improve the global intensity, contrast and local colors of the image by controlling over-intensity and artificial colors that may result from histogram equalization. The last part is the attention module, utilized to help the proposed framework have a synergistic combination of the previous modules. The attention module is designed to combine the advantages of the previous modules and evade their drawbacks. Experiments to objectively and subjectively evaluate the performance of the proposed model show that the proposed model is superior to existing underwater image enhancement methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000233",
    "keywords": [
      "Adaptive histogram equalization",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Contrast (vision)",
      "Geology",
      "Histogram",
      "Histogram equalization",
      "Image (mathematics)",
      "Image enhancement",
      "Oceanography",
      "Underwater"
    ],
    "authors": [
      {
        "surname": "Khandouzi",
        "given_name": "Ali"
      },
      {
        "surname": "Ezoji",
        "given_name": "Mehdi"
      }
    ]
  },
  {
    "title": "Copy Move Forgery detection and localisation robust to rotation using block based Discrete Cosine Transform and eigenvalues",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104075",
    "abstract": "The contemporary era faces a widespread issue with digital image forgery, posing a significant challenge due to its ease and the broad reach enabled by high-speed internet. This manipulation of images carries substantial socio-political implications globally. Hence, robust digital image forensic methods are critical for detecting such forgeries. This article presents an innovative algorithm specifically designed to detect and locate copy-move duplication within digital images. By utilising the Discrete Cosine Transform and eigenvalues as distinguishing features, the algorithm precisely identifies and pinpoints replicated image regions from overlapping pixel blocks. Uniquely, another cumulative DCT feature enhances the algorithm’s ability to discern duplicated regions, even when subjected to post-processing rotation attacks. Experiments using various datasets demonstrate that this method outperforms avant-garde techniques in detecting and localising forgeries, showcasing promising results. This approach contributes significantly to the field of digital image forensics, providing a valuable tool for identifying and localising manipulated content.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000300",
    "keywords": [
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Digital forensics",
      "Digital image",
      "Discrete cosine transform",
      "Feature (linguistics)",
      "Geometry",
      "Image (mathematics)",
      "Image processing",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Rotation (mathematics)"
    ],
    "authors": [
      {
        "surname": "Shehin",
        "given_name": "A.U."
      },
      {
        "surname": "Sankar",
        "given_name": "Deepa"
      }
    ]
  },
  {
    "title": "A domain generalized person re-identification algorithm based on meta-bond domain alignment☆",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104054",
    "abstract": "Domain Generalization (DG) model is an important tool to improve the robustness of person re-identification algorithm, but the domain gap makes it difficult to transfer knowledge cross-domain effectively. To solve the above problems, this paper proposes a generalization model based on a Meta-Bond Domain Alignment (M−BDA) model. To learn a generalizable model, a meta-learning strategy is introduced to simulate the training–testing process of the domain generalization. Then a bond-domain module is constructed in the training to align the source domain, which can reduce the domain gap between the source domain and the target domain, and facilitate the knowledge transfer. Finally, the bond-domain loss is counted on the feature space of the bond-domain to prevent the generated bond-domain from overfitting to the source domain. The experimental results show that the proposed algorithm achieves better performance in DukeMTMC-ReID, Market1501 and MSMT17 alternating as the source and the target domain tasks. In the generalization experiments of Market-1501 → DukeMTMC-ReID, mAP and Rank-1 increased by 7.7 % and 3.6 %, respectively, and in the generalization experiments of DukeMTMC-ReID → Market-1501, mAP and Rank-1 are increased by 9.3 % and 3.5 %, respectively, which are significantly better than the newer representative algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000099",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Combinatorics",
      "Computer science",
      "Domain (mathematical analysis)",
      "Gene",
      "Generalization",
      "Mathematical analysis",
      "Mathematics",
      "Overfitting",
      "Rank (graph theory)",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Baohua"
      },
      {
        "surname": "Wu",
        "given_name": "Dongyang"
      },
      {
        "surname": "Lu",
        "given_name": "Xiaoqi"
      },
      {
        "surname": "Li",
        "given_name": "Yongxiang"
      },
      {
        "surname": "Gu",
        "given_name": "Yu"
      },
      {
        "surname": "Li",
        "given_name": "Jianjun"
      },
      {
        "surname": "Wang",
        "given_name": "Jingyu"
      }
    ]
  },
  {
    "title": "Context-based modeling for accurate logo detection in complex environments",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104061",
    "abstract": "Logo detection involves the tasks of locating and classifying logo objects in images and videos, and has been widely applied in the real world. However, most existing approaches rely on general object detection strategies that do not fully utilize the unique characteristics of logos. This can lead to sub-optimal performance in complex environments, especially when logos are small or have varying sizes and shapes. We observe that logos belonging to the same category often share similar context information, such as background dependency of the logo. This motivates us to incorporate contextual information to improve logo detection. Our proposed method, Context-based Modeling Enhancement Network (CME-Net), aims to enhance the distinctive region feature of logos using contextual information. We achieve this by modeling both the logo and its background region to extract their contextual information. This contextual information serves as a guide for enhancing the saliency of the distinctive regions within the logo image. To further improve the accuracy of detection, we have implemented a scale feature balance strategy. This strategy solves the problem of losing scale information caused by enhancement, ensuring that all scales are appropriately considered. Additionally, noise generated during the enhancement process is also effectively suppressed. By effectively leveraging contextual information, our method successfully tackles the challenge of accurately locating logo objects. Our extensive experiments on four public benchmark datasets demonstrate that CME-Net improves the accuracy of logo detection in complex environments.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000166",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Feature (linguistics)",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Linguistics",
      "Logo (programming language)",
      "Machine learning",
      "Noise (video)",
      "Object detection",
      "Operating system",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Process (computing)",
      "Programming language",
      "Quantum mechanics",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "Jia",
        "given_name": "Zhixiang"
      },
      {
        "surname": "Hou",
        "given_name": "Sujuan"
      },
      {
        "surname": "Li",
        "given_name": "Peng"
      }
    ]
  },
  {
    "title": "BNDCNet: Bilateral nonlocal decoupled convergence network for semantic segmentation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2023.104028",
    "abstract": "The perceptual scope of deep convolutional neural networks is inherently confined to a local scale due to the inherent limitations of convolution operations. This confinement subsequently hampers a comprehensive understanding of intricate scenes. In response, an innovative approach called Bilateral Non-Local Decoupled Convergence Network (BNDCNet) is proposed to facilitate contextual interaction of global information. The proposed module decouples the information from the input feature map and uses a bilateral non-local architecture for system processing. This strategy facilitates inter-pixel interaction and aggregates global information. An important aspect of our approach involves the computation of adaptive convolutional channel weights for the feature map. This innovation greatly improves the efficacy and performance of the model. The proposed method achieved high performance on the competitive scene-parsing datasets CamVid, Cityscapes, and KITTI, and thus demonstrated effectiveness and generality. Code has been released: https://github.com/Mantee0810/BNDC.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032300278X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Ye",
        "given_name": "Mengting"
      },
      {
        "surname": "Chen",
        "given_name": "Zhenxue"
      },
      {
        "surname": "Guo",
        "given_name": "Yixin"
      },
      {
        "surname": "Yu",
        "given_name": "Kaili"
      },
      {
        "surname": "Liu",
        "given_name": "Longcheng"
      },
      {
        "surname": "Wu",
        "given_name": "Q.M. Jonathan"
      }
    ]
  },
  {
    "title": "Facial attribute editing method combined with parallel GAN for attribute separation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2023.104031",
    "abstract": "Facial attribute editing encounters problems of incorrect changes to face regions and artifacts in generated images. We propose a facial attribute editing method combined with parallel GAN for attribute separation. First, the method integrates the U2-net encoder and Trans-GAN decoder as a model encoder to extract and generate facial spatial information effectively. Second, RGB images and semantic mask images are used to train a parallel generator and discriminator respectively. Semantic consistency loss is introduced to ensure that the two branches have consistent semantic output and achieve the effect of convergence in the same direction parallel generator and discriminator. The proposed model, trained on the CelebAMask-HQ original dataset and verified by the CelebA dataset, adopts the separation of the face mask image and the background mask image, to improve the correct rate of face attribute editing. Compared with existing facial attribute editing methods, the proposed method is capable of balancing attribute editing ability and details preservation ability. It can accurately edit the target attribute area and greatly improve the quality of facial images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032300281X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Separation (statistics)"
    ],
    "authors": [
      {
        "surname": "Wei",
        "given_name": "Jia"
      },
      {
        "surname": "Wang",
        "given_name": "Wei"
      }
    ]
  },
  {
    "title": "Feature rectification and enhancement for no-reference image quality assessment",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2023.104030",
    "abstract": "The assessment of image is a critical task in computer vision, particularly in a no-referenced context, which presents numerous challenges. Though Deep neural networks have exhibited excellent performance on no-reference quality assessment (NR-IQA), the distortion-aware NR-IQA methods are limited to specific distortion type and struggle to handle realistic distortion scenarios. In this work, we propose a feature rectification and enhancement convolutional neural network namely FREIQA for NR-IQA. Our approach targets to fully utilize the multi-stage semantic features to produce a well fused and rectified score vector for effective image quality regression. Specifically, the multi-stage semantic features extracted by a pre-trained backbone would be rectified with a multi-level channel attention module before integrating to a global score vector. The fused score vector then will be gradually optimized through the perception feature provided by the multi-stage semantic feature, and finally sent to quality regression network to obtain the image quality score. Experimental results on public benchmarks including LIVE, TID2013, CSIQ, LIVEC, KADID-10k, KonIQ-10k and Waterloo exploration database show that our FREIQA achieve the state-of-the-art performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002808",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Electrical engineering",
      "Engineering",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Image quality",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quality (philosophy)",
      "Quantum mechanics",
      "Rectification",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Wei"
      },
      {
        "surname": "Huang",
        "given_name": "Daoquan"
      },
      {
        "surname": "Yao",
        "given_name": "Yang"
      },
      {
        "surname": "Shen",
        "given_name": "Zhuonan"
      },
      {
        "surname": "Zhang",
        "given_name": "Hua"
      },
      {
        "surname": "Yan",
        "given_name": "Chenggang"
      },
      {
        "surname": "Zheng",
        "given_name": "Bolun"
      }
    ]
  },
  {
    "title": "Recovering sign bits of DCT coefficients in digital images as an optimization problem",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2023.104045",
    "abstract": "Recovering unknown, missing, damaged, distorted, or lost information in DCT coefficients is a common task in multiple applications of digital image processing, including image compression, selective image encryption, and image communication. This paper investigates the recovery of sign bits in DCT coefficients of digital images, by proposing two different approximation methods to solve a mixed integer linear programming (MILP) problem, which is NP-hard in general. One method is a relaxation of the MILP problem to a linear programming (LP) problem, and the other splits the original MILP problem into some smaller MILP problems and an LP problem. We considered how the proposed methods can be applied to JPEG-encoded images and conducted extensive experiments to validate their performances. The experimental results showed that the proposed methods outperformed other existing methods by a substantial margin, both according to objective quality metrics and our subjective evaluation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032300295X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Data compression",
      "Discrete cosine transform",
      "Encryption",
      "Image (mathematics)",
      "Image compression",
      "Image processing",
      "Image quality",
      "Integer programming",
      "JPEG",
      "Linear programming",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Psychology",
      "Relaxation (psychology)",
      "Sign (mathematics)",
      "Social psychology"
    ],
    "authors": [
      {
        "surname": "Lin",
        "given_name": "Ruiyuan"
      },
      {
        "surname": "Liu",
        "given_name": "Sheng"
      },
      {
        "surname": "Jiang",
        "given_name": "Jun"
      },
      {
        "surname": "Li",
        "given_name": "Shujun"
      },
      {
        "surname": "Li",
        "given_name": "Chengqing"
      },
      {
        "surname": "Kuo",
        "given_name": "C.-C. Jay"
      }
    ]
  },
  {
    "title": "Chronological Gazelle Optimization with Deep Learning-Based pixel prediction for video steganography in H.264 video for defence applications",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2023.104024",
    "abstract": "With the recent progress in Information technology and the internet, there has been an increase in violations of information security and privacy, particularly in the defence domain. In this work, a secure video steganography method using deep learning-based pixel prediction in H.264 video is presented. The pixels in the keyframe on which the secret image is embedded are predicted using a Deep Convolutional Neural Network (DCNN) optimized by the Chronological Gazelle optimization algorithm (CGOA). Later, embedding is carried out using Haar Wavelet Transform (HWT). The security of the proposed steganography technique has been analysed by performing steganalysis using a Convolutional Neural Network (CNN). The efficiency of the approach is examined by considering evaluation measures, like structural Similarity Index measure (SSIM), Normalized Correlation (NC), Peak Signal to Noise Ratio (PSNR), Bit Error Rate (BER), and Mean Squared Error (MSE), and has attained values of 0.979, 0.974, 49.624, 4.655, and 0.790, revealing imperceptibility and robustness.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002742",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Pattern recognition (psychology)",
      "Pixel",
      "Steganography"
    ],
    "authors": [
      {
        "surname": "Sriram",
        "given_name": "K.V."
      },
      {
        "surname": "Havaldar",
        "given_name": "R.H."
      }
    ]
  },
  {
    "title": "Octagonal lattice-based triangulated shape descriptor engaging second-order derivatives supplementing image retrieval",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2023.104005",
    "abstract": "Erstwhile shape description schemes lack primarily in establishing trade-offs with accuracy and computational load. Accordingly, a lightweight shape descriptor offering precise definition and compaction of high-frequency features is contributed in this paper using a simple geometrical shape for localization and shape characterization. Initially, the input image is octagonally tessellated and triangularly decomposed into sub-regions whose side-wise differences are evaluated and subjected to second-order differentiation to produce three high-frequency values representing triangle corners. The resultant is processed by the law of sines to yield localized shape features exhibiting congruence and is reiterated on the residual regions, followed by a novel octal encoding scheme encompassing maximal variations in the localized regions. The resulting features are globally fabricated into shape histograms in a non-overlapping manner representing the shape vector. This scheme validated on widely popular benchmark shape datasets demonstrates superior retrieval and recognition accuracies greater than 93% which is lacking in its competitors.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002559",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Geodesy",
      "Geology",
      "Geometry",
      "Histogram",
      "Image (mathematics)",
      "Mathematics",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Kanimozhi",
        "given_name": "M."
      },
      {
        "surname": "Sudhakar",
        "given_name": "M.S."
      }
    ]
  },
  {
    "title": "Weakly-supervised cloud detection and effective cloud removal for remote sensing images",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2023.104006",
    "abstract": "We propose a one-stop automatic cloud processing scheme for RSI, including weakly supervised cloud detection and effective cloud removal. First, to avoid using massive expensive paired-labeled data, we use the idea of adversarial training to detect clouds, that is, randomly selecting some unpaired cloud and cloudless images to alternatively train a spectral-normalized Markov discriminator to obtain cloud masks. Second, a two-stage downsampling-restoring-upsampling-refining scheme is used to remove detected clouds. In order to further improve the rationality and fineness, we construct a fractional-order anisotropic filter kernel and design a convolution process to impose spatial regularization constraints on the loss construction at each stage, taking to account confidence values and structural priorities. Comprehensive experimental results show that our proposed networks are superior to the SOTA methods in terms of objective indicators and subjective performance. In addition, the performance of our one-stop approach in automatically detecting and eliminating clouds is also very satisfactory.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002560",
    "keywords": [
      "Artificial intelligence",
      "Cloud computing",
      "Combinatorics",
      "Computer science",
      "Image (mathematics)",
      "Kernel (algebra)",
      "Mathematics",
      "Operating system",
      "Upsampling"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Xiuhong"
      },
      {
        "surname": "Gou",
        "given_name": "Tiankun"
      },
      {
        "surname": "Lv",
        "given_name": "Zhiyong"
      },
      {
        "surname": "Li",
        "given_name": "Leida"
      },
      {
        "surname": "Jin",
        "given_name": "Haiyan"
      }
    ]
  },
  {
    "title": "Intra-Inter Region Adaptive Graph Convolutional Networks for skeleton-based action recognition",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2023.104020",
    "abstract": "To effectively capture the spatio-temporal dependencies of the skeletal data, graph convolution has been widely applied. However, it tends to emphasize the dependence relationship between adjacent joints and does not consider long-distance dependence relationships among joints. Another problem is single-structure temporal convolution, which is difficult to extract global temporal features. To address the above issues, we propose Intra-Inter Region Adaptive Graph Convolutional Networks (IIR-AGCN), which models the long-distance relationships of skeleton sequences at temporal and spatial scales. Our contributions are three-fold: First, to enhance global topological learning capabilities of graph convolution, we propose a regional-coupled attention module, which divides joint features into multiple sub-regions, and then constructs coupled relationships between intra-inter regions by self-attention mechanism, which realizes global joint information interaction. Second, to address the issue of difficulty in extracting global temporal features, we propose a pyramid multi-scale temporal module that extracts deep multi-scale temporal features and implements adaptive cross-scale feature fusion. Third, IIR-AGCN adopts a two-stream architecture, evaluating performances on two large-scale human skeleton datasets, NTU-RGB+D 60 and NTU-RGB+D 120, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002705",
    "keywords": [
      "Algorithm",
      "Architectural engineering",
      "Artificial intelligence",
      "Artificial neural network",
      "Cartography",
      "Computer science",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Deep learning",
      "Engineering",
      "Feature (linguistics)",
      "Geography",
      "Graph",
      "Joint (building)",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "RGB color model",
      "Scale (ratio)",
      "Skeleton (computer programming)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Wenting"
      },
      {
        "surname": "Wang",
        "given_name": "Chuanxu"
      },
      {
        "surname": "Zhang",
        "given_name": "Zhe"
      },
      {
        "surname": "Lin",
        "given_name": "Guocheng"
      },
      {
        "surname": "Sun",
        "given_name": "Yue"
      }
    ]
  },
  {
    "title": "Corner-to-Center long-range context model for efficient learned image compression",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2023.103990",
    "abstract": "In the framework of learned image compression, the context model plays a pivotal role in capturing the dependencies among latent representations. To reduce the decoding time resulting from the serial autoregressive context model, the parallel context model has been proposed as an alternative that necessitates only two passes during the decoding phase, thus facilitating efficient image compression in real-world scenarios. However, performance degradation occurs due to its incomplete casual context. To tackle this issue, we conduct an in-depth analysis of the performance degradation observed in existing parallel context models, focusing on two aspects: the Quantity and Quality of information utilized for context prediction and decoding. Based on such analysis, we propose the Corner-to-Center transformer-based Context Model (C 3 M) designed to enhance context and latent predictions and improve rate–distortion performance. Specifically, we leverage the logarithmic-based prediction order to predict more context features from corner to center progressively. In addition, to enlarge the receptive field in the analysis and synthesis transformation, we use the Long-range Crossing Attention Module (LCAM) in the encoder/decoder to capture the long-range semantic information by assigning the different window shapes in different channels. Extensive experimental evaluations show that the proposed method is effective and outperforms the state-of-the-art parallel methods. Finally, according to the subjective analysis, we suggest that improving the detailed representation in transformer-based image compression is a promising direction to be explored.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002407",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Autoregressive model",
      "Biology",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Context model",
      "Data compression",
      "Decoding methods",
      "Encoder",
      "Image (mathematics)",
      "Image compression",
      "Image processing",
      "Leverage (statistics)",
      "Mathematics",
      "Object (grammar)",
      "Operating system",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Statistics",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Sui",
        "given_name": "Yang"
      },
      {
        "surname": "Ding",
        "given_name": "Ding"
      },
      {
        "surname": "Pan",
        "given_name": "Xiang"
      },
      {
        "surname": "Xu",
        "given_name": "Xiaozhong"
      },
      {
        "surname": "Liu",
        "given_name": "Shan"
      },
      {
        "surname": "Yuan",
        "given_name": "Bo"
      },
      {
        "surname": "Chen",
        "given_name": "Zhenzhong"
      }
    ]
  },
  {
    "title": "A no-reference image quality assessment model based on neighborhood component analysis and Gaussian process",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2023.104041",
    "abstract": "In this paper, a human visual system (HVS) based no-reference image quality assessment model (NR-IQA) is proposed. This is an objective NR-IQA model that uses perceptual features such as structure and orientation to compute the loss of naturalness in the image. An adaptive feature extraction process is modeled to capture the underlying significant features of the images along with the noise. Moreover, the optimal subset of features is derived using the neighborhood component analysis and further used as inputs to the Gaussian process for regression. The various experimentations are carried out on LIVE, TID2008, and TID2013 databases to test the effectiveness of the proposed approach. It is observed that the presented model exhibits a competitive performance in comparison with the existing IQA approaches. Moreover, the computation complexity and run-time show the effectuality of the proposed approach. Further, the experiments show that the predicted score matches with human perceptions. Thus, the proposed model is more accurate, less complex, independent of distortions, and well-suited for real-time applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002912",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Component (thermodynamics)",
      "Computation",
      "Computer science",
      "Data mining",
      "Epistemology",
      "Feature (linguistics)",
      "Feature extraction",
      "Gaussian",
      "Gaussian network model",
      "Gaussian process",
      "Geometry",
      "Human visual system model",
      "Image (mathematics)",
      "Image quality",
      "Linguistics",
      "Mathematics",
      "Naturalness",
      "Noise (video)",
      "Operating system",
      "Orientation (vector space)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Process (computing)",
      "Quality (philosophy)",
      "Quantum mechanics",
      "Thermodynamics"
    ],
    "authors": [
      {
        "surname": "Rajevenceltha",
        "given_name": "J."
      },
      {
        "surname": "Gaidhane",
        "given_name": "Vilas H."
      }
    ]
  },
  {
    "title": "GSD-YOLOX: Lightweight and more accurate object detection models",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2023.104009",
    "abstract": "We present an enhanced YOLOX model for vehicle detection, addressing issues of slow detection speed, high parameter counts, and complex computations. Our model significantly improves inference speed while maintaining high detection accuracy. We introduce two lightweight modules, DG and DS, to reduce model size and enhance detection speed. The DG module eliminates redundant features during feature extraction, and the DS module optimizes the performance of DG, enhancing feature extraction efficiency. We utilize the CIoU loss function for accurate bounding box regression. Additionally, we introduce the Chinese Chongqing Vehicle Detection Benchmark (CCVTDB) dataset to address dataset limitations. Our lightweight model achieves an impressive 83.36% detection accuracy on CCVTDB at 65 FPS with 4.4 million parameters. Compared to the original model, our approach improves detection speed by 30% and reduces model size by 51%, while maintaining substantial detection performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002596",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Bounding overwatch",
      "Computation",
      "Computer science",
      "Feature (linguistics)",
      "Feature extraction",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Inference",
      "Linguistics",
      "Minimum bounding box",
      "Object detection",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Gao",
        "given_name": "Xinghua"
      },
      {
        "surname": "Yu",
        "given_name": "Anning"
      },
      {
        "surname": "Tan",
        "given_name": "Jia"
      },
      {
        "surname": "Gao",
        "given_name": "Xingzhong"
      },
      {
        "surname": "Zeng",
        "given_name": "Xiaoping"
      },
      {
        "surname": "Wu",
        "given_name": "Chen"
      }
    ]
  },
  {
    "title": "Style Elimination and Information Restitution for generalizable person re-identification",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104048",
    "abstract": "Domain generalizable person re-identification (DG ReID) aims to obtain a model that can be applied directly to unseen domains once trained on a set of source domains (datasets collected from different camera networks). Among current DG ReID methods, instance normalization is a promising solution to reduce the effect of domain bias, but it inevitably filters out some discriminative information. Besides, since most pioneering approaches cannot effectively address the loss of discriminative information, this paper proposes a Style Elimination and Information Restitution (SEIR) module and constructs a generalizable framework. Specifically, we utilize instance normalization to reinforce the generalization capability on unseen domains. Then, the instance-specific mean and variance are employed to construct a feature vector that includes discriminative and style information. Besides, an encoder–decoder structure is designed to restitute the discriminative information to ensure a high recognition rate. Finally, a dual optimization strategy is devised to update the proposed module by simulating a real train-test process to enhance the generalization robustness further and prevent SEIR from overfitting to the source domain. Extensive experimental results demonstrate that the proposed method outperforms the state-of-the-art methods by a large margin on the public DG ReID benchmarks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000038",
    "keywords": [
      "Art",
      "Biology",
      "Botany",
      "Computer science",
      "Identification (biology)",
      "Law",
      "Literature",
      "Political science",
      "Restitution",
      "Style (visual arts)"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Qian"
      },
      {
        "surname": "Yu",
        "given_name": "Wentao"
      },
      {
        "surname": "Ji",
        "given_name": "Tangyu"
      }
    ]
  },
  {
    "title": "Surveillance video synopsis framework base on tube set",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104057",
    "abstract": "Video synopsis technology can shorten the length of the video, which has attracted wide attention. However, due to the limitation of object extraction technology and the difficulty of preserving interactivity, synopsis videos will lose the semantic information of the original video. To address the above problems, we propose a video synopsis framework based on tube sets. Firstly, we propose a video tracking algorithm based on Yolov4 and Kalman Filter, which can effectively alleviate the problem that it is difficult for object tracking technology to extract occluded objects. Secondly, we propose a method that combines moving direction and dynamic threshold to judge interactivity. Accurately judging interactivity can ensure that the interactive objects are not separated in the synopsis video. Finally, we propose a tube set mapping model (TSMM) that can rearrange with the tube set as the basic unit. Experimental results demonstrate that our method can achieve superior performance than other state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000129",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Interactivity",
      "Kalman filter",
      "Multimedia",
      "Object (grammar)",
      "Pedagogy",
      "Programming language",
      "Psychology",
      "Set (abstract data type)",
      "Tracking (education)",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Yunzuo"
      },
      {
        "surname": "Zhu",
        "given_name": "Pengfei"
      },
      {
        "surname": "Zheng",
        "given_name": "Tingting"
      },
      {
        "surname": "Yu",
        "given_name": "Puze"
      },
      {
        "surname": "Wang",
        "given_name": "Jianming"
      }
    ]
  },
  {
    "title": "Sea-Pix-GAN: Underwater image enhancement using adversarial neural network",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2023.104021",
    "abstract": "In the last decade, the exploration of deep-sea ecosystems has surged, offering exciting prospects for discovering untapped resources such as medical drugs, food and energy sources, and renewable energy products. Consequently, research in underwater image processing has witnessed substantial growth. However, underwater imaging poses significant challenges, particularly without sophisticated, specialized cameras. Traditional cameras are impacted by absorption and scattering in the aquatic environment, producing hazy images with a blue–green tint. This phenomenon holds implications for marine research and other disciplines that rely on underwater imaging. While hardware advancements have been made over the years, image processing remains a valuable, cost-effective, and practical approach for underwater enhancement. Despite the existence of state-of-the-art techniques for underwater enhancement and restoration, their performance is often inconsistent. While some methods excel in contrast restoration, color restoration remains a pervasive challenge. In this paper, we introduce Sea-Pix-GAN, a Generative Adversarial Network (GAN)-based model that addresses these issues in underwater image enhancement. We redefine the problem as an image-to-image translation task and tailor the objective and loss functions to achieve color, content, and style transfer. The model is trained on a large dataset of underwater scenes, encompassing the diverse color dynamics of underwater subjects. Sea-Pix-GAN demonstrates promising results in restoring color, contrast, texture, and saturation. To validate its effectiveness, we compare the performance of Sea-Pix-GAN quantitatively based on metrics like PSNR, SSIM, and UIQM and qualitatively against several existing techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002717",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Contrast (vision)",
      "Environmental science",
      "Geology",
      "Image (mathematics)",
      "Image enhancement",
      "Image processing",
      "Image restoration",
      "Oceanography",
      "Underwater"
    ],
    "authors": [
      {
        "surname": "Chaurasia",
        "given_name": "Dhiraj"
      },
      {
        "surname": "Chhikara",
        "given_name": "Prateek"
      }
    ]
  },
  {
    "title": "DRC: Chromatic aberration intensity priors for underwater image enhancement",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104065",
    "abstract": "Underwater imaging technology is a crucial tool for monitoring marine flora and fauna. However, selective light absorption and scattering properties of water make underwater imagery frequently appear blurred and exhibit color biases, hindering the extraction of vital aquacultural insights. To address this challenge, we propose a method, namely DRC, which is a holistic approach to enhancing underwater image clarity and color fidelity. This method comprises three integral components: D-procedure, R-procedure, and C-procedure. The D-procedure intricately accounts for the trichromatic underwater attenuation dynamics, formulating a chromatic aberration intensity prior. This prior counters the disparities in degradation levels seen in conventional single-channel prior depth estimations, achieving a dynamic depth representation approaching binocular image precision. The R-procedure, utilizing an adaptive dark-pixel prior, pinpoints corresponding points across varied depth zones to counteract backscattering, thereby mitigating the image’s hazy appearance. The C-procedure bolsters image luminance and color fidelity through opponent channel rectification and amalgamates pronounced image contrasts and intricate details via Gaussian pyramid fusion. The method was tested on several publicly available datasets and compared with nine popular underwater image enhancement techniques. Both subjective and objective assessments underscore the superiority of our DRC method over existing underwater image enhancement techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000208",
    "keywords": [
      "Artificial intelligence",
      "Brightness",
      "Channel (broadcasting)",
      "Chromatic scale",
      "Color correction",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Geology",
      "Image (mathematics)",
      "Oceanography",
      "Optics",
      "Physics",
      "Underwater"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Qian"
      },
      {
        "surname": "He",
        "given_name": "Zongxin"
      },
      {
        "surname": "Zhang",
        "given_name": "Dehuan"
      },
      {
        "surname": "Zhang",
        "given_name": "Weishi"
      },
      {
        "surname": "Lin",
        "given_name": "Zifan"
      },
      {
        "surname": "Sohel",
        "given_name": "Ferdous"
      }
    ]
  },
  {
    "title": "FDNet: Feature decoupling for single-stage pose estimation in complex scenes",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2023.104007",
    "abstract": "Severe error accumulation is a key reason that makes existing multi-person pose estimation challenging in complex scenes. Single-stage pose estimation methods detect human and keypoints in parallel, avoiding serial error accumulation compared to two-stage methods. However, two distinct tasks share identical features in the existing method, which causes error accumulation. Therefore, we propose a Feature Decoupling Network (FDNet) to further extend the pipeline of single-stage methods and reconstruct task-specific features, including Feature Decoupling Module (FDM), Human Aware Loss (HAL) and Keypoint Aware Loss (KAL). FDM can adaptively perceive spatial and channel features and allocate separate feature domains for various tasks. To learn distinctive feature representation for compact human bodies, HAL and KAL measure and suppress feature similarities and distances for different human and keypoints, thus alienating feature interference. Experiments on crowded datasets show that our method is superior, outperforming the state-of-the-art method CID by 1.5%, 0.8% on OCHuman and CrowdPose.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002572",
    "keywords": [
      "Aerospace engineering",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Control engineering",
      "Decoupling (probability)",
      "Engineering",
      "Feature (linguistics)",
      "Law",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pipeline (software)",
      "Political science",
      "Politics",
      "Pose",
      "Programming language",
      "Representation (politics)",
      "Single stage"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Qianqian"
      },
      {
        "surname": "Liu",
        "given_name": "Qiong"
      }
    ]
  },
  {
    "title": "HFA-GTNet: Hierarchical Fusion Adaptive Graph Transformer network for dance action recognition",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2023.104038",
    "abstract": "Dance action recognition is a hot research topic in computer vision. However, current skeleton-based action recognition methods face difficulties in capturing the adequate spatial structure and temporal variations of dance actions, resulting in lower recognition accuracy. In this paper, we propose a Hierarchical Fusion Adaptive Graph Transformer network (HFA-GTNet) for dance action recognition. A Hierarchical Spatial Attention (HSAtt) module is designed to extract different levels of spatial feature information from joint to parts to group, it can effectively learn high-order dependency relationships from local joints to global poses in dance actions. Secondly, to extract the joint variations in dance actions at different speeds, we have designed a Temporal Fusion Attention (TFAtt) module. This module learns the short-term and long-term temporal dependencies among joints across frames. Additionally, to capture the variations in motion patterns and dance styles among different dancers, we introduce an Adaptive Component (AdaptC). Finally, we evaluate our model on two self-built dance datasets, MSDanceAction and InDanceAction, and demonstrate its superior performance compared to other state-of-the-art methods in dance action recognition.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002882",
    "keywords": [
      "Action recognition",
      "Art",
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Computer vision",
      "Dance",
      "Electrical engineering",
      "Engineering",
      "Feature (linguistics)",
      "Graph",
      "Linguistics",
      "Literature",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Theoretical computer science",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Jia",
        "given_name": "Ru"
      },
      {
        "surname": "Zhao",
        "given_name": "Li"
      },
      {
        "surname": "Yang",
        "given_name": "Rui"
      },
      {
        "surname": "Yang",
        "given_name": "Honghong"
      },
      {
        "surname": "Wu",
        "given_name": "Xiaojun"
      },
      {
        "surname": "Zhang",
        "given_name": "Yumei"
      },
      {
        "surname": "Li",
        "given_name": "Peng"
      },
      {
        "surname": "Su",
        "given_name": "Yuping"
      }
    ]
  },
  {
    "title": "Stereo matching algorithm based on improved census transform and minimum spanning tree cost aggregation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2023.104023",
    "abstract": "Existing stereo matching algorithms suffer from issues such as susceptibility to distortion, weak noise resistance, and a high rate of mismatches in regions with weak textures and discontinuous disparities. To address these challenges, this paper proposes a stereo matching algorithm based on an improved census transform and minimum spanning tree (MST) cost aggregation. In the cost calculation phase, we employ a Gaussian-weighted transformation window and incorporate gradient and edge information to perform weighted fusion of the results. In the cost aggregation process, we introduce a collaborative adaptive window. Each pixel acquires information from the support window of the guided filter (GF) and other pixels within the MST. Furthermore, we integrate the SLIC superpixel segmentation algorithm into MST construction. These two components work synergistically to assign appropriate adaptive weights to pixels, facilitating coordinated cost volume aggregation. Different optimization methods are applied to address mismatched points of various types in post-disparity processing.Performance evaluation using the Middlebury dataset and KITTI dataset demonstrates that our proposed algorithm not only enhances matching accuracy in regions with discontinuous disparities and weak textures but also exhibits significantly improved robustness to interference. Additionally, the resulting disparity map displays smoother edges.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002730",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Gene",
      "Matching (statistics)",
      "Mathematics",
      "Minimum spanning tree",
      "Pattern recognition (psychology)",
      "Pixel",
      "Robustness (evolution)",
      "Spanning tree",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Xi",
        "given_name": "Dian"
      },
      {
        "surname": "Yang",
        "given_name": "Hengzhan"
      },
      {
        "surname": "Tan",
        "given_name": "Bo"
      }
    ]
  },
  {
    "title": "Exploring a context-gated network for effective image deraining",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104060",
    "abstract": "The existing deraining methods have obtained noteworthy improvements, but it is a challenging problem to extend the methods for complicated rain conditions where rain streaks exhibit different distribution densities, sizes, shapes, etc. The main challenges are the ability to fully explore and utilize the multi-scale context information of rain streaks that maintain both global structure completeness and local detail accurateness. To this end, this paper proposes an Exploring Context-Gated Network, known as ECGNet. To adequately explore the richer context information, the proposed method consists of two key elements: context-enhanced feature block (CEFB) and multi-scale-gated aggregation block (MGAB). Specifically, the various scale features can be captured by CEFB with the multi-scale operation, to better remove the rain streaks and effectively restore the local detail textures. Subsequently, the captured features from different spaces are sent to MGAB, to aggregate and transmit these different scale features from the encoder to the decoder and reduce the transmission loss of information. Massive experiments on the commonly used benchmarks have demonstrated that the proposed method obtains more appealing performances against other competitive methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000154",
    "keywords": [
      "Aggregate (composite)",
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Composite material",
      "Computer science",
      "Computer security",
      "Context (archaeology)",
      "Data mining",
      "Encoder",
      "Feature (linguistics)",
      "Geology",
      "Geometry",
      "Key (lock)",
      "Linguistics",
      "Materials science",
      "Mathematics",
      "Operating system",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Scale (ratio)",
      "Spatial contextual awareness",
      "Telecommunications",
      "Transmission (telecommunications)"
    ],
    "authors": [
      {
        "surname": "Song",
        "given_name": "Tianyu"
      },
      {
        "surname": "Li",
        "given_name": "Pengpeng"
      },
      {
        "surname": "Fan",
        "given_name": "Shumin"
      },
      {
        "surname": "Jin",
        "given_name": "Jiyu"
      },
      {
        "surname": "Jin",
        "given_name": "Guiyue"
      },
      {
        "surname": "Fan",
        "given_name": "Lei"
      }
    ]
  },
  {
    "title": "Neighbor2Global: Self-supervised image denoising for Poisson-Gaussian noise",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104049",
    "abstract": "Sub-sampled pairs generated from noisy images solve the lack of noisy-clean pairs in denoising model training. However, real sub-sampled image pairs with signal-dependent Poisson-Gaussian noise and the approximation of neighboring pixels lead to denoising performance degradation. For this reason, a novel self-supervised Neighbor2Global is proposed to train an efficient denoising model for real-world images denoising. Firstly, the problems faced by model training are analyzed theoretically, and a GAT-based image generation strategy is introduced to make the sub-sampled pair from a single noisy image approximately independent. Secondly, a complementary training strategy is presented to train the denoising network on sub-sampled GAT image pairs, GAT images and PD-transformed GAT images, with improved reconstruction loss and optimized regulation for better performance. Extensive experiments have been conducted on both synthetic and real-world datasets to validate the superior performance of the proposed method in balancing detailed texture and global information, as well as weakening over-smoothing.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032400004X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Gaussian",
      "Gaussian noise",
      "Image (mathematics)",
      "Image denoising",
      "Mathematics",
      "Noise (video)",
      "Noise reduction",
      "Pattern recognition (psychology)",
      "Physics",
      "Poisson distribution",
      "Quantum mechanics",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Qiuqiu"
      },
      {
        "surname": "Xing",
        "given_name": "Yuanxiu"
      },
      {
        "surname": "Song",
        "given_name": "Linlin"
      }
    ]
  },
  {
    "title": "Hybrid CNN-transformer based meta-learning approach for personalized image aesthetics assessment",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2023.104044",
    "abstract": "Personalized Image Aesthetics Assessment (PIAA) is highly subjective, as people's aesthetic preferences vary greatly. Traditional generic models struggle to capture the unique preferences of each individual, and PIAA often deals with limited samples from individual users. Furthermore, it requires a holistic consideration of diverse visual features in images, including both local and global features. To address these challenges, we propose an innovative network that combines the power of transformer and Convolutional Neural Networks (CNNs) with Meta-Learning for PIAA (TCML-PIAA). Firstly, we leverage both Vision Transformer blocks and CNNs to extract long-term and short-term dependencies, mining richer and heterogeneous aesthetic attributes from these two branches. Secondly, to effectively fuse these distinct features, we introduce an Aesthetic Feature Interaction Module (AFIM), designed to seamlessly integrate the aesthetic features extracted from CNNs and ViT, enabling the interaction and fusion of aesthetic information across different modalities. We also incorporate a Channel-Spatial Attention Module (CSAM), embedding it within both the CNNs and the AFIM to enhance the perception of different regions in images, further exploring the aesthetic cues in images. Experimental results demonstrate that our TCML-PIAA outperforms existing state-of-the-art methods on benchmark databases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002948",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Embedding",
      "Feature extraction",
      "Feature learning",
      "Leverage (statistics)",
      "Machine learning",
      "Modalities",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Perception",
      "Physics",
      "Psychology",
      "Quantum mechanics",
      "Social science",
      "Sociology",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Yan",
        "given_name": "Xingao"
      },
      {
        "surname": "Shao",
        "given_name": "Feng"
      },
      {
        "surname": "Chen",
        "given_name": "Hangwei"
      },
      {
        "surname": "Jiang",
        "given_name": "Qiuping"
      }
    ]
  },
  {
    "title": "Neighbor2Global: Self-supervised image denoising for Poisson-Gaussian noise",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104049",
    "abstract": "Sub-sampled pairs generated from noisy images solve the lack of noisy-clean pairs in denoising model training. However, real sub-sampled image pairs with signal-dependent Poisson-Gaussian noise and the approximation of neighboring pixels lead to denoising performance degradation. For this reason, a novel self-supervised Neighbor2Global is proposed to train an efficient denoising model for real-world images denoising. Firstly, the problems faced by model training are analyzed theoretically, and a GAT-based image generation strategy is introduced to make the sub-sampled pair from a single noisy image approximately independent. Secondly, a complementary training strategy is presented to train the denoising network on sub-sampled GAT image pairs, GAT images and PD-transformed GAT images, with improved reconstruction loss and optimized regulation for better performance. Extensive experiments have been conducted on both synthetic and real-world datasets to validate the superior performance of the proposed method in balancing detailed texture and global information, as well as weakening over-smoothing.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032400004X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Gaussian",
      "Gaussian noise",
      "Image (mathematics)",
      "Image denoising",
      "Mathematics",
      "Noise (video)",
      "Noise reduction",
      "Pattern recognition (psychology)",
      "Physics",
      "Poisson distribution",
      "Quantum mechanics",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Qiuqiu"
      },
      {
        "surname": "Xing",
        "given_name": "Yuanxiu"
      },
      {
        "surname": "Song",
        "given_name": "Linlin"
      }
    ]
  },
  {
    "title": "Depth as attention to learn image representations for visual localization, using monocular images",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2023.104012",
    "abstract": "Image retrieval algorithms are widely used in visual localization tasks. In visual localization, we can benefit from retrieving the images depicting same landmark taken from a pose similar to the query. However, state-of-the-art image retrieval algorithms are optimized mainly for landmark retrieval, and do not take camera pose into account. To address this limitation, we propose novel Depth Attention Network (DeAttNet). DeAttNet leverages both visual and depth information in learning a global image representation. Depth varies for similar features captured from different camera poses. Based on this insight, we employ depth within an attention mechanism to discern and emphasize the salient regions. In our method, we utilize monocular depth estimation algorithms to render depth maps. Compared to RGB only image descriptors, significant improvements are obtained with the proposed method on Mapillary Street Level Sequences, Pittsburgh and Cambridge Landmark datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002626",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Image retrieval",
      "Landmark",
      "Law",
      "Monocular",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "RGB color model",
      "Representation (politics)",
      "Salient"
    ],
    "authors": [
      {
        "surname": "Hettiarachchi",
        "given_name": "Dulmini"
      },
      {
        "surname": "Tian",
        "given_name": "Ye"
      },
      {
        "surname": "Yu",
        "given_name": "Han"
      },
      {
        "surname": "Kamijo",
        "given_name": "Shunsuke"
      }
    ]
  },
  {
    "title": "LL-WSOD: Weakly supervised object detection in low-light",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2023.104010",
    "abstract": "Weakly supervised detection performs significantly lower than the fully supervised methods due to the lack of detailed and precise annotations. Especially, its performance deteriorates more severely in low-light conditions with the lack of low-light datasets. To overcome these issues, we propose a new Low-Light Weakly Supervised Object Detection (LL-WSOD) framework. First, we propose a Progressive Low-light NoiseModule (PLNM) to train the model progressively with low light using the common datasets with normal light, greatly reducing the training difficulty. Next, a Residual Self-refinement Low-light Rebuild Module (ResSLRM) is proposed to allow convolutional neural networks to learn sharper features by rebuilding low-light features into normal-light images. Finally, a Pseudo Boundingbox Assisted Learning Module (PBALM) is designed to perform better low-light training using salient priors. The results show that the proposed LL-WSOD algorithm effectively detects objects under low-light conditions and achieves great results on the real low-light dataset ExDark.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002602",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Residual",
      "Salient"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Han"
      },
      {
        "surname": "Wang",
        "given_name": "Yongfang"
      },
      {
        "surname": "Yang",
        "given_name": "Yingjie"
      }
    ]
  },
  {
    "title": "Indoor semantic segmentation based on Swin-Transformer",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2023.103991",
    "abstract": "In recent years, with the rapid development of Transformer in the field of natural language processing, many researchers have realized its potential and gradually applied it to the field of computer vision, with a proliferation of theoretical approaches represented by Vision Transformer (ViT) and Data-efficient image Transformer (DeiT). On the basis of ViT, the famous Swin-Transformer was proposed as one of the best computer vision neural network backbones, which can be widely used in tasks such as image classification, target detection and video recognition. However, in the field of image segmentation, the semantic segmentation of indoor scenes is still very challenging due to the wide variety of objects, large differences in object sizes, and a large number of overlapping objects with occlusion. Aiming at the problem that the existing semantic segmentation of RGB-D indoor scenes cannot effectively fuse multimodal features, in this paper, we propose a novel indoor semantic segmentation algorithm based on Swin-Transformer. It attempts to apply Swin-Transformer to the field of indoor RGBD semantic segmentation, and tests the performance of the model by conducting extensive experiments on the mainstream indoor semantic segmentation datasets NYU-Depth V2 and SUN RGB-D. The experimental results show that the Swin-L RGB+Depth setting achieves 52.44% MIoU on the NYU-Depth V2 data and 51.15% MIoU on the SUN RGB-D data set, which reflects an excellent performance in the field of indoor semantic segmentation. The improved performance of the Depth features on the indoor semantic segmentation model has also been demonstrated in the experiments by controlling the type of input features. Our source code is publicly available at https://github.com/YunpingZheng/ISSSW.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002419",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Electrical engineering",
      "Engineering",
      "Image segmentation",
      "Pattern recognition (psychology)",
      "RGB color model",
      "Scale-space segmentation",
      "Segmentation",
      "Segmentation-based object categorization",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Zheng",
        "given_name": "Yunping"
      },
      {
        "surname": "Xu",
        "given_name": "Yuan"
      },
      {
        "surname": "Shu",
        "given_name": "Shiqiang"
      },
      {
        "surname": "Sarem",
        "given_name": "Mudar"
      }
    ]
  },
  {
    "title": "Texture and motion aware perception in-loop filter for AV1",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2023.104025",
    "abstract": "Lossy compression introduces artifacts, and many conventional in-loop filters have been adopted in the AV1 standard to reduce these artifacts. Researchers have explored deep learning-based filters to remove artifacts in the compression loop. However, the high computational complexity of CNN-based filters remains a challenge. In this paper, a Texture- and Motion-Aware Perception (TMAP) in-loop filter is proposed to addresses this issue by selectively applying CNNs to texture-rich and high-motion regions, while utilizing non-learning methods to detect these regions. The proposed method introduces a new CNN structure, the Dense-Dual-Field Network (DDFN), which leverages a larger receptive field to enhance the quality of reconstructed frames by incorporating more contextual information. Furthermore, to improve perceptual quality, a novel loss function integrating wavelet-based perceptual information is presented. Experimental results demonstrate the superiority of our proposed models over other lightweight CNN models, and the effectiveness of the perceptual loss function is validated using the VMAF metric.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002754",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Compression artifact",
      "Computer science",
      "Computer vision",
      "Economics",
      "Filter (signal processing)",
      "Image (mathematics)",
      "Image compression",
      "Image processing",
      "Lossy compression",
      "Metric (unit)",
      "Neuroscience",
      "Operations management",
      "Pattern recognition (psychology)",
      "Perception",
      "Texture (cosmology)",
      "Wavelet"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Tianqi"
      },
      {
        "surname": "Huang",
        "given_name": "Hong"
      },
      {
        "surname": "Lei",
        "given_name": "Zhijun"
      },
      {
        "surname": "Fang",
        "given_name": "Ruogu"
      },
      {
        "surname": "Wu",
        "given_name": "Dapeng"
      }
    ]
  },
  {
    "title": "HFA-GTNet: Hierarchical Fusion Adaptive Graph Transformer network for dance action recognition",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2023.104038",
    "abstract": "Dance action recognition is a hot research topic in computer vision. However, current skeleton-based action recognition methods face difficulties in capturing the adequate spatial structure and temporal variations of dance actions, resulting in lower recognition accuracy. In this paper, we propose a Hierarchical Fusion Adaptive Graph Transformer network (HFA-GTNet) for dance action recognition. A Hierarchical Spatial Attention (HSAtt) module is designed to extract different levels of spatial feature information from joint to parts to group, it can effectively learn high-order dependency relationships from local joints to global poses in dance actions. Secondly, to extract the joint variations in dance actions at different speeds, we have designed a Temporal Fusion Attention (TFAtt) module. This module learns the short-term and long-term temporal dependencies among joints across frames. Additionally, to capture the variations in motion patterns and dance styles among different dancers, we introduce an Adaptive Component (AdaptC). Finally, we evaluate our model on two self-built dance datasets, MSDanceAction and InDanceAction, and demonstrate its superior performance compared to other state-of-the-art methods in dance action recognition.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002882",
    "keywords": [
      "Action recognition",
      "Art",
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Computer vision",
      "Dance",
      "Electrical engineering",
      "Engineering",
      "Feature (linguistics)",
      "Graph",
      "Linguistics",
      "Literature",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Theoretical computer science",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Jia",
        "given_name": "Ru"
      },
      {
        "surname": "Zhao",
        "given_name": "Li"
      },
      {
        "surname": "Yang",
        "given_name": "Rui"
      },
      {
        "surname": "Yang",
        "given_name": "Honghong"
      },
      {
        "surname": "Wu",
        "given_name": "Xiaojun"
      },
      {
        "surname": "Zhang",
        "given_name": "Yumei"
      },
      {
        "surname": "Li",
        "given_name": "Peng"
      },
      {
        "surname": "Su",
        "given_name": "Yuping"
      }
    ]
  },
  {
    "title": "Subspace learning machine (SLM): Methodology and performance evaluation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104058",
    "abstract": "Inspired by the feedforward multilayer perceptron (FF-MLP), decision tree (DT) and extreme learning machine (ELM), a new classification model, called the subspace learning machine (SLM), is proposed in this work. SLM first identifies a discriminant subspace, S 0 , by examining the discriminant power of each input feature. Then, it uses probabilistic projections of features in S 0 to yield 1D subspaces and finds the optimal partition for each of them. This is equivalent to partitioning S 0 with hyperplanes. A criterion is developed to choose the best q partitions that yield 2 q partitioned subspaces among them. We assign S 0 to the root node of a decision tree and the intersections of 2 q subspaces to its child nodes of depth one. The partitioning process is recursively applied at each child node to build an SLM tree. When the samples at a child node are sufficiently pure, the partitioning process stops and each leaf node makes a prediction. The idea can be generalized to regression, leading to the subspace learning regressor (SLR). Furthermore, ensembles of SLM/SLR trees can yield a stronger predictor. Extensive experiments are conducted for performance benchmarking among SLM/SLR trees, ensembles and classical classifiers/regressors.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000130",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Combinatorics",
      "Computer science",
      "Decision tree",
      "Engineering",
      "Extreme learning machine",
      "Geometry",
      "Hyperplane",
      "Linear subspace",
      "Machine learning",
      "Mathematics",
      "Node (physics)",
      "Partition (number theory)",
      "Pattern recognition (psychology)",
      "Structural engineering",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Fu",
        "given_name": "Hongyu"
      },
      {
        "surname": "Yang",
        "given_name": "Yijing"
      },
      {
        "surname": "Mishra",
        "given_name": "Vinod K."
      },
      {
        "surname": "Kuo",
        "given_name": "C.-C. Jay"
      }
    ]
  },
  {
    "title": "Learning a Holistic-Specific color transformer with Couple Contrastive constraints for underwater image enhancement and beyond",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104059",
    "abstract": "Underwater images suffer from different types of degradation due to medium characteristics and interfere with underwater tasks. While deep learning methods based on the Convolutional Neural Network (CNN) excel at detection tasks, they have inherent limitations when it comes to handling long-range dependencies. The enhanced images generated by these methods often have problems such as color cast, artificial traces and insufficient contrast. To address these limitations, we present a novel Holistic-Specific attention (HSA) mechanism based on the Vision Transformer (ViT). This mechanism allows us to capture global information in finer detail and perform initial enhancements on underwater images. Notably, even when combined with ViT, CNNs do not always approach the ideal state of image enhancement, as reference images themselves may involve human intervention. To tackle this, we design a loss function that incorporates contrastive learning, using the source image as a negative example. This approach guides the enhancement results to be closer to the ideal enhancement state while keeping away from the degraded state, not just closer to the reference. Additionally, we introduce patch-based contrastive learning to address the shortcomings of image-based methods in fine-detail correction. Our extensive qualitative and quantitative experiments demonstrate that the proposed method outperforms state-of-the-art techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000142",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Geology",
      "Machine learning",
      "Oceanography",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Transformer",
      "Underwater",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Wei",
        "given_name": "Debin"
      },
      {
        "surname": "Xie",
        "given_name": "Hongji"
      },
      {
        "surname": "Zhang",
        "given_name": "Zengxi"
      },
      {
        "surname": "Yan",
        "given_name": "Tiantian"
      }
    ]
  },
  {
    "title": "Video reversible data hiding: A systematic review",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2023.104029",
    "abstract": "Reversible data hiding (RDH), also known as lossless data hiding, ensures the losslessly recovery of both the cover medium and the secret message at the decoder. For the video medium, RDH can also provide the effective protection and has attracted a lot of attention due to its unique reversibility. This paper aims to present a systematic review about the existing video RDH methods and the corresponding practical applications. We summarize the basic frameworks and the current optimization aspects in video RDH methods, including (1) the determination for the suitable embedding position, (2) the drift distortion minimization, (3) the extension for security improvement. Then, the fundamental trade-off for the evaluation metrics is quantified, and the performance comparison of the classical methods is given. Finally, the future research directions are discussed to exploit the further idea for this area.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002791",
    "keywords": [
      "Algorithm",
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Computer network",
      "Computer science",
      "Computer security",
      "Cover (algebra)",
      "Data compression",
      "Distortion (music)",
      "Embedding",
      "Engineering",
      "Exploit",
      "Extension (predicate logic)",
      "Information hiding",
      "Lossless compression",
      "Mechanical engineering",
      "Minification",
      "Programming language"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Jiaqi"
      },
      {
        "surname": "Ou",
        "given_name": "Bo"
      }
    ]
  },
  {
    "title": "Multi-scale convolutional neural networks and saliency weight maps for infrared and visible image fusion",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2023.104015",
    "abstract": "Image fusion is the fusion of multiple images from the same scene to produce a more informative image, and infrared and visible image fusion is an important branch of image fusion. To tackle the issues of diminished luminosity in the infrared target, inconspicuous target features, and blurred texture of the fused image after the fusion of infrared and visible images. This paper introduces a novel effective fusion framework that merges multi-scale Convolutional Neural Networks (CNN) with saliency weight maps. First, the method measures the source image features to estimate the initial saliency weight map. Then, the initial weight map is segmented and optimized using a guided filter before being further processed by CNN. Next, a trained Siamese convolutional network is used to solve the two key problems of activity measure and weight assignment. Meanwhile, a multi-layer fusion strategy is designed to effectively retain the luminance of the infrared target and the texture information in the visible background. Finally, adaptive adjustment of the fusion coefficients is achieved by employing saliency. The experimental results show that the method outperforms the state-of-the-art algorithms in terms of both subjective visual quality and objective evaluation effects.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002651",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Filter (signal processing)",
      "Fusion",
      "Image (mathematics)",
      "Image fusion",
      "Infrared",
      "Linguistics",
      "Luminance",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Chenxuan"
      },
      {
        "surname": "He",
        "given_name": "Yunan"
      },
      {
        "surname": "Sun",
        "given_name": "Ce"
      },
      {
        "surname": "Chen",
        "given_name": "Bingkun"
      },
      {
        "surname": "Cao",
        "given_name": "Jie"
      },
      {
        "surname": "Wang",
        "given_name": "Yongtian"
      },
      {
        "surname": "Hao",
        "given_name": "Qun"
      }
    ]
  },
  {
    "title": "Fast intra coding in AVS3 based on direct non-first pre-coding skip",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2023.104027",
    "abstract": "The third generation of Audio Video Coding Standard (AVS3) adopts sophisticated block partitioning structure with a multi-type tree, which includes Quad-Tree (QT), Binary-Tree (BT), and Extend Quad-Tree (EQT). During multi-type tree recursive block partitioning and pre-coding process of encoding, each Coding Unit (CU) of each Size at each Location (CUeSeL) of the picture is repeatedly pre-coded many times when visiting each branch of each tree. Although there are early termination mechanism, the number of visits for each CUeSeL is still very high and up to more than fifty. This process brings very high coding complexity. In order to reduce the coding complexity of AVS3, this paper proposes a method named Direct Non-First Pre-coding Skip (DNFPS) for a CUeSeL to directly and completely skip all non-first pre-coding under specific conditions. The conditions include, but are not limited to, those related to the first pre-coding. The contributions of the paper are as follows. In I pictures, for a CUeSeL with top-left location (x, y), if both x and y are multiples of 64 or if a set of conditions related to first pre-coding is satisfied, then each non-first pre-coding is skipped, and the coding result of the first pre-coding is reused for each non-first pre-coding skipped. In non-I pictures, each non-first pre-coding is skipped, and the coding result of the first pre-coding is reused for each non-first pre-coding skipped, provided a set of conditions related to first pre-coding is satisfied. The experimental results demonstrate that DNFPS has a negligible impact on coding efficiency under All Intra (AI) configurations, and coding runtime is reduced by 16 %.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002778",
    "keywords": [
      "Algorithm",
      "Coding (social sciences)",
      "Coding tree unit",
      "Computer science",
      "Mathematics",
      "Statistics",
      "Tunstall coding",
      "Variable-length code"
    ],
    "authors": [
      {
        "surname": "Cao",
        "given_name": "Xueyan"
      },
      {
        "surname": "Lin",
        "given_name": "Tao"
      },
      {
        "surname": "Zhao",
        "given_name": "Liping"
      },
      {
        "surname": "Yang",
        "given_name": "Yufen"
      },
      {
        "surname": "Zhou",
        "given_name": "Kailun"
      },
      {
        "surname": "Wei",
        "given_name": "Hu"
      },
      {
        "surname": "Chen",
        "given_name": "Xianyi"
      }
    ]
  },
  {
    "title": "High-low level task combination for object detection in foggy weather conditions",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2023.104042",
    "abstract": "For the object detection task in foggy weather conditions, image dehazing network is often used as preprocessing method to get a clear input. However, there is not strictly a strong positive correlation between image dehazing task and object detection task. Moreover, the preprocessing module can increase the inference time of the whole model to a certain extent. To alleviate these problems, we propose a novel High-Low level task combination network (HLNet) based on multitask learning, which can learn both high-level and low-level tasks. Specially, instead of restoring the features to clear pixel-wise feature space like common image dehazing method, we opt to perform a restoration in feature level to mitigate the influence of the Batch Normalization (BN) layer of encoder on dehazing task. HLNet jointly learn dehazing task and detection task in an end-to-end fashion, which ensures that the weather-specific information in latent feature space is suppressed. Moreover, we applied the HLNet framework on three different object detection networks, including R e t i n a N n e t , Y O L O v 3 and Y O L O v 5 s network, and achieved improvements of 1.7 percent, 2.3 percent, and 1.2 percent in m A P respectively. The experimental results demonstrate the effectiveness and generalization ability of our proposed HLNet framework in real foggy scenarios.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002924",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Economics",
      "Encoder",
      "Feature (linguistics)",
      "Generalization",
      "Inference",
      "Linguistics",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Multi-task learning",
      "Object detection",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Preprocessor",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Ke"
      },
      {
        "surname": "Wu",
        "given_name": "Fei"
      },
      {
        "surname": "Zhan",
        "given_name": "Zhenfei"
      },
      {
        "surname": "Luo",
        "given_name": "Jun"
      },
      {
        "surname": "Pu",
        "given_name": "Huayan"
      }
    ]
  },
  {
    "title": "Texture-aware and color-consistent learning for underwater image enhancement",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104051",
    "abstract": "Texture and color are pivotal factors for evaluating the quality of underwater image enhancement. However, current methods for enhancing underwater images still exhibit deficiencies in the restoration of texture and color. Diverging from previous approaches, we conduct heuristic modeling specifically targeting color and texture, culminating in the proposal of a texture-aware and color-consistent network (TACC-Net). Specifically, it first obtains preliminary enhanced images of more global features to facilitate feature decoupling. Then, we designed a color feature decoupling (CFD) sub-module to decouple the preliminary enhanced images into color-consistent color histograms to generate textured gray images. Finally, we designed a color space adjustment embedding (CSAE) sub-module, which is used to evaluate the similarity between features when embedding color information, and adjust the embedding of color information on textures. The experimental results indicate that our method enhances images and exhibits strong competitiveness in improving visual quality.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000063",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Color balance",
      "Color correction",
      "Color histogram",
      "Color image",
      "Color normalization",
      "Color quantization",
      "Color space",
      "Computer science",
      "Computer vision",
      "Embedding",
      "Feature (linguistics)",
      "Geography",
      "Image (mathematics)",
      "Image processing",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Underwater"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Shuteng"
      },
      {
        "surname": "Cheng",
        "given_name": "Zheng"
      },
      {
        "surname": "Fan",
        "given_name": "Guodong"
      },
      {
        "surname": "Gan",
        "given_name": "Min"
      },
      {
        "surname": "Chen",
        "given_name": "C.L. Philip"
      }
    ]
  },
  {
    "title": "CTFCD: Channel transformer based on full convolutional decoder for single image deraining",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2023.103992",
    "abstract": "Although convolutional neural network and visual transformer have been successfully applied in various field of computer vision, there is little work combining them to construct an efficient network model to solve image deraining tasks. Convolutional neural network is utilized to extract the features from each region, while visual transformer is utilized to extract the context information between local features. Due to the limitations in computational resources and processing time, it is difficult for visual transformer to process high-resolution images, which hinders the application of visual transformer in devices with limited hardware resources. The purpose of this article is to utilize the advantages of to design a lightweight encoder-decoder network for real-time image deraining. Firstly, a novel channel Transformer module is designed to obtain global contextual information, where deep separable convolution is utilized to extract multi-scale local features and a Transformer encoder is constructed by stacking Transformer modules. Secondly, a decoder based on a fully convolution is designed to adopt mask attention and inverted bottleneck convolution to achieve progressive feature fusion and feature reconstruction, which significantly reduces computational complexity and memory requirement. A large number of experimental results have verified that the proposed method has superior performance compared with other state-of-the art methods, while the computational cost and parameter quantity are much smaller than those of similar methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002420",
    "keywords": [
      "Artificial intelligence",
      "Bottleneck",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Electrical engineering",
      "Embedded system",
      "Encoder",
      "Engineering",
      "Operating system",
      "Pattern recognition (psychology)",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Tan",
        "given_name": "Shaohan"
      },
      {
        "surname": "Chen",
        "given_name": "Hui"
      },
      {
        "surname": "Zhu",
        "given_name": "Songhao"
      }
    ]
  },
  {
    "title": "Multiple integration model for single-source domain generalizable person re-identification",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2023.104037",
    "abstract": "Domain generalizable (DG) person re-identification (re-ID) aims to train a model on labeled source domains which can perform well on invisible target domains. Because of the distribution shifts between different domains, it is a challenging task. Existing methods address this challenge by using multiple source domains to train a model which requires more data, manual labor, and computation. In contrast, we pay attention to the single-source DG re-ID task, that is, only one source domain data is accessible for training. However, due to the limited availability of training data, this task is more difficult. In this paper, a novel MulTiple Integration (MTI) model is introduced for single-source DG person re-ID. By integrating multiple reliable perturbations, the generalization performance can be improved. Specifically, MTI model contains two types of integration modules, one is shallow-level compensation (SLC) and the other is deep-level integration (DLI). For SLC, according to the idea of continual learning, the shallow-level information of the ImageNet pre-trained ResNet-50 branch is introduced and fused with the shallow-level information of our backbone network. In this way, massive information in ImageNet can be used to prevent the disastrous forgetting of the pre-trained information, and information compensation can be provided for backbone network. Additionally, we propose a hybrid integrated normalization layer to fuse information and improve the model’s generalization performance. For DLI, a wave transformer block is introduced in the deep layer of the backbone, which can integrate the information of a batch images and contain reliable disturbance, so that the robustness of the model can be promoted. Extensive experimental results demonstrate the superiority of our model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002870",
    "keywords": [
      "Anthropology",
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Economics",
      "Electrical engineering",
      "Engineering",
      "Fuse (electrical)",
      "Generalization",
      "Identification (biology)",
      "Machine learning",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Multi-source",
      "Normalization (sociology)",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Sociology",
      "Statistics",
      "Task (project management)",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Jia"
      },
      {
        "surname": "Li",
        "given_name": "Yanfeng"
      },
      {
        "surname": "Chen",
        "given_name": "Luyifu"
      },
      {
        "surname": "Chen",
        "given_name": "Houjin"
      },
      {
        "surname": "Peng",
        "given_name": "Wanru"
      }
    ]
  },
  {
    "title": "Secret image sharing in the encrypted domain",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2023.104013",
    "abstract": "Nowadays, digital images have a greatly widespread and profound influence on human life, and its security protection problem is increasingly prominent. Image encryption is one of the most effective solutions for image protection but may result in secret concentration or single point of failure. Secret image sharing (SIS) with loss tolerance provides a feasible method to protect secret images in a distributed way. However, existing SIS schemes are most utilized in natural images, and the application of SIS to encrypted images which are widely used in practical scenarios is rarely discussed. Thus, this paper proposes a novel concept of SIS in the encrypted domain (SIS-ED) for the first time on the basis of synthesizing the merits of SIS and image encryption. SIS-ED integrating the encryption and SIS means applying general or user-defined encryption algorithms to encrypt the secret image in the plaintext domain, and in the encrypted domain using appropriate SIS schemes on the encrypted image to share secrets. SIS-ED can prevent secret images from being reconstructed illegally and enhance security against malicious attacks such as differential attacks, which can be widely applied in practical scenarios with high-security requirements such as authentication, blockchain, and cloud systems. In addition, we present a concrete SIS-ED scheme to instantiate the SIS-ED model. The proposed SIS-ED scheme with high security can realize lossless recovery without pixel expansion, and reduced-size shares. Theoretical analysis, experiments, and comparisons are conducted to show the effectiveness and advantages of SIS-ED.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002638",
    "keywords": [
      "Computer science",
      "Computer security",
      "Computer vision",
      "Cryptography",
      "Data compression",
      "Encryption",
      "Homomorphic secret sharing",
      "Image (mathematics)",
      "Image sharing",
      "Lossless compression",
      "Plaintext",
      "Secret sharing",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Rui"
      },
      {
        "surname": "Yang",
        "given_name": "Guozheng"
      },
      {
        "surname": "Yan",
        "given_name": "Xuehu"
      },
      {
        "surname": "Luo",
        "given_name": "Shengyang"
      },
      {
        "surname": "Han",
        "given_name": "Qiang"
      }
    ]
  },
  {
    "title": "Effective image tampering localization with multi-scale ConvNeXt feature fusion",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2023.103981",
    "abstract": "With the widespread use of powerful image editing tools, image tampering becomes easy and realistic. Existing image forensic methods still face challenges of low generalization performance and robustness. In this letter, we propose an effective image tampering localization scheme based on ConvNeXt encoder and multi-scale Feature Fusion (ConvNeXtFF). Stacked ConvNeXt blocks are utilized as an encoder to capture hierarchical multi-scale features, which are then fused in decoder for locating tampered pixels accurately. Combined loss function and effective data augmentation strategies are adopted to further improve the model performance. Extensive experimental results show that both localization accuracy and robustness of the ConvNeXtFF scheme outperform other state-of-the-art ones. The source code is available at https://github.com/multimediaFor/ConvNeXtFF.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002316",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Code (set theory)",
      "Computer science",
      "Computer vision",
      "Encoder",
      "Feature (linguistics)",
      "Gene",
      "Generalization",
      "Image (mathematics)",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Programming language",
      "Robustness (evolution)",
      "Set (abstract data type)",
      "Source code"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Haochen"
      },
      {
        "surname": "Cao",
        "given_name": "Gang"
      },
      {
        "surname": "Zhao",
        "given_name": "Mo"
      },
      {
        "surname": "Tian",
        "given_name": "Huawei"
      },
      {
        "surname": "Lin",
        "given_name": "Weiguo"
      }
    ]
  },
  {
    "title": "An efficient adaptive Masi entropy multilevel thresholding algorithm based on dynamic programming",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2023.104008",
    "abstract": "Masi entropy multilevel thresholding can utilize additive and non-extensive information in images to effectively segment a complex image. However, the entropy index of Masi entropy cannot be selected automatically, and the time complexity of the multilevel algorithm by exhaustive searching grows exponentially with the increase of the threshold numbers. To address these two problems, an adaptive entropy index selection strategy based on image histogram information is proposed first. To improve the computation efficiency, an efficient solution for the adaptive Masi entropy multilevel thresholding algorithm based on dynamic programming (DP + AMasi) is also proposed. The DP + AMasi algorithm is compared with the Masi entropy multilevel thresholding algorithm by exhaustive search and state-of-the-art metaheuristic algorithms on three benchmark datasets. The effectiveness of the DP + AMasi is verified by fitness function values, Uniformity Measure, Davies Bouldin index, and CPU run time. In addition, the Wilcoxon test is used to analyze the differences between algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002584",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Dynamic programming",
      "Entropy (arrow of time)",
      "Histogram",
      "Image (mathematics)",
      "Mathematics",
      "Physics",
      "Quantum mechanics",
      "Thresholding"
    ],
    "authors": [
      {
        "surname": "Lei",
        "given_name": "Bo"
      },
      {
        "surname": "Li",
        "given_name": "Jinming"
      },
      {
        "surname": "Wang",
        "given_name": "Ningning"
      },
      {
        "surname": "Yu",
        "given_name": "Haiyan"
      }
    ]
  },
  {
    "title": "Subdomain alignment based open-set domain adaptation image classification",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104047",
    "abstract": "Domain adaptation has achieved great success in using labeled source domain samples to identify unlabeled target domain samples. Here, we aim to solve the open-set domain adaptation, which is different from the closed-set domain adaptation in that it contains categories in target domain that do not appear in source domain. To solve this problem, this paper proposes open-set domain adaptation model based on subdomain alignment, which uses variable weights for discriminative training of unknown samples in target domain. Aiming at the distribution differences between domains, the model aligns the distributions of the category subspaces of source and target domains, enhancing the distribution similarity within the subspaces of the same category. Through experiments on different domain adaptation datasets, the results show that the model proposed in this paper effectively improves the accuracy of open-set domain adaptation classification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000026",
    "keywords": [
      "Adaptation (eye)",
      "Algorithm",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Discriminative model",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Geometry",
      "Image (mathematics)",
      "Linear subspace",
      "Mathematical analysis",
      "Mathematics",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Programming language",
      "Set (abstract data type)",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Ji",
        "given_name": "Kangkang"
      },
      {
        "surname": "Zhang",
        "given_name": "Qingliang"
      },
      {
        "surname": "Zhu",
        "given_name": "Songhao"
      }
    ]
  },
  {
    "title": "A small object detection algorithm based on feature interaction and guided learning",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2023.104011",
    "abstract": "At present, for the diverse target sizes, different angles, and complex environment in aerial images, how to detect small objects effectively in aerial images is still a challenge task. In order to further improve the detection accuracy and reduce the missing rate, in this paper, we proposes a small target detection algorithm based on feature interaction and guided learning. Firstly, the feature guided alignment module is designed to deal with the problem of information loss and overlap. Secondly, the misallocation of small object samples is optimized by introducing gaussian sample allocation strategy. Then the interactive parallel detection header is designed to solve the detection task conflict problem. Finally, the new regression loss function is designed to enhance the localization ability of the target. The experimental results on the Tinyperson dataset and Visdrone dataset show that this model can improve the detection accuracy and reduce the missing rate of small targets effectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002614",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Chemistry",
      "Chromatography",
      "Computer network",
      "Computer science",
      "Economics",
      "Evolutionary biology",
      "Feature (linguistics)",
      "Function (biology)",
      "Gaussian",
      "Header",
      "Linguistics",
      "Management",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Sample (material)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Shao",
        "given_name": "Xiang-Ying"
      },
      {
        "surname": "Guo",
        "given_name": "Ying"
      },
      {
        "surname": "Wang",
        "given_name": "You-Wei"
      },
      {
        "surname": "Bao",
        "given_name": "Zheng-Wei"
      },
      {
        "surname": "Wang",
        "given_name": "Ji-Yu"
      }
    ]
  },
  {
    "title": "Residual attention fusion network for video action recognition",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2023.103987",
    "abstract": "Human action recognition in videos is a fundamental and important topic in computer vision, and modeling spatial–temporal dynamics in a video is crucial for action classification. In this paper, a novel attention module named Channel-wise Non-local Attention Module (CNAM) is proposed to highlight the important features both spatially and temporally. Besides, another new attention module named Channel-wise Attention Recalibration Module (CARM) is developed to focus on capturing discriminative features at channel level. Based on these two attention modules, a novel convolutional neural network named Residual Attention Fusion Network (RAFN) is proposed to model long-range temporal structure and capture more discriminative action features at the same time. More specifically, first, a sparse temporal sampling strategy is adopted to uniformly sample video data as input to RAFN along the temporal dimension. Secondly, the attention modules CNAM and CARM are plugged into residual network for highlighting important action regions around actors. Finally, the classification scores of four streams of RAFN are combined by late fusion. The experimental results on HMDB51 and UCF101 demonstrate the effectiveness and excellent recognition performance of our proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002377",
    "keywords": [
      "Action (physics)",
      "Action recognition",
      "Algorithm",
      "Artificial intelligence",
      "Channel (broadcasting)",
      "Class (philosophy)",
      "Computer network",
      "Computer science",
      "Convolutional neural network",
      "Dimension (graph theory)",
      "Discriminative model",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Pure mathematics",
      "Quantum mechanics",
      "Residual"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Ao"
      },
      {
        "surname": "Yi",
        "given_name": "Yang"
      },
      {
        "surname": "Liang",
        "given_name": "Daan"
      }
    ]
  },
  {
    "title": "SICNet: Learning selective inter-slice context via Mask-Guided Self-knowledge distillation for NPC segmentation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104053",
    "abstract": "Accurate segmentation of nasopharyngeal carcinoma (NPC) in magnetic resonance (MR) images is crucial for radiotherapy planning. However, vanilla 2D/3D deep convolutional networks fail to gain satisfying NPC segmentation results, since 3D methods suffer from inter-slice discontinuities and 2D methods lack inter-slice context learning. To address this problem, this paper proposes a 2.5D learning Selective Inter-slice Context Network (SICNet) for NPC segmentation. The proposed SICNet comprises a main encoder that extracts target features from the middle slice and an auxiliary encoder that learns inter-slice context from adjacent slices. The features from two encoders are fused by the Inter-slice Context Fusion (ICF) module and then sent to the decoder for predictions. Furthermore, a knowledge distillation module named Mask-Guided Self-knowledge Distillation (MGSD) is proposed to help the auxiliary encoder learn context selectively. Specifically, with the guidance of mask labels, MGSD tells the auxiliary encoder where to look and how to learn, thus achieving selective inter-slice context learning and boosting segmentation accuracy. The proposed SICNet achieves a DSC of 74.38 ± 11.99, an HD95 of 9.31 ± 2.68, and an ASSD of 1.46 ± 0.76 on an in-house NPC dataset, outperforming other state-of-the-art segmentation methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000087",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Deep learning",
      "Encoder",
      "Operating system",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Jinhong"
      },
      {
        "surname": "Li",
        "given_name": "Bin"
      },
      {
        "surname": "Qiu",
        "given_name": "Qianhui"
      },
      {
        "surname": "Mo",
        "given_name": "Hongqiang"
      },
      {
        "surname": "Tian",
        "given_name": "Lianfang"
      }
    ]
  },
  {
    "title": "SDCS-CF: Saliency-driven localization and cascade scale estimation for visual tracking",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2023.104040",
    "abstract": "Discriminative correlation filtering (DCF)-based trackers have demonstrated remarkable results in the field of visual tracking. Nevertheless, in most DCF-based trackers: (i) they apply correlation operations across the entire search area features without discriminative weights, rendering them highly susceptible to background interference; and (ii) the fixed aspect ratio scale search strategy is inadequate for accurate scale estimation under irregular scale variations. To overcome these challenges, this study proposes a new correlation filter through saliency-driven localization and cascaded scale estimation (SDCS-CF). Specifically, a U-like network is devised to generate a pixel-wise saliency map. This map is then multiplied element-wise with search area features to accentuate the target attribute, mitigating distractors and increasing localization confidence. Furthermore, a cascaded scale estimation model consisting of three one-dimensional filters is designed to refine the scale estimation process and improving the robustness. Extensive experimental results on three public datasets demonstrate that the proposed SDCS-CF outperforms most DCF-based trackers.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002900",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "BitTorrent tracker",
      "Cascade",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Computer vision",
      "Correlation",
      "Discriminative model",
      "Eye tracking",
      "Gene",
      "Geometry",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Pixel",
      "Quantum mechanics",
      "Rendering (computer graphics)",
      "Robustness (evolution)",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Peng"
      },
      {
        "surname": "Wang",
        "given_name": "Qinghui"
      },
      {
        "surname": "Dou",
        "given_name": "Jie"
      },
      {
        "surname": "Dou",
        "given_name": "Lei"
      }
    ]
  },
  {
    "title": "DUIANet: A double layer U-Net image hiding method based on improved Inception module and attention mechanism",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2023.104035",
    "abstract": "Image hiding is the process of hiding secret image into cover image, and revealing secret image from steganographic image. However, the quality of generated images and the ability to resist steganalysis detection of steganographic image can be further improved. We propose a novel double-layer U-Net image hiding method based on improved Inception and attention mechanism. The secret image undergoes Haar wavelet transform, hiding network hides the wavelet sub-band of secret image into cover image. We use improved Inception while cascading convolution and attention mechanisms to better extract and fuse information, resulting in high-quality steganographic image. In the revealing stage, Attention mechanism can help networks extract more features about secret image, then we using inverse Haar wavelet transform to revealing high-quality secret image. The experimental results show that the method can obtain high quality images and have good security. When hiding multiple images, the method still has good performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002857",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Cover (algebra)",
      "Discrete wavelet transform",
      "Engineering",
      "Haar wavelet",
      "Image (mathematics)",
      "Image quality",
      "Information hiding",
      "Mechanical engineering",
      "Pattern recognition (psychology)",
      "Steganalysis",
      "Steganography",
      "Wavelet",
      "Wavelet transform"
    ],
    "authors": [
      {
        "surname": "Duan",
        "given_name": "Xintao"
      },
      {
        "surname": "Wu",
        "given_name": "Guoming"
      },
      {
        "surname": "Li",
        "given_name": "Chun"
      },
      {
        "surname": "Li",
        "given_name": "Zhuang"
      },
      {
        "surname": "Qin",
        "given_name": "Chuan"
      }
    ]
  },
  {
    "title": "A hierarchical probabilistic underwater image enhancement model with reinforcement tuning",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104052",
    "abstract": "Underwater Image Enhancement (UIE) is a challenging problem due to the complex underwater environment. Traditional UIE methods can hardly adapt to various underwater environments. Deep learning-based UIE methods are more powerful but often rely on a large deal of real-world underwater images with distortion-free reference images. This gives rise to two issues: First, the reference images are highly uncertain because the ground-truth images cannot be are captured directly in underwater environment. Second, learning-based methods may lack generalization ability for diverse underwater environments. To tackle these issues, we propose HPUIE-RL, a hierarchical probabilistic UIE model facilitated by reinforcement learning. This model integrates UNet with hierarchical probabilistic modules to produce various enhanced candidate images that reflect the uncertainty of the enhancement. Then, a reinforcement learning fine-tuning framework is designed to fine-tune the pretrained model in an unsupervised manner, which responds to the dynamic underwater environment. Experiments on real-world datasets from diverse underwater environments demonstrate that our HPUIE-RL model outperforms state-of-the-art UIE methods regarding visual and quantitative performance and generalizability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000075",
    "keywords": [
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Distortion (music)",
      "Generalizability theory",
      "Generalization",
      "Geology",
      "Ground truth",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Oceanography",
      "Probabilistic logic",
      "Reinforcement learning",
      "Statistics",
      "Underwater"
    ],
    "authors": [
      {
        "surname": "Song",
        "given_name": "Wei"
      },
      {
        "surname": "Shen",
        "given_name": "Zhihao"
      },
      {
        "surname": "Zhang",
        "given_name": "Minghua"
      },
      {
        "surname": "Wang",
        "given_name": "Yan"
      },
      {
        "surname": "Liotta",
        "given_name": "Antonio"
      }
    ]
  },
  {
    "title": "Fast HEVC inter-frame coding based on LSTM neural network technology",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104056",
    "abstract": "High Efficiency Video Coding (HEVC) is the most commonly used video coding standard. However, its high coding complexity is a heavy burden for real-time video applications. But, coding tools designed based on traditional coding frameworks have reached limits. Furthermore, existing low-complexity video coding methods have not thoroughly analyzed the characteristics of compressed video, making it impossible to develop targeted models to reduce coding complexity. Therefore, in this research, a fast HEVC inter-frame coding technique is proposed. Firstly, we perform a characteristics analysis of HEVC inter-frame coding to explore the correlation between video frames. Secondly, we develop an Inter-Frame Feature Transfer-Long Short-Term Memory (IFFT-LSTM) model to obtain the optimal coding tree unit (CTU) partition structure. Thirdly, we embed the IFFT-LSTM model into the HEVC test platform. The experimental results show that suggested method can effectively reduce the HEVC inter-frame coding complexity with a small amount of rate-distortion (RD) performance loss while maintaining acceptable subjective video quality.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000117",
    "keywords": [
      "Algorithm",
      "Algorithmic efficiency",
      "Artificial intelligence",
      "Coding (social sciences)",
      "Coding tree unit",
      "Computer science",
      "Computer vision",
      "Context-adaptive binary arithmetic coding",
      "Data compression",
      "Decoding methods",
      "Intra-frame",
      "Mathematics",
      "Multiview Video Coding",
      "Pixel",
      "Real-time computing",
      "Statistics",
      "Video processing",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Chang"
      }
    ]
  },
  {
    "title": "A small object detection algorithm based on feature interaction and guided learning",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2023.104011",
    "abstract": "At present, for the diverse target sizes, different angles, and complex environment in aerial images, how to detect small objects effectively in aerial images is still a challenge task. In order to further improve the detection accuracy and reduce the missing rate, in this paper, we proposes a small target detection algorithm based on feature interaction and guided learning. Firstly, the feature guided alignment module is designed to deal with the problem of information loss and overlap. Secondly, the misallocation of small object samples is optimized by introducing gaussian sample allocation strategy. Then the interactive parallel detection header is designed to solve the detection task conflict problem. Finally, the new regression loss function is designed to enhance the localization ability of the target. The experimental results on the Tinyperson dataset and Visdrone dataset show that this model can improve the detection accuracy and reduce the missing rate of small targets effectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002614",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Chemistry",
      "Chromatography",
      "Computer network",
      "Computer science",
      "Economics",
      "Evolutionary biology",
      "Feature (linguistics)",
      "Function (biology)",
      "Gaussian",
      "Header",
      "Linguistics",
      "Management",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Sample (material)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Shao",
        "given_name": "Xiang-Ying"
      },
      {
        "surname": "Guo",
        "given_name": "Ying"
      },
      {
        "surname": "Wang",
        "given_name": "You-Wei"
      },
      {
        "surname": "Bao",
        "given_name": "Zheng-Wei"
      },
      {
        "surname": "Wang",
        "given_name": "Ji-Yu"
      }
    ]
  },
  {
    "title": "Residual attention fusion network for video action recognition",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2023.103987",
    "abstract": "Human action recognition in videos is a fundamental and important topic in computer vision, and modeling spatial–temporal dynamics in a video is crucial for action classification. In this paper, a novel attention module named Channel-wise Non-local Attention Module (CNAM) is proposed to highlight the important features both spatially and temporally. Besides, another new attention module named Channel-wise Attention Recalibration Module (CARM) is developed to focus on capturing discriminative features at channel level. Based on these two attention modules, a novel convolutional neural network named Residual Attention Fusion Network (RAFN) is proposed to model long-range temporal structure and capture more discriminative action features at the same time. More specifically, first, a sparse temporal sampling strategy is adopted to uniformly sample video data as input to RAFN along the temporal dimension. Secondly, the attention modules CNAM and CARM are plugged into residual network for highlighting important action regions around actors. Finally, the classification scores of four streams of RAFN are combined by late fusion. The experimental results on HMDB51 and UCF101 demonstrate the effectiveness and excellent recognition performance of our proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002377",
    "keywords": [
      "Action (physics)",
      "Action recognition",
      "Algorithm",
      "Artificial intelligence",
      "Channel (broadcasting)",
      "Class (philosophy)",
      "Computer network",
      "Computer science",
      "Convolutional neural network",
      "Dimension (graph theory)",
      "Discriminative model",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Pure mathematics",
      "Quantum mechanics",
      "Residual"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Ao"
      },
      {
        "surname": "Yi",
        "given_name": "Yang"
      },
      {
        "surname": "Liang",
        "given_name": "Daan"
      }
    ]
  },
  {
    "title": "SICNet: Learning selective inter-slice context via Mask-Guided Self-knowledge distillation for NPC segmentation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104053",
    "abstract": "Accurate segmentation of nasopharyngeal carcinoma (NPC) in magnetic resonance (MR) images is crucial for radiotherapy planning. However, vanilla 2D/3D deep convolutional networks fail to gain satisfying NPC segmentation results, since 3D methods suffer from inter-slice discontinuities and 2D methods lack inter-slice context learning. To address this problem, this paper proposes a 2.5D learning Selective Inter-slice Context Network (SICNet) for NPC segmentation. The proposed SICNet comprises a main encoder that extracts target features from the middle slice and an auxiliary encoder that learns inter-slice context from adjacent slices. The features from two encoders are fused by the Inter-slice Context Fusion (ICF) module and then sent to the decoder for predictions. Furthermore, a knowledge distillation module named Mask-Guided Self-knowledge Distillation (MGSD) is proposed to help the auxiliary encoder learn context selectively. Specifically, with the guidance of mask labels, MGSD tells the auxiliary encoder where to look and how to learn, thus achieving selective inter-slice context learning and boosting segmentation accuracy. The proposed SICNet achieves a DSC of 74.38 ± 11.99, an HD95 of 9.31 ± 2.68, and an ASSD of 1.46 ± 0.76 on an in-house NPC dataset, outperforming other state-of-the-art segmentation methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000087",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Deep learning",
      "Encoder",
      "Operating system",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Jinhong"
      },
      {
        "surname": "Li",
        "given_name": "Bin"
      },
      {
        "surname": "Qiu",
        "given_name": "Qianhui"
      },
      {
        "surname": "Mo",
        "given_name": "Hongqiang"
      },
      {
        "surname": "Tian",
        "given_name": "Lianfang"
      }
    ]
  },
  {
    "title": "SDCS-CF: Saliency-driven localization and cascade scale estimation for visual tracking",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2023.104040",
    "abstract": "Discriminative correlation filtering (DCF)-based trackers have demonstrated remarkable results in the field of visual tracking. Nevertheless, in most DCF-based trackers: (i) they apply correlation operations across the entire search area features without discriminative weights, rendering them highly susceptible to background interference; and (ii) the fixed aspect ratio scale search strategy is inadequate for accurate scale estimation under irregular scale variations. To overcome these challenges, this study proposes a new correlation filter through saliency-driven localization and cascaded scale estimation (SDCS-CF). Specifically, a U-like network is devised to generate a pixel-wise saliency map. This map is then multiplied element-wise with search area features to accentuate the target attribute, mitigating distractors and increasing localization confidence. Furthermore, a cascaded scale estimation model consisting of three one-dimensional filters is designed to refine the scale estimation process and improving the robustness. Extensive experimental results on three public datasets demonstrate that the proposed SDCS-CF outperforms most DCF-based trackers.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002900",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "BitTorrent tracker",
      "Cascade",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Computer vision",
      "Correlation",
      "Discriminative model",
      "Eye tracking",
      "Gene",
      "Geometry",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Pixel",
      "Quantum mechanics",
      "Rendering (computer graphics)",
      "Robustness (evolution)",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Peng"
      },
      {
        "surname": "Wang",
        "given_name": "Qinghui"
      },
      {
        "surname": "Dou",
        "given_name": "Jie"
      },
      {
        "surname": "Dou",
        "given_name": "Lei"
      }
    ]
  },
  {
    "title": "A hierarchical probabilistic underwater image enhancement model with reinforcement tuning",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104052",
    "abstract": "Underwater Image Enhancement (UIE) is a challenging problem due to the complex underwater environment. Traditional UIE methods can hardly adapt to various underwater environments. Deep learning-based UIE methods are more powerful but often rely on a large deal of real-world underwater images with distortion-free reference images. This gives rise to two issues: First, the reference images are highly uncertain because the ground-truth images cannot be are captured directly in underwater environment. Second, learning-based methods may lack generalization ability for diverse underwater environments. To tackle these issues, we propose HPUIE-RL, a hierarchical probabilistic UIE model facilitated by reinforcement learning. This model integrates UNet with hierarchical probabilistic modules to produce various enhanced candidate images that reflect the uncertainty of the enhancement. Then, a reinforcement learning fine-tuning framework is designed to fine-tune the pretrained model in an unsupervised manner, which responds to the dynamic underwater environment. Experiments on real-world datasets from diverse underwater environments demonstrate that our HPUIE-RL model outperforms state-of-the-art UIE methods regarding visual and quantitative performance and generalizability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000075",
    "keywords": [
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Distortion (music)",
      "Generalizability theory",
      "Generalization",
      "Geology",
      "Ground truth",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Oceanography",
      "Probabilistic logic",
      "Reinforcement learning",
      "Statistics",
      "Underwater"
    ],
    "authors": [
      {
        "surname": "Song",
        "given_name": "Wei"
      },
      {
        "surname": "Shen",
        "given_name": "Zhihao"
      },
      {
        "surname": "Zhang",
        "given_name": "Minghua"
      },
      {
        "surname": "Wang",
        "given_name": "Yan"
      },
      {
        "surname": "Liotta",
        "given_name": "Antonio"
      }
    ]
  },
  {
    "title": "MFCANet: A road scene segmentation network based on Multi-Scale feature fusion and context information aggregation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104055",
    "abstract": "Road scene segmentation is the basic task of autonomous driving. Recent representative scene segmentation methods adopt the full convolutional network based on the encoder-decoder. However, the framework can cause the loss of image fine-grained information in the process of down-sampling, feature extraction and feature fusion, resulting in blurred boundary details and chaotic segmentation effect. In this work, a road scene segmentation network based on multi-scale feature fusion and context information aggregation is proposed, in which context information is used to guide feature fusion and enhance semantic feature extraction. Three plug-and-play modules are designed to extract multi-scale features with strong semantic information from high-level features, which compensate for the loss of spatial information in the upper sampling stage, and capture the information dependence among pixels to improve pixel-by-pixel segmentation. Experimental results on Camvid and Cityscapes show that the proposed multi-scale feature fusion and context information aggregation network (MFCANet) can achieve satisfactory performance compared with the state-of-the-art segmentation methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000105",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Encoder",
      "Feature (linguistics)",
      "Feature extraction",
      "Geography",
      "Image segmentation",
      "Linguistics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Scale-space segmentation",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yunfeng"
      },
      {
        "surname": "Zhou",
        "given_name": "Yi"
      },
      {
        "surname": "Wu",
        "given_name": "Hao"
      },
      {
        "surname": "Liu",
        "given_name": "Xiyu"
      },
      {
        "surname": "Zhai",
        "given_name": "Xiaodi"
      },
      {
        "surname": "Sun",
        "given_name": "Kuizhi"
      },
      {
        "surname": "Tian",
        "given_name": "Chengliang"
      },
      {
        "surname": "Zhao",
        "given_name": "Haixia"
      },
      {
        "surname": "Li",
        "given_name": "Tao"
      },
      {
        "surname": "Jia",
        "given_name": "Wenguang"
      },
      {
        "surname": "Zhang",
        "given_name": "Yan"
      }
    ]
  },
  {
    "title": "Human skin detection: An unsupervised machine learning way",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104046",
    "abstract": "Researchers have been involved for decades in search of an efficient skin detection method. However, current methods have not overcome the significant challenges of skin detection, such as variation of illumination, various skin tones of different ethnic groups, and many others. This research proposed a clustering and region-growing-based skin detection method to overcome these limitations. Together with significant insight, these methods result in a more effective algorithm. The insight concerns the capability to dynamically define the number of clusters in a collection of pixels organized as images. In Clustering for most problem domains, the number of clusters is fixed prior and does not perform effectively over a wide variety of data contents. Therefore, this research paper proposed a skin detection method that validated the above findings. The proposed method assigns the number of clusters based on image properties and ultimately allows freedom from manual thresholds or other manual operations. The dynamic determination of clustering outcomes allows for greater automation of skin detection when dealing with uncertain real-world conditions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000014",
    "keywords": [
      "Artificial intelligence",
      "Astrophysics",
      "Automation",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Engineering",
      "Image (mathematics)",
      "Machine learning",
      "Mechanical engineering",
      "Pattern recognition (psychology)",
      "Physics",
      "Pixel",
      "Variation (astronomy)",
      "Variety (cybernetics)"
    ],
    "authors": [
      {
        "surname": "Islam",
        "given_name": "ABM Rezbaul"
      },
      {
        "surname": "Alammari",
        "given_name": "Ali"
      },
      {
        "surname": "Buckles",
        "given_name": "Bill"
      }
    ]
  },
  {
    "title": "DPAFD-net: A dual-path adaptive fusion dehazing network",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2023.104018",
    "abstract": "Image dehazing is an ill-posed problem that has been extensively studied in recent years. Unfortunately, most existing deep dehazing models have high computational complexity and lack the dynamic adjustment of details, which hinders their application to high-resolution images in computational vision tasks. In this paper, we propose an efficient dual-path adaptive fusion dehazing network (DPAFD-Net) to directly restore a clear image from a hazy input. Moreover, we propose a pure subnetwork with encoder and decoder structures to further extract the structural information and progressively restore the haze-free image. To evaluate the effectiveness of the proposed method, we validate our approach on synthetic and real hazy images, where our method performs favourably against the state-of-the-art dehazing approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002687",
    "keywords": [
      "Algorithm",
      "Art",
      "Artificial intelligence",
      "Computational complexity theory",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Dual (grammatical number)",
      "Encoder",
      "Image (mathematics)",
      "Literature",
      "Operating system",
      "Path (computing)",
      "Subnetwork"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Chenyang"
      },
      {
        "surname": "Jing",
        "given_name": "Hongyuan"
      },
      {
        "surname": "Wei",
        "given_name": "Shuang"
      },
      {
        "surname": "Chen",
        "given_name": "Jiaxing"
      },
      {
        "surname": "Shang",
        "given_name": "Xinna"
      },
      {
        "surname": "Chen",
        "given_name": "Aidong"
      }
    ]
  },
  {
    "title": "Deep-MDS framework for recovering the 3D shape of 2D landmarks from a single image",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2023.104032",
    "abstract": "Using 3D reconstruction techniques within computer vision frameworks can result in more robust and accurate solutions. However, the main challenge lies in the high computation and memory resources required by such methods. To reduce the complexity of these frameworks, a practical solution is to use geometric landmarks instead of the entire image. Therefore, in this paper the problem of 3D shape recovery of a set of standard 2D landmarks, on a single human face image is faced using multi-dimensional scaling (MDS) approach to find a 3D embedding for a set of standard 2D points. Hence, MDS approach is used for the first time in this study to establish an unbiased mapping from 2D landmark space to the corresponding 3D shape space. A deep neural network learns the pairwise 3D dissimilarity among standard 2D landmarks. This scheme leads to find a symmetric dissimilarity matrix, to be fed into the MDS approach to appropriately recovering the 3D shape of corresponding 2D landmarks. In the case of complex input image formations like posedness or perspective projection causing occlusion in the input image, an autoencoder component is used in the proposed framework, as an occlusion removal part, which turns different input views of the human face into a profile view. The results of performance evaluation using variety of synthetic and real-world human face datasets, including NoW dataset, Besel Face Model (BFM), CelebA, CoMA - FLAME, and CASIA-3D, indicates the superiority of the proposed framework, despite its small number of training parameters, with the related state-of-the-art and recent 3D shape recovery of landmark methods from the literature, in terms of unbiasedness, independent projection, efficiency and accuracy. Further, ablation study is performed to find the best training scheme of the deep learning components. All codes and public data of our paper are publicly available for research purposes at https://github.com/s2kamyab/DeepMDS.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002821",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Image (mathematics)"
    ],
    "authors": [
      {
        "surname": "Kamyab",
        "given_name": "Shima"
      },
      {
        "surname": "Azimifar",
        "given_name": "Zohreh"
      }
    ]
  },
  {
    "title": "CalD3r and MenD3s: Spontaneous 3D facial expression databases",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2023.104033",
    "abstract": "In the last couple of decades, the research on 3D facial expression recognition has been fostered by the creation of tailored databases containing prototypical expressions of different individuals and by the advances in cost effective acquisition technologies. Though, most of the currently available databases consist of exaggerated facial expressions, due to the imitation principle which they rely on. This makes these databases only partially employable for real world applications such as human-computer interaction for smart products and environments, health, and industry 4.0, as algorithms learn on these ‘inflated’ data which do not respond to ecological validity requirements. In this work, we present two novel 2D + 3D spontaneous facial expression databases of young adults with different geographical origin, in which emotions have been evoked thanks to affective images of the acknowledged IAPS and GAPED databases, and verified with participants’ self-reports. To the best of our knowledge, these are the first three-dimensional facial databases with emotions elicited by validated affective stimuli.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002833",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Database",
      "Expression (computer science)",
      "Facial expression",
      "Programming language"
    ],
    "authors": [
      {
        "surname": "Ulrich",
        "given_name": "Luca"
      },
      {
        "surname": "Marcolin",
        "given_name": "Federica"
      },
      {
        "surname": "Vezzetti",
        "given_name": "Enrico"
      },
      {
        "surname": "Nonis",
        "given_name": "Francesca"
      },
      {
        "surname": "Mograbi",
        "given_name": "Daniel C."
      },
      {
        "surname": "Scurati",
        "given_name": "Giulia Wally"
      },
      {
        "surname": "Dozio",
        "given_name": "Nicolò"
      },
      {
        "surname": "Ferrise",
        "given_name": "Francesco"
      }
    ]
  },
  {
    "title": "REQA: Coarse-to-fine assessment of image quality to alleviate the range effect",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2023.104043",
    "abstract": "Blind image quality assessment (BIQA) of User Generated Content (UGC) suffers from the range effect, which indicates that on the overall quality range, mean opinion score (MOS) and predicted MOS (pMOS) are well correlated, but when focusing on a particular narrow range, the correlation is lower. To tackle this problem, a novel method is proposed from coarse-grained metric to fine-grained prediction. Concretely, we utilize global context features and local detailed features for the multi-scale distortion perception. Then, to further boost the ability of fine-grained assessment, we introduce the feedback mechanism, which is in accord with Human Vision System (HVS), to perceive detailed distortions gradually. Also, two coarse-to-fine loss functions are proposed to facilitate the feedback perception progress: a rank-and-gradient loss for coarse-grained metric keeps the assessment rank and gradient consistency between pMOS and MOS; a multi-level tolerance loss following the curriculum learning strategy is proposed to make a fine-grained prediction. Both coarse-grained and fine-grained experiments demonstrate that the proposed method outperforms the state-of-the-art ones, which validates that our method effectively alleviates the range effect. The codes are available at https://github.com/huofushuo/REQA.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002936",
    "keywords": [
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Biology",
      "Combinatorics",
      "Composite material",
      "Computer network",
      "Computer science",
      "Consistency (knowledge bases)",
      "Context (archaeology)",
      "Distortion (music)",
      "Economics",
      "Epistemology",
      "Image (mathematics)",
      "Image quality",
      "Machine learning",
      "Materials science",
      "Mathematics",
      "Metric (unit)",
      "Neuroscience",
      "Operations management",
      "Paleontology",
      "Perception",
      "Philosophy",
      "Quality (philosophy)",
      "Range (aeronautics)",
      "Rank (graph theory)",
      "Ranking (information retrieval)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Bingheng"
      },
      {
        "surname": "Huo",
        "given_name": "Fushuo"
      }
    ]
  },
  {
    "title": "Multimodal fusion hierarchical self-attention network for dynamic hand gesture recognition",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2023.104019",
    "abstract": "Recent improvements in dynamic hand gesture recognition have seen a shift from traditional convolutional architectures to attention-based networks. These attention networks have been proven to outclass CNN + LSTM architectures, showing higher accuracy as well as reduced model parameters. Especially, skeleton-based attention networks have been shown to outperform visual-based networks due to the rich information from skeleton-based hand features. However, there is an opportunity to introduce complementary features from other modalities like RGB, depth, and optical flow images to enhance the recognition capability of skeleton-based networks. This paper aims to explore the addition of a multimodal fusion network to a skeleton-based Hierarchical Self-Attention Network (MF-HAN) and test for increased model effectiveness. Unlike traditional fusion techniques, this fusion network uses features derived from other sources of multimodal data in a reduced feature space using a cross-attention layer. The model outperforms its root model and other state-of-the-art models on the SHREC’17 track dataset, especially in the 28 gestures setting by more than 1 % in gesture classification accuracy. The experimentation was tested on the DHG dataset as well.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002699",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Gesture",
      "Gesture recognition",
      "Image (mathematics)",
      "Linguistics",
      "Optical flow",
      "Pattern recognition (psychology)",
      "Philosophy",
      "RGB color model"
    ],
    "authors": [
      {
        "surname": "Balaji",
        "given_name": "Pranav"
      },
      {
        "surname": "Ranjan Prusty",
        "given_name": "Manas"
      }
    ]
  },
  {
    "title": "Learning informative and discriminative semantic features for robust facial expression recognition",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2024.104062",
    "abstract": "Facial expression recognition (FER) becomes challenging in real-world scenarios, which requires learning informative and discriminative features from challenging datasets to obtain robust facial expression recognition. In this paper, we propose an Informative and Discriminative Semantic Features Learning (IDSFL) network for FER against occlusion and head pose in the wild. Specifically, IDSFL aims to mine informative and discriminative semantic features from both low and high levels learned features to learn robust representations. First, a multi-channel feature (MCF) modulator incorporating low-level Gabor features is introduced to learn informative semantic features by capturing adequate diverse and detailed information. Additionally, a specific emotion-aware (SEA) module is proposed to learn discriminative semantic features by aggregating high-level emotion-specific features to focus on each expression category. Thus, IDSFL can collaboratively learn informative and discriminative representations. Extensive experiments on challenging in-the-wild datasets, including RAF-DB, FERPlus and AffectNet-7, demonstrate that our proposed method outperforms most state-of-the-art FER methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320324000178",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Discriminative model",
      "Expression (computer science)",
      "Facial expression",
      "Feature (linguistics)",
      "Feature learning",
      "Focus (optics)",
      "Linguistics",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Programming language",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Tan",
        "given_name": "Yumei"
      },
      {
        "surname": "Xia",
        "given_name": "Haiying"
      },
      {
        "surname": "Song",
        "given_name": "Shuxiang"
      }
    ]
  },
  {
    "title": "A survey on just noticeable distortion estimation and its applications in video coding",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2024",
    "doi": "10.1016/j.jvcir.2023.104034",
    "abstract": "With the developing explosion in video data delivery, perceptual video coding (PVC) plays an increasingly significant role in video compression. The just noticeable distortion (JND) reflects the tolerance limit of human visual system (HVS) to coding distortion directly, resulting in that JND-based PVC is the most important branch of video coding. This paper provides an extensive overview of JND estimation and JND-based PVC, so as to make interested readers aware of the status quo. The main contribution of this article can be briefly outlined as follows. Firstly, the general description of JND concepts and most existing computation models for JND are to be reviewed systematically. Secondly, most related works about JND-based perceptual image and video coding schemes are introduced, including JND-based coding preprocessing and JND-based codec embedding. Thirdly, in addition to a thorough summary of JND estimation and JND-based PVC, possible future directions and opportunities are analyzed and discussed.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320323002845",
    "keywords": [
      "Artificial intelligence",
      "Codec",
      "Coding (social sciences)",
      "Computer science",
      "Mathematics",
      "Preprocessor",
      "Statistics",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Guoxiang"
      },
      {
        "surname": "Wang",
        "given_name": "Hongkui"
      },
      {
        "surname": "Li",
        "given_name": "Hui"
      },
      {
        "surname": "Yu",
        "given_name": "Li"
      },
      {
        "surname": "Yin",
        "given_name": "Haibing"
      },
      {
        "surname": "Xu",
        "given_name": "Haifeng"
      },
      {
        "surname": "Ye",
        "given_name": "Zhen"
      },
      {
        "surname": "Song",
        "given_name": "Junfeng"
      }
    ]
  }
]