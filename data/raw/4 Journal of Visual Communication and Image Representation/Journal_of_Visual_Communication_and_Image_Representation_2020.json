[
  {
    "title": "Analytical derivatives for differentiable renderer: 3D pose estimation by silhouette consistency",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102960",
    "abstract": "Differentiable renderer is widely used in optimization-based 3D reconstruction which requires gradients for optimization. The existing differentiable renderers obtain gradients via numerical techniques. However, these methods are inaccurate and inefficient. Motivated by this fact, we propose a differentiable renderer with analytical gradients. The main obstacle of traditional renderer being differentiable is the discrete sampling operation of rasterization. To obtain a differentiable rasterization renderer, we define pixel intensity as a double integral over the pixel grid, and then derive the analytical gradients with respect to vertices. 3D pose estimation by multi-viewpoint silhouettes is conducted to reveal the effectiveness and efficiency of the proposed method. Experimental results show that 3D pose estimation without 3D and 2D joint supervision can produce competitive results. The findings also indicate that the proposed method has higher accuracy and efficiency than previous differentiable renderers.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301851",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Differentiable function",
      "Mathematical analysis",
      "Mathematics",
      "Pixel",
      "Pose",
      "Rendering (computer graphics)"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Zaiqiang"
      },
      {
        "surname": "Jiang",
        "given_name": "Wei"
      },
      {
        "surname": "Yu",
        "given_name": "Hongyan"
      }
    ]
  },
  {
    "title": "Weakly supervised instance segmentation using multi-stage erasing refinement and saliency-guided proposals ordering",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102957",
    "abstract": "Weakly supervised instance segmentation is a new research topic in the field of computer vision. Compared with fully supervised instance segmentation, weakly supervised methods use weaker data annotations such as points, scribbles or class labels which are easy to obtain. Among these annotations, image-level instance segmentation using only class labels as supervision is the most challenging task. In this paper, we propose a novel weakly supervised instance segmentation framework using a multi-stage erasing refinement method and a saliency-guided proposals ordering method. Firstly, the multi-stage erasing refinement method is exploited to enhance the instance representation by iteratively discovering separate object-related regions, so as to obtain more complete discriminative regions. Then, the saliency-guided proposals ordering method utilizes the saliency map to alleviate the background noise and better select the object proposals for generating the instance segmentation result. Experimental results on the PASCAL VOC 2012 dataset and the COCO dataset demonstrate that our framework achieves superior performance compared with the state-of-the-art weakly supervised instance segmentation models and the ablation study shows the effectiveness of the proposed two methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032030184X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Class (philosophy)",
      "Computer science",
      "Discriminative model",
      "Image segmentation",
      "Law",
      "Machine learning",
      "Object (grammar)",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Programming language",
      "Representation (politics)",
      "Scale-space segmentation",
      "Segmentation",
      "Supervised learning"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Zheng"
      },
      {
        "surname": "Liu",
        "given_name": "Zhi"
      },
      {
        "surname": "Li",
        "given_name": "Gongyang"
      },
      {
        "surname": "Ye",
        "given_name": "Linwei"
      },
      {
        "surname": "Zhou",
        "given_name": "Lei"
      },
      {
        "surname": "Wang",
        "given_name": "Yang"
      }
    ]
  },
  {
    "title": "Exploiting object features in deep gaze prediction models",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102931",
    "abstract": "The human visual system analyzes the complex scenes rapidly. It devotes the limited perceptual resources to the most salient subsets and/or objects of scenes while ignoring their less salient parts. Gaze prediction models try to predict the human eye fixations (human gaze) under free-viewing conditions while imitating the attentive mechanism. Previous studies on saliency benchmark datasets have shown that visual attention is affected by the salient objects of the scenes and their features. These features include the identity, the location, and the visual features of objects in the scenes, beside to the context of the input image. Moreover, the human eye fixations often converge to the specific parts of salient objects in the scenes. In this paper, we propose a deep gaze prediction model using object detection via image segmentation. It uses some deep neural modules to find the identity, location, and visual features of the salient objects in the scenes. In addition, we introduce a deep module to capture the prior bias of human eye fixations. To evaluate our model, several challenging saliency benchmark datasets are used in the experiments. We also conduct an ablation study to show the effectiveness of our proposed modules and its architecture. Despite its fewer parameters, our model has comparable, or even better performance on some datasets, to the state-of-the-art saliency models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301620",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Eye tracking",
      "Gaze",
      "Geodesy",
      "Geography",
      "Human visual system model",
      "Image (mathematics)",
      "Neuroscience",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Perception",
      "Psychology",
      "Salient",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Zabihi",
        "given_name": "Saman"
      },
      {
        "surname": "Mansoori",
        "given_name": "Eghbal"
      },
      {
        "surname": "Yazdi",
        "given_name": "Mehran"
      }
    ]
  },
  {
    "title": "A view-free image stitching network based on global homography",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102950",
    "abstract": "Image stitching is a traditional but challenging computer vision task, aiming to obtain a seamless panoramic image. Recently, researchers begin to study the image stitching task using deep learning. However, the existing learning methods assume a relatively fixed view during the image capturing, thus show a poor generalization ability to flexible view cases. To address the above problem, we present a cascaded view-free image stitching network based on a global homography. This novel image stitching network does not have any restriction on the view of images and it can be implemented in three stages. In particular, we first estimate a global homography between two input images from different views. And then we propose a structure stitching layer to obtain the coarse stitching result using the global homography. In the last stage, we design a content revision network to eliminate ghosting effects and refine the content of the stitching result. To enable efficient learning on various views, we also present a method to generate synthetic datasets for network training. Experimental results demonstrate that our method can achieve almost 100% elimination of artifacts in overlapping areas at the cost of acceptable slight distortions in non-overlapping areas, compared with traditional methods. In addition, the proposed method is view-free and more robust especially in a scene where feature points are difficult to detect.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301784",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Economics",
      "Feature (linguistics)",
      "Ghosting",
      "Homography",
      "Image (mathematics)",
      "Image stitching",
      "Linguistics",
      "Management",
      "Mathematics",
      "Panorama",
      "Philosophy",
      "Projective space",
      "Projective test",
      "Statistics",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Nie",
        "given_name": "Lang"
      },
      {
        "surname": "Lin",
        "given_name": "Chunyu"
      },
      {
        "surname": "Liao",
        "given_name": "Kang"
      },
      {
        "surname": "Liu",
        "given_name": "Meiqin"
      },
      {
        "surname": "Zhao",
        "given_name": "Yao"
      }
    ]
  },
  {
    "title": "Multiple depth-levels features fusion enhanced network for action recognition",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102929",
    "abstract": "As a challenging task of video classification, action recognition has become a significant topic of computer vision community. The most popular methods based on two-stream architecture up to now are still simply fusing the prediction scores of each stream. In that case, the complementary characteristics of two streams cannot be fully utilized and the effect of shallower features is often overlooked. In addition, the equal treatment to features may weaken the role of the feature contributing significantly to the classification. Accordingly, a novel network called Multiple Depth-levels Features Fusion Enhanced Network (MDFFEN) is proposed. It improves on two aspects of two-stream architecture. In terms of the two-stream interaction mechanism, multiple depth-levels features fusion (MDFF) is formed to aggregate spatial–temporal features extracted from several sub-modules of original two streams by spatial–temporal features fusion (STFF). And with respect to further refining the spatiotemporal features, we propose a group-wise spatial-channel enhance (GSCE) module to highlight the meaningful regions and expressive channels automatically by priority assignment. The competitive results are achieved after we validate MDFFEN on three public challenging action recognition datasets, HDMB51, UCF101 and ChaLearn LAP IsoGD.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301607",
    "keywords": [
      "Action (physics)",
      "Action recognition",
      "Aggregate (composite)",
      "Architecture",
      "Art",
      "Artificial intelligence",
      "Class (philosophy)",
      "Composite material",
      "Computer science",
      "Economics",
      "Feature (linguistics)",
      "Fusion",
      "Linguistics",
      "Management",
      "Materials science",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Task (project management)",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Shengquan"
      },
      {
        "surname": "Kong",
        "given_name": "Jun"
      },
      {
        "surname": "Jiang",
        "given_name": "Min"
      },
      {
        "surname": "Liu",
        "given_name": "Tianshan"
      }
    ]
  },
  {
    "title": "A large-scale remote sensing database for subjective and objective quality assessment of pansharpened images",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102947",
    "abstract": "Pansharpening is a process to fuse a low spatial resolution multispectral image and a high spatial resolution panchromatic image to produce a high-resolution multispectral image. Quality assessment of pansharpened images is challenging due to without actual reference images. There are two main types of assessment methods: reduced resolution (RR) assessment based on Wald’s protocol, and full resolution (FR) assessment without reference. Currently, it is lack of large-scale benchmark databases for subjective and objective performance evaluation of different image pansharpening methods. In this paper, we construct a large-scale database named Pansharpened Remote Sensing Image Quality Database (PRSIQD) from both qualitative and quantitative perspectives, which contains 13,620 pansharpened images acquired from IKONOS, QuickBird, Gaofen-1, WorldView-2, WorldView-3 and WorldView-4 satellite sensors. In addition, we have comprehensively analyzed the advantages and disadvantages of the existing pansharpening quality assessment methods on different satellite sensors, thematic datasets and bands.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301760",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Cartography",
      "Computer science",
      "Computer vision",
      "Database",
      "Geography",
      "Image (mathematics)",
      "Image quality",
      "Image resolution",
      "Multispectral image",
      "Panchromatic film",
      "Remote sensing",
      "Scale (ratio)",
      "Thematic map"
    ],
    "authors": [
      {
        "surname": "Xiong",
        "given_name": "Yiming"
      },
      {
        "surname": "Shao",
        "given_name": "Feng"
      },
      {
        "surname": "Meng",
        "given_name": "Xiangchao"
      },
      {
        "surname": "Jiang",
        "given_name": "Qiuping"
      },
      {
        "surname": "Sun",
        "given_name": "Weiwei"
      },
      {
        "surname": "Fu",
        "given_name": "Randi"
      },
      {
        "surname": "Ho",
        "given_name": "Yo-Sung"
      }
    ]
  },
  {
    "title": "Image splicing localization using residual image and residual-based fully convolutional network",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102967",
    "abstract": "Fully convolutional networks (FCNs) have been efficiently applied in splicing localization. However, the existing FCN-based methods still have three drawbacks: (a) their performance in detecting image details is unsatisfactory; (b) deep FCNs are difficult to train; (c) results of multiple FCNs are merged using fixed parameters to weigh their contributions. So, an improved method is proposed. Firstly, both the original spliced image and its corresponding residual image are regarded as the inputs of the network. Secondly, the residual block is introduced into FCN as residual-based FCN (RFCN) to make the network easier to optimize. Thirdly, three different RFCNs are merged to enhance locating maps with two learnable weight parameters. Besides, condition random field is introduced into the whole network to improve the results further. Experimental results on five datasets show that the proposed method performs better than some existing methods in localization ability, generalization ability, and robustness against additional operations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301929",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Block (permutation group theory)",
      "Chemistry",
      "Computer science",
      "Gene",
      "Geometry",
      "Image (mathematics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Residual",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Beijing"
      },
      {
        "surname": "Qi",
        "given_name": "Xiaoming"
      },
      {
        "surname": "Zhou",
        "given_name": "Yang"
      },
      {
        "surname": "Yang",
        "given_name": "Guanyu"
      },
      {
        "surname": "Zheng",
        "given_name": "Yuhui"
      },
      {
        "surname": "Xiao",
        "given_name": "Bin"
      }
    ]
  },
  {
    "title": "A heuristic framework for perceptual saliency prediction",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102913",
    "abstract": "Saliency prediction can be regarded as the human spontaneous activity. The most effective saliency model should highly approximate the response of viewers to the perceived information. In the paper, we exploit the perception response for saliency detection and propose a heuristic framework to predict salient region. First, to find the perceptually meaningful salient regions, an orientation selectivity based local feature and a visual Acuity based global feature are proposed to jointly predict candidate salient regions. Subsequently, to further boost the accuracy of saliency map, we introduce a visual error sensitivity based operator to activate the meaningful salient regions from a local and global perspective. In addition, an adaptive fusion method based on free energy principle is designed to combine the sub-saliency maps from each image channel to obtain the final saliency map. Experimental results on five natural and emotional datasets demonstrate the superiority of the proposed method compared to twelve state-of-the-art algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301516",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Contrast (vision)",
      "Electronic engineering",
      "Engineering",
      "Feature (linguistics)",
      "Geometry",
      "Heuristic",
      "Image (mathematics)",
      "Linguistics",
      "Mathematics",
      "Neuroscience",
      "Orientation (vector space)",
      "Pattern recognition (psychology)",
      "Perception",
      "Perspective (graphical)",
      "Philosophy",
      "Psychology",
      "Salient",
      "Sensitivity (control systems)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yongfang"
      },
      {
        "surname": "Ye",
        "given_name": "Peng"
      },
      {
        "surname": "Xia",
        "given_name": "Yumeng"
      },
      {
        "surname": "An",
        "given_name": "Ping"
      }
    ]
  },
  {
    "title": "Exploiting multigranular salient features with hierarchical multi-mode attention network for pedestrian re-IDentification",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102914",
    "abstract": "In this paper, we propose an end-to-end hierarchical-based multi-mode attention network and adaptive fusion (HMAN-HAF) strategy to learn different-level salient features for re-ID tasks. First, according to each layer’s characteristics, a hierarchical multi-mode attention network (HMAN) is designed to adopt different attention models for different-level salient feature learning. Specifically, refined channel-wise attention (CA) is adopted to capture high-level valuable semantic information, an attentive region model (AR) is used to detect salient regions in the low layer, and fused attention (FA) is designed to capture the salient regions of valuable channels in the middle layer. Second, a hierarchical adaptive fusion (HAF) is constructed to fulfill the complementary strengths of different-level salient features. Experimental results demonstrate that the proposed method outperforms the state-of-the-art methods on the following challenging benchmarks: Market-1501, DukeMTMC-reID and CUHK03.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301528",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Computer vision",
      "Engineering",
      "Human–computer interaction",
      "Identification (biology)",
      "Mode (computer interface)",
      "Pedestrian",
      "Salient",
      "Transport engineering"
    ],
    "authors": [
      {
        "surname": "Geng",
        "given_name": "Yanbing"
      },
      {
        "surname": "Lian",
        "given_name": "Yongjian"
      },
      {
        "surname": "Zhou",
        "given_name": "Mingliang"
      },
      {
        "surname": "Kong",
        "given_name": "Yixue"
      },
      {
        "surname": "Zhu",
        "given_name": "Yinong"
      }
    ]
  },
  {
    "title": "Automated work efficiency analysis for smart manufacturing using human pose tracking and temporal action localization",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102948",
    "abstract": "In this paper, we aim to develop an automatic system to monitor and evaluate worker’s efficiency for smart manufacturing based on human pose tracking and temporal action localization. First, we explore the generative adversarial networks (GANs) to achieve significantly improved estimation of human body joints. Second, we formulate the automated worker efficiency analysis into a temporal action localization problem in which the action video performed by the worker is matched against a reference video performed by a teacher. We extract invariant spatio-temporal features from the human body pose sequences and perform cross-video matching using dynamic time warping. Our proposed human pose estimation method achieves state-of-the-art performance on the benchmark dataset. Our automated work efficiency analysis is able to achieve action localization with an average IoU (intersection over union) score large than 0.9. This represents one of the first systems to provide automated worker efficiency evaluation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301759",
    "keywords": [
      "Action (physics)",
      "Action recognition",
      "Aerospace engineering",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Class (philosophy)",
      "Computer science",
      "Computer vision",
      "Dynamic time warping",
      "Engineering",
      "Geodesy",
      "Geography",
      "Intersection (aeronautics)",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Pedagogy",
      "Physics",
      "Pose",
      "Psychology",
      "Quantum mechanics",
      "Tracking (education)"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Hao"
      },
      {
        "surname": "Ning",
        "given_name": "Guanghan"
      },
      {
        "surname": "Zhao",
        "given_name": "Zhiqun"
      },
      {
        "surname": "Huang",
        "given_name": "Zhongchao"
      },
      {
        "surname": "He",
        "given_name": "Zhihai"
      }
    ]
  },
  {
    "title": "SAR-NAS: Skeleton-based action recognition via neural architecture searching",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102942",
    "abstract": "This paper presents a study of automatic design of neural network architectures for skeleton-based action recognition. Specifically, we encode a skeleton-based action instance into a tensor and carefully define a set of operations to build two types of network cells: normal cells and reduction cells. The recently developed DARTS (Differentiable Architecture Search) is adopted to search for an effective network architecture that is built upon the two types of cells. All operations are 2D based in order to reduce the overall computation and search space. Experiments on the challenging NTU RGB+D and Kinectics datasets have verified that most of the networks developed to date for skeleton-based action recognition are likely not compact and efficient. The proposed method provides an approach to search for such a compact network that is able to achieve comparative or even better performance than the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301711",
    "keywords": [
      "Action (physics)",
      "Algorithm",
      "Architecture",
      "Art",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Computation",
      "Computer science",
      "ENCODE",
      "Gene",
      "Geometry",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Reduction (mathematics)",
      "Set (abstract data type)",
      "Skeleton (computer programming)",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Haoyuan"
      },
      {
        "surname": "Hou",
        "given_name": "Yonghong"
      },
      {
        "surname": "Wang",
        "given_name": "Pichao"
      },
      {
        "surname": "Guo",
        "given_name": "Zihui"
      },
      {
        "surname": "Li",
        "given_name": "Wanqing"
      }
    ]
  },
  {
    "title": "Depth segmentation in real-world scenes based on U–V disparity analysis",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102920",
    "abstract": "Depth segmentation has the challenge of separating the objects from their supporting surfaces in a noisy environment. To address the issue, a novel segmentation scheme based on disparity analysis is proposed. First, we transform a depth scene into the corresponding U-V disparity map. Then, we conduct a region-based detection method to divide the object region into several targets in the processed U-disparity map. Thirdly, the horizontal plane regions may be mapped as slant lines in the V-disparity map, the Random Sample Consensus (RANSAC) algorithm is improved to fit such multiple lines. Moreover, noise regions are reduced by image processing strategies during the above processes. We respectively evaluate our approach on both real-world scenes and public data sets to verify the flexibility and generalization. Sufficient experimental results indicate that the algorithm can efficiently segment and label a full-view scene into a group of valid regions as well as removing surrounding noise regions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301541",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Depth map",
      "Generalization",
      "Image (mathematics)",
      "Image segmentation",
      "Mathematical analysis",
      "Mathematics",
      "Noise (video)",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "RANSAC",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Xiaohan"
      },
      {
        "surname": "Chen",
        "given_name": "Lu"
      },
      {
        "surname": "Li",
        "given_name": "Shuang"
      },
      {
        "surname": "Zhou",
        "given_name": "Xiang"
      }
    ]
  },
  {
    "title": "An adaptive spatio-temporal perception aware quantization algorithm for AVS2",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102917",
    "abstract": "Adaptive quantization proves to be an effective tool to improve coding performance. In this paper, we propose an adaptive spatiotemporal perception aware quantization algorithm to increase subjective coding performance. To measure the spatiotemporally perceptual redundancy, the perceptual complexity models are firstly established with spatial and temporal characteristics respectively. With the help of the models, the adaptive spatial and temporal quantization parameter (QP) offsets are then calculated for each coding tree unit (CTU), respectively. Finally, the perceptually optimal Lagrange multiplier of each CTU is determined with the spatial–temporal QP offset. Experimental results show that the proposed algorithm reduces 8.6% and 8.4% Bjontegaard-Delta Rate (BD-Rate) with Structural Similarity Index Metric (SSIM) in average over the second generation of Audio Video Coding Standard (AVS2) reference software RD17.0 in Low-Delay-P (LDP) and Random-Access (RA) configurations, respectively. The subjective assessment proves that the proposed algorithm can reduce the bitrates with the same subjective quality significantly.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301553",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Coding (social sciences)",
      "Computer science",
      "Computer vision",
      "Lagrange multiplier",
      "Mathematical optimization",
      "Mathematics",
      "Neuroscience",
      "Offset (computer science)",
      "Perception",
      "Programming language",
      "Psychology",
      "Quantization (signal processing)",
      "Reference software",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Yan",
        "given_name": "Yunyao"
      },
      {
        "surname": "Xiang",
        "given_name": "Guoqing"
      },
      {
        "surname": "Li",
        "given_name": "Yuan"
      },
      {
        "surname": "Xie",
        "given_name": "Xiaodong"
      },
      {
        "surname": "Jia",
        "given_name": "Huizhu"
      }
    ]
  },
  {
    "title": "Multi-stage all-zero block detection for HEVC coding using machine learning",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102945",
    "abstract": "Compared with deadzone hard-decision quantization (HDQ), rate-distortion optimized quantization (RDOQ) in HEVC brings non-negligible coding gain, however consumes considerable computations caused by exhaustive search over multiple candidates to determine optimal output level. Benefiting from efficient prediction in HEVC, transform blocks are frequently quantized to all zero, especially in small-size blocks. It is worthwhile to detect all zero block (AZB) for transform blocks to bypass subsequent computation-intensive RDOQ. Traditional thresholding based AZB detection algorithms are well-suited for deadzone quantized blocks, however miss partial optimal results in RDOQ and suffer from more or less accuracy degradation in RDOQ. This paper proposes a novel multi-stage AZB detection algorithm for RDOQ blocks with good tradeoff between complexity and accuracy. At the first stage, genuine all zero blocks (G_AZB) which are quantized to all zero both in HDQ and RDOQ are prejudged by comparison with conservative threshold determined by mathematical derivation for deadzone HDQ. At the second stage, an adaptive threshold model is built using adaptive deadzone offset by simulating the behavior patterns existing in RDOQ, aiming to further detect the pseudo AZB (P_AZB) which are quantized to all zero in RDOQ however not all zero in HDQ. At the final stage, machine learning based detection is proposed to classify the remaining “cunning” all zero blocks using eight distinguished RDO-related features, by which subtle working mechanism in RDOQ is leveraged. The experimental results demonstrate that the proposed algorithm achieves up to 7.471% total coding computation saving with 0.064% BD-RATE increment compared with RDOQ on average. Moreover, the average FNR and FPR detection accuracies are 6.3% and 6.5% respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301747",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Coding (social sciences)",
      "Computation",
      "Computer science",
      "Dead zone",
      "Geology",
      "Image (mathematics)",
      "Mathematics",
      "Oceanography",
      "Offset (computer science)",
      "Programming language",
      "Quantization (signal processing)",
      "Statistics",
      "Thresholding"
    ],
    "authors": [
      {
        "surname": "Yin",
        "given_name": "Haibing"
      },
      {
        "surname": "Yang",
        "given_name": "Haoyun"
      },
      {
        "surname": "Huang",
        "given_name": "Xiaofeng"
      },
      {
        "surname": "Wang",
        "given_name": "Hongkui"
      },
      {
        "surname": "Yan",
        "given_name": "Chenggang"
      }
    ]
  },
  {
    "title": "Improved-StoryGAN for sequential images visualization",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102956",
    "abstract": "Story visualization is a novel and challenging topic that intersects computer vision and natural language processing, which needs to generate sequential images based on a story. It is related to text-to-image generation and video generation. Apart from ensuring the quality of the results, the synthesized images of story visualization are supposed to be consistent with each other and reflect the input story. In order to improve the performance of generated sequential images, we have developed the baseline model StoryGAN. Firstly, we use Dilated Convolution in the discriminators to expand the receptive field of the convolution kernel in the feature maps, thus enhancing the quality of the generated sequential images. In addition, Weighted Activation Degree (WAD) is introduced in the discriminators to provide a robust evaluation in view of similarity between the generated images and the target story, which results in enhancement on the consistency between the generated images and the target story. Last but not least, Bi-GRU stores the historical and future information of each sentence to effectively extract the textual features. What’s more, in order to make full use of the features of the long story features, Gated Convolution is used to replace the original MLP in the Initial State Encoder to improve the consistence between the generated sequential images. Experimental results and visual sequential images demonstrate the outperformance of the model we develop, compared with the other models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301826",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Convolution (computer science)",
      "Encoder",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Kernel (algebra)",
      "Linguistics",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Sentence",
      "Similarity (geometry)",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Chunye"
      },
      {
        "surname": "Kong",
        "given_name": "Liya"
      },
      {
        "surname": "Zhou",
        "given_name": "Zhiping"
      }
    ]
  },
  {
    "title": "Recognition of occupational therapy exercises and detection of compensation mistakes for Cerebral Palsy",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102970",
    "abstract": "Depth camera-based virtual rehabilitation systems are gaining attention in occupational therapy for cerebral palsy patients. When developing such a system, domain-specific exercise recognition is vital. To design such a gesture recognition method, some obstacles need to be overcome: detection of gestures not related to the defined exercise set and recognition of incorrect exercises performed by the patients to compensate for their lack of ability. We propose a framework based on hidden Markov models for the recognition of upper extremity functional exercises. We determine critical compensation mistakes together with restrictions for classifying these mistakes with the help of occupational therapists. We first eliminate undefined gestures by evaluating two models that produce adaptive threshold values. Then we utilize specific negative models based on feature thresholding and train them for each exercise to detect compensation mistakes. We perform various tests using our method in a laboratory environment under the supervision of occupational therapists.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301905",
    "keywords": [
      "Artificial intelligence",
      "Cerebral palsy",
      "Compensation (psychology)",
      "Computer science",
      "Domain (mathematical analysis)",
      "Feature (linguistics)",
      "Gesture",
      "Gesture recognition",
      "Hidden Markov model",
      "Image (mathematics)",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Medicine",
      "Philosophy",
      "Physical medicine and rehabilitation",
      "Physical therapy",
      "Programming language",
      "Psychoanalysis",
      "Psychology",
      "Rehabilitation",
      "Set (abstract data type)",
      "Thresholding"
    ],
    "authors": [
      {
        "surname": "Ongun",
        "given_name": "Mehmet Faruk"
      },
      {
        "surname": "Güdükbay",
        "given_name": "Uğur"
      },
      {
        "surname": "Aksoy",
        "given_name": "Selim"
      }
    ]
  },
  {
    "title": "Semi-supervised cross-modal representation learning with GAN-based Asymmetric Transfer Network",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102899",
    "abstract": "In this paper, we proposed a semi-supervised common representation learning method with GAN-based Asymmetric Transfer Network (GATN) for cross modality retrieval. GATN utilizes the asymmetric pipeline to guarantee the semantic consistency and adopt (Generative Adversarial Network) GAN to fit the distributions of different modalities. Specifically, the common representation learning across modalities includes two stages: (1) the first stage, GATN trains source mapping network to learn the semantic representation of text modality by supervised method; and (2) the second stage, GAN-based unsupervised modality transfer method is proposed to guide the training of target mapping network, which includes generative network (target mapping network) and discriminative network. Experimental results on three widely-used benchmarks show that GATN have achieved better performance comparing with several existing state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301413",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Law",
      "Materials science",
      "Modal",
      "Parallel computing",
      "Political science",
      "Politics",
      "Polymer chemistry",
      "Representation (politics)",
      "Transfer (computing)",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Lei"
      },
      {
        "surname": "Chen",
        "given_name": "Leiting"
      },
      {
        "surname": "Ou",
        "given_name": "Weihua"
      },
      {
        "surname": "Zhou",
        "given_name": "Chuan"
      }
    ]
  },
  {
    "title": "Resolving overlapping convex objects in silhouette images by concavity analysis and Gaussian process",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102962",
    "abstract": "This paper introduces a novel method for segmentation of clustered partially overlapping convex objects in silhouette images. The proposed method involves three main steps: pre-processing, contour evidence extraction, and contour estimation. Contour evidence extraction starts by recovering contour segments from a binarized image by detecting concave points. After this the contour segments which belong to the same objects are grouped. The grouping is formulated as a combinatorial optimization problem and solved using the branch and bound algorithm. Finally, the full contours of the objects are estimated by a Gaussian process regression method. The experiments on a challenging dataset consisting of nanoparticles demonstrate that the proposed method outperforms three current state-of-art approaches in overlapping convex objects segmentation. The method relies only on edge information and can be applied to any segmentation problems where the objects are partially overlapping and have a convex shape.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301863",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Enhanced Data Rates for GSM Evolution",
      "Gaussian",
      "Geometry",
      "Image segmentation",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics",
      "Process (computing)",
      "Quantum mechanics",
      "Regular polygon",
      "Scale-space segmentation",
      "Segmentation",
      "Segmentation-based object categorization",
      "Silhouette"
    ],
    "authors": [
      {
        "surname": "Zafari",
        "given_name": "Sahar"
      },
      {
        "surname": "Murashkina",
        "given_name": "Mariia"
      },
      {
        "surname": "Eerola",
        "given_name": "Tuomas"
      },
      {
        "surname": "Sampo",
        "given_name": "Jouni"
      },
      {
        "surname": "Kälviäinen",
        "given_name": "Heikki"
      },
      {
        "surname": "Haario",
        "given_name": "Heikki"
      }
    ]
  },
  {
    "title": "Convolutional neural network with adaptive inferential framework for skeleton-based action recognition",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102925",
    "abstract": "In the task of skeleton-based action recognition, CNN-based methods represent the skeleton data as a pseudo image for processing. However, it still remains as a critical issue of how to construct the pseudo image to model the spatial dependencies of the skeletal data. To address this issue, we propose a novel convolutional neural network with adaptive inferential framework (AIF-CNN) to exploit the dependencies among the skeleton joints. We particularly investigate several initialization strategies to make the AIF effective with each strategy introducing the different prior knowledge. Extensive experiments on the dataset of NTU RGB+D and Kinetics-Skeleton demonstrate that the performance is improved significantly by integrating the different prior information. The source code is available at: https://github.com/hhe-distance/AIF-CNN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301589",
    "keywords": [
      "Action recognition",
      "Artificial intelligence",
      "Class (philosophy)",
      "Code (set theory)",
      "Computer science",
      "Computer security",
      "Construct (python library)",
      "Convolutional neural network",
      "Exploit",
      "Human skeleton",
      "Image (mathematics)",
      "Initialization",
      "Machine learning",
      "Operating system",
      "Pattern recognition (psychology)",
      "Programming language",
      "Set (abstract data type)",
      "Skeleton (computer programming)",
      "Source code"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Hong’en"
      },
      {
        "surname": "Su",
        "given_name": "Hang"
      },
      {
        "surname": "Chang",
        "given_name": "Zhigang"
      },
      {
        "surname": "Yu",
        "given_name": "Mingyang"
      },
      {
        "surname": "Gao",
        "given_name": "Jialin"
      },
      {
        "surname": "Li",
        "given_name": "Xinzhe"
      },
      {
        "surname": "Zheng",
        "given_name": "Shibao"
      }
    ]
  },
  {
    "title": "Improving the representation of image descriptions for semantic image retrieval with RDF",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102934",
    "abstract": "The past few years have witnessed a surge of interest in many topics at the intersection of natural language processing and computer vision. In particular, using objects together with their attributes and relations to represent images or interpret languages has been proved useful across a wide variety of applications. The goal of this work is to provide an improved RDF-based model to represent images for enhancing textual based image retrieval. We use natural language processing tools to obtain a set of objects, attributes and relations; and then model them into graphical structures with RDF-based model. We also conduct some preliminary experiments to show how to handle textual based image retrieval for complex queries or multilingual queries. The experimental results show that our approach improves the representation of image descriptions, which is suitable for enhancing image retrieval with high-level semantics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301644",
    "keywords": [
      "Aerospace engineering",
      "Artificial intelligence",
      "Automatic image annotation",
      "Computer science",
      "Engineering",
      "Image (mathematics)",
      "Image retrieval",
      "Information retrieval",
      "Intersection (aeronautics)",
      "Law",
      "Natural language processing",
      "Political science",
      "Politics",
      "Programming language",
      "RDF",
      "Representation (politics)",
      "Semantic Web",
      "Semantics (computer science)",
      "Set (abstract data type)",
      "Variety (cybernetics)"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Hua"
      },
      {
        "surname": "Guo",
        "given_name": "AiBin"
      },
      {
        "surname": "Ni",
        "given_name": "Wenlong"
      },
      {
        "surname": "Cheng",
        "given_name": "Yan"
      }
    ]
  },
  {
    "title": "AP-GAN: Predicting skeletal activity to improve early activity recognition",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102923",
    "abstract": "Early activity recognition is a classification task before the completion of activity. The study of early activity recognition is beneficial to avoid serious result. Previous studies have focused on extracting effective activity features and modeling for quick and accurate classification. It is challenging because of lack of available information. In order to get a firm basis for judgment, this paper adds an activity prediction module prior to recognition module. The main task of the module is to predict subsequent motions according to observed motions. To avoid motion blur, the structure of GAN (Generative Adversarial Networks) is used to generate the predicted motions. Compared with the traditional deep learning model, dilated neural network has advantages in large-span spatiotemporal feature modeling. The dilated RNN (Recurrent Neural Networks) and CNN (Convolutional Neural Networks) are introduced to the recognition module. In order to make the activity prediction and recognition modules work together, this paper designs and introduces a hard class mining mechanism to improve the learning ability of hard class samples. The proposed method is validated on four skeletal activity datasets and achieves state-of-the-art accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301577",
    "keywords": [
      "Activity recognition",
      "Artificial intelligence",
      "Artificial neural network",
      "Class (philosophy)",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Engineering",
      "Feature (linguistics)",
      "Generative adversarial network",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Recurrent neural network",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Cui",
        "given_name": "Ran"
      },
      {
        "surname": "Hua",
        "given_name": "Gang"
      },
      {
        "surname": "Wu",
        "given_name": "Jingran"
      }
    ]
  },
  {
    "title": "Local relation network with multilevel attention for visual question answering",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102762",
    "abstract": "With the tremendous success of the visual question answering (VQA) tasks, visual attention mechanisms have become an indispensable part of VQA models. However, these attention-based methods do not consider any relationship among regions, which is crucial for the thorough understanding of the image by the model. We propose local relation networks for generating context-aware image features for each image region, which contain information on the relationship among the other image regions. Furthermore, we propose a multilevel attention mechanism to combine semantic information from the LRNs and the original image regions, rendering the decision of the model more reasonable. With these two measures, we improve the region representation and achieve better attentive effect and VQA performance. We conduct numerous experiments on the COCO-QA dataset and the largest VQA v2.0 benchmark dataset. Our model achieves competitive results, proving the effectiveness of our proposed LRNs and multilevel attention mechanism through visual demonstrations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300122",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Data mining",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Information retrieval",
      "Machine learning",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Question answering",
      "Relation (database)",
      "Rendering (computer graphics)"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Bo"
      },
      {
        "surname": "Yao",
        "given_name": "Zeng"
      },
      {
        "surname": "Zhang",
        "given_name": "Yinghui"
      },
      {
        "surname": "Yu",
        "given_name": "Lejun"
      }
    ]
  },
  {
    "title": "Contour and region harmonic features for sub-local facial expression recognition",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102949",
    "abstract": "Expression recognition relies on intensity, edges, and geometry that overlooks the actual shape curvatures of facial regions. This paper presents a novel two-stage approach to distinguish seven expressions on the basis of eleven different facial areas. The combination of contour and region harmonics is used to develop the interrelationship of sub-local areas in the human face for expression recognition. We applied a multi-class support vector machine (SVM) with subject dependent k-fold cross-validation to classify the human emotions into expressions. We tested our proposed method on three public facial expression datasets for sub-local regions in human face and achieved 94.90%, 93.43%, and 92.57% recognition rate for the CK+, CFEE, and MUG datasets respectively. Experiments show that the contour and region harmonics have high classification power and can be computed efficiently. Our method provides higher accuracy, less computing time, and less memory space than existing techniques, including deep learning.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301772",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Expression (computer science)",
      "Face (sociological concept)",
      "Facial expression",
      "Facial recognition system",
      "Harmonic",
      "Harmonics",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Social science",
      "Sociology",
      "Spherical harmonics",
      "Support vector machine",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Raza Shahid",
        "given_name": "Ali"
      },
      {
        "surname": "Khan",
        "given_name": "Sheheryar"
      },
      {
        "surname": "Yan",
        "given_name": "Hong"
      }
    ]
  },
  {
    "title": "Learning discriminative update adaptive spatial-temporal regularized correlation filter for RGB-T tracking",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102881",
    "abstract": "The RGB-T trackers based on correlation filter framework have been extensively investigated for that they can track targets more accurately in most complex scenes. However, the performance of these trackers is limited when facing some specific challenging scenarios, such as occlusion and background clutter. For different tracking targets, most of these trackers utilize fixed regularization constraint to build the filter model, which is obviously unreasonable to effectively present the appearance changes and characteristics of a specific target. In addition, they adopt a simple model update mechanism based on linear interpolation, which can easily lead to model degradation in challenging scenarios, resulting in tracker drift. To solve the above problems, we propose a novel adaptive spatial-temporal regularized correlation filter model to learn an appropriate regularization for achieving robust tracking and a relative peak discriminative method for model updating to avoid the model degradation. Besides, to make better integrate the unique advantages of the two modes and adapt the changing appearance of the target, an adaptive weighting ensemble scheme and a multi-scale search mechanism are adopted, respectively. To optimize the proposed model, we designed an efficient ADMM algorithm, which greatly improved the efficiency. Extensive experiments have been carried out on two available datasets, RGBT234 and RGBT210, and the experimental results indicate that the tracker proposed by us performs favorably in both accuracy and robustness against the state-of-the-art RGB-T trackers.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301279",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "BitTorrent tracker",
      "Chemistry",
      "Clutter",
      "Computer science",
      "Computer vision",
      "Discriminative model",
      "Eye tracking",
      "Gene",
      "Medicine",
      "Pattern recognition (psychology)",
      "RGB color model",
      "Radar",
      "Radiology",
      "Regularization (linguistics)",
      "Robustness (evolution)",
      "Telecommunications",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Feng",
        "given_name": "Mingzheng"
      },
      {
        "surname": "Song",
        "given_name": "Kechen"
      },
      {
        "surname": "Wang",
        "given_name": "Yanyan"
      },
      {
        "surname": "Liu",
        "given_name": "Jie"
      },
      {
        "surname": "Yan",
        "given_name": "Yunhui"
      }
    ]
  },
  {
    "title": "Steganography using a 3-player game",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102910",
    "abstract": "Image steganography aims to securely embed secret information into cover images. Until now, adaptive embedding algorithms such as S-UNIWARD or Mi-POD, were among the most secure and most often used methods for image steganography. With the arrival of deep learning and more specifically, Generative Adversarial Networks (GAN), new steganography techniques have appeared. Among them is the 3-player game approach, where three networks compete against each other. In this paper, we propose three different architectures based on the 3-player game. The first architecture is proposed as a rigorous alternative to two recent publications. The second takes into account stego noise power. Finally, our third architecture enriches the second one with a better interaction between embedding and extracting networks. Our method achieves better results compared to existing works Hayes and Danezis (2017), Zhu et al. (2018), and paves the way for future research on this topic.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301486",
    "keywords": [],
    "authors": [
      {
        "surname": "Yedroudj",
        "given_name": "Mehdi"
      },
      {
        "surname": "Comby",
        "given_name": "Frédéric"
      },
      {
        "surname": "Chaumont",
        "given_name": "Marc"
      }
    ]
  },
  {
    "title": "Light field all-in-focus image fusion based on spatially-guided angular information",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102878",
    "abstract": "Compared to traditional 2D images, light field images record both spatial and angular information of the scene, which can provide more data for image fusion. In this paper, a light field all-in-focus image fusion algorithm based on spatially-guided angular information is proposed. In the proposed method, the initial weight maps carrying the angular information are calculated by comparing the block variance of the 4D light field data. The initial weight maps are then guided by digital refocused images carrying the spatial information to obtain the refined weight maps. In the refocused image multi-scale decomposition, the micro-lens calibration error is considered and the additional edge layers are extracted to suppress the edge artifacts. Experiments demonstrate the effectiveness of the proposed algorithm. Quantitative evaluation results show that the proposed algorithm performs the best in the feature-based index and structural similarity-based index without sacrificing the information and perceptual sharpness of the fused image.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301243",
    "keywords": [
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Computer science",
      "Computer vision",
      "Enhanced Data Rates for GSM Evolution",
      "Field (mathematics)",
      "Focus (optics)",
      "Geometry",
      "Image (mathematics)",
      "Image fusion",
      "Image resolution",
      "Light field",
      "Mathematics",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Pure mathematics",
      "Similarity (geometry)",
      "Spatial analysis",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Yingchun"
      },
      {
        "surname": "Wang",
        "given_name": "Yumei"
      },
      {
        "surname": "Liang",
        "given_name": "Jie"
      },
      {
        "surname": "Bajić",
        "given_name": "Ivan V."
      },
      {
        "surname": "Wang",
        "given_name": "Anhong"
      }
    ]
  },
  {
    "title": "Automatic foreground extraction from imperfect backgrounds using multi-agent consensus equilibrium",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102907",
    "abstract": "Extracting accurate foreground objects from a scene is an essential step for many video applications. Traditional background subtraction algorithms can generate coarse estimates, but generating high quality masks requires professional softwares with significant human interventions, e.g., providing trimaps or labeling key frames. We propose an automatic foreground extraction method in applications where a static but imperfect background is available. Examples include filming and surveillance where the background can be captured before the objects enter the scene or after they leave the scene. Our proposed method is very robust and produces significantly better estimates than state-of-the-art background subtraction, video segmentation and alpha matting methods. The key innovation of our method is a novel information fusion technique. The fusion framework allows us to integrate the individual strengths of alpha matting, background subtraction and image denoising to produce an overall better estimate. Such integration is particularly important when handling complex scenes with imperfect background. We show how the framework is developed, and how the individual components are built. Extensive experiments and ablation studies are conducted to evaluate the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301474",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Background subtraction",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Foreground detection",
      "Imperfect",
      "Key (lock)",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Segmentation",
      "Subtraction"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Xiran"
      },
      {
        "surname": "Juang",
        "given_name": "Jason"
      },
      {
        "surname": "Chan",
        "given_name": "Stanley H."
      }
    ]
  },
  {
    "title": "Adult-child 3D backward face aging model (3D B-FAM)",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102803",
    "abstract": "Face aging has been widely considered in many studies regarding all the potential applications. However, the de-aging known as the rejuvenation or backward modeling has recently received more attention. Previous studies mainly focused on rejuvenating faces from aged adults into young adults using two-dimensional (2D) models. In this work, we propose an extension of a previous 2D adult-child B-FAM into 3D model. This model allows a digital face appearance rejuvenation within a range of [75–3] years old. To evaluate the performances of the proposed approach, first, we proposed two performance evaluation modes, namely: Generic Perception Based and Biometric Verification Mode. Then, the performances have been evaluated over our own 3D database, called Face Time-Machine database constructed using 75 females and 70 males, leading to 500 textured surface meshes. Finally, results show that they are perceptually satisfying and system performance increases by using the faces obtained from our model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300535",
    "keywords": [
      "Artificial intelligence",
      "Biometrics",
      "Composite material",
      "Computer graphics (images)",
      "Computer science",
      "Face (sociological concept)",
      "Gerontology",
      "Human–computer interaction",
      "Materials science",
      "Medicine",
      "Mode (computer interface)",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Perception",
      "Polygon mesh",
      "Psychology",
      "Range (aeronautics)",
      "Rejuvenation",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Heravi",
        "given_name": "Farnaz Majid Zadeh"
      },
      {
        "surname": "Nait-Ali",
        "given_name": "Amine"
      }
    ]
  },
  {
    "title": "Adaptive deep feature aggregation using Fourier transform and low-pass filtering for robust object retrieval",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102860",
    "abstract": "With the rapid development of deep learning techniques, convolutional neural networks (CNN) have been widely investigated for the feature representations in the image retrieval task. However, the key step in CNN-based retrieval, i.e., feature aggregation has not been solved in a robust and general manner when tackling different kinds of images. In this paper, we present a deep feature aggregation method for image retrieval using the Fourier transform and low-pass filtering, which can adaptively compute the weights for each feature map with discrimination. Specifically, the low-pass filtering can preserve the semantic information in each feature map by transforming images to the frequency domain. In addition, we develop three adaptive methods to further improve the robustness of feature aggregation, i.e., Region of Interests (ROI) selection, spatial weighting and channel weighting. Experimental results demonstrate the superiority of the proposed method in comparison with other state-of-the-art, in achieving robust and accurate object retrieval under five benchmark datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301115",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Fourier transform",
      "Gene",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Image retrieval",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Medicine",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Radiology",
      "Robustness (evolution)",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Ziyao"
      },
      {
        "surname": "Wang",
        "given_name": "Xinsheng"
      },
      {
        "surname": "Li",
        "given_name": "Chen"
      },
      {
        "surname": "Zeng",
        "given_name": "Ming"
      },
      {
        "surname": "Li",
        "given_name": "Zhongyu"
      }
    ]
  },
  {
    "title": "Real-time long-term tracker with tracking–verification–detection–refinement",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102896",
    "abstract": "Long-term tracking is one of the most challenging problems in computer vision. In this paper, we make full use of the Discriminative Correlation Filter (DCF), and propose a real-time long-term tracker by exploiting a joint tracking–verification–detection–refinement framework. We utilize a DCF which is updated aggressively to estimate translation and scale variation of the target. Subsequently, a passively updated DCF checks the reliability of the tracking result. Once the result is not reliable, we evoke the proposed optimized candidate detector to generate a small number of relatively high quality candidates. Finally, one DCF with an adaptive online learning rate is adopted to refine the predictions that the sparse candidates inferred. In addition, we employ a selection mechanism for the correlation responses to maintain reliable samples effectively. Extensive experiments show that the proposed method performs favorably against lots of state-of-the-art methods while running more than 30 frames per second on single CPU.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301383",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Detector",
      "Discriminative model",
      "Pattern recognition (psychology)",
      "Pedagogy",
      "Physics",
      "Power (physics)",
      "Psychology",
      "Quantum mechanics",
      "Reliability (semiconductor)",
      "Telecommunications",
      "Term (time)",
      "Tracking (education)"
    ],
    "authors": [
      {
        "surname": "Liao",
        "given_name": "Jiawen"
      },
      {
        "surname": "Qi",
        "given_name": "Chun"
      },
      {
        "surname": "Cao",
        "given_name": "Jianzhong"
      },
      {
        "surname": "Ren",
        "given_name": "Long"
      },
      {
        "surname": "Zhang",
        "given_name": "Gaopeng"
      }
    ]
  },
  {
    "title": "Deep gradual flash fusion for low-light enhancement",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102903",
    "abstract": "In this paper, we propose gradual flash fusion, a new imaging concept that enables acquisition of pseudo multi-exposure images in a passive manner. This means that our gradual flash capture does not require any user-side manipulation (taking multiple shots or varying camera settings). Continuous high-speed capture naturally contains different intensities of flash in a single shooting. The captured gradual flash images, containing different information of the same scene, are fused to generate higher-quality images, especially in a low light scenario. For gradual flash fusion, we use a Generative Adversarial Network (GAN) based approach, where the generator is a tailored convolutional Auto-Encoder for image fusion. For the training, we build a custom dataset comprising gradual flash images and corresponding ground truths. This enables supervised learning, unlike most conventional image fusion studies. Experimental results demonstrate that gradual flash fusion achieves artifact-free and noise-free results resembling ground truth, owing to supervised adversarial fusion.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301449",
    "keywords": [
      "Artifact (error)",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Encoder",
      "Flash (photography)",
      "Fusion",
      "Generative adversarial network",
      "Generator (circuit theory)",
      "Ground truth",
      "Image (mathematics)",
      "Image fusion",
      "Linguistics",
      "Noise (video)",
      "Operating system",
      "Optics",
      "Philosophy",
      "Physics",
      "Power (physics)",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Jae-Woo"
      },
      {
        "surname": "Ryu",
        "given_name": "Je-Ho"
      },
      {
        "surname": "Kim",
        "given_name": "Jong-Ok"
      }
    ]
  },
  {
    "title": "Semantic granularity metric learning for visual search",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102871",
    "abstract": "Existing metric learning methods often do not consider different granularity in visual similarity. However, in many domains, images exhibit similarity at multiple granularities with visual semantic concepts, e . g . fashion demonstrates similarity ranging from clothing of the exact same instance to similar looks/design or common category. Therefore, training image triplets/pairs inherently possess different degree of information. Nevertheless, the existing methods often treat them with equal importance which hinder capturing underlying granularities in image similarity. In view of this, we propose a new semantic granularity metric learning (SGML) that develops a novel idea of detecting and leveraging attribute semantic space and integrating it into deep metric learning to capture multiple granularities of similarity. The proposed framework simultaneously learns image attributes and embeddings with multitask-CNN where the tasks are linked by semantic granularity similarity mapping to leverage correlations between the tasks. To this end, we propose a new soft-binomial deviance loss that effectively integrates informativeness of training samples into metric-learning on-the-fly during training. Compared to recent ensemble-based methods, SGML is conceptually elegant, computationally simple yet effective. Extensive experiments on benchmark datasets demonstrate its superiority e . g . , 1–4.5%-Recall@1 improvement over the state-of-the-arts (Kim et al., 2018; Cakir et al., 2019) on DeepFashion-Inshop dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032030119X",
    "keywords": [],
    "authors": [
      {
        "surname": "Manandhar",
        "given_name": "Dipu"
      },
      {
        "surname": "Bastan",
        "given_name": "Muhammet"
      },
      {
        "surname": "Yap",
        "given_name": "Kim-Hui"
      }
    ]
  },
  {
    "title": "Transformed denoising autoencoder prior for image restoration",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102927",
    "abstract": "Image restoration problem is generally ill-posed, which can be alleviated by learning image prior. Inspired by the considerable performance of utilizing priors in pixel domain and wavelet domain jointly, we propose a novel transformed denoising autoencoder as prior (TDAEP). The core idea behind TDAEP is to enhance the classical denoising autoencoder (DAE) via transform domain, which captures complementary information from multiple views. Specifically, 1-level nonorthogonal wavelet coefficients are used to form 4-channel feature images. Moreover, a 5-channel tensor is obtained by stacking the original image under the pixel domain and 4-channel feature images under the wavelet domain. Then we train the transformed DAE (TDAE) with the 5-channel tensor as the network input. The optimized image prior is obtained based on the trained autoencoder, and it is incorporated into an iterative restoration procedure with the aid of the auxiliary variable technique. The resulting model is affiliationed by proximal gradient descent technique. Numerous experiments demonstrated that the TDAEP outperforms a set of image restoration benchmark algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301590",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Autoencoder",
      "Bayesian probability",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Domain (mathematical analysis)",
      "Feature (linguistics)",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Image processing",
      "Image restoration",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Noise reduction",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Prior probability",
      "Wavelet"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Jinjie"
      },
      {
        "surname": "He",
        "given_name": "Zhuonan"
      },
      {
        "surname": "Liu",
        "given_name": "Xiaodong"
      },
      {
        "surname": "Wang",
        "given_name": "Yuhao"
      },
      {
        "surname": "Wang",
        "given_name": "Shanshan"
      },
      {
        "surname": "Liu",
        "given_name": "Qiegen"
      }
    ]
  },
  {
    "title": "An improved noise loss correction algorithm for learning from noisy labels",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102930",
    "abstract": "Despite excellent performance in image classification researches, the training of the deep neural networks (DNN) needs a large set of clean data with accurate annotations. The collection of a dataset is easy, but annotating the collected data is difficult on the contrary. There are many image data on the websites, which contain inaccurate annotations, but trainings on these datasets may make networks easier to over-fit noisy data and cause performance degradation. In this work, we propose an improved joint optimization framework for noise correction, which uses the Combination of Mix-up entropy and Kullback-Leibler entropy (CMKL) as the loss function. The new loss function can achieve better fine-tuning results after updating all label annotations. The experimental results on publicly available CIFAR-10 dataset and Clothing1M dataset show superior performance of our approach compared with other state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301619",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Data mining",
      "Deep neural networks",
      "Entropy (arrow of time)",
      "Evolutionary biology",
      "Function (biology)",
      "Image (mathematics)",
      "Information loss",
      "Machine learning",
      "Noise (video)",
      "Pattern recognition (psychology)",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Set (abstract data type)",
      "Training set"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Qian"
      },
      {
        "surname": "Lee",
        "given_name": "Feifei"
      },
      {
        "surname": "Wang",
        "given_name": "Ya-gang"
      },
      {
        "surname": "Miao",
        "given_name": "Ran"
      },
      {
        "surname": "Chen",
        "given_name": "Lei"
      },
      {
        "surname": "Chen",
        "given_name": "Qiu"
      }
    ]
  },
  {
    "title": "Gray-level image denoising with an improved weighted sparse coding",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102895",
    "abstract": "The nonlocal self-similarity of images means that groups of similar patches have low-dimensional property. The property has been previously used for image denoising, with particularly notable success via sparse coding. However, only a few studies have focused on the varying statistics of noise in different similar patches during the iterative denoising process. This has motivated us to introduce an improved weighted sparse coding for gray-level image denoising in this paper. On the basis of traditional sparse coding, we introduce a weight matrix to account for the noise variation characteristics of different similar patches, while introduce another weight matrix to make full use of the sparsity priors of natural images. The Maximum A-Posterior estimation (MAP) is used to obtain the closed-form solution of the proposed method. Experimental results demonstrate the competitiveness of the proposed method compared with that of state-of-the-art methods in both the objective and perceptual quality.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301401",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Bayesian probability",
      "Coding (social sciences)",
      "Computer science",
      "Image (mathematics)",
      "Image denoising",
      "Mathematics",
      "Multiview Video Coding",
      "Neural coding",
      "Noise reduction",
      "Non-local means",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Prior probability",
      "Sparse approximation",
      "Statistics",
      "Video denoising",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Ou",
        "given_name": "Yang"
      },
      {
        "surname": "Luo",
        "given_name": "Jianqiao"
      },
      {
        "surname": "Li",
        "given_name": "Bailin"
      },
      {
        "surname": "Swamy",
        "given_name": "M.N.S."
      }
    ]
  },
  {
    "title": "Real-time sepsis severity prediction on knowledge graph deep learning networks for the intensive care unit",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102901",
    "abstract": "Sepsis is the third-highest mortality disease in intensive care units (ICUs). In this paper, we proposed a deep learning model for predicting the severity of sepsis patients. Most existing models based on attention mechanisms do not fully utilize knowledge graph based information for different organ systems, such that might constitute crucial features for predicting the severity of sepsis patients. Therefore, we have employed a medical knowledge graph as a reliable and robust source of side information. End-to-end neural networks that incorporate analyses of various organ systems simultaneously and intuitively were developed in the proposed model to reflect upon the condition of patients in a timely fashion. We have developed a pre-training technique in the proposed model to combine it with labeled data by multi-task learning. Experimental results on real-world clinical datasets, MIMIC-III and eIR, demonstrate that our model outperforms state-of-the-art models in predicting the severity of sepsis patients.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301425",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Data mining",
      "Deep learning",
      "Graph",
      "Immunology",
      "Intensive care medicine",
      "Intensive care unit",
      "Machine learning",
      "Medicine",
      "Sepsis",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Qing"
      },
      {
        "surname": "Li",
        "given_name": "Lili"
      },
      {
        "surname": "Zhong",
        "given_name": "Jiang"
      },
      {
        "surname": "Huang",
        "given_name": "L. Frank"
      }
    ]
  },
  {
    "title": "Novel shrinking residual convolutional neural network for efficient accurate stereo matching",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102872",
    "abstract": "For stereo matching based on patch comparing using convolutional neural networks (CNNs), the matching cost estimation is highly dependent on the network structure, and the patch comparing is time consuming for traditional CNNs. Accordingly, we propose a stereo matching method based on a novel shrinking residual CNN, which consists of convolutional layers and skip-connection layers, and the size of the fully connected layers decreases progressively. Firstly, a layer-by-layer shrinking size model is adopted for the full-connection layers to greatly increase the running speed. Secondly, the convolutional layer and the residual structure are fused to improve patch comparing. Finally, the Loss function is re-designed to give higher weights to hard-classified examples compared with the standard cross entropy loss. Experimental results on KITTI2012 and KITTI2015 demonstrate that the proposed method can improve the operation speed while maintaining high accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301218",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Matching (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Residual",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Lei",
        "given_name": "Junfeng"
      },
      {
        "surname": "Dong",
        "given_name": "Yuxuan"
      },
      {
        "surname": "Zhao",
        "given_name": "Tao"
      },
      {
        "surname": "xiao",
        "given_name": "Jinsheng"
      },
      {
        "surname": "Chen",
        "given_name": "Yunhua"
      },
      {
        "surname": "Li",
        "given_name": "Bijun"
      }
    ]
  },
  {
    "title": "A survey on analysis and implementation of state-of-the-art haze removal techniques",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102912",
    "abstract": "Haze is a poor-quality state described by the opalescent appearance of the atmosphere which reduces the visibility. It is caused by high concentrations of atmospheric air pollutants, such as dust, smoke and other particles that scatter and absorb sunlight. The poor visibility can result in the failure of multiple computer vision applications such as smart transport systems, image processing, object detection, surveillance etc. One of the major issues in the field of image processing is the restoration of images that are corrupted due to different degradations. Typically, the images or videos captured in the outside environment have low contrast, colour fade and restricted visibility due to suspended particles of the atmosphere that directly influence the image quality. This can cause difficulty in identifying the objects in the captured hazy images or frames. To address this problem, several image dehazing techniques have been developed in the literature, each of which has its own advantages and limitations, but effective image restoration remains a challenging task. In recent times, various learning (Machine learning & Deep learning) based methods greatly condensed the drawbacks of manual design of haze related features and reduces the difficulty in efficient restoration of images with less computational time and cost. The current state-of-the-art methods for haze free images, mainly from the last decade, are thoroughly examined in this survey. Moreover, this paper systematically summarizes the hardware implementations of various haze removal methods in real time. It is with the hope that this current survey acts as a reference for researchers in this scientific area and to provide a direction for future improvements based on current achievements.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301504",
    "keywords": [
      "Artificial intelligence",
      "Atmosphere (unit)",
      "Computer science",
      "Computer vision",
      "Geography",
      "Haze",
      "Image (mathematics)",
      "Image processing",
      "Image quality",
      "Image restoration",
      "Meteorology",
      "Visibility"
    ],
    "authors": [
      {
        "surname": "Harish Babu",
        "given_name": "G."
      },
      {
        "surname": "Venkatram",
        "given_name": "N."
      }
    ]
  },
  {
    "title": "Transparency-guided ensemble convolutional neural network for the stratification between pseudoprogression and true progression of glioblastoma multiform in MRI",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102880",
    "abstract": "For patients with glioblastoma multiform (GBM), differentiating pseudoprogression (PsP) from true tumor progression (TTP) is a challenging and time-consuming task for radiologists. Although deep neural networks can automatically diagnose PsP and TTP, lacking of interpretability has always been its major drawback. To overcome these shortcomings and produce more reliable outcomes, we propose a transparency-guided ensemble convolutional neural network (CNN) to automatically discriminate PsP and TTP in magnetic resonance imaging (MRI). A total of 84 patients with GBM were enrolled in the study. First, three typical convolutional neutral networks, namely VGG, ResNet and DenseNet, were trained to distinguish PsP and TTP. Subsequently, we used class-specific gradient information from convolutional layers to highlight the important regions in MRI scans. And radiologists selected the most lesion-relevant layer for each CNN. Finally, the selected layers are utilized to guide the construction of a multi-scale ensemble CNN whose classification accuracy reached 90.20%, and whose specificity is promoted 20% than that of a single CNN. The results demonstrate the presented network can enhance the reliability and accuracy of CNNs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301267",
    "keywords": [
      "Artificial intelligence",
      "Cancer research",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Glioblastoma",
      "Interpretability",
      "Magnetic resonance imaging",
      "Medicine",
      "Pattern recognition (psychology)",
      "Radiology"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Xiaoming"
      },
      {
        "surname": "Zhou",
        "given_name": "Xiaobo"
      },
      {
        "surname": "Qian",
        "given_name": "Xiaohua"
      }
    ]
  },
  {
    "title": "TRBACF: Learning temporal regularized correlation filters for high performance online visual object tracking",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102882",
    "abstract": "Correlation filter-based trackers (CFTs) have recently shown remarkable performance in the field of visual object tracking. The advantage of these trackers originates from their ability to convert time-domain calculations into frequency domain calculations. However, a significant problem of these CFTs is that the model is insufficiently robust when the tracking scenarios are too complicated, meaning that the ideal tracking performance cannot be acquired. Recent work has attempted to resolve this problem by reducing the boundary effects from modeling the foreground and background of the object target effectively (e.g., CFLB, BACF, and CACF). Although these methods have demonstrated reasonable performance, they are often affected by occlusion, deformation, scale variation, and other challenging scenes. In this study, considering the relationship between the current frame and the previous frame of a moving object target in a time series, we propose a temporal regularization strategy to improve the BACF tracker (denoted as TRBACF), a typical representative of the aforementioned trackers. The TRBACF tracker can efficiently adjust the model to adapt the change of the tracking scenes, thereby enhancing its robustness and accuracy. Moreover, the objective function of our TRBACF tracker can be solved by an improved alternating direction method of multipliers, which can speed up the calculation in the Fourier domain. Extensive experimental results demonstrate that the proposed TRBACF tracker achieves competitive tracking performance compared with state-of-the-art trackers.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301255",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "BitTorrent tracker",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Eye tracking",
      "Filter (signal processing)",
      "Fourier domain",
      "Fourier transform",
      "Gene",
      "Mathematical analysis",
      "Mathematics",
      "Object (grammar)",
      "Pedagogy",
      "Psychology",
      "Robustness (evolution)",
      "Tracking (education)",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Yuan",
        "given_name": "Di"
      },
      {
        "surname": "Shu",
        "given_name": "Xiu"
      },
      {
        "surname": "He",
        "given_name": "Zhenyu"
      }
    ]
  },
  {
    "title": "Looking ahead: Joint small group detection and tracking in crowd scenes",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102876",
    "abstract": "Small group detection and tracking in crowd scenes are basis for high level crowd analysis tasks. However, it suffers from the ambiguities in generating proper groups and in handling dynamic changes of group configurations. In this paper, we propose a novel delay decision-making based method for addressing the above problems, motivated by the idea that these ambiguities can be solved using rich temporal context. Specifically, given individual detections, small group hypotheses are generated. Then candidate group hypotheses across consecutive frames and their potential associations are built in a tree. By seeking for the best non-conflicting subset from the hypothesis tree, small groups are determined and simultaneously their trajectories are got. So this framework is called joint detection and tracking. This joint framework reduces the ambiguities in small group decision and tracking by looking ahead for several frames. However, it results in the unmanageable solution space because the number of track hypotheses grows exponentially over time. To solve this problem, effective pruning strategies are developed, which can keep the solution space manageable and also improve the credibility of small groups. Experiments on public datasets demonstrate the effectiveness of our method. The method achieves the state-of-the-art performance even in noisy crowd scenes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032030122X",
    "keywords": [
      "Agronomy",
      "Archaeology",
      "Architectural engineering",
      "Artificial intelligence",
      "Biology",
      "Chemistry",
      "Computer science",
      "Context (archaeology)",
      "Credibility",
      "Decision tree",
      "Engineering",
      "Geography",
      "Group (periodic table)",
      "Joint (building)",
      "Law",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Organic chemistry",
      "Pedagogy",
      "Political science",
      "Pruning",
      "Psychology",
      "Space (punctuation)",
      "Tracking (education)",
      "Tree (set theory)"
    ],
    "authors": [
      {
        "surname": "Ma",
        "given_name": "Qiulin"
      },
      {
        "surname": "Zou",
        "given_name": "Qi"
      },
      {
        "surname": "Wang",
        "given_name": "Nan"
      },
      {
        "surname": "Guan",
        "given_name": "Qingji"
      },
      {
        "surname": "Pei",
        "given_name": "Yanting"
      }
    ]
  },
  {
    "title": "OSED: Object-specific edge detection",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102918",
    "abstract": "Object-specific edge detection (OSED) aims to detect object edges in an image along with classify the edge into object or non-object. It prunes edges which are not belonging to the object class for following processing, such as, feature matching for object detection, localization and three-dimensional reconstruction. In this paper, an OSED method that combines region proposal detectors with deep supervision nets to identify object-specific edges is proposed. It minimizes errors of object proposal by learning from hidden layers. Additionally, it combines features from different scales to detect object edges. In order to evaluate the performance of the OSED, we present two datasets which are captured in real scenes. The OSED method demonstrates a high accuracy of 90% and a high speed of 0.5 s for an image whose size is 512 × 448 pixels on the proposed datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032030153X",
    "keywords": [
      "Artificial intelligence",
      "Cognitive neuroscience of visual object recognition",
      "Computer science",
      "Computer vision",
      "Edge detection",
      "Enhanced Data Rates for GSM Evolution",
      "Face detection",
      "Facial recognition system",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Image processing",
      "Linguistics",
      "Matching (statistics)",
      "Mathematics",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Statistics",
      "Viola–Jones object detection framework"
    ],
    "authors": [
      {
        "surname": "Xiao",
        "given_name": "Ling"
      },
      {
        "surname": "Wu",
        "given_name": "Bo"
      },
      {
        "surname": "Hu",
        "given_name": "Youmin"
      }
    ]
  },
  {
    "title": "Translating video into language by enhancing visual and language representations",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102875",
    "abstract": "It is a fundamental task of translating videos into natural language automatically by computer. At present, the models for video description based on deep learning have made a great breakthrough. However, the static information loss is serious during encoding stage for motion feature of videos, and the linguistic feature from LSTM network lack personalized expression, leading to inappropriate words and poor semantics in generation sentences. In this work, a model with enhanced features of visual and language is proposed to address the challenges. First, static features of video frames from the first LSTM layer are incorporated, then fed into another LSTM layer according by frame sequence. Second, the feature of word is combined with the output of LSTM network for predicted probability of candidate word on each time step. The experimental results demonstrate effectiveness of the proposed approach with competitive performance compared with other state-of-the-art methods on various metrics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301231",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Economics",
      "Encoding (memory)",
      "Feature (linguistics)",
      "Frame (networking)",
      "Language model",
      "Layer (electronics)",
      "Linguistics",
      "Management",
      "Natural language",
      "Natural language processing",
      "Organic chemistry",
      "Philosophy",
      "Programming language",
      "Semantics (computer science)",
      "Speech recognition",
      "Task (project management)",
      "Telecommunications",
      "Word (group theory)"
    ],
    "authors": [
      {
        "surname": "Tang",
        "given_name": "Pengjie"
      },
      {
        "surname": "Tan",
        "given_name": "Yunlan"
      },
      {
        "surname": "Li",
        "given_name": "Jinzhong"
      },
      {
        "surname": "Tan",
        "given_name": "Bin"
      }
    ]
  },
  {
    "title": "Multiple objects tracking by a highly decisive three-frame differencing-combined-background subtraction method with GMPFM-GMPHD filters and VGG16-LSTM classifier",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102905",
    "abstract": "Tracking of moving vehicles and pedestrians is the most important application in traffic surveillance videos. This study develops a highly efficient and fast multi-object tracking method using three-frame differencing-combined-background subtraction (TFDCBS)-coupled-automatic and fast histogram-entropy-based thresholding (HEBT) method together with GMPFM-GMPHD filters and VGG16-LSTM classifier. Here TFDCBS-HEBT methods identify the targeted objects with enclosed 3D bounding boxes and extracts multiple features from the raw images. Maximum number of error-free extracted multiple features (key points, multiple local convolutions, corners, and descriptors) are processed subsequently for object tracking by GMPFM-GMPHD Filters and an upgraded VGG16- LSTM classifier. The proposed method has been validated on KITTI 3D bounding box-dataset and its performance compared with three state-of-the-art tracking methods. Highest values of several performance parameters and the lowest computation time clearly demonstrate the promising feature of our new method for its application towards a fast and effective multi-target tracking of moving objects.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301450",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Background subtraction",
      "Classifier (UML)",
      "Computation",
      "Computer science",
      "Computer vision",
      "Feature extraction",
      "Frame rate",
      "Histogram",
      "Image (mathematics)",
      "Minimum bounding box",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Pixel",
      "Thresholding",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Chandrasekar",
        "given_name": "K. Silpaja"
      },
      {
        "surname": "Geetha",
        "given_name": "P."
      }
    ]
  },
  {
    "title": "Scalable Hash From Triplet Loss Feature Aggregation For Video De-duplication",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102908",
    "abstract": "The producing, sharing and consuming life cycle of video content creates massive amount of duplicates in video segments due to variable bit rate representation and fragmentation in the playbacks. The inefficiency of this duplicates to storage and communication motivate researchers in both academia and industry to come up with computationally efficient video deduplication solutions for storage and CDN providers. Moreover, the increasing demands of high resolution and quality aggravate the status of heavy burden of cluster storage side and restricted bandwidth resources. Hence, video de-duplication in storage and transmission is becoming an important feature for video cloud storage and Content Delivery Network (CDN) service providers. Despite of the necessity of optimizing the multimedia data de-duplication approach, it is a challenging task because we should match as many as possible duplicated videos under not removing videos by mistake. The current video de-duplication schemes mostly relies on the URL based solution, which is not able to deal with non-cacheable content like video, which the same piece of content may have totally different URL identification and fragmentation and different quality representations further complicate the problem. In this paper, we propose a novel content based video segmentation identification scheme that is invariant to the underlying codec and operational bit rates, it computes robust features from a triplet loss deep learning network that captures the invariance of the same content under different coding tools and strategy, while a scalable hashing solution is developed based on Fisher Vector aggregation of the convolutional features from the Triplet loss network. Our simulation results demonstrate the great improvement in terms of large scale video repository de-duplication compared with state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301462",
    "keywords": [
      "Computer network",
      "Computer science",
      "Computer security",
      "Data deduplication",
      "Database",
      "Economics",
      "Hash function",
      "Metric (unit)",
      "Operations management",
      "Scalability",
      "Scalable Video Coding",
      "Video quality"
    ],
    "authors": [
      {
        "surname": "Jia",
        "given_name": "Wei"
      },
      {
        "surname": "Li",
        "given_name": "Li"
      },
      {
        "surname": "Li",
        "given_name": "Zhu"
      },
      {
        "surname": "Zhao",
        "given_name": "Shuai"
      },
      {
        "surname": "Liu",
        "given_name": "Shan"
      }
    ]
  },
  {
    "title": "Weakly supervised single image dehazing",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102897",
    "abstract": "Single image dehazing is a critical image pre-processing step for many practical vision systems. Most existing dehazing methods solve this problem utilizing various of hand-crafted priors or by supervised training on the synthetic hazy image information (such as haze-free image, transmission map and atmospheric light). However, the assumptions on the hand-crafted priors are easily violated and collecting realistic transmission map and atmospheric light are unpractical. In this paper, we propose a novel weakly supervised network based on the multi-level multi-scale block. The proposed network reduces the constraint on the training data and automatically estimates the transmission map and the atmospheric light as well as the intermediate haze-free image without using any realistic transmission map and atmospheric light as supervision. Moreover, the estimated intermediate haze-free image helps to generate accurate transmission map and atmospheric light by embedding the physical-model, which presents reliable restoration of the final haze-free image. In particular, our network also can be trained on the real-world dataset to fine-tune the model and the fine-tuning operation improves the dehazing performance on the real-world dataset. Quantitative and qualitative experimental results demonstrate the proposed method performs on par with the supervised methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301395",
    "keywords": [
      "Artificial intelligence",
      "Bayesian probability",
      "Block (permutation group theory)",
      "Computer science",
      "Computer vision",
      "Constraint (computer-aided design)",
      "Embedding",
      "Geometry",
      "Haze",
      "Image (mathematics)",
      "Mathematics",
      "Meteorology",
      "Physics",
      "Prior probability",
      "Telecommunications",
      "Transmission (telecommunications)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Cong"
      },
      {
        "surname": "Fan",
        "given_name": "Wanshu"
      },
      {
        "surname": "Wu",
        "given_name": "Yutong"
      },
      {
        "surname": "Su",
        "given_name": "Zhixun"
      }
    ]
  },
  {
    "title": "(t, k, n) XOR-based visual cryptography scheme with essential shadows",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102911",
    "abstract": "Visual cryptography scheme (VCS) shares a binary secret image into multiple shadows, only qualified set of shadows can reveal the secret image by stacking operation. However, VCS suffers the problems of low visual quality of the revealed image and large shadow size. A (t, k, n) XOR-based visual cryptography scheme (XVCS) shares the secret image into n shadows including t essentials and n-t non-essentials. A qualified set of shadows contains any k shadows including t essentials. The revealing process is implemented by XOR operation on the involved shadows. In this paper, we propose a construction method for (t, k, n)-XVCS with essential shadows. The secret image can be revealed perfectly, and the shadow size is small compared with VCS. Theoretical analysis and experimental results show the security and effectiveness of the proposed scheme.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301498",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Binary number",
      "Bitwise operation",
      "Computer science",
      "Computer vision",
      "Cryptography",
      "Image (mathematics)",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Process (computing)",
      "Programming language",
      "Psychology",
      "Psychotherapist",
      "Scheme (mathematics)",
      "Secret sharing",
      "Set (abstract data type)",
      "Shadow (psychology)",
      "Visual cryptography"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Peng"
      },
      {
        "surname": "Ma",
        "given_name": "Jianfeng"
      },
      {
        "surname": "Ma",
        "given_name": "Quan"
      }
    ]
  },
  {
    "title": "Intra mode prediction for H.266/FVC video coding based on convolutional neural network",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2019.102686",
    "abstract": "The next-generation video compression standard H.266/Future Video Coding (FVC) provides high compression efficiency in terms of the cost of computing the optimal intra mode from 67 modes. We propose an intra mode prediction method based on a convolutional neural network (CNN). An input image set of 20 × 20 blocks is used to train the CNN; the CNN is used to predict the best classes of intra mode direction. The CNN architecture comprises two convolutional layers and a fully connected layer. Compared with the default fast search method in FVC, the proposed method can achieve a 0.033% decrease in Bjøntegaard delta bit rate (BDBR) with only a slight increase in time.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320319303074",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Coding (social sciences)",
      "Computer science",
      "Convolutional code",
      "Convolutional neural network",
      "Data compression",
      "Decoding methods",
      "Mathematics",
      "Mode (computer interface)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Lin",
        "given_name": "Ting-Lan"
      },
      {
        "surname": "Liang",
        "given_name": "Kai-Wen"
      },
      {
        "surname": "Huang",
        "given_name": "Jing-Ya"
      },
      {
        "surname": "Tu",
        "given_name": "Yu-Liang"
      },
      {
        "surname": "Chang",
        "given_name": "Pao-Chi"
      }
    ]
  },
  {
    "title": "Real-time license plate detection and recognition using deep convolutional neural networks",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102773",
    "abstract": "Automatic License Plate Recognition (ALPR) is an important task with many applications in Intelligent Transportation and Surveillance systems. This work presents an end-to-end ALPR method based on a hierarchical Convolutional Neural Network (CNN). The core idea of the proposed method is to identify the vehicle and the license plate region using two passes on the same CNN, and then to recognize the characters using a second CNN. The recognition CNN massively explores the use of synthetic and augmented data to cope with limited training datasets, and our results show that the augmentation process significantly increases the recognition rate. In addition, we present a novel temporal coherence technique to better stabilize the OCR output in videos. Our method was tested with publicly available datasets containing Brazilian and European license plates, achieving accuracy rates better than competitive academic methods and a commercial system.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300237",
    "keywords": [
      "Artificial intelligence",
      "Civil engineering",
      "Coherence (philosophical gambling strategy)",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Economics",
      "Engineering",
      "Intelligent transportation system",
      "License",
      "Machine learning",
      "Management",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics",
      "Process (computing)",
      "Quantum mechanics",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Silva",
        "given_name": "Sergio Montazzolli"
      },
      {
        "surname": "Jung",
        "given_name": "Claudio Rosito"
      }
    ]
  },
  {
    "title": "Vision-based optimization of the generalized predictive active disturbance rejection controller",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2019.102728",
    "abstract": "The batching system of the integrated mixing and spreading equipment for MOH material is a nonlinear system with large uncertainty. It is difficult for conventional control strategies to meet the requirements for system performance. This research combines generalized predictive control and active disturbance rejection technique to propose a new generalized predictive active disturbance rejection controller (GPADRC) used in the batching system of MOH material. For the nonlinearity and uncertainty of the batching system, the extended state observer in the active disturbance rejection technique is used for estimation and compensation. The batching system model is converted into an integrator form, based on which the use of generalized predictive control can greatly reduce the impact of nonlinear models and uncertainties on the controller. Aiming at the problem that the parameters of the proposed new controller are numerous and difficult to tune, the adaptive genetic algorithm is used to realize the automatic tuning of the parameters. The simulation experiment shows that the designed GPADRC can well adapt to the working conditions of the batching system and can meet the requirements for various control indicators. At the same time, the adaptive genetic algorithm can realize the rapid tuning of the controller parameters, which reduce the difficulty and time consumption of the tuning process, and improve the applicability and achievability of the designed controller.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320319303499",
    "keywords": [
      "Agronomy",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Biology",
      "Compensation (psychology)",
      "Computer network",
      "Computer science",
      "Control (management)",
      "Control engineering",
      "Control theory (sociology)",
      "Controller (irrigation)",
      "Disturbance (geology)",
      "Engineering",
      "Integrator",
      "Model predictive control",
      "Nonlinear system",
      "Operating system",
      "Paleontology",
      "Physics",
      "Process (computing)",
      "Psychoanalysis",
      "Psychology",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Yikun"
      },
      {
        "surname": "Jiao",
        "given_name": "Shengjie"
      },
      {
        "surname": "Li",
        "given_name": "Jiabo"
      }
    ]
  },
  {
    "title": "A novel approach for space debris recognition based on the full information vectors of star points",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2019.102716",
    "abstract": "The recognition and detection of space debris has become one of significant research fields recently. Compared with natural images, effective information are very few contained in star images. In the past years, the gray values of star points and the continuity of sequential star images are utilized by numerous algorithms to carry out the recognition and detection through fusion of consecutive star images, which have been achieved good performance. However, with the rapid increase of star image data, those algorithms seem to be inadequate in recognition ability. In this paper, we propose one novel approach based on the full information vectors of star points to recognize moving targets with the machine learning method which is never utilized in space debris recognition field. Besides gray values, we further deeply excavate the characteristics of each star point in a single frame by the equal probability density curve of Gaussian distribution. The elliptical pattern characteristic vectors of star points can be input into the machine learning method for classification of static stars and moving targets in a single frame. Finally, trajectories of moving targets can be determined within 3 frames by the full information vectors. Therefore, traditional processing methods are abandoned and the proposed brand new approach redefines the recognition technical route of space debris. The experimental results demonstrate that moving targets can be successfully recognized in a single frame and the coverage rate of moving targets can reach 100%. Compared with other traditional methods, the proposed approach has better performance and more robustness.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320319303372",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Debris",
      "Frame (networking)",
      "Gaussian",
      "Gene",
      "Geography",
      "Geometry",
      "Mathematical analysis",
      "Mathematics",
      "Meteorology",
      "Pattern recognition (psychology)",
      "Physics",
      "Point (geometry)",
      "Quantum mechanics",
      "Robustness (evolution)",
      "Space debris",
      "Star (game theory)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Du",
        "given_name": "Yun"
      },
      {
        "surname": "Wen",
        "given_name": "Desheng"
      },
      {
        "surname": "Liu",
        "given_name": "Guizhong"
      },
      {
        "surname": "Qiu",
        "given_name": "Shi"
      },
      {
        "surname": "Yao",
        "given_name": "Dalei"
      },
      {
        "surname": "Yi",
        "given_name": "Hongwei"
      },
      {
        "surname": "Liu",
        "given_name": "Meiying"
      }
    ]
  },
  {
    "title": "Three stage deep network for 3D human pose reconstruction by exploiting spatial and temporal data via its 2D pose",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102866",
    "abstract": "3D Human Pose Reconstruction (HPR) is a challenging task due to less availability of 3D ground truth data and projection ambiguity. To address these limitations, we propose a three-stage deep network having the workflow of 2D Human Pose Estimation (HPE) followed by 3D HPR; which utilizes the proposed Frame Specific Pose Estimation (FSPE), Multi-Stage Cascaded Feature Connection (MSCFC) and Feature Residual Connection (FRC) Sub-level Strategies. In the first stage, the FSPE concept with the MSCFC strategy has been used for 2D HPE. In the second stage, the basic deep learning concepts like convolution, batch normalization, ReLU, and dropout have been utilized with the FRC Strategy for spatial 3D reconstruction. In the last stage, LSTM deep architecture has been used for temporal refinement. The effectiveness of the technique has been demonstrated on MPII, Human3.6M, and HumanEva-I datasets. From the experiments, it has been observed that the proposed method gives competitive results to the recent state-of-the-art techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301164",
    "keywords": [
      "Algorithm",
      "Anthropology",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Convolution (computer science)",
      "Database",
      "Deep learning",
      "Feature (linguistics)",
      "Linguistics",
      "Normalization (sociology)",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pose",
      "Residual",
      "Sociology",
      "Stage (stratigraphy)",
      "Workflow"
    ],
    "authors": [
      {
        "surname": "Verma",
        "given_name": "Pratishtha"
      },
      {
        "surname": "Srivastava",
        "given_name": "Rajeev"
      }
    ]
  },
  {
    "title": "Detail retaining convolutional neural network for image denoising",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102774",
    "abstract": "Compared with the traditional image denoising method, although the convolutional neural network (CNN) has better denoising performance, there is an important issue that has not been well resolved: the residual image obtained by learning the difference between noisy image and clean image pairs contains abundant image detail information, resulting in the serious loss of detail in the denoised image. In this paper, in order to relearn the lost image detail information, a mathematical model is deducted from a minimization problem and an end-to-end detail retaining CNN (DRCNN) is proposed. Unlike most denoising methods based on CNN, DRCNN is not only focus to image denoising, but also the integrity of high frequency image content. DRCNN needs less parameters and storage space, therefore it has better generalization ability. Moreover, DRCNN can also adapt to different image restoration tasks such as blind image denoising, single image superresolution (SISR), blind deburring and image inpainting. Extensive experiments show that DRCNN has a better effect than some classic and novel methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300249",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Feature detection (computer vision)",
      "Generalization",
      "Image (mathematics)",
      "Image denoising",
      "Image processing",
      "Image restoration",
      "Inpainting",
      "Mathematical analysis",
      "Mathematics",
      "Noise reduction",
      "Non-local means",
      "Pattern recognition (psychology)",
      "Residual"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Xiaoxia"
      },
      {
        "surname": "Xiao",
        "given_name": "Juan"
      },
      {
        "surname": "Zhou",
        "given_name": "Yingyue"
      },
      {
        "surname": "Ye",
        "given_name": "Yuanzheng"
      },
      {
        "surname": "Lv",
        "given_name": "Nianzu"
      },
      {
        "surname": "Wang",
        "given_name": "Xueyuan"
      },
      {
        "surname": "Wang",
        "given_name": "Shunli"
      },
      {
        "surname": "Gao",
        "given_name": "ShaoBing"
      }
    ]
  },
  {
    "title": "Human action recognition based on convolutional neural network and spatial pyramid representation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2019.102722",
    "abstract": "Detecting and recognizing human action in natural scenarios, such as indoor and outdoor, is a significant technique in computer vision and intelligent systems, which is widely applied in video surveillance, pedestrian tracking and human-computer interaction. Conventional approaches have been proposed based on various features and achieved impressive performance. However, these methods failed to cope with partial occlusion and changes of posture. In order to address these limitations, we propose a novel human action recognition method. More specifically, in order to capture image spatial composition, we leverage a three-level spatial pyramid feature extraction scheme, where each pyramid is encoded by local features. Thereafter, regions generated by a proposal algorithm are fed into a dual-aggregation net for deep representation extraction. Afterwards, both local features and deep features are fused to describe each image. To describe human action category, we design a metric CXQDA based on Cosine measure and Cross-view Quadratic Discriminant Analysis (XQDA) to calculate the similarity among different action categories. Experimental results demonstrate that our proposed method can effectively cope with object scale variations, partial occlusion and achieve competitive performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320319303438",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Feature extraction",
      "Geometry",
      "Law",
      "Leverage (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Pyramid (geometry)",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Xiao",
        "given_name": "Jihai"
      },
      {
        "surname": "Cui",
        "given_name": "Xiaohong"
      },
      {
        "surname": "Li",
        "given_name": "Feng"
      }
    ]
  },
  {
    "title": "☆ Discriminative dictionary learning algorithm based on sample diversity and locality of atoms for face recognition",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102763",
    "abstract": "Dictionary learning is one of the most important algorithms for face recognition. However, many dictionary learning algorithms for face recognition have the problems of small sample and weak discriminability. In this paper, a novel discriminative dictionary learning algorithm based on sample diversity and locality of atoms is proposed to solve the problems. The rational sample diversity is implemented by alternative samples and new error model to alleviate the small sample size problem. Moreover, locality can leads to sparsity and strong discriminability. In this paper, to enhance the dictionary discrimination and to reduce the influence of noise, the graph Laplacian matrix of atoms is used to keep the local information of the data. At the same, the relational theory is presented. A large number of experiments prove that the proposed algorithm can achieve more high performance than some state-of-the-art algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300134",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Dictionary learning",
      "Discriminative model",
      "Face (sociological concept)",
      "Facial recognition system",
      "Graph",
      "Laplacian matrix",
      "Linguistics",
      "Locality",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Sample (material)",
      "Social science",
      "Sociology",
      "Sparse approximation",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Shigang"
      },
      {
        "surname": "Wang",
        "given_name": "Yuhong"
      },
      {
        "surname": "Wu",
        "given_name": "Xiaosheng"
      },
      {
        "surname": "Li",
        "given_name": "Jun"
      },
      {
        "surname": "Lei",
        "given_name": "Tao"
      }
    ]
  },
  {
    "title": "Multi-scale active patches fusion based on spatiotemporal LBP-TOP for micro-expression recognition",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102862",
    "abstract": "Micro-expressions are spontaneous emotions appearing on a face that is hard to conceal and thus making them different from normal facial expressions both in duration and subtlety. This paper investigates a challenging issue in micro-expression, where not all facial regions contribute equally to effective representation. Consequently, we proposed a multi-scale active patches fusion-based spatiotemporal LBP-TOP descriptor that considers the active contributions for different region area in faces. For the feature procedure, we exploit the average value of all patches under each scale to obtain the threshold that selectively fuses the local and global features. On the other hand, an improved weighted sparse representation based dual augmented Lagrange multiplier is adopted for the classification to remit the problem of sparse coefficients obtained by the traditional sparse representation algorithm. We conduct comprehensive experiments on CASME II and SAMM datasets and the accuracies respectively reach 77.30% and 58.82% using LOSO cross-validation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301139",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Computer science",
      "Dual (grammatical number)",
      "Expression (computer science)",
      "Face (sociological concept)",
      "Facial expression",
      "Facial expression recognition",
      "Facial recognition system",
      "Feature (linguistics)",
      "Fusion",
      "Lagrange multiplier",
      "Law",
      "Linguistics",
      "Literature",
      "Mathematical optimization",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Political science",
      "Politics",
      "Programming language",
      "Quantum mechanics",
      "Representation (politics)",
      "Scale (ratio)",
      "Social science",
      "Sociology",
      "Sparse approximation"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Zhe"
      },
      {
        "surname": "Hu",
        "given_name": "Zheng-ping"
      },
      {
        "surname": "Zhao",
        "given_name": "Mengyao"
      },
      {
        "surname": "Li",
        "given_name": "Shufang"
      }
    ]
  },
  {
    "title": "Reversible data hiding in JPEG bitstream using optimal VLC mapping",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102821",
    "abstract": "The traditional RDH method for JPEG bitstream is conducted by building the mapping between the variable length codes (VLC). However, the capacity is limited, and the file size may not be well preserved as the capacity is increased. This is because that the trade-off between the capacity and the file size has not been deeply investigated, neither explicitly formulated nor appropriately optimized. In this paper, we propose to take the file size preservation into consideration and minimize the file size increase for a given capacity. We use the value transfer matrix to simulate a theoretical model and then design some optimization rules to reach the reversible solution. Consequently, a better reversible VLC mapping can be obtained in terms of both the capacity and the file size preservation. The experimental results show that the proposed method can increase the capacity with a relatively low cost of file size increase.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300717",
    "keywords": [
      "Algorithm",
      "Bitstream",
      "Computer science",
      "Data compression",
      "Decoding methods",
      "File size",
      "JPEG",
      "Operating system"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Cheng"
      },
      {
        "surname": "Ou",
        "given_name": "Bo"
      },
      {
        "surname": "Tian",
        "given_name": "Huawei"
      },
      {
        "surname": "Qin",
        "given_name": "Zheng"
      }
    ]
  },
  {
    "title": "No-reference image sharpness assessment based on discrepancy measures of structural degradation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102861",
    "abstract": "The discrepancy between an image and its “reblurred” version indicates the extent of blur in the image. This paper presents a novel no-reference image sharpness evaluator leveraging the discrepancy measures of structural degradation in both the spatial and wavelet domains. Specifically, local structural degradation of an input image is characterized by the discrepancy measures of orientation selectivity-based visual patterns and log-Gabor filter responses between the image and its corresponding reblurred version respectively. Considering the influence of viewing distance on image quality, the global sharpness discrepancy is measured through inter-resolution self-similarities. Finally, the computed discrepancies are utilized as sharpness-aware features and then a support vector regressor is employed to map the feature vectors into quality scores. The performance of the proposed method is evaluated on six public image quality databases, including two real blurred image databases. Experimental results demonstrate that our proposed method achieves state-of-the-art performances across all these databases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301127",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Filter (signal processing)",
      "Gabor filter",
      "Geometry",
      "Image (mathematics)",
      "Image processing",
      "Image quality",
      "Image resolution",
      "Image restoration",
      "Linguistics",
      "Mathematics",
      "Orientation (vector space)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Wavelet"
    ],
    "authors": [
      {
        "surname": "Cai",
        "given_name": "Hao"
      },
      {
        "surname": "Wang",
        "given_name": "Mingjie"
      },
      {
        "surname": "Mao",
        "given_name": "Wendong"
      },
      {
        "surname": "Gong",
        "given_name": "Minglun"
      }
    ]
  },
  {
    "title": "Deep hierarchical encoding model for sentence semantic matching",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102794",
    "abstract": "Sentence semantic matching (SSM) always plays a critical role in natural language processing. Measuring the intrinsic semantic similarity among sentences is very challenging and has not been substantially addressed. The latest SSM research usually relies on a shallow text representation and interaction between sentence pairs, which might not be enough to capture the complex semantic features and lead to limited performance. To capture more semantic context features and interactions, we propose a hierarchical encoding model (HEM) for sentence representation, further enhanced by a hierarchical matching mechanism for sentence interaction. Given two sentences, HEM generates intermediate and final representations in encoding layer, which are further handled by a novel hierarchical matching mechanism to capture more multi-view interactions in matching layer. The comprehensive experiments demonstrate that our model is capable to capture more sentence semantic features and interactions, which significantly outperforms the existing state-of-the-art neural models on the public real-world dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300444",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Chemistry",
      "Computer science",
      "Context (archaeology)",
      "Encoding (memory)",
      "Law",
      "Layer (electronics)",
      "Matching (statistics)",
      "Mathematics",
      "Natural language processing",
      "Organic chemistry",
      "Paleontology",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Semantic matching",
      "Semantic similarity",
      "Sentence",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Lu",
        "given_name": "Wenpeng"
      },
      {
        "surname": "Zhang",
        "given_name": "Xu"
      },
      {
        "surname": "Lu",
        "given_name": "Huimin"
      },
      {
        "surname": "Li",
        "given_name": "Fangfang"
      }
    ]
  },
  {
    "title": "Human motion quality assessment toward sophisticated sports scenes based on deeply-learned 3D CNN model",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2019.102702",
    "abstract": "Video may be subject to various distortions during acquisition, processing, compression, storage, transmission, and reproduction, and it results in reduced visual quality. In complex sports scenes under big data environment, the human body's movements are even more so. The quality of human motion can intuitively affect the human visual experience. Therefore, it is necessary to determine an intelligent quality assessment model to evaluate human motion in complex motion scenarios under big data environment. It can be used to dynamically monitor and adjust video quality, and it can be used for algorithms and parameter settings in motion image processing systems. With the popularity of deep learning, convolutional neural networks have become a very important method in the field of computer vision research. Based on the 2D-CNN algorithm, we propose a 3D convolutional neural network model for human motion quality assessment in complex motion scenarios. The model captures the pose characteristics, motion trajectory, video brightness and contrast in time and space. The model feeds back the reference and distorted video pairs into the network, with each output layer acting as a feature map. The local similarity between the feature maps obtained from the reference video and the distorted video is then calculated and combined to obtain a global image quality score. Experiments show that the model can achieve competitive performance in big data environment for video quality assessment.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320319303232",
    "keywords": [
      "Artificial intelligence",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Epistemology",
      "Human motion",
      "Motion (physics)",
      "Philosophy",
      "Quality (philosophy)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Yedong"
      },
      {
        "surname": "He",
        "given_name": "Hongmei"
      },
      {
        "surname": "Zhang",
        "given_name": "Zhixin"
      }
    ]
  },
  {
    "title": "Artificial intelligence based ensemble approach for intrusion detection systems",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2019.102736",
    "abstract": "Internet attacks pose a severe threat to most of the online resources and are a prime concern of security administrators these days. In spite of many efforts, the security techniques are unable to detect the intrusions accurately. Most of the methods suffer from the limitations of a high false positive rate, low detection rate and provide one solution which lacks the classification trade-offs. In this work, an effective two-stage method is proposed to produce a pool of non-dominating solutions or Pareto optimal solutions as base models and their ensembles for detecting the intrusions accurately. It generates Pareto optimal solutions to a chromosome structure in stage 1 formulating Pareto front. Whereas, another approximation to the Pareto front of optimal solutions is made to obtain non-dominating ensembles in the second stage. The final prediction ensemble solutions are computed from individual predictions using majority voting approach. Applicability of the suggested method is validated using benchmark dataset NSL-KDD dataset. The experimental results show that the recommended method provides better results than conventional ensemble techniques. The recommended method is also adequate to generate Pareto optimal solutions that address the issue of improving detection accuracy for minority as well as majority attack classes along with handling classification tradeoff problem. The proposed method resulted detection accuracy of 97% with FPR of 2% for KDD dataset respectively. The most attractive feature of the proposed method is that both generation of base classifier and their ensemble thereof are multi-objective in nature addressing the issue of low detection accuracy and classification tradeoffs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320319303578",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Ensemble forecasting",
      "Ensemble learning",
      "Geodesy",
      "Geography",
      "Intrusion detection system",
      "Machine learning",
      "Majority rule",
      "Mathematical optimization",
      "Mathematics",
      "Multi-objective optimization",
      "Pareto optimal",
      "Pareto principle"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Hongwei"
      },
      {
        "surname": "Li",
        "given_name": "Mingzhao"
      },
      {
        "surname": "Zhao",
        "given_name": "Haoyu"
      }
    ]
  },
  {
    "title": "Low-resource automatic cartoon image creation from limited samples",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102863",
    "abstract": "In this work, a framework that can automatically create cartoon images with low computation resources and small training datasets is proposed. The proposed system performs region segmentation and learns a region relationship tree from each learning image. The segmented regions are clustered automatically with an enhanced clustering mechanism with no prior knowledge of number of clusters. According to the topology represented by region relationship tree and clustering results, the regions are reassembled to create new images. A swarm intelligence optimization procedure is designed to coordinate the regions to the optimized sizes and positions in the created image. Rigid deformation using moving least squares is performed on the regions to generate more variety for created images. Compared with methods based on Generative Adversarial Networks, the proposed framework can create better images with limited computation resources and a very small amount of training samples.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301140",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Computation",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Image segmentation",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Particle swarm optimization",
      "Pattern recognition (psychology)",
      "Segmentation",
      "Swarm intelligence",
      "Tree (set theory)"
    ],
    "authors": [
      {
        "surname": "Cheng",
        "given_name": "Hsu-Yung"
      },
      {
        "surname": "Yu",
        "given_name": "Chih-Chang"
      }
    ]
  },
  {
    "title": "Learning multiple instance deep representation for objects tracking",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2019.102737",
    "abstract": "Object tracking has been widely used in various intelligent systems, such as pedestrian tracking, autonomous vehicles. To solve the problem that appearance changes and occlusion may lead to poor tracking performance, we propose a multiple instance learning (MIL) based method for object tracking. To achieve this task, we first manually label the first several frames of video stream in image level, which can indicate that whether a target object in the video stream. Then, we leverage a pre-trained convolutional neural network that has rich prior information to extract deep representation of target object. Since the location of the same object in adjacent frames is similar, we introduce a particle filter to predict the location of target object within a specific region. Comprehensive experiments have shown the effectiveness of our proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732031930358X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Law",
      "Pattern recognition (psychology)",
      "Pedagogy",
      "Political science",
      "Politics",
      "Psychology",
      "Representation (politics)",
      "Tracking (education)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Chunyu"
      },
      {
        "surname": "Li",
        "given_name": "Gang"
      }
    ]
  },
  {
    "title": "Application of multimedia technology in water conservancy and hydropower engineering",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2019.102707",
    "abstract": "Multimedia covers a wide range. In general, digital audio production, animation video production, website production, and even game development can all be attributed to multimedia. The definition of multimedia narrowly defined, that is, the project with interactive program development as the main object of this paper, such as interactive CD production, touch screen presentation production, etc. Of course, there will still be a lot of content related to graphic design, animation, video processing, audio production and so on. The application of multimedia technology in water conservancy and hydro-power engineering is characterized by a variety of media means to represent the design, construction process and post-construction scene of water conservancy and hydro-power projects and to simulate the phenomenon in the project, such as the performance of water conservancy and hydro-power projects. Hub layout, structure of main buildings, dam flood discharge, rubber dam dam overflow, sluice dispatching process, ship lock crossing process, etc. In the water conservancy and hydro-power project, computer multimedia technology has been widely used from the general design proposal to the entire pivot project demonstration system. This paper mainly introduces the design and development of the multimedia demonstration system for water conservancy and hydro-power projects.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320319303281",
    "keywords": [
      "Computer science",
      "Electrical engineering",
      "Engineering",
      "Hydropower",
      "Multimedia"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Jingfeng"
      },
      {
        "surname": "Zhang",
        "given_name": "Jing"
      }
    ]
  },
  {
    "title": "Research on the parallelization of image quality analysis algorithm based on deep learning",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2019.102709",
    "abstract": "Image quality assessment is an indispensable in computer vision applications, such as image classification, image parsing. With the development of Internet, image data acquisition becomes more conveniently. However, image distortion is inevitable due to imperfect image acquisition system, image transmission medium and image recording equipment. Traditional image quality assessment algorithms only focus on low-level visual features such as color or texture, which could not encode high-level features effectively. CNN-based methods have shown satisfactory results in image quality assessment. However, existing methods have problems such as incomplete feature extraction, partial image block distortion, and inability to determine scores. So in this paper, we propose a novel framework for image quality assessment based on deep learning. We incorporate both low-level visual features and high-level semantic features to better describe images. And image quality is analyzed in a parallel processing mode. Experiments are conducted on LIVE and TID2008 datasets demonstrate the proposed model can predict the quality of the distorted image well, and both SROCC and PLCC can reach 0.92 or higher.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732031930330X",
    "keywords": [
      "Amplifier",
      "Artificial intelligence",
      "Automatic image annotation",
      "Bandwidth (computing)",
      "Block (permutation group theory)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Digital image processing",
      "Distortion (music)",
      "Epistemology",
      "Feature (linguistics)",
      "Feature detection (computer vision)",
      "Feature extraction",
      "Focus (optics)",
      "Geometry",
      "Image (mathematics)",
      "Image processing",
      "Image quality",
      "Image texture",
      "Linguistics",
      "Mathematics",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quality (philosophy)"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Jui-Chan"
      },
      {
        "surname": "Huang",
        "given_name": "Hao-Chen"
      },
      {
        "surname": "Liu",
        "given_name": "Hsin-Hung"
      }
    ]
  },
  {
    "title": "NERNet: Noise estimation and removal network for image denoising",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102851",
    "abstract": "While some denoising methods based on deep learning achieve superior results on synthetic noise, they are far from dealing with photographs corrupted by realistic noise. Denoising on real-world noisy images faces more significant challenges due to the source of it is more complicated than synthetic noise. To address this issue, we propose a novel network including noise estimation module and removal module (NERNet). The noise estimation module automatically estimates the noise level map corresponding to the information extracted by symmetric dilated block and pyramid feature fusion block. The removal module focuses on removing the noise from the noisy input with the help of the estimated noise level map. Dilation selective block with attention mechanism in the removal module adaptively not only fuses features from convolution layers with different dilation rates, but also aggregates the global and local information, which is benefit to preserving more details and textures. Experiments on two datasets of synthetic noise and three datasets of realistic noise show that NERNet achieves competitive results in comparison with other state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301024",
    "keywords": [
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Dilation (metric space)",
      "Gaussian noise",
      "Geometry",
      "Gradient noise",
      "Image (mathematics)",
      "Mathematics",
      "Noise (video)",
      "Noise floor",
      "Noise measurement",
      "Noise reduction",
      "Pattern recognition (psychology)",
      "Value noise"
    ],
    "authors": [
      {
        "surname": "Guo",
        "given_name": "Bingyang"
      },
      {
        "surname": "Song",
        "given_name": "Kechen"
      },
      {
        "surname": "Dong",
        "given_name": "Hongwen"
      },
      {
        "surname": "Yan",
        "given_name": "Yunhui"
      },
      {
        "surname": "Tu",
        "given_name": "Zhibiao"
      },
      {
        "surname": "Zhu",
        "given_name": "Liu"
      }
    ]
  },
  {
    "title": "Credit risk assessment of P2P lending platform towards big data based on BP neural network",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2019.102730",
    "abstract": "Peer-to-peer (P2P) lending platform plays a significant role in modern financial systems. However, due to improper supervision, credit risk is inevitable. In this paper, we analyze the traditional financial risk and information technology risk of P2P lending platform. In order to evaluate the performance of assessment algorithms, we present a BP neural network-based algorithm for lending risk assessment. To achieve our task, we crawled large-scale lending data for 2015–2019. Logistic regression is used to compare with BP neural network method. Experimental results show that BP neural network-based algorithm outperforms traditional Logistic regression algorithm and the proposed method can effectively reduce investor risk.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320319303517",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Big data",
      "Business",
      "Computer science",
      "Credit risk",
      "Data mining",
      "Engineering",
      "Finance",
      "Logistic regression",
      "Machine learning",
      "Order (exchange)",
      "Physics",
      "Quantum mechanics",
      "Scale (ratio)",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Guo",
        "given_name": "Yiping"
      }
    ]
  },
  {
    "title": "Cross-domain representation learning by domain-migration generative adversarial network for sketch based image retrieval",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102835",
    "abstract": "Sketch based image retrieval (SBIR), which uses free-hand sketches to search the images containing similar objects/scenes, is attracting more and more attentions as sketches could be got more easily with the development of touch devices. However, this task is difficult as the huge differences between sketches and images. In this paper, we propose a cross-domain representation learning framework to reduce these differences for SBIR. This framework aims to transfer sketches to images with the information learned both in the sketch domain and image domain by the proposed domain migration generative adversarial network (DMGAN). Furthermore, to reduce the representation gap between the generated images and natural images, a similarity learning network (SLN) is also proposed with the new designed loss function incorporating semantic information. Extensive experiments have been done from different aspects, including comparison with state-of-the-art methods. The results show that the proposed DMGAN and SLN really work for SBIR.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300857",
    "keywords": [
      "Adversarial system",
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Domain (mathematical analysis)",
      "Generative adversarial network",
      "Generative grammar",
      "Image (mathematics)",
      "Law",
      "Mathematical analysis",
      "Mathematics",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Sketch"
    ],
    "authors": [
      {
        "surname": "Bai",
        "given_name": "Cong"
      },
      {
        "surname": "Chen",
        "given_name": "Jian"
      },
      {
        "surname": "Ma",
        "given_name": "Qing"
      },
      {
        "surname": "Hao",
        "given_name": "Pengyi"
      },
      {
        "surname": "Chen",
        "given_name": "Shengyong"
      }
    ]
  },
  {
    "title": "Clustering adaptive canonical correlations for high-dimensional multi-modal data",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102815",
    "abstract": "Multi-modal canonical correlation analysis (MCCA) is an important joint dimension reduction method and has been widely applied to clustering tasks of multi-modal data. MCCA-based clustering is usually dimension reduction of high-dimensional data followed by clustering of low-dimensional data. However, the two-stage clustering is difficult to ensure the adaptability of dimension reduction and clustering, which will affect the final clustering performance. To solve the issue, we propose a novel clustering adaptive multi-modal canonical correlations (CAMCCs) method, which constructs a unified optimization model of multi-modal correlation learning and clustering. The method not only realizes discriminant learning of correlation projection directions under unsupervised cases, but also is able to directly obtain class labels of multi-modal data. Additionally, the method also realizes out-of-sample extension in class labels. Solutions of CAMCCs are optimized by an iterative way, and we analyze its convergence. Extensive experimental results on various datasets have demonstrated the effectiveness of the method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300651",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Canonical correlation",
      "Chemistry",
      "Cluster analysis",
      "Clustering high-dimensional data",
      "Computer science",
      "Correlation clustering",
      "Data mining",
      "Dimension (graph theory)",
      "Dimensionality reduction",
      "Mathematics",
      "Modal",
      "Pattern recognition (psychology)",
      "Polymer chemistry",
      "Projection (relational algebra)",
      "Pure mathematics"
    ],
    "authors": [
      {
        "surname": "Su",
        "given_name": "Shuzhi"
      },
      {
        "surname": "Fang",
        "given_name": "Xianjin"
      },
      {
        "surname": "Yang",
        "given_name": "Gaoming"
      },
      {
        "surname": "Ge",
        "given_name": "Bin"
      },
      {
        "surname": "Zheng",
        "given_name": "Ping"
      }
    ]
  },
  {
    "title": "Hierarchical Learning-Guided human motion quality assessment in big data environment",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2019.102700",
    "abstract": "Image information may be distorted during acquisition, processing, compression, and transmission. It is necessary to propose an intelligent image quality assessment model toward big data environment to quantify the degree of distortion of the image. This paper proposes a quality assessment model for human motion images. In complex scenes, the human body's action posture can be taken as an important feature point. Usually, in different scenes, the parts that affect the quality of the human body's posture are different. In other words, the weights of feature points that affect quality are different in different scenarios. However, due to the categorization of human movements, we can learn the quality assessment methods of different types of movements through sample training. Inspired by feature learning in the field of machine learning, we propose a hierarchical quality learning approach. We cast quality assessment as quality feature learning and layer by layer. The hierarchical quality learning method is based on deep reinforcement learning. The key part is that the method focuses on the region that containing more information on the features of the quality and enlarges the region layer by layer. Finally, we can determine the part of the body that affects the quality assessment. We compare this method with the subjective quality assessment results of the human observers and find that the proposed method achieves effective performance in big data environment to evaluate human motion quality.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320319303219",
    "keywords": [
      "Artificial intelligence",
      "Big data",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Environmental science",
      "Human motion",
      "Motion (physics)",
      "Motion capture",
      "Physics",
      "Quality (philosophy)",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Zhiwei"
      },
      {
        "surname": "Li",
        "given_name": "Yan"
      },
      {
        "surname": "Luo",
        "given_name": "Shiguang"
      }
    ]
  },
  {
    "title": "Polygonal approximation based on coarse-grained parallel genetic algorithm",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2019.102717",
    "abstract": "This paper proposes to apply coarse-grained parallel genetic algorithm (CGPGA) to solve polygonal approximation problem. Chromosomes are used to represent digital curves and genes correspond to points of curves. This method divides the whole population into several subpopulations, each of which performs evolutionary process independently. After every migration interval number of generations, these subpopulations exchange their information with each other. Inspired by the designing theory of ensemble learning in machine learning, this paper further improves the basic CGPGA through adopting different but effective genetic algorithms, respectively, in different subpopulations. Both the diversity among different subpopulations and the accuracy in each individual subpopulation are ensured. Experimental results, based on four benchmark curves and four real image curves extracted from the lake maps, show that the basic CGPGA outperforms the used genetic algorithm, and further the improved CGPGA (ICGPGA) is more effective than the basic CGPGA, in terms of the quality of best solutions, the average solutions, and the variance of best solutions. Especially for those larger approximation problems, the ICGPGA is more remarkably superior to some representative genetic algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320319303384",
    "keywords": [
      "Accounting",
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Business",
      "Combinatorics",
      "Computer science",
      "Demography",
      "Genetic algorithm",
      "Geodesy",
      "Geography",
      "Interval (graph theory)",
      "Machine learning",
      "Mathematics",
      "Operating system",
      "Population",
      "Process (computing)",
      "Sociology",
      "Variance (accounting)"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Zhaobin"
      },
      {
        "surname": "Zhao",
        "given_name": "Chunxia"
      },
      {
        "surname": "Liu",
        "given_name": "Bin"
      }
    ]
  },
  {
    "title": "Spatial-temporal saliency action mask attention network for action recognition",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102846",
    "abstract": "Recently, video action recognition about two-stream network is still a popular research topic in computer vision. However, most of current two-stream-based methods have two redundancy issues, including: inter-frame redundancy and intra-frame redundancy. To solve the above problems, a Spatial-Temporal Saliency Action Mask Attention network (STSAMANet) is built for action recognition. First, this paper introduces a key-frame mechanism to eliminate inter-frame redundancy. This mechanism can compute key frames on each video sequence to get the greatest difference between frames. Then, Mask R-CNN detection technology is introduced to build a saliency attention layer to eliminate intra-frame redundancy. This layer is to focus on the saliency human body and objects for each action class. We experiment on two public video action datasets, i.e., the UCF101 dataset and Penn Action dataset to verify the effectiveness of our method in action recognition.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300973",
    "keywords": [
      "Action recognition",
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer network",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Frame (networking)",
      "Key (lock)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Redundancy (engineering)"
    ],
    "authors": [
      {
        "surname": "Jiang",
        "given_name": "Min"
      },
      {
        "surname": "Pan",
        "given_name": "Na"
      },
      {
        "surname": "Kong",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "Application of fuzzy image restoration in criminal investigation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2019.102704",
    "abstract": "The advancement of science and technology has a positive effect on the development of law disciplines. The development of algorithms and artificial intelligence also has a certain impact on judicial practice. Image restoration is a significant technique in image processing. It aims to objectively restore the content or quality of the original image from the degraded image. Image degradation is always generated in image transmission, such as distortion, blur. In modern video surveillance system, image restoration is significant for criminal investigation. However, image restoration based on conventional filter algorithms cannot achieve satisfactory performance. Thus, we first introduce the image restoration algorithms based on different degradation model. Then, we propose some applications of fuzzy image restoration in criminal investigation. We conduct experiments on both degraded images and videos and experimental results have shown the effectiveness of fuzzy image restoration applying to the criminal investigation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320319303256",
    "keywords": [
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Distortion (music)",
      "Fuzzy logic",
      "Image (mathematics)",
      "Image processing",
      "Image restoration"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Shuo"
      }
    ]
  },
  {
    "title": "Water leakage image recognition of shield tunnel via learning deep feature representation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2019.102708",
    "abstract": "With the development of urban metro, the research on structural diseases of shield tunnels has been becoming a hot research topic, especially the leakage water diseases. Deep learning-based algorithms have shown impressive performance in image processing domain, such as image classification, image recognition or image retrieval. In this paper, we propose a novel image recognition algorithm for water leakage diseases of shield tunnels based on deep learning algorithm. Water leakage images are classified into six categories, each of which are extracted deep representation for image recognition. We compare our method with Otsu algorithm (OA), Region Growing Algorithm (RGA), and Watershed Algorithm (WA) to show the effectiveness of our proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320319303293",
    "keywords": [
      "Artificial intelligence",
      "Composite material",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Economics",
      "Feature (linguistics)",
      "Geology",
      "Image (mathematics)",
      "Image processing",
      "Leakage (economics)",
      "Linguistics",
      "Macroeconomics",
      "Materials science",
      "Pattern recognition (psychology)",
      "Petrology",
      "Philosophy",
      "Shield",
      "Water leakage",
      "Watershed"
    ],
    "authors": [
      {
        "surname": "Xiong",
        "given_name": "Leijin"
      },
      {
        "surname": "Zhang",
        "given_name": "Dingli"
      },
      {
        "surname": "Zhang",
        "given_name": "Yu"
      }
    ]
  },
  {
    "title": "Multi-path connected network for medical image segmentation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102852",
    "abstract": "In recent years, deep learning has been successfully applied to medical image segmentation. However, as the network extends deeper, the consecutive downsampling operations will lead to more loss of spatial information. In addition, the limited data and diverse targets increase the difficulty for medical image segmentation. To address these issues, we propose a multi-path connected network (MCNet) for medical segmentation problems. It integrates multiple paths generated by pyramid pooling into the encoding phase to preserve semantic information and spatial details. We utilize multi-scale feature extractor block (MFE block) in the encoder to obtain large and multi-scale receptive fields. We evaluated MCNet on three medical datasets with different image modalities. The experimental results show that our method achieves better performance than the state-of-the-art approaches. Our model has strong feature learning ability and is robust to capture different scale targets. It can achieve satisfactory results while using only 0.98 million (M) parameters.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301036",
    "keywords": [
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Encoder",
      "Encoding (memory)",
      "Feature (linguistics)",
      "Geometry",
      "Image (mathematics)",
      "Image segmentation",
      "Linguistics",
      "Mathematics",
      "Operating system",
      "Path (computing)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Pooling",
      "Programming language",
      "Pyramid (geometry)",
      "Quantum mechanics",
      "Scale (ratio)",
      "Segmentation",
      "Upsampling"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Dan"
      },
      {
        "surname": "Hu",
        "given_name": "Guoqing"
      },
      {
        "surname": "Lyu",
        "given_name": "Chengzhi"
      }
    ]
  },
  {
    "title": "Rate-distortion-complexity optimization for x265",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102870",
    "abstract": "Rate-distortion optimization (RDO) is conventionally based on the analysis of rate-distortion (R-D) curve to minimize the coding distortion under the coding bits constraint. However, it is necessary to consider the computational complexity in the RDO process. In this paper, we obtain the Confidence LEvel - Computational complexity (CLEC) curves which indicate the characteristics of coding tree units (CTUs). Based on the CLEC curves, a rate-distortion-complexity optimization (RDCO) algorithm is proposed to optimize R-D under given computational complexity and achieve the optimal coding performance for x265. Experimental results demonstrate that the proposed algorithm can achieve a wide range of encoding speed under a given quantization parameter (QP) whereas the original x265 can only achieve a few fixed encoding speeds, and the proposed algorithm can reduce the BD-rate and increase the BD-PSNR by 6.59% and 0.13 dB on average under the same requirements of encoding speeds as the original x265.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301206",
    "keywords": [
      "Algorithm",
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Coding (social sciences)",
      "Computational complexity theory",
      "Computer network",
      "Computer science",
      "Data compression",
      "Distortion (music)",
      "Encoding (memory)",
      "Mathematical optimization",
      "Mathematics",
      "Multiview Video Coding",
      "Operating system",
      "Process (computing)",
      "Quantization (signal processing)",
      "Rate distortion",
      "Rate–distortion optimization",
      "Rate–distortion theory",
      "Statistics",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Saiping"
      },
      {
        "surname": "Yang",
        "given_name": "Fuzheng"
      },
      {
        "surname": "Wan",
        "given_name": "Shuai"
      }
    ]
  },
  {
    "title": "Volume preserving image segmentation with entropy regularized optimal transport and its applications in deep learning",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102845",
    "abstract": "Image segmentation with a volume constraint is an important prior for many real applications. In this work, we present a novel volume preserving image segmentation algorithm, which is based on the entropy and Total Variation (TV) regularized optimal transport theory. The volume and classification constraints can be regarded as two measures preserving constraints in the optimal transport. By studying the dual problem, we develop a simple but efficient dual algorithm for our model. Moreover, to be different from many variational based image segmentation algorithms, the proposed algorithm can be directly unrolled to a new Volume Preserving and TV regularized softmax (VPTV-softmax) layer for semantic segmentation in the popular Deep Convolution Neural Network (DCNN). The experiment results show that our proposed model is very competitive and can improve the performance of many semantic segmentation networks such as the popular U-net and DeepLabv3+.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300961",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Constraint (computer-aided design)",
      "Convolutional neural network",
      "Deep learning",
      "Geometry",
      "Image segmentation",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Principle of maximum entropy",
      "Quantum mechanics",
      "Scale-space segmentation",
      "Segmentation",
      "Softmax function",
      "Volume (thermodynamics)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Haifeng"
      },
      {
        "surname": "Liu",
        "given_name": "Jun"
      },
      {
        "surname": "Cui",
        "given_name": "Li"
      },
      {
        "surname": "Huang",
        "given_name": "Haiyang"
      },
      {
        "surname": "Tai",
        "given_name": "Xue-Cheng"
      }
    ]
  },
  {
    "title": "Salient object detection via two-stage absorbing Markov chain based on background and foreground",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2019.102727",
    "abstract": "This paper proposes a saliency detection method via two-stage absorbing Markov chain based on background and foreground for detecting salient objects in images. Firstly, image preprocessing is performed, followed by convex hull construction and superpixel segmentation, to prepare for subsequent processing. Secondly, according to the boundary connectivity, the superpixels with lower background probability value in the candidate boundary background set B 0 are deleted, and the boundary background set B 1 is obtained. With the saliency values of the nodes in the boundary-prior saliency map S b g 1 , the background seeds are added appropriately in the region outside the candidate boundary background set B 0 and the convex hull H , and the background seed set B is obtained after update. Then, the background-absorbing Markov chain is constructed to generate background-absorbing saliency map S b g 2 . By fusing the saliency maps S b g 1 and S b g 2 , the first-stage background-based saliency map S bg is obtained. Thirdly, in the range of the convex hull H , the foreground seed set F is determined according to the saliency map S bg . Then, the foreground-absorbing Markov chain is constructed, to obtain the second-stage foreground-absorbing saliency map S fg . Finally, the saliency maps S bg and S fg of the two stages are combined to obtain a fused saliency map S , and the final saliency map S ∗ is obtained after optimization through smoothing mechanism. Compared with the traditional methods, the performance of the proposed method is significantly improved. The proposed method is tested on three public image datasets, and it shows great accuracy in detecting salient objects.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320319303487",
    "keywords": [
      "Absorbing Markov chain",
      "Artificial intelligence",
      "Background image",
      "Boundary (topology)",
      "Computer science",
      "Computer vision",
      "Convex hull",
      "Geometry",
      "Image (mathematics)",
      "Image segmentation",
      "Machine learning",
      "Markov chain",
      "Markov model",
      "Markov property",
      "Mathematical analysis",
      "Mathematics",
      "Object detection",
      "Pattern recognition (psychology)",
      "Preprocessor",
      "Programming language",
      "Regular polygon",
      "Saliency map",
      "Segmentation",
      "Set (abstract data type)",
      "Smoothing"
    ],
    "authors": [
      {
        "surname": "Tang",
        "given_name": "Wei"
      },
      {
        "surname": "Wang",
        "given_name": "Zhijian"
      },
      {
        "surname": "Zhai",
        "given_name": "Jiyou"
      },
      {
        "surname": "Yang",
        "given_name": "Zhangjing"
      }
    ]
  },
  {
    "title": "Protein secondary structure prediction based on integration of CNN and LSTM model",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102844",
    "abstract": "Protein structure prediction is an important issue in computational biology, and protein secondary structure prediction is the basis for protein three-dimensional structure prediction. A protein secondary structure prediction method based on convolutional neural networks (CNN) and Long Short-Term Memory (LSTM) is proposed in this paper. The architecture of CNN has two convolutional layers, one max-pooling layer and one ReLU activation layer. The feature maps extracted from second convolutional layer are used to feed to softmax classifier, and the first probability output is obtained. The LSTM model has a sequence layer and a last layer. The feature is extracted from last layer and input to random forest classifier to get the second probability output. The two probabilistic outputs are weighted and integrated to obtain the prediction model EN-CSLR in this paper. Based on the advantages of integration of the two models, cross-validation experiments are performed on the 25pdb dataset, and Q 3 reaches 80.18%, which is higher than using only one model. The experimental results show that the features extracted from CNN and LSTM models can effectively improve the accuracy of protein secondary structure prediction.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032030095X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Chemistry",
      "Classifier (UML)",
      "Computer science",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Layer (electronics)",
      "Linguistics",
      "Machine learning",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pooling",
      "Probabilistic logic",
      "Random forest",
      "Softmax function"
    ],
    "authors": [
      {
        "surname": "Cheng",
        "given_name": "Jinyong"
      },
      {
        "surname": "Liu",
        "given_name": "Yihui"
      },
      {
        "surname": "Ma",
        "given_name": "Yuming"
      }
    ]
  },
  {
    "title": "Taguchi-TOPSIS based HOG parameter selection for complex background sign language recognition",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102834",
    "abstract": "This paper presents an approach to design Indian Sign Language (ISL) recognition system for complex background. In many applications, Histogram of Oriented Gradients (HOG) have been proved to be effective. However, it is observed that the choice of HOG parameters affects the feature vector size and its classification capability. The objective is to select the parameter values in order to have maximal accuracy at a minimal computational time and reduced feature vector size. A combined Taguchi and Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS) based decision-making technique is applied to determine the values of these parameters. Results show that the combined TOPSIS-Taguchi based technique is effective in selecting the parameter combination to get high overall performance. For the acquired ISL complex background dataset, the selected values of parameters are further used to obtain multi-level HOG resulting in the overall accuracy of 92% for 280 features.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300845",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Feature (linguistics)",
      "Feature selection",
      "Histogram",
      "Ideal solution",
      "Image (mathematics)",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Operations research",
      "Orthogonal array",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Sign (mathematics)",
      "Similarity (geometry)",
      "Statistics",
      "TOPSIS",
      "Taguchi methods",
      "Thermodynamics"
    ],
    "authors": [
      {
        "surname": "Joshi",
        "given_name": "Garima"
      },
      {
        "surname": "Singh",
        "given_name": "Sukhwinder"
      },
      {
        "surname": "Vig",
        "given_name": "Renu"
      }
    ]
  },
  {
    "title": "A novel fast intra mode decision for versatile video coding",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102849",
    "abstract": "Abstract The latest video coding standard Versatile Video Coding (VVC) obtains superior coding efficiency compared to the High Efficiency Video Coding (HEVC), which is achieved by incorporating more effective and complex new coding tools. In this paper, we propose a novel fast intra mode decision algorithm for VVC, including following two strategies: (1) the correlation between the optimal modes of the adjacent blocks and the modes selected in the rough modes decision (RMD) process is analyzed and applied to reduce the modes in the candidate list; (2) modes in the candidate list are sorted in ascending order according to the modes’ cost calculated in the RMD process. An early termination method is proposed for terminating the optimal prediction mode decision process based on this new order early. These two strategies are incorporated into intra coding to reduce the coding complexity. Since these two strategies do not add any additional computational complexity, the proposed fast algorithm can achieve more complexity reduction. The experimental results show that the complexity reduction of the proposed algorithm is up to 44.74% compared to VVC reference software VTM2.0, and averagely 30.59% encoding time saving with 0.86% BDBR increase.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301000",
    "keywords": [
      "Algorithm",
      "Algorithmic efficiency",
      "Coding (social sciences)",
      "Coding tree unit",
      "Computational complexity theory",
      "Computer engineering",
      "Computer science",
      "Decoding methods",
      "Mathematics",
      "Programming language",
      "Reference software",
      "Software",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Yamei"
      },
      {
        "surname": "Yu",
        "given_name": "Li"
      },
      {
        "surname": "Wang",
        "given_name": "Hongkui"
      },
      {
        "surname": "Li",
        "given_name": "Tiansong"
      },
      {
        "surname": "Wang",
        "given_name": "Shengwei"
      }
    ]
  },
  {
    "title": "High-quality face image generation based on generative adversarial networks",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2019.102719",
    "abstract": "Conventional face image generation using generative adversarial networks (GAN) is limited by the quality of generated images since generator and discriminator use the same backpropagation network. In this paper, we discuss algorithms that can improve the quality of generated images, that is, high-quality face image generation. In order to achieve stability of network, we replace MLP with convolutional neural network (CNN) and remove pooling layers. We conduct comprehensive experiments on LFW, CelebA datasets and experimental results show the effectiveness of our proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320319303402",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Artificial neural network",
      "Backpropagation",
      "Computer science",
      "Convolutional neural network",
      "Detector",
      "Discriminator",
      "Epistemology",
      "Face (sociological concept)",
      "Generative adversarial network",
      "Generative grammar",
      "Generator (circuit theory)",
      "Image (mathematics)",
      "Image quality",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Pooling",
      "Power (physics)",
      "Quality (philosophy)",
      "Quantum mechanics",
      "Social science",
      "Sociology",
      "Stability (learning theory)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Zhixin"
      },
      {
        "surname": "Pan",
        "given_name": "Xuhua"
      },
      {
        "surname": "Jiang",
        "given_name": "Shuhao"
      },
      {
        "surname": "Zhao",
        "given_name": "Peijun"
      }
    ]
  },
  {
    "title": "Radar remote sensing image retrieval algorithm based on improved Sobel operator",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2019.102720",
    "abstract": "Aiming at the time-consuming problem caused by large computational load of radar image retrieval, based on blocking histogram, Sobel edge detection operator and gray level co-occurrence matrix (GLCCM), new radar remote sensing image retrieval algorithm based on improved Sobel operator is proposed. Firstly, the Sobel edge detection algorithm is used to process the image, the edge image is acquired, the radar remote sensing image is analyzed from different angles, and then the different radar remote sensing images are transformed. Then, based on the above processing, Radar Remote Sensing Image Retrieval Algorithm is acquired; finally, the plurality of statistic of the matrix is recorded as a feature vector describing the radar image, and the image is retrieved according to the feature vector of the radar image. Through a large number of experiments, Radar Remote Sensing Image Retrieval algorithm can greatly reduce the retrieval time, and it also has a good retrieval effect for images with rich texture.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320319303414",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Edge detection",
      "Geology",
      "Histogram",
      "Image (mathematics)",
      "Image gradient",
      "Image processing",
      "Image retrieval",
      "Radar",
      "Radar imaging",
      "Remote sensing",
      "Sobel operator",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Guobin"
      },
      {
        "surname": "Jiang",
        "given_name": "Zhiyong"
      },
      {
        "surname": "Kamruzzaman",
        "given_name": "M.M."
      }
    ]
  },
  {
    "title": "Image understanding via learning weakly-supervised cross-modal semantic translation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102789",
    "abstract": "Fusing cross-modal features is significant for image understanding, which aims at describing objects inside an image by optimally combining multiple visual channels. In the literature, low-level based multimodal feature fusion have achieved impressive performance. However, the semantic gap is a big limitation, i.e., these methods cannot reflect the how humans perceive image semantic objects. Supervised learning-based methods require intolerably expensive manual labeling, which is not a good choice in practice. To alleviate these limitations, we present an image understanding method by learning weakly-supervised based cross-modal semantic translation. More specifically, we design a manifold embedding algorithm to automatically translate image-level text semantic labels into several pixel-level image regions. Subsequently, we leverage a three-level spatial pyramid model to extract both local and global features of objects from training images. Afterwards, these cross-modal features are seamlessly concatenated to form a multiple feature matrix. Afterwards, these cross-modal features are seamlessly concatenated to form a multiple feature matrix. The feature matrix can be employed to learn a kernel SVM and ranking SVM for image classification and retrieval respectively. Comprehensive experiments on image recognition, classification and retrieval have demonstrated the effectiveness of our method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300390",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Combinatorics",
      "Computer science",
      "Embedding",
      "Feature (linguistics)",
      "Feature extraction",
      "Image (mathematics)",
      "Image retrieval",
      "Image translation",
      "Kernel (algebra)",
      "Leverage (statistics)",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Modal",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Polymer chemistry",
      "Semantic feature",
      "Semantic gap",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Shen",
        "given_name": "Guorong"
      }
    ]
  },
  {
    "title": "E-commerce personalized recommendation analysis by deeply-learned clustering",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2019.102735",
    "abstract": "With the development of Internet, personalized recommendation has played an important role in human modern lives. Since the number of users’ data is always large-scale, traditional algorithms cannot effectively cope with e-commerce personalized recommendation tasks. This paper proposes an e-commerce product personalized recommendation system based on learning clustering representation. Traditional kNN method has limitation in selecting adjacent object set. Thus, we introduce neighbor factor and time function and leverage dynamic selection model to select the adjacent object set. We combine RNN as well as attention mechanism to design the e-commerce product recommendation system. Comprehensive experimental results have shown the effectiveness of our proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320319303566",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "E-commerce",
      "Information retrieval",
      "Recommender system",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Kai"
      },
      {
        "surname": "Zhang",
        "given_name": "Tiantian"
      },
      {
        "surname": "Xue",
        "given_name": "Tianqiao"
      },
      {
        "surname": "Lu",
        "given_name": "Yu"
      },
      {
        "surname": "Na",
        "given_name": "Sang-Gyun"
      }
    ]
  },
  {
    "title": "Quality-related English text classification based on recurrent neural network",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2019.102724",
    "abstract": "With the rapid development of artificial intelligence technology, text categorization technology is becoming more and more mature. However, text categorization in real situations still faces various unconstrained conditions. English text is an important part of text information, it is also an important way for people to get information from abroad. How can everyone get the desired content from the massive data quickly and accurately, it has become a hot issue in current research. This paper improves the current text categorization algorithm based on English quality-related text categorization. The design and implementation of text categorization system are illustrated with an example of English quality-related text categorization system, complete the research work of text categorization algorithm. The core work of this paper is to mine, classify and analyze large amounts of data in English text by using the method of combining cyclic neural network with quality. Finally, the essential features of high quality English texts are obtained. Traditional English text categorization algorithm if the amount of training data is large, it is easy to show some defects such as unclear feature items. In view of these problems, in order to improve the accuracy and flexibility of English text categorization, this paper proposes a quality-related English text categorization method based on cyclic neural network. A mechanism combining attention is proposed to improve the problem of label disorder and make the structure of the model more flexible. The model proposed in this paper is compared and optimized. Experiments show that the accuracy of neural text classification based on quality classification can reach about 96%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320319303451",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Categorization",
      "Computer science",
      "Epistemology",
      "Feature (linguistics)",
      "Flexibility (engineering)",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Natural language processing",
      "Philosophy",
      "Quality (philosophy)",
      "Statistics",
      "Text categorization"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Cheng"
      },
      {
        "surname": "Wang",
        "given_name": "Xiaofang"
      }
    ]
  },
  {
    "title": "Video action recognition based on visual rhythm representation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102771",
    "abstract": "Advances in video acquisition and storage technologies have promoted a great demand for automatic recognition of actions. The use of cameras for security and surveillance purposes has applications in several scenarios, such as airports, parks, banks, stations, roads, hospitals, supermarkets, industries, stadiums, schools. An inherent difficulty of the problem is the complexity of the scene under usual recording conditions, which may contain complex background and motion, multiple people on the scene, interactions with other actors or objects, and camera motion. Most recent databases are built primarily with shared recordings on YouTube and with snippets of movies, situations where these obstacles are not restricted. Another difficulty is the impact of the temporal dimension since it expands the size of the data, increasing computational cost and storage space. In this work, we present a methodology of volume description using the Visual Rhythm (VR) representation. This technique reshapes the original volume of the video into an image, where two-dimensional descriptors are computed. We investigated different strategies for constructing the representation by combining configurations in several image domains and traversing directions of the video frames. From this, we propose two feature extraction methods, Naïve Visual Rhythm (Naïve VR) and Visual Rhythm Trajectory Descriptor (VRTD). The first approach is the straightforward application of the technique in the original video volume, forming a holistic descriptor that considers action events as patterns and formats in the visual rhythm image. The second variation focuses on the analysis of small neighborhoods obtained from the process of dense trajectories, which allows the algorithm to capture details unnoticed by the global description. We tested our methods in eight public databases, one of hand gestures (SKIG), two in first person (DogCentric and JPL), and five in third person (Weizmann, KTH, MuHAVi, UCF11 and HMDB51). The results show that the developed techniques are able to extract motion elements along with format and appearance information, achieving competitive accuracy rates compared to state-of-the-art action recognition approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300213",
    "keywords": [
      "Action (physics)",
      "Aesthetics",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Dimension (graph theory)",
      "Feature (linguistics)",
      "Geodesy",
      "Geography",
      "Law",
      "Linguistics",
      "Mathematics",
      "Motion (physics)",
      "Operating system",
      "Philosophy",
      "Physics",
      "Political science",
      "Politics",
      "Process (computing)",
      "Pure mathematics",
      "Quantum mechanics",
      "Representation (politics)",
      "Rhythm",
      "Traverse",
      "Volume (thermodynamics)"
    ],
    "authors": [
      {
        "surname": "Moreira",
        "given_name": "Thierry Pinheiro"
      },
      {
        "surname": "Menotti",
        "given_name": "David"
      },
      {
        "surname": "Pedrini",
        "given_name": "Helio"
      }
    ]
  },
  {
    "title": "Structured feature sparsity training for convolutional neural network compression",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102867",
    "abstract": "Convolutional neural networks (CNNs) with large model size and computing operations are difficult to be deployed on embedded systems, such as smartphones or AI cameras. In this paper, we propose a novel structured pruning method, termed the structured feature sparsity training (SFST), to speed up the inference process and reduce the memory usage of CNNs. Unlike other existing pruning methods, which require multiple iterations of pruning and retraining to ensure stable performance, SFST only needs to fine-tune the pretrained model with additional regularization on the less important features and then prune them, no multiple pruning and retraining needed. SFST can be deployed to a variety of modern CNN architectures including VGGNet, ResNet and MobileNetv2. Experimental results on CIFAR, SVHN, ImageNet and MSTAR benchmark dataset demonstrate the effectiveness of our scheme, which achieves superior performance over the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301176",
    "keywords": [
      "Agronomy",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Business",
      "Computer science",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Geodesy",
      "Geography",
      "Inference",
      "International trade",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pruning",
      "Regularization (linguistics)",
      "Retraining",
      "Scheme (mathematics)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Wei"
      },
      {
        "surname": "Zhu",
        "given_name": "Liqiang"
      }
    ]
  },
  {
    "title": "Multimodal hand gesture recognition combining temporal and pose information based on CNN descriptors and histogram of cumulative magnitudes",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102772",
    "abstract": "In this paper, we present a new approach for dynamic hand gesture recognition. Our goal is to integrate spatiotemporal features extracted from multimodal data captured by the Kinect sensor. In case the skeleton data is not provided, we apply a novel skeleton estimation method to compute temporal features. Furthermore, we introduce an effective method to extract a fixed number of keyframes to reduce the processing time. To extract pose features from RGB-D data, we take advantage of two different approaches: (1) Convolutional Neural Networks and (2) Histogram of Cumulative Magnitudes. We test different integration methods to fuse the extracted spatiotemporal features to boost recognition performance in a linear SVM classifier. Extensive experiments prove the effectiveness and feasibility of the proposed framework for hand gesture recognition.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300225",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Gesture",
      "Gesture recognition",
      "Histogram",
      "Image (mathematics)",
      "Pattern recognition (psychology)",
      "RGB color model",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Escobedo Cardenas",
        "given_name": "Edwin Jonathan"
      },
      {
        "surname": "Chavez",
        "given_name": "Guillermo Camara"
      }
    ]
  },
  {
    "title": "Candidate region correlation for video action detection",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102818",
    "abstract": "The rapid development of deep learning has prompted the development of video action detection technology. However, the accuracy of current video action detection algorithms can be improved further. Previous work has improved feature extraction by optimizing the network structure. In addition, the features of the candidate regions have been optimized by changing the representation of the regions. Although these methods have achieved promising results, they fail to consider the correlation among different candidate regions, generating uninformative (even redundant) candidate regions, and thus usually decrease the detection performance in practice. To address this problem, in this paper we propose a self-attention mechanism for candidate regions, which can help pursue the most informative regions. We obtain the region correlation by simultaneously determining the spatial and temporal correlation among different candidate regions. In addition, we focus on how to apply the correlation to optimize the original candidate region features and improve video action detection accuracy. The experimental results show the promising improvement achieved by our method over the state-of-the-art solutions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300687",
    "keywords": [
      "Action (physics)",
      "Artificial intelligence",
      "Computer science",
      "Correlation",
      "Data mining",
      "Feature (linguistics)",
      "Feature extraction",
      "Focus (optics)",
      "Geometry",
      "Law",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Political science",
      "Politics",
      "Quantum mechanics",
      "Representation (politics)",
      "Spatial correlation",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Yeguang"
      },
      {
        "surname": "Zhang",
        "given_name": "Mingyuan"
      },
      {
        "surname": "Hu",
        "given_name": "Liang"
      },
      {
        "surname": "Li",
        "given_name": "Jun"
      },
      {
        "surname": "Wang",
        "given_name": "Deqing"
      }
    ]
  },
  {
    "title": "Neural machine translation with Gumbel Tree-LSTM based encoder",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102811",
    "abstract": "Neural machine translation has improved the translation accuracy greatly and received great attention of the machine translation community. Tree-based translation models aim to model the syntactic or semantic relation among long-distance words or phrases in a sentence. However, it faces the difficulties of expensive manual annotation cost and poor automatic annotation accuracy. In this paper, we focus on how to encode a source sentence into a vector in a unsupervised-tree way and then decode it into a target sentence. Our model incorporates Gumbel Tree-LSTM, which can learn how to compose tree structures from plain text without any tree annotation. We evaluate the proposed model on both spoken and news corpora, and show that the performance of our proposed model outperforms the attentional seq2seq model and the Transformer base model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300614",
    "keywords": [
      "Algorithm",
      "Annotation",
      "Artificial intelligence",
      "Binary tree",
      "Computer science",
      "Encoder",
      "Machine translation",
      "Mathematical analysis",
      "Mathematics",
      "Natural language processing",
      "Operating system",
      "Physics",
      "Quantum mechanics",
      "Sentence",
      "Speech recognition",
      "Transformer",
      "Tree (set theory)",
      "Tree structure",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Su",
        "given_name": "Chao"
      },
      {
        "surname": "Huang",
        "given_name": "Heyan"
      },
      {
        "surname": "Shi",
        "given_name": "Shumin"
      },
      {
        "surname": "Jian",
        "given_name": "Ping"
      },
      {
        "surname": "Shi",
        "given_name": "Xuewen"
      }
    ]
  },
  {
    "title": "An image similarity descriptor for classification tasks",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102847",
    "abstract": "We develop an image similarity descriptor for an image pair, based on deep features. The development consists of two parts - selecting the deep layer whose features are to be included in the descriptor, and a representation of the similarity between the images in the pair. The selection of the deep layer follows a sparse representation of the feature maps followed by multi-output support vector regression. The similarity representation is based on a novel correlation between the histograms of the feature maps of the two images. Experiments to demonstrate the effectiveness of the proposed descriptor are carried out on four applications that can be cast as classification tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300985",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Contextual image classification",
      "Image (mathematics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Liangliang"
      },
      {
        "surname": "Rajan",
        "given_name": "Deepu"
      }
    ]
  },
  {
    "title": "Weakly-supervised large-scale image modeling for sport scenes and its applications",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2019.102718",
    "abstract": "Image modeling towards sport scenes plays an important role in sport image classification and analysis. Traditional algorithms for sport image modeling required carefully hand-crafted features, which cannot be popularized in practical application, especially with the emergence of massive-scale data. Weakly-supervised learning algorithms have shown effectiveness in modeling data with image-level labels. Thus, in this paper, we propose a weakly-supervised learning based method for sport image modeling without utilizing bounding box annotations, which can be used for various sport image applications. More specifically, we first collect large-scale sport images from existing datasets and Internet, and we annotate them at image-level labels. Subsequently, we leverage region proposal generation algorithm to select discriminative regions that can effectively represent the category of images. Each region is fed into a pre-trained CNN architecture to extract deep representation. Afterwards, we design an improved multiple discriminant analysis (MDA) algorithm to project these datapoints to a subspace that can more easily to distinguish different sport categories. Comprehensive experiments have shown the effectiveness and robustness of our proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320319303396",
    "keywords": [
      "Artificial intelligence",
      "Cartography",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Geography",
      "Image (mathematics)",
      "Mathematics",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "Lu",
        "given_name": "Congsheng"
      },
      {
        "surname": "Zhai",
        "given_name": "Feng"
      }
    ]
  },
  {
    "title": "An improved DCT-based JND estimation model considering multiple masking effects",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102850",
    "abstract": "The just noticeable distortion (JND) in the contour and orderly regions is easy to be overestimated and that in the disorderly areas is usually underestimated. In order to estimate the JND threshold more accurately, this paper proposes an improved DCT-based JND estimation model considering multiple masking effects properly. The contributions of this paper are characterized by twofold. On the one hand, a mean absolute difference based (MAD-based) block classification method is developed at first to classify image blocks into plain, contour and texture types accurately and quickly. And the JND model for contrast masking effect (CM-JND) is constructed as a modulation factor based on the MAD of each block. On the other hand, we propose a distance-based disorder evaluation metric to measure the disorder intensity in block level. Then, the JND model for the disorderly concealment effect (DC-JND) is proposed based on our psychological experiment. Finally, the total JND estimation threshold is modeled by fusing the spatial contrast sensitivity function, the luminance adaptation effect, the CM and DC effects. Experimental results show that the proposed DCT-based JND estimation model outperforms existing models in performance and complexity. Specifically, the proposed model shows more tolerance for distortions, lower computational complexity with better perceptual quality than other JND models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320301012",
    "keywords": [
      "Amplifier",
      "Art",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Block (permutation group theory)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Contrast (vision)",
      "Discrete cosine transform",
      "Distortion (music)",
      "Economics",
      "Geometry",
      "Image (mathematics)",
      "Just-noticeable difference",
      "Luminance",
      "Masking (illustration)",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Hongkui"
      },
      {
        "surname": "Yu",
        "given_name": "Li"
      },
      {
        "surname": "Yin",
        "given_name": "Haibing"
      },
      {
        "surname": "Li",
        "given_name": "Tiansong"
      },
      {
        "surname": "Wang",
        "given_name": "Shengwei"
      }
    ]
  },
  {
    "title": "Quality-guided video aesthetics assessment with social media context",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2019.102643",
    "abstract": "Media aesthetic assessment is a key technique in computer vision, which is widely applied in computer game rendering, video/image classification. Low-level and high-level features fusion-based video aesthetic assessment algorithms have achieved impressive performance, which outperform photo- and motion-based algorithms, however, these methods only focus on aesthetic features of single-frame while ignore the inherent relationship between adjacent frames. Therefore, we propose a novel video aesthetic assessment framework, where structural cues among frames are well encoded. Our method consists of two components: aesthetic features extraction and structure correlation construction. More specifically, we incorporate both low-level and high-level visual features to construct aesthetic features, where salient regions are extracted for content understanding. Subsequently, we develop a structure correlation-based algorithm to evaluate the relationship among adjacent frames, where frames with similar structure property should have a strong correlation coefficient. Afterwards, a kernel multi-SVM is trained for video classification and high aesthetic video selection. Comprehensive experiments demonstrate the effectiveness of our method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320319302640",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Correlation",
      "Economics",
      "Frame (networking)",
      "Geometry",
      "Key frame",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Rendering (computer graphics)",
      "Salient",
      "Support vector machine",
      "Telecommunications",
      "Video quality"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Chao"
      },
      {
        "surname": "Liu",
        "given_name": "Sitong"
      },
      {
        "surname": "Li",
        "given_name": "Huizi"
      }
    ]
  },
  {
    "title": "No-reference stereoscopic images quality assessment method based on monocular superpixel visual features and binocular visual features ☆",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102848",
    "abstract": "No-reference quality assessment of images has received considerable attention. However, the accuracy of such assessment remains questionable because of its weak biological basis. In this paper, we propose a novel quality assessment model based on the superpixel index and biological binocular mechanisms. The technical contributions of our model are the introduction of local monocular superpixel features and three global binocular visual features. We utilize monocular superpixel segmentation to extract two types of entropies as the local visual features for accurate quality-aware feature extraction. In addition, natural scene statistics features are extracted from the binocular visual information to complement the local monocular features and quantify the naturalness of the stereoscopic images. Finally, a regression model is learned to evaluate the quality of the stereoscopic images. Experimental results from three popular databases demonstrate that the proposed model has a more reliable performance than earlier models in terms of prediction accuracy and generalizability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300997",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Generalizability theory",
      "Linguistics",
      "Mathematics",
      "Monocular",
      "Naturalness",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Segmentation",
      "Statistics",
      "Stereoscopy"
    ],
    "authors": [
      {
        "surname": "Zheng",
        "given_name": "Zhi"
      },
      {
        "surname": "Liu",
        "given_name": "Yun"
      },
      {
        "surname": "Liu",
        "given_name": "Yun"
      },
      {
        "surname": "Huang",
        "given_name": "Baoqing"
      },
      {
        "surname": "Yu",
        "given_name": "Hongwei"
      }
    ]
  },
  {
    "title": "Generating video animation from single still image in social media based on intelligent computing",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102812",
    "abstract": "Bringing a single still image into reality is a challenging topic in computer animation because the driven and structural information in single still image is inadequate. In this paper, we present an image animating method for enhancing single still image in social media with virtual realistic and animated motions without prior information. We imitate the interaction between the active objects in an image and their neighboring passive objects. The existing actions in the image and the virtual specified force are employed to animate the active objects. Observing that the change between two subsequent motions of the active objects derives a motion tendency, we can calculate a virtual driving force based on the motion tendency. By virtue of the virtual driving force, the stochastic motion texture is used to animate the passive objects. Finally, the convolutional neural network is employed to optimize the virtual motion animations. In this way, the proposed method produces visually natural results while guaranteeing motion harmony between active objects and passive objects. To demonstrate the applicability and rationality of virtual animation driving force, our method generates several animations from still images in Social Media.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300626",
    "keywords": [
      "Animation",
      "Artificial intelligence",
      "Computer animation",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Motion (physics)",
      "Virtual image",
      "Virtual reality"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Tao"
      },
      {
        "surname": "Liang",
        "given_name": "Chao"
      },
      {
        "surname": "Min",
        "given_name": "Geyong"
      },
      {
        "surname": "Li",
        "given_name": "Keqin"
      },
      {
        "surname": "Xiao",
        "given_name": "Chunxia"
      }
    ]
  },
  {
    "title": "An embedding strategy on fusing multiple image features for data hiding in multiple images",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102822",
    "abstract": "Data hiding in multiple images has been a significant research direction in information security. How to reasonably design the embedding strategy to spread the payload among multiple images is still an open issue. In this paper, we propose an embedding strategy on fusing multiple features. We utilize the typical characteristic parameters of gray level co-occurrence matrix, the image entropy and the shape parameter to describe image complexity. Furthermore, we combine with the number of cover images, the number of cover images assigned to steganographer and the size of cover image to estimate the steganographic capacity of each image. The strategy is implemented together with some state-of-the-art single image steganographic algorithms. Experimental results demonstrate that the security performance of the proposed strategy is higher than that of the state-of-the-art embedding strategy against the blind universal pooled steganalysis.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300729",
    "keywords": [
      "Artificial intelligence",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Cover (algebra)",
      "Embedding",
      "Engineering",
      "Entropy (arrow of time)",
      "Image (mathematics)",
      "Information hiding",
      "Mathematics",
      "Mechanical engineering",
      "Network packet",
      "Pattern recognition (psychology)",
      "Payload (computing)",
      "Physics",
      "Quantum mechanics",
      "Steganalysis",
      "Steganography"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Junxue"
      },
      {
        "surname": "Liao",
        "given_name": "Xin"
      }
    ]
  },
  {
    "title": "Analysis of financial business model towards big data and its applications",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2019.102729",
    "abstract": "Finance service based on big data faces many issues, such as fraud, credit. In this paper, we study the development of financial business model under the big data. We first analyze the impact mechanism of big data finance on customer information protection of commercial banks. Customer information has the characteristics of large amount of information, high value of data and strong destructive data leakage. Then, we propose two solutions towards issues of finance service including face anti-spoofing algorithm and financial risk evaluation. Experiments show the effectiveness of our proposed method in improving the reliability and security of modern big data finance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320319303505",
    "keywords": [
      "Big data",
      "Business",
      "Business model",
      "Computer science",
      "Data mining",
      "Face (sociological concept)",
      "Finance",
      "Financial services",
      "Marketing",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Reliability (semiconductor)",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yupeng"
      }
    ]
  },
  {
    "title": "Hierarchical learning using deep optimum-path forest",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102823",
    "abstract": "Bag-of-Visual Words (BoVW) and deep learning techniques have been widely used in several domains, which include computer-assisted medical diagnoses. In this work, we are interested in developing tools for the automatic identification of Parkinson’s disease using machine learning and the concept of BoVW. The proposed approach concerns a hierarchical-based learning technique to design visual dictionaries through the Deep Optimum-Path Forest classifier. The proposed method was evaluated in six datasets derived from data collected from individuals when performing handwriting exams. Experimental results showed the potential of the technique, with robust achievements.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300730",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Deep learning",
      "Handwriting",
      "Machine learning",
      "Medical diagnosis",
      "Medicine",
      "Path (computing)",
      "Pathology",
      "Pattern recognition (psychology)",
      "Programming language"
    ],
    "authors": [
      {
        "surname": "Afonso",
        "given_name": "Luis C.S."
      },
      {
        "surname": "Pereira",
        "given_name": "Clayton R."
      },
      {
        "surname": "Weber",
        "given_name": "Silke A.T."
      },
      {
        "surname": "Hook",
        "given_name": "Christian"
      },
      {
        "surname": "Falcão",
        "given_name": "Alexandre X."
      },
      {
        "surname": "Papa",
        "given_name": "João P."
      }
    ]
  },
  {
    "title": "Hierarchical learning using deep optimum-path forest",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102823",
    "abstract": "Bag-of-Visual Words (BoVW) and deep learning techniques have been widely used in several domains, which include computer-assisted medical diagnoses. In this work, we are interested in developing tools for the automatic identification of Parkinson’s disease using machine learning and the concept of BoVW. The proposed approach concerns a hierarchical-based learning technique to design visual dictionaries through the Deep Optimum-Path Forest classifier. The proposed method was evaluated in six datasets derived from data collected from individuals when performing handwriting exams. Experimental results showed the potential of the technique, with robust achievements.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300730",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Deep learning",
      "Handwriting",
      "Machine learning",
      "Medical diagnosis",
      "Medicine",
      "Path (computing)",
      "Pathology",
      "Pattern recognition (psychology)",
      "Programming language"
    ],
    "authors": [
      {
        "surname": "Afonso",
        "given_name": "Luis C.S."
      },
      {
        "surname": "Pereira",
        "given_name": "Clayton R."
      },
      {
        "surname": "Weber",
        "given_name": "Silke A.T."
      },
      {
        "surname": "Hook",
        "given_name": "Christian"
      },
      {
        "surname": "Falcão",
        "given_name": "Alexandre X."
      },
      {
        "surname": "Papa",
        "given_name": "João P."
      }
    ]
  },
  {
    "title": "Driver fatigue detection based on deeply-learned facial expression representation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2019.102723",
    "abstract": "Driver fatigue detection is a significant application in smart cars. In order to improve the accuracy and timeliness of driver fatigue detection, a fatigue detection algorithm based on deeply-learned facial expression analysis is proposed. Specifically, the face key point detection model is first trained by multi block local binary patterns (MB-LBP) and Adaboost classifier. Subsequently, the eyes and mouth state are detected by using the trained model to detect the 24 facial features. Afterwards, we calculate the number of two parameters that can describe the driver's fatigue state and the proportion of the closed eye time within the unit time (PERCLOS) and yawning frequency. Finally, the fuzzy inference system is utilized to deduce the driver's fatigue state (normal, slight fatigue, severe fatigue). Experimental results show that the proposed algorithm can detect driver fatigue degree quickly and accurately.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732031930344X",
    "keywords": [
      "AdaBoost",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Computer vision",
      "Face (sociological concept)",
      "Face detection",
      "Facial expression",
      "Facial recognition system",
      "Histogram",
      "Image (mathematics)",
      "Local binary patterns",
      "Pattern recognition (psychology)",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Zhongmin"
      },
      {
        "surname": "Peng",
        "given_name": "Yuxi"
      },
      {
        "surname": "Hu",
        "given_name": "Wenjin"
      }
    ]
  },
  {
    "title": "An extension of the differential image foresting transform and its application to superpixel generation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2019.102748",
    "abstract": "The Image Foresting Transform (IFT) is a graph-based framework to develop image operators based on optimum connectivity between a root set and the remaining nodes, according to a given path-cost function. Its applications involve a variety of tasks, such as segmentation, boundary tracking, skeletonization, filtering, among others. The Differential Image Foresting Transform (DIFT) allows multiple IFT executions for different root sets and a same monotonically incremental path-cost function, making the processing time proportional to the number of modified nodes. In this paper, we extend the DIFT algorithm for non-monotonically incremental functions with root-based increases. This proposed extension, called Generalized DIFT (GDIFT), has been successfully used as the core part of some modern superpixels methods with state-of-the-art results. Experimental results show considerable efficiency gains over the sequential flow of IFTs for the generation of superpixels, also avoiding inconsistencies in image segmentation, which could occur with the regular DIFT algorithm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320319303694",
    "keywords": [
      "Aerospace engineering",
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Differential (mechanical device)",
      "Engineering",
      "Evolutionary biology",
      "Extension (predicate logic)",
      "Function (biology)",
      "Image (mathematics)",
      "Image segmentation",
      "Mathematical analysis",
      "Mathematics",
      "Monotonic function",
      "Path (computing)",
      "Pattern recognition (psychology)",
      "Programming language",
      "Segmentation",
      "Skeletonization"
    ],
    "authors": [
      {
        "surname": "Condori",
        "given_name": "Marcos A.T."
      },
      {
        "surname": "Cappabianco",
        "given_name": "Fábio A.M."
      },
      {
        "surname": "Falcão",
        "given_name": "Alexandre X."
      },
      {
        "surname": "Miranda",
        "given_name": "Paulo A.V."
      }
    ]
  },
  {
    "title": "Automated standardization of images of Drosophila embryos",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102758",
    "abstract": "Modeling expression patterns of Drosophila, in space and time, plays a critical role to understand the development of multicellular organisms. In confocal microscopy, to produce precise quantitative data it is frequently necessary to process and analyze large amounts of digital images. Automatic preprocessing is a crucial step in this scenario, essential to standardize significant features such as orientation, size, position, direction, lighting condition and texture of embryo images. Even though a lot of efforts have been made, a robust embryo standardization strategy is still needed. In this paper, we propose the method Embrystandar. It is designed to remove background artifacts and standardize the direction and orientation of a Drosophila embryo through a sequence of automatic operations. To test its potential for large-scale image processing, Embrystandar was applied in different databases. It showed to be robust and precise, reaching more than 90% success rate.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300080",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Cell biology",
      "Computer science",
      "Computer vision",
      "Drosophila (subgenus)",
      "Embryo",
      "Gene",
      "Genetics",
      "Geometry",
      "Mathematics",
      "Operating system",
      "Orientation (vector space)",
      "Pattern recognition (psychology)",
      "Preprocessor",
      "Process (computing)",
      "Standardization"
    ],
    "authors": [
      {
        "surname": "de Sousa",
        "given_name": "Daniela Justiniano"
      },
      {
        "surname": "Cardoso",
        "given_name": "Maira Arruda"
      },
      {
        "surname": "Bisch",
        "given_name": "Paulo Mascarello"
      },
      {
        "surname": "Lopes",
        "given_name": "Francisco José Pereira"
      },
      {
        "surname": "Travençolo",
        "given_name": "Bruno Augusto Nassif"
      }
    ]
  },
  {
    "title": "BURSTS: A bottom-up approach for robust spotting of texts in scenes",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102843",
    "abstract": "In this paper, we present a bottom-up approach for robust spotting of texts in scenes. In the proposed technique, character candidates are first detected using our proposed character detector, which leverages on the strengths of an Extremal Region (ER) detector and an Aggregate Channel Feature (ACF) detector for high character detection recall. The real characters are then identified by using a novel convolutional neural network (CNN) filter for high character detection precision. A hierarchical clustering algorithm is designed which combines multiple visual and geometrical features to group characters into word proposal regions for word recognition. The proposed technique has been evaluated on several scene text spotting datasets and experiments show superior spotting performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300948",
    "keywords": [
      "Aggregate (composite)",
      "Artificial intelligence",
      "Character (mathematics)",
      "Cluster analysis",
      "Composite material",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Detector",
      "Feature (linguistics)",
      "Filter (signal processing)",
      "Geometry",
      "Keyword spotting",
      "Linguistics",
      "Materials science",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Speech recognition",
      "Spotting",
      "Telecommunications",
      "Word (group theory)"
    ],
    "authors": [
      {
        "surname": "Fan",
        "given_name": "Jiayuan"
      },
      {
        "surname": "Chen",
        "given_name": "Tao"
      },
      {
        "surname": "Zhou",
        "given_name": "Feng"
      }
    ]
  },
  {
    "title": "Hyperspectral image quality based on convolutional network of multi-scale depth",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2019.102721",
    "abstract": "Hyperspectral imagery has been widely used in military and civilian research fields such as crop yield estimation, mineral exploration, and military target detection. However, for the limited imaging equipment and the complex imaging environment of hyperspectral images, the spatial resolution of hyperspectral images is still relatively low, which limits the application of hyperspectral images. So, studying the data characteristics of hyperspectral images deeply and improving the spatial resolution of hyperspectral images is an important prerequisite for accurate interpretation and wide application of hyperspectral images. The purpose of this paper is to deal with super-resolution of the hyperspectral image quickly and accurately, and maintain the spectral characteristics of the hyperspectral image, makes the spectral separability of the substrate in the original image remains unchanged after super-resolution processing. This paper first learns the mapping relationship between the spectral difference of low-resolution hyperspectral image and the spectral difference of the corresponding high-resolution hyperspectral image based on multiple scale convolutional neural network, Thus, apply this mapping relationship to the input low-resolution hyperspectral image generally, getting the corresponding high resolution spectral difference. Constrained space by using the image of reconstructed spectral difference, this requires the low-resolution hyperspectral image generated by the reconstructed image is to be close to the input low-resolution hyperspectral image in space, so that the whole process becomes a closed circulation system where the low-resolution hyperspectral image generation of high-resolution hyperspectral images, then back to low-resolution hyperspectral images. This innovative design further enhances the super-resolution performance of the algorithm. The experimental results show that the hyperspectral image super-resolution method based on convolutional neural network improves the input image spatial information, and the super-resolution performance of the model is above 90%, which can maintain the spectral information well.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320319303426",
    "keywords": [
      "Artificial intelligence",
      "Astronomy",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Full spectral imaging",
      "Geography",
      "Hyperspectral imaging",
      "Image resolution",
      "Pattern recognition (psychology)",
      "Physics",
      "Remote sensing",
      "Resolution (logic)",
      "Spectral line",
      "Spectral resolution"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Lei"
      },
      {
        "surname": "Sun",
        "given_name": "Min"
      },
      {
        "surname": "Ren",
        "given_name": "Xiang"
      },
      {
        "surname": "Li",
        "given_name": "Xiuxian"
      },
      {
        "surname": "Zhang",
        "given_name": "Qiaoru"
      },
      {
        "surname": "Ma",
        "given_name": "Li"
      },
      {
        "surname": "Li",
        "given_name": "Yongning"
      },
      {
        "surname": "Song",
        "given_name": "Mo"
      }
    ]
  },
  {
    "title": "Histopathological image classification through discriminative feature learning and mutual information-based multi-channel joint sparse representation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102799",
    "abstract": "Histopathological image classification is a very challenging task because of the biological heterogeneities and rich geometrical structures. In this paper, we propose a novel histopathological image classification framework, which includes the discriminative feature learning and the mutual information-based multi-channel joint sparse representation. We first propose a stack-based discriminative prediction sparse decomposition (SDPSD) model by incorporating the class labels information to predict deep discriminant features automatically. Subsequently, a mutual information-based multi-channel joint sparse model (MIMCJSM) is presented to jointly encode the common component and particular components of the discriminative features. Especially, the main advantage of the MIMCJSM is the construction of a joint dictionary using a mutual information criterion, which contains a common sub-dictionary and three particular sub-dictionaries. Based on the joint dictionary, the MIMCJSM captures the relationship of multi-channel features, which can improve discriminative ability of joint sparse representation coefficients. Finally, the joint sparse representation coefficients of different levels can be aggregated using the spatial pyramid matching (SPM) model, and the linear support vector machine (SVM) is used as the classifier. Experimental results on ADL and BreaKHis datasets demonstrate that our proposed framework consistently performs better than popular existing classification frameworks. Additionally, it can show promising strong-robustness performance for histopathological image classification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300493",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Classifier (UML)",
      "Computer science",
      "Contextual image classification",
      "Discriminative model",
      "Feature (linguistics)",
      "Feature extraction",
      "Gene",
      "Image (mathematics)",
      "Linguistics",
      "Mutual information",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Robustness (evolution)",
      "Sparse approximation",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Xiao."
      },
      {
        "surname": "Tang",
        "given_name": "Hongzhong."
      },
      {
        "surname": "Zhang",
        "given_name": "Dongbo."
      },
      {
        "surname": "Liu",
        "given_name": "Ting."
      },
      {
        "surname": "Mao",
        "given_name": "Lizhen."
      },
      {
        "surname": "Chen",
        "given_name": "Tianyu."
      }
    ]
  },
  {
    "title": "Chronological pattern indexing: An efficient feature extraction method for hand gesture recognition with Leap Motion",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102842",
    "abstract": "Recently, Hand-Gesture-Recognition (HGR) systems has appreciably change the way of interaction between humans and computers thanks to advanced sensor technologies like the Leap-Motion-Controller (LMC). Despite the success achieved by many state-of-the-art methods, they have not worked on the rich temporal information existing in the sequential hand gesture data and characterizing the discriminative representation of different hand gesture classes. In this paper, we suggest a novel Chronological-Pattern-Indexing (CPI) approach which encodes the temporal orders of patterns for hand gesture time series data acquired by the LMC sensor. We extract a set of temporal patterns from different optimized projections. Then, we compare their temporal order and we encode the whole sequence with the index of the first coming pattern. We repeat these steps until we generate an efficient feature vector modeling the chronological dynamics of the hand gesture. The experiments demonstrate the potential of the proposed CPI approach for HGR systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300936",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Discriminative model",
      "ENCODE",
      "Feature (linguistics)",
      "Feature extraction",
      "Gene",
      "Gesture",
      "Gesture recognition",
      "Law",
      "Linguistics",
      "Motion (physics)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Search engine indexing",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Ameur",
        "given_name": "Safa"
      },
      {
        "surname": "Ben Khalifa",
        "given_name": "Anouar"
      },
      {
        "surname": "Bouhlel",
        "given_name": "Med Salim"
      }
    ]
  },
  {
    "title": "Probabilistic color visual cryptography schemes for black and white secret images",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102793",
    "abstract": "Color-black-and-white visual cryptography scheme (CBW-VCS) is a methodology that utilizes colors to alleviate the pixel expansion problem. In a general ( k , n ) CBW-VCS, when k and n become larger, the pixel expansion increases dramatically. In this paper, two constructions for constituting a ( k , n ) threshold probabilistic CBW-VCS (PCBW-VCS) are introduced, where the generated color shares are non-expansible. The two proposed constructions are proven to be valid constructions which satisfy the security and contrast conditions. Theoretical analysis and sufficient experiments are demonstrated to shown the effectiveness and advantages of the proposed PCBW-VCSs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300432",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Contrast (vision)",
      "Cryptography",
      "Gene",
      "Mathematics",
      "Pixel",
      "Probabilistic logic",
      "Secret sharing",
      "Theoretical computer science",
      "Visual cryptography",
      "White (mutation)"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Xiaotian"
      },
      {
        "surname": "Yang",
        "given_name": "Ching-Nung"
      }
    ]
  },
  {
    "title": "Low overhead spatiotemporal video compression over smartphone based Delay Tolerant Network",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102813",
    "abstract": "In this paper we provide a lightweight video compression scheme, Low Overhead Spatio-Temporal Video compression (Lost-Vision) scheme which is done through inter-frame and intra-frame compression. In inter-frame compression redundant frames are removed by a proposed interpolation search-based method and a lightweight edge detection technique. Then intra-frame compression is done by a proposed adaptive column dropping technique modifying an existing technique namely ICCD. At the receiver end, we propose two reconstruction filters targeting to improve reconstruction quality. Performance of our scheme in terms of energy efficiency and reconstruction quality is evaluated both theoretically and practically. In practical implementation, the proposed video compression scheme is assessed in a real environment with different terrains using a smartphones/tablet-based DTN-like network. A Comparison of our scheme with three recent works on video compression shows our scheme's dominance over the competing works with 52%, 45.6% and 53% energy in saving yet maintaining acceptable reconstruction quality.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300638",
    "keywords": [
      "Artificial intelligence",
      "Composite material",
      "Compression (physics)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Data compression",
      "Frame (networking)",
      "Inter frame",
      "Materials science",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Overhead (engineering)",
      "Real-time computing",
      "Reference frame",
      "Residual frame",
      "Scheme (mathematics)",
      "Video compression picture types",
      "Video processing",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Pal",
        "given_name": "Tamal"
      },
      {
        "surname": "Bit",
        "given_name": "Sipra Das"
      }
    ]
  },
  {
    "title": "PixelHop: A successive subspace learning (SSL) method for object recognition",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2019.102749",
    "abstract": "A new machine learning methodology, called successive subspace learning (SSL), is introduced in this work. SSL contains four key ingredients: (1) successive near-to-far neighborhood expansion; (2) unsupervised dimension reduction via subspace approximation; (3) supervised dimension reduction via label-assisted regression (LAG); and (4) feature concatenation and decision making. An image-based object classification method, called PixelHop, is proposed to illustrate the SSL design. It is shown by experimental results that the PixelHop method outperforms the classic CNN model of similar model complexity in three benchmarking datasets (MNIST, Fashion MNIST and CIFAR-10). Although SSL and deep learning (DL) have some high-level concept in common, they are fundamentally different in model formulation, the training process and training complexity. Extensive discussion on the comparison of SSL and DL is made to provide further insights into the potential of SSL.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320319303700",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Concatenation (mathematics)",
      "Deep learning",
      "Dimension (graph theory)",
      "Dimensionality reduction",
      "Feature (linguistics)",
      "Geometry",
      "Linguistics",
      "MNIST database",
      "Machine learning",
      "Mathematics",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pure mathematics",
      "Reduction (mathematics)",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Yueru"
      },
      {
        "surname": "Kuo",
        "given_name": "C.-C. Jay"
      }
    ]
  },
  {
    "title": "Adaptive single image dehazing method based on support vector machine",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102792",
    "abstract": "A dehazing method often only shows good results when processing the image for a certain haze concentration. So an adaptive hazy image dehazing method based on SVM is proposed. The innovation points are as follows: Firstly, combining the characteristics of the degraded images of haze weather, the dark channel histogram and texture features of the input images are extracted to form the feature vectors. These are trained by supervised learning through SVM algorithm to realize automatic binary classification of images; Secondly, the defined dehazing methods are called to process the classified result as a hazy image and the same quality evaluation indexes are used to evaluate each image output by different dehazing methods. Then, it outputs the highest evaluation image after haze removal. Finally, the output image is classified again by SVM until the image reaches the clearest it can be. The experimental results show that the proposed algorithm exhibits good contrast, brightness and color saturation from the visual effect. Also the scene adaptability and robustness of the algorithm are improved.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300420",
    "keywords": [
      "Adaptability",
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Brightness",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Contrast (vision)",
      "Ecology",
      "Gene",
      "Haze",
      "Histogram",
      "Image (mathematics)",
      "Meteorology",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Robustness (evolution)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Gui",
        "given_name": "Bian"
      },
      {
        "surname": "Zhu",
        "given_name": "Yuhua"
      },
      {
        "surname": "Zhen",
        "given_name": "Tong"
      }
    ]
  },
  {
    "title": "Data encryption based blockchain and privacy preserving mechanisms towards big data",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2019.102741",
    "abstract": "Blockchain is a key technique which can support Bitcoin. Blockchain is a decentralized infrastructure that uses chained data structure to verify and store data, and uses distributed node consensus mechanism to generate and update data. Blockchain has become a hot research topic since its attributes of decentralization, verifiability and anti-tampering. To stimulate the development of Blockchain, we conduct a comprehensive research on Blockchain. Specifically, we discuss various mainstream consensus mechanisms used in blockchain technology, and thoroughly analyze anonymity and privacy protection in digital currency. Aiming at data encryption mechanism, we discuss existing anonymity and privacy protection schemes. Our discussion can further promote the development of Blockchain.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320319303621",
    "keywords": [
      "Anonymity",
      "Big data",
      "Blockchain",
      "Computer science",
      "Computer security",
      "Cryptocurrency",
      "Data mining",
      "Decentralization",
      "Encryption",
      "Immutability",
      "Information privacy",
      "Key (lock)",
      "Law",
      "Political science"
    ],
    "authors": [
      {
        "surname": "Guo",
        "given_name": "Leiyong"
      },
      {
        "surname": "Xie",
        "given_name": "Hui"
      },
      {
        "surname": "Li",
        "given_name": "Yu"
      }
    ]
  },
  {
    "title": "Diverse receptive field network with context aggregation for fast object detection",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102770",
    "abstract": "Current context-utilizing detectors are all based on two-stage approaches. However, their computational efficiency and context quality extremely depend on the accuracy of proposal-generating methods, which limits their performance and makes them hardly perform real-time detection. In this work, we present a context-exploited method that integrates features in different receptive fields to obtain contextual representation. Based on this idea, we put forward the multi-branch diverse receptive field modules (DRF modules) and their design principles to encode context. To further utilize contextual information for fast object detection, we propose a one-stage diverse receptive field network (DRFNet). In DRFNet, the DRF modules are first applied to capture rich context as the basis, then a parallel structure is constructed to exploit the context at different scales along with DRF modules. Comprehensive experiments indicate that the context introduced by our methods improves the detection performance and DRFNet achieves a good trade-off between speed and accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300201",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Context (archaeology)",
      "Context model",
      "ENCODE",
      "Exploit",
      "Field (mathematics)",
      "Gene",
      "Law",
      "Mathematics",
      "Object (grammar)",
      "Object detection",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Pure mathematics",
      "Receptive field",
      "Representation (politics)",
      "Spatial contextual awareness"
    ],
    "authors": [
      {
        "surname": "Xie",
        "given_name": "Shaorong"
      },
      {
        "surname": "Liu",
        "given_name": "Chang"
      },
      {
        "surname": "Gao",
        "given_name": "Jiantao"
      },
      {
        "surname": "Li",
        "given_name": "Xiaomao"
      },
      {
        "surname": "Luo",
        "given_name": "Jun"
      },
      {
        "surname": "Fan",
        "given_name": "Baojie"
      },
      {
        "surname": "Chen",
        "given_name": "Jiahong"
      },
      {
        "surname": "Pu",
        "given_name": "Huayan"
      },
      {
        "surname": "Peng",
        "given_name": "Yan"
      }
    ]
  },
  {
    "title": "Combining polar harmonic transforms and 2D compound chaotic map for distinguishable and robust color image zero-watermarking algorithm",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102804",
    "abstract": "Although zero-watermarking can provide an effective and distortion-free scheme for image copyright protection, its robustness and discriminability do not meet expectations in existing methods. Some cannot resist effectively geometric attacks, others do not consider the discriminability and equalization. For that reason, this paper proposes a robust and distinguishable color image zero-watermarking algorithm based on polar harmonic transforms (PHTs) and compound chaotic map. In the proposed algorithm, firstly three PHTs moments of an image are computed simultaneously and accurate moments are selected for the robustness. Then, content-based binary feature sequence is acquired by judging the relation between magnitudes of adjacent moments for the discriminability. Finally, compound chaotic map is employed to encrypt copyright logo for ensuring security and scramble binary feature sequence for improving the equalization. Experimental results show that the proposed zero-watermarking algorithm has good equalization and discriminability, and an advantage in robustness compared with other zero-watermarking and traditional watermarking.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300547",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Binary number",
      "Biochemistry",
      "Chaotic",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Digital watermarking",
      "Feature (linguistics)",
      "Gene",
      "Image (mathematics)",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Robustness (evolution)",
      "Scrambling"
    ],
    "authors": [
      {
        "surname": "Kang",
        "given_name": "Xiaobing"
      },
      {
        "surname": "Zhao",
        "given_name": "Fan"
      },
      {
        "surname": "Chen",
        "given_name": "Yajun"
      },
      {
        "surname": "Lin",
        "given_name": "Guangfeng"
      },
      {
        "surname": "Jing",
        "given_name": "Cuining"
      }
    ]
  },
  {
    "title": "Massive-scale visual information retrieval towards city residential environment surveillance",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2019.102739",
    "abstract": "Urban residential environment surveillance plays an important role in modern intelligent city. Satellite images have been applied in various fields, and the analysis and processing of satellite images has become an important means to obtain the information perceived by satellites. This paper focuses on city residential environment surveillance based on massive-scale visual information retrieval. Since the shortcomings of low contrast, blurred boundary, large amount of information and susceptibility to noise, the performance of satellite image segmentation is not satisfactory, which will affect residential environment surveillance. We design an improved rough set fuzzy C-means clustering algorithm combined with ant colony algorithm. More specifically, satellite images are classified based on the gradient of pixels according to the indistinguishable relation of the image combined with rough set theory. Then, the traditional fuzzy set-based fuzzy C-means clustering algorithm is applied to the satellite image segmentation technology. Subsequently, the improved algorithm-quantum ant colony algorithm and rough set fuzzy clustering C-means algorithm are combined to achieve accurate segmentation of satellite images. Afterwards, we propose a satellite image retrieval algorithm, which can assist city residential environment surveillance. Comprehensive experiment show that our proposed method is effective and robust in residential environment surveillance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320319303608",
    "keywords": [
      "Aerospace engineering",
      "Ant colony optimization algorithms",
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Engineering",
      "Fuzzy clustering",
      "Fuzzy logic",
      "Image segmentation",
      "Pixel",
      "Rough set",
      "Satellite",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Yuzhe"
      },
      {
        "surname": "Xu",
        "given_name": "Zhiyi"
      }
    ]
  },
  {
    "title": "Wavelet based medical image super resolution using cross connected residual-in-dense grouped convolutional neural network",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102819",
    "abstract": "In clinical analysis and diagnosis, high resolution (HR) computed tomography (CT) images are required for proper treatment of a patient. Developing HR medical images by X-ray CT devices require extended radiation exposure with large radiative dosages, putting the patient at potential risk of inducing cancer. So, radiation exposure should be reduced. However, photon starvation and beam hardening in low-dose X-rays will cause severe artifacts. Thus, an accurate reconstruction of low-dose X-ray CT images is required. To this end, we propose a wavelet based multi-channel and multi-scale cross connected residual-in-dense grouped convolutional neural network (WCRDGCNN) for accurate super resolution (SR) of medical images. The adopted filter groups reduce the connection weights, thereby reducing the computational complexity. Gradient vanishing problem is tackled by using residual and dense skip connections. The extensive experimentation results on benchmark datasets show that our method outperforms the state-of-the-art SR methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300699",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Geodesy",
      "Geography",
      "Pattern recognition (psychology)",
      "Residual",
      "Wavelet"
    ],
    "authors": [
      {
        "surname": "Amaranageswarao",
        "given_name": "Gadipudi"
      },
      {
        "surname": "Deivalakshmi",
        "given_name": "S."
      },
      {
        "surname": "Ko",
        "given_name": "Seok-Bum"
      }
    ]
  },
  {
    "title": "Hand pose estimation in object-interaction based on deep learning for virtual reality applications",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102802",
    "abstract": "Hand Pose Estimation aims to predict the position of joints on a hand from an image, and it has become popular because of the emergence of VR/AR/MR technology. Nevertheless, an issue surfaces when trying to achieve this goal, since a hand tends to cause self-occlusion or external occlusion easily as it interacts with external objects. As a result, there have been many projects dedicated to this field for a better solution of this problem. This paper develops a system that accurately estimates a hand pose in 3D space using depth images for VR applications. We propose a data-driven approach of training a deep learning model for hand pose estimation with object interaction. In the convolutional neural network (CNN) training procedure, we design a skeleton-difference loss function, which effectively can learn the physical constraints of a hand. Also, we propose an object-manipulating loss function, which considers knowledge of the hand-object interaction, to enhance performance. In the experiments we have conducted for hand pose estimation under different conditions, the results validate the robustness and the performance of our system and show that our method is able to predict the joints more accurately in challenging environmental settings. Such appealing results may be attributed to the consideration of the physical joint relationship as well as object information, which in turn can be applied to future VR/AR/MR systems for more natural experience.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300523",
    "keywords": [
      "3D pose estimation",
      "Articulated body pose estimation",
      "Artificial intelligence",
      "Augmented reality",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Gene",
      "Object (grammar)",
      "Pose",
      "Robustness (evolution)",
      "Virtual image",
      "Virtual reality"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Min-Yu"
      },
      {
        "surname": "Ting",
        "given_name": "Pai-Wen"
      },
      {
        "surname": "Tang",
        "given_name": "Ya-Hui"
      },
      {
        "surname": "Chou",
        "given_name": "En-Te"
      },
      {
        "surname": "Fu",
        "given_name": "Li-Chen"
      }
    ]
  },
  {
    "title": "Locality preserving projection based on Euler representation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102796",
    "abstract": "Locality preserving projection (LPP) is a widely used linear dimensionality reduction method, which preserves the locality structure of the original data. Motivated by the fact that kernel technique can capture nonlinear similarity of features and help to improve separability between nearby data points, this paper proposes locality preserving projection model based on Euler representation (named as ELPP). This model first projects the data into a complex space with Euler representation, then learns the dimensionality reduction projection with preserving locality structure in this complex space. We also extend ELPP to F-ELPP by replacing the squared F-norm with F-norm, which will weaken the exaggerated errors and be more robustness to outliers. The optimization algorithms of the two models are given, and the convergence of F-ELPP is proved. A large number of experiments on several public databases have demonstrated that the two proposed models have good robustness and feature extraction ability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300468",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Dimensionality reduction",
      "Gene",
      "Linguistics",
      "Locality",
      "Mathematics",
      "Outlier",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Long",
        "given_name": "Tianhang"
      },
      {
        "surname": "Sun",
        "given_name": "Yanfeng"
      },
      {
        "surname": "Gao",
        "given_name": "Junbin"
      },
      {
        "surname": "Hu",
        "given_name": "Yongli"
      },
      {
        "surname": "Yin",
        "given_name": "Baocai"
      }
    ]
  },
  {
    "title": "An effective hybrid pruning architecture of dynamic convolution for surveillance videos",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102798",
    "abstract": "The large-scale surveillance videos analysis becomes important as the development of the intelligent city; however, the heavy computational resources necessary for the state-of-the-art deep learning model makes real-time processing hard to be implemented. As the characteristic of high scene similarity generally existing in surveillance videos, we propose an effective compression architecture called dynamic convolution, which can reuse the previous feature maps to reduce the calculation amount; and combine with filter pruning to further speed up the performance. In this paper, we tested the presented method on 45 surveillance videos with various scenes. The experimental results show that the hybrid pruning architecture can reduce up to 80.4% of FLOPs while preserving the precision within 1.34% mAP; furthermore, the method can improve the processing speed up to 2.8 times compared to the traditional Single Shot MultiBox Detection.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300481",
    "keywords": [
      "Agronomy",
      "Architecture",
      "Art",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Ecology",
      "FLOPS",
      "Feature (linguistics)",
      "Filter (signal processing)",
      "Geometry",
      "Image (mathematics)",
      "Linguistics",
      "Mathematics",
      "Parallel computing",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pruning",
      "Reduction (mathematics)",
      "Reuse",
      "Similarity (geometry)",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Tsai",
        "given_name": "Chun-Ya"
      },
      {
        "surname": "Gao",
        "given_name": "De-Qin"
      },
      {
        "surname": "Ruan",
        "given_name": "Shanq-Jang"
      }
    ]
  },
  {
    "title": "Massive-scale image retrieval based on deep visual feature representation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2019.102738",
    "abstract": "This paper proposes an image retrieval algorithm towards massive-scale multimedia data. In order to be consistent with human visual system, we first design a color attention function to describe the important of different image patches. Subsequently, we combine color and texture to construct candidate regions, which will be fed into a deep neural network (DNN) for deep representation extraction. Then, we design a similarity function to calculate the distance among different images, where top-ranking images are considered as the required images. Experimental results show the effectiveness and robustness of our proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320319303591",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Deep neural networks",
      "Feature extraction",
      "Gene",
      "Geometry",
      "Image (mathematics)",
      "Image retrieval",
      "Law",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Pyramid (geometry)",
      "Representation (politics)",
      "Robustness (evolution)",
      "Similarity (geometry)",
      "Visual Word"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Hongpeng"
      }
    ]
  },
  {
    "title": "Human-computer interaction based on face feature localization",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2019.102740",
    "abstract": "Human-computer interaction is the way in which humans and machines communicate information. With the rapid development of deep learning technology, the technology of human-computer interaction has also made a corresponding breakthrough. In the past, the way human-computer interaction was mostly relied on hardware devices. Through the coordinated work of multiple sensors, people and machines can realize information interaction. However, as theoretical technology continues to mature, algorithms for human-computer interaction are also being enriched. The popularity of convolutional neural networks has made image processing problems easier to solve. Therefore, real-time human-computer interaction can be performed by using image processing, and intelligent of human-computer interaction can be realized. The main idea of this paper is to use the real-time capture of face images and video information to image the face image information. We perform feature point positioning based on the feature points of the face image. We perform expression recognition based on the feature points that are located. At the same time, we perform ray tracing for the identified human eye area. The feature points of the face and the corresponding expressions and implementation movements represent the user's use appeal. Therefore, we can analyze the user's use appeal by locating the face feature area. We define the corresponding action information for specific user face features. We extract the user's corresponding information according to the user's face features, and perform human-computer interaction according to the user's information.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732031930361X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Face (sociological concept)",
      "Face detection",
      "Facial recognition system",
      "Feature (linguistics)",
      "Feature extraction",
      "Geometry",
      "Human–computer interaction",
      "Linguistics",
      "Mathematics",
      "Operating system",
      "Philosophy",
      "Point (geometry)",
      "Social science",
      "Sociology",
      "Tracing"
    ],
    "authors": [
      {
        "surname": "Shi",
        "given_name": "Yan"
      },
      {
        "surname": "Zhang",
        "given_name": "Zijun"
      },
      {
        "surname": "Huang",
        "given_name": "Kaining"
      },
      {
        "surname": "Ma",
        "given_name": "Wudi"
      },
      {
        "surname": "Tu",
        "given_name": "Shanshan"
      }
    ]
  },
  {
    "title": "Semi-automatic 2D-to-3D video conversion based on background sprite generation",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102801",
    "abstract": "This paper presents a technique for semi-automatic 2D-to-3D stereo video conversion, which is known to provide user intervention in assigning foreground/background depths for key frames and then get depth maps for non-key frames via automatic depth propagation. Our algorithm treats foreground and background separately. For foregrounds, kernel pixels are identified and then used as the seeds for graph-cut segmentation for each non-key frame independently, resulting in results not limited by objects’ motion activity. For backgrounds, all video frames, after foregrounds being removed, are integrated into a common background sprite model (BSM) based on a relay-frame-based image registration algorithm. Users can then draw background depths for BSM in an integrated manner, thus reducing human efforts significantly. Experimental results show that our method is capable of retaining more faithful foreground depth boundaries (by 1.6–2.7 dB) and smoother background depths than prior works. This advantage is helpful for 3D display and 3D perception.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300511",
    "keywords": [
      "2D to 3D conversion",
      "Artificial intelligence",
      "Background subtraction",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Cut",
      "Image (mathematics)",
      "Image segmentation",
      "Integral imaging",
      "Pixel",
      "Segmentation",
      "Sprite (computer graphics)",
      "Stereo display"
    ],
    "authors": [
      {
        "surname": "Lie",
        "given_name": "Wen-Nung"
      },
      {
        "surname": "Chiu",
        "given_name": "Shao-Ting"
      },
      {
        "surname": "Chen",
        "given_name": "Yi-Kai"
      },
      {
        "surname": "Chiang",
        "given_name": "Jui-Chiu"
      }
    ]
  },
  {
    "title": "Visual object tracking using sparse context-aware spatio-temporal correlation filter",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102820",
    "abstract": "This paper presents a novel sparse context-aware spatio-temporal correlation filter tracker (SCAST) method for robust visual object tracking. Different from the existing trackers, this paper introduce an l 1 multi-scale regularization parameter-based correlation filter that reduces the boundary effect due to partial occlusions, illumination and scale variations. At each iteration, the l 1 regularization parameter is updated through spatial knowledge of each correlation filter coefficient. Besides, the contextual information acquired from the target region can lead to determining the accurate localization of the target. Moreover, contextual information has combined with spatio-temporal factor to achieve the better performance. Further, an objective function is designed with system constraints to ensure the applicability of the model and the optimal solution is derived by utilizing the alternating direction method of multiplier, which leads to low computational cost. Finally, the feasibility and superiority of proposed tracker algorithm is evaluated through three benchmark dataset: OTB-2013, OTB-2015, and TempleColor-128.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300705",
    "keywords": [
      "Artificial intelligence",
      "BitTorrent tracker",
      "Computer science",
      "Computer vision",
      "Correlation",
      "Eye tracking",
      "Filter (signal processing)",
      "Geometry",
      "Mathematics",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Regularization (linguistics)",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Elayaperumal",
        "given_name": "Dinesh"
      },
      {
        "surname": "Joo",
        "given_name": "Young Hoon"
      }
    ]
  },
  {
    "title": "Perceptual visual quality assessment using deeply-learned gaze shifting kernel",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2019.102701",
    "abstract": "Image quality assessment (IQA) is a useful technique in computer vision and machine intelligence. It is widely applied in image retrieval, image clustering and image recognition. IQA algorithms generally rely on human visual system (HVS), which can reflect how human perceive salient regions in the image. In this paper, we leverage both low-level features and high-level semantic features to select salient regions, which will be concatenated to form GSPs by the designed saliency-constraint algorithm to mimic human visual system. We design an enhanced IQA index based on the GSPs to calculate the simialrity between reference image and test image to achieve image quality assessment. Experiments demonstrate that our IQA method can achieve satisfactory performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320319303220",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Gaze",
      "Human visual system model",
      "Image (mathematics)",
      "Image quality",
      "Kernel (algebra)",
      "Leverage (statistics)",
      "Mathematics",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Perception",
      "Psychology",
      "Salient"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Feng"
      },
      {
        "surname": "Huang",
        "given_name": "Shiwang"
      },
      {
        "surname": "Long",
        "given_name": "Renyan"
      },
      {
        "surname": "Zhang",
        "given_name": "Tiantian"
      },
      {
        "surname": "Na",
        "given_name": "Sang-Gyun"
      }
    ]
  },
  {
    "title": "Perceptual visual quality assessment using deeply-learned gaze shifting kernel",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2019.102701",
    "abstract": "Image quality assessment (IQA) is a useful technique in computer vision and machine intelligence. It is widely applied in image retrieval, image clustering and image recognition. IQA algorithms generally rely on human visual system (HVS), which can reflect how human perceive salient regions in the image. In this paper, we leverage both low-level features and high-level semantic features to select salient regions, which will be concatenated to form GSPs by the designed saliency-constraint algorithm to mimic human visual system. We design an enhanced IQA index based on the GSPs to calculate the simialrity between reference image and test image to achieve image quality assessment. Experiments demonstrate that our IQA method can achieve satisfactory performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320319303220",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Gaze",
      "Human visual system model",
      "Image (mathematics)",
      "Image quality",
      "Kernel (algebra)",
      "Leverage (statistics)",
      "Mathematics",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Perception",
      "Psychology",
      "Salient"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Feng"
      },
      {
        "surname": "Huang",
        "given_name": "Shiwang"
      },
      {
        "surname": "Long",
        "given_name": "Renyan"
      },
      {
        "surname": "Zhang",
        "given_name": "Tiantian"
      },
      {
        "surname": "Na",
        "given_name": "Sang-Gyun"
      }
    ]
  },
  {
    "title": "Reversible data hiding in binary images by flipping pattern pair with opposite center pixel",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102816",
    "abstract": "In this paper, a novel RDH scheme by flipping pattern pairs with opposite center pixel (PPOCPs) in binary images is proposed, aiming at decreasing the distortion while increasing the embedding payload. First, 25 patterns in the 3 × 3 block are designed which construct the PPOCPs according to the distance level providing a guarantee for reversibility. Then, a balanced score is designed which considers both visual distortion and embedding payload to select the optimal PPOCP, and the secret messages are embedded in the optimal PPOCP. For the receiver, the secret messages can be extracted precisely and the original binary image can be recovered by scanning the optimal PPOCP. PPOCP is a novel RDH model which fully considers the visual distortion caused by flipping pixels. Experimental results demonstrate the feasibility of the proposed RDH method for binary images, and the visual quality is satisfactory under high embedding payload and smallest pure flipping rate.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300663",
    "keywords": [
      "Algorithm",
      "Amplifier",
      "Arithmetic",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Binary data",
      "Binary image",
      "Binary number",
      "Block (permutation group theory)",
      "Combinatorics",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Construct (python library)",
      "Distortion (music)",
      "Embedding",
      "Image (mathematics)",
      "Image processing",
      "Information hiding",
      "Mathematics",
      "Network packet",
      "Pattern recognition (psychology)",
      "Payload (computing)",
      "Pixel",
      "Programming language",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Yin",
        "given_name": "Xiaolin"
      },
      {
        "surname": "Lu",
        "given_name": "Wei"
      },
      {
        "surname": "Zhang",
        "given_name": "JunHong"
      },
      {
        "surname": "Chen",
        "given_name": "Jianfei"
      },
      {
        "surname": "Liu",
        "given_name": "Wanteng"
      }
    ]
  },
  {
    "title": "Relative view based holistic-separate representations for two-person interaction recognition using multiple graph convolutional networks",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102833",
    "abstract": "In this paper, we focus on recognizing person-person interactions using skeletal data captured from depth sensors. First, we propose a novel and efficient view transformation scheme. The skeletal interaction sequence is re-observed under a new coordinate system, which is invariant to various setups and capturing views of depth cameras as well as the position or facing orientation exchange between two persons. Second, we propose concise and discriminative interaction representations simply composed of the joint locations from two persons. Proposed representations are efficient to describe both the holistic interactive scene and individual poses performed by each subject separately. Third, we introduce the graph convolutional networks(GCN) to directly learn proposed skeletal interaction representations. Moreover, we design a multiple GCN-based model to provide the final class score. Extensive experimental results on three skeletal action datasets NTU RGB+D 60, NTU RGB+D 120 and SBU consistently demonstrate the superiority of our interaction recognition method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300833",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Coordinate system",
      "Discriminative model",
      "Graph",
      "Invariant (physics)",
      "Mathematical physics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "RGB color model",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Xing"
      },
      {
        "surname": "Li",
        "given_name": "Yanshan"
      },
      {
        "surname": "Guo",
        "given_name": "Tianyu"
      },
      {
        "surname": "Xia",
        "given_name": "Rongjie"
      }
    ]
  },
  {
    "title": "An efficient tensor completion method via truncated nuclear norm",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102791",
    "abstract": "Tensor completion aims to recover missing entries from partial observations for multi-dimensional data. Traditional tensor completion algorithms process the dimensional data by unfolding the tensor into matrices, which breaks the inherent correlation and dependencies in multiple channels and lead to critical information loss. In this paper, we propose a novel tensor completion model for visual multi-dimensional data completion under the tensor singular value decomposition (t-SVD) framework. In the proposed method, tensor is treated as a whole and a truncated nuclear norm regularization is employed to exploit the structural properties in a tensor and hidden information existing among the adjacent channels of a tensor. Besides, we introduce a weighted tensor to adjust the residual error of each frontal slices in consideration of their different recovery statistics. It does enhance the sparsity of all unfoldings of the tensor and accelerates the convergence of the proposed method. Experimental results on various visual datasets demonstrate the promising performance of the proposed method in comparison with the state-of-the-art tensor completion methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300419",
    "keywords": [
      "Algorithm",
      "Computer science",
      "Eigenvalues and eigenvectors",
      "Law",
      "Mathematics",
      "Matrix norm",
      "Norm (philosophy)",
      "Physics",
      "Political science",
      "Pure mathematics",
      "Quantum mechanics",
      "Residual",
      "Singular value decomposition",
      "Tensor (intrinsic definition)",
      "Tensor decomposition"
    ],
    "authors": [
      {
        "surname": "Song",
        "given_name": "Yun"
      },
      {
        "surname": "Li",
        "given_name": "Jie"
      },
      {
        "surname": "Chen",
        "given_name": "Xi"
      },
      {
        "surname": "Zhang",
        "given_name": "Dengyong"
      },
      {
        "surname": "Tang",
        "given_name": "Qiang"
      },
      {
        "surname": "Yang",
        "given_name": "Kun"
      }
    ]
  },
  {
    "title": "A novel haze image steganography method via cover-source switching",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102814",
    "abstract": "In realistic outdoor scenarios, image sensors tend to suffer from various weather conditions (e.g., haze, rain, etc.),which make the images of the same scene taken at different times may be different. Therefore, one should be able to securely embed secret messages into these images by making use of the variations of the weather effects. Inspired by some recent natural steganography algorithms, this paper presents a novel haze image steganography method, which embeds messages through adjusting the weather effects of an input haze image, making it resemble the same image captured under another weather condition. The proposed steganography method consists of three parts: (1) model parameter estimation of the input haze image, (2) haze effects adjustment according to the atmospheric scattering model, (3) message embedding using the floating-point adjusted haze image. 10,000 haze images captured under different haze conditions in various scenarios were used to test the proposed steganography algorithm. The experimental results show that the proposed steganography algorithm is more secure than S-UNIWARD and HILL for steganalyzers who only have raw haze images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032030064X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Cover (algebra)",
      "Embedding",
      "Engineering",
      "Geography",
      "Haze",
      "Image (mathematics)",
      "Mechanical engineering",
      "Meteorology",
      "Steganography"
    ],
    "authors": [
      {
        "surname": "Qi",
        "given_name": "Baojun"
      },
      {
        "surname": "Yang",
        "given_name": "Chunfang"
      },
      {
        "surname": "Tan",
        "given_name": "Lei"
      },
      {
        "surname": "Luo",
        "given_name": "Xiangyang"
      },
      {
        "surname": "Liu",
        "given_name": "Fenlin"
      }
    ]
  },
  {
    "title": "Learning spatial hierarchies of high-level features in deep neural network",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102817",
    "abstract": "This paper addresses a new approach to learn perceptual grouping of the extracted features of the convolutional neural network (CNN) to represent the structure contained in the image. In CNN, the spatial hierarchies between the high-level features are not taken into account. To do so, the perceptual grouping of features is utilized. To consider the intra-relationship between feature maps, modified Guided Co-occurrence Block (mGCoB) is proposed. This block preserves the joint co-occurrence of two features in the spatial domain and it prevents the co-adaptation. Also, to preserve the interrelationship in each feature map, the principle of common region grouping is utilized which states that the features which are located in the same feature map tend to be grouped together. To consider it, an MFC block is proposed. To evaluate the proposed approach, it is applied to some known semantic segmentation and image classification datasets that achieve superior performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300675",
    "keywords": [
      "Adaptation (eye)",
      "Architectural engineering",
      "Artificial intelligence",
      "Artificial neural network",
      "Block (permutation group theory)",
      "Computer science",
      "Convolutional neural network",
      "Domain (mathematical analysis)",
      "Engineering",
      "Feature (linguistics)",
      "Geometry",
      "Image (mathematics)",
      "Joint (building)",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Razzaghi",
        "given_name": "Parvin"
      },
      {
        "surname": "Abbasi",
        "given_name": "Karim"
      },
      {
        "surname": "Bayat",
        "given_name": "Pegah"
      }
    ]
  },
  {
    "title": "End-to-end DeepNCC framework for robust visual tracking",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102800",
    "abstract": "In this paper, we propose an NCC-based object tracking deep framework, which can be well initialized with the limited target samples in the first frame. The proposed framework contains a pretrained model, online feature fine-tuning layers and tracking processes. The pretrained model provides rich feature representations while online feature fine-tuning layers select discriminative and generic features for the tracked object. We choose normalized cross-correlation as a template tracking layer to perform the tracking process. To enable the learned features representation closely coordinated to the tracked target, we jointly train the feature representation network and tracking processes. In online tracking, an adaptive template and a fixed template are fused to find the optimal tracking results. Scale estimation and a high-confidence model update scheme are perfectly integrated into the framework to adapt to the target appearance changes. The extensive experiments demonstrate that the proposed tracker achieves superior performance compared with other state-of-the-art trackers.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032030050X",
    "keywords": [
      "Active appearance model",
      "Artificial intelligence",
      "BitTorrent tracker",
      "Computer science",
      "Computer vision",
      "Discriminative model",
      "Eye tracking",
      "Feature (linguistics)",
      "Frame (networking)",
      "Image (mathematics)",
      "Law",
      "Linguistics",
      "Object (grammar)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Pedagogy",
      "Philosophy",
      "Political science",
      "Politics",
      "Process (computing)",
      "Psychology",
      "Representation (politics)",
      "Telecommunications",
      "Tracking (education)",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Dai",
        "given_name": "Kaiheng"
      },
      {
        "surname": "Wang",
        "given_name": "Yuehuan"
      }
    ]
  },
  {
    "title": "Retraction notice to “Efficient object analysis by leveraging deeply-trained object proposals prediction model” [J. Vis. Commun. Image R. 61 (2019) 218–224]",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102837",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300882",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Information retrieval",
      "Law",
      "Notice",
      "Object (grammar)",
      "Object based",
      "Political science"
    ],
    "authors": [
      {
        "surname": "Dai",
        "given_name": "Bo"
      },
      {
        "surname": "Wang",
        "given_name": "Yanbo"
      },
      {
        "surname": "Yao",
        "given_name": "Yiyang"
      },
      {
        "surname": "Ye",
        "given_name": "Weijing"
      },
      {
        "surname": "Chen",
        "given_name": "Ting"
      }
    ]
  },
  {
    "title": "Research on image feature extraction and retrieval algorithms based on convolutional neural network",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2019.102705",
    "abstract": "With the rapid development of mobile Internet and digital technology, people are more and more keen to share pictures on social networks, and online pictures have exploded. How to retrieve similar images from large-scale images has always been a hot issue in the field of image retrieval, and the selection of image features largely affects the performance of image retrieval. The Convolutional Neural Networks (CNN), which contains more hidden layers, has more complex network structure and stronger ability of feature learning and expression compared with traditional feature extraction methods. By analyzing the disadvantage that global CNN features cannot effectively describe local details when they act on image retrieval tasks, a strategy of aggregating low-level CNN feature maps to generate local features is proposed. The high-level features of CNN model pay more attention to semantic information, but the low-level features pay more attention to local details. Using the increasingly abstract characteristics of CNN model from low to high. This paper presents a probabilistic semantic retrieval algorithm, proposes a probabilistic semantic hash retrieval method based on CNN, and designs a new end-to-end supervised learning framework, which can simultaneously learn semantic features and hash features to achieve fast image retrieval. Using convolution network, the error rate is reduced to 14.41% in this test set. In three open image libraries, namely Oxford, Holidays and ImageNet, the performance of traditional SIFT-based retrieval algorithms and other CNN-based image retrieval algorithms in tasks are compared and analyzed. The experimental results show that the proposed algorithm is superior to other contrast algorithms in terms of comprehensive retrieval effect and retrieval time.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320319303268",
    "keywords": [
      "Artificial intelligence",
      "Automatic image annotation",
      "Computer science",
      "Computer security",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Feature extraction",
      "Hash function",
      "Image (mathematics)",
      "Image retrieval",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Probabilistic logic",
      "Scale-invariant feature transform",
      "Semantic feature",
      "Visual Word"
    ],
    "authors": [
      {
        "surname": "Peng",
        "given_name": "Xushan"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiaoming"
      },
      {
        "surname": "Li",
        "given_name": "Yongping"
      },
      {
        "surname": "Liu",
        "given_name": "Bangquan"
      }
    ]
  },
  {
    "title": "Retraction notice to “Image quality tendency modeling by fusing multiple visual cues” [J. Vis. Commun. Image R. 62 (2019) 117–128]",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102841",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300924",
    "keywords": [
      "Artificial intelligence",
      "Cognitive psychology",
      "Computer science",
      "Computer vision",
      "Epistemology",
      "Image (mathematics)",
      "Image quality",
      "Law",
      "Notice",
      "Philosophy",
      "Political science",
      "Psychology",
      "Quality (philosophy)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Wenjie"
      },
      {
        "surname": "Yao",
        "given_name": "Yiyang"
      },
      {
        "surname": "Wang",
        "given_name": "Jinxiong"
      },
      {
        "surname": "Xiang",
        "given_name": "Xinyu"
      },
      {
        "surname": "Shu",
        "given_name": "Peng"
      }
    ]
  },
  {
    "title": "Retraction notice to “Moving object surveillance using object proposals and background prior prediction” [J. Vis. Commun. Image Represent. 61 (2019) 85–92]",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102838",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300894",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Law",
      "Notice",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Political science"
    ],
    "authors": [
      {
        "surname": "Yao",
        "given_name": "Yiyang"
      },
      {
        "surname": "Liu",
        "given_name": "Peizhen"
      },
      {
        "surname": "Sun",
        "given_name": "Xiaowei"
      },
      {
        "surname": "Zhang",
        "given_name": "Luming"
      }
    ]
  },
  {
    "title": "JPEG quantization step estimation with coefficient histogram and spectrum analyses",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102795",
    "abstract": "This paper proposes a new method for estimating quantization steps (QSs) from an image that has been previously JPEG-compressed and stored in a lossless format. In this method, DCT coefficients of each frequency band of JPEG-compressed image are aggregated in the QS and its multiples. The entire estimation process can be grouped into two categories: alternating and direct current bands. Considering that DCT coefficients under different QSs show different periodicity, QS estimation for each band is then further divided into three steps, which involve identifying whether the QS is one, two, or another value. For each step, the periodicity of DCT coefficients can be well exploited with the analyses of the DCT-coefficient histogram and its corresponding frequency magnitude spectrum. Experimental results demonstrate the efficacy of the proposed method and the superiority in QS estimation for previously JPEG-compressed images, especially in the case that the actual QSs are higher than two.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300456",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Data compression",
      "Discrete cosine transform",
      "Histogram",
      "Image (mathematics)",
      "Image compression",
      "Image processing",
      "JPEG",
      "Lossless JPEG",
      "Lossless compression",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Quantization (signal processing)"
    ],
    "authors": [
      {
        "surname": "Yao",
        "given_name": "Heng"
      },
      {
        "surname": "Wei",
        "given_name": "Hongbin"
      },
      {
        "surname": "Qiao",
        "given_name": "Tong"
      },
      {
        "surname": "Qin",
        "given_name": "Chuan"
      }
    ]
  },
  {
    "title": "Cross-level reinforced attention network for person re-identification",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102775",
    "abstract": "Attention mechanism is a simple and effective method to enhance discriminative performance of person re-identification (Re-ID). Most of previous attention-based works have difficulty in eliminating the negative effects of meaningless information. In this paper, a universal module, named Cross-level Reinforced Attention (CLRA), is proposed to alleviate this issue. Firstly, we fuse features of different semantic levels using adaptive weights. The fused features, containing richer spatial and semantic information, can better guide the generation of subsequent attention module. Then, we combine hard and soft attention to improve the ability to extract important information in spatial and channel domains. Through the CLRA, the network can aggregate and propagate more discriminative semantic information. Finally, we integrate the CLRA with Harmonious Attention CNN (HA-CNN) and form a novel Cross-level Reinforced Attention CNN (CLRA-CNN) to optimize person Re-ID. Experiment results on several public benchmarks show that the proposed method achieves state-of-the-art performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300250",
    "keywords": [
      "Aggregate (composite)",
      "Artificial intelligence",
      "Attention network",
      "Biology",
      "Botany",
      "Channel (broadcasting)",
      "Composite material",
      "Computer network",
      "Computer science",
      "Discriminative model",
      "Electrical engineering",
      "Engineering",
      "Fuse (electrical)",
      "Identification (biology)",
      "Machine learning",
      "Materials science",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Jiang",
        "given_name": "Min"
      },
      {
        "surname": "Li",
        "given_name": "Cong"
      },
      {
        "surname": "Kong",
        "given_name": "Jun"
      },
      {
        "surname": "Teng",
        "given_name": "Zhende"
      },
      {
        "surname": "Zhuang",
        "given_name": "Danfeng"
      }
    ]
  },
  {
    "title": "Retraction notice to “Analysis of security operation and maintenance system using privacy utility in media environment” [J. Vis. Commun. Image Represent. 56 (2018) 177–181]",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102840",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300912",
    "keywords": [
      "Business",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Image (mathematics)",
      "Internet privacy",
      "Law",
      "Notice",
      "Political science"
    ],
    "authors": [
      {
        "surname": "Jiang",
        "given_name": "Zhengwei"
      },
      {
        "surname": "Chen",
        "given_name": "Guoen"
      },
      {
        "surname": "Jin",
        "given_name": "Xueqi"
      },
      {
        "surname": "Wang",
        "given_name": "Yueqiang"
      }
    ]
  },
  {
    "title": "Retraction notice to “Deeply fusing multimodal features in hypergraph” [J. Vis. Commun. Image Represent. 62 (2019) 97–104]",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102836",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300869",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Hypergraph",
      "Image (mathematics)",
      "Law",
      "Mathematics",
      "Notice",
      "Political science"
    ],
    "authors": [
      {
        "surname": "Yao",
        "given_name": "Wei"
      },
      {
        "surname": "Jiang",
        "given_name": "Ying"
      },
      {
        "surname": "Lu",
        "given_name": "Wenda"
      },
      {
        "surname": "Chen",
        "given_name": "Jun"
      },
      {
        "surname": "Xie",
        "given_name": "Linchao"
      }
    ]
  },
  {
    "title": "Retraction notice to “Camera network analysis for visual surveillance in electric industrial context“ [J. Vis. Commun. Image Represent. 56 (2018) 201–206]",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102839",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300900",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Geography",
      "Image (mathematics)",
      "Law",
      "Notice",
      "Political science"
    ],
    "authors": [
      {
        "surname": "Jiang",
        "given_name": "Zhengwei"
      }
    ]
  },
  {
    "title": "No-reference quality metric for contrast-distorted image based on gradient domain and HSV space",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102797",
    "abstract": "Image quality assessment (IQA) plays an important role in digital image forensics. Due to the occurrence of contrast distortion during image acquisition and manipulation, IQA for contrast is a major issue. And it is vital for benchmarking and optimizing the image tampering detection and contrast-enhancement algorithms. In this paper, a new no-reference/blind image quality assessment (IQA) metric is proposed for evaluating image contrast. This research seeks for the inter-relationship between contrast distortion and visual perception quality. The comprehensive quality metric is obtained by combining local binary pattern (LBP) descriptor on gradient domain with color moment on HSV color space. And a prediction model is trained with support vector regression (SVR). Extensive analysis and cross validation are performed on four contrast relevant image databases, which validates the superiority of our proposed blind technique over state-of-the-art no-reference IQA methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032030047X",
    "keywords": [
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Color space",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Contrast (vision)",
      "Distortion (music)",
      "Economics",
      "HSL and HSV",
      "Image (mathematics)",
      "Image quality",
      "Mathematics",
      "Medicine",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Support vector machine",
      "Virology",
      "Virus"
    ],
    "authors": [
      {
        "surname": "Lyu",
        "given_name": "Wenjing"
      },
      {
        "surname": "Lu",
        "given_name": "Wei"
      },
      {
        "surname": "Ma",
        "given_name": "Ming"
      }
    ]
  },
  {
    "title": "Pure intra mode decision in HEVC using optimized firefly algorithm",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102766",
    "abstract": "High Efficiency Video Coding (HEVC) is the latest encoder that increased the intra modes from 9 to 35 to efficiently handle the contents of the video. The HEVC’s test model (HM) selects the optimal intra mode using the brute force method which increases the complexity of HEVC. This work, firstly, investigates the feasibility of firefly algorithm (FFA) due to its exploration and exploitation characteristics to expedite the intra mode decision in HEVC. Secondly, a novel objective function is formulated for FFA to efficiently compute the brightness for intra modes in FFA. Thirdly, the parameters of FFA are made dynamic to adjust according to the contents of video sequences. Simulation results demonstrate that the nature inspired algorithm, FFA, pays off by saving a minimum of 27% of the total coding time on average and doesn’t sacrifice quality by limiting Bjontegaard delta bit rate (BD-BR) increase to only 0.98% on average.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032030016X",
    "keywords": [
      "Algorithm",
      "Algorithmic efficiency",
      "Biology",
      "Bit rate",
      "Brightness",
      "Brute force",
      "Coding (social sciences)",
      "Computer science",
      "Computer security",
      "Encoder",
      "Engineering",
      "Firefly algorithm",
      "Firefly protocol",
      "Limiting",
      "Mathematics",
      "Mechanical engineering",
      "Operating system",
      "Optics",
      "Particle swarm optimization",
      "Physics",
      "Real-time computing",
      "Simulation",
      "Statistics",
      "Zoology"
    ],
    "authors": [
      {
        "surname": "Tariq",
        "given_name": "Junaid"
      },
      {
        "surname": "Armghan",
        "given_name": "Ammar"
      },
      {
        "surname": "Ijaz",
        "given_name": "Amir"
      },
      {
        "surname": "Ashraf",
        "given_name": "Imran"
      }
    ]
  },
  {
    "title": "A real time expert system for anomaly detection of aerators based on computer vision and surveillance cameras",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102767",
    "abstract": "Aerators are essential and crucial auxiliary devices in intensive culture, especially in industrial culture in China. In this paper, we propose a real-time expert system for anomaly detection of aerators based on computer vision technology and existing surveillance cameras. The expert system includes two modules, i.e., object region detection and working state detection. First, we present a small object region detection method based on the region proposal idea. Moreover, we propose a novel algorithm called reference frame Kanade-Lucas-Tomasi (RF-KLT) algorithm for motion feature extraction in fixed regions. Then, we describe a dimension reduction method of time series for establishing a feature dataset with obvious boundaries between classes. Finally, we use machine learning algorithms to build the feature classifier. The proposed expert system can realize real-time, robust and cost-free anomaly detection of aerators in both the actual video dataset and the augmented video dataset. Demo is available at https://youtu.be/xThHRwu_cnI.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300171",
    "keywords": [
      "Anomaly detection",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Feature extraction",
      "Frame (networking)",
      "Linguistics",
      "Object detection",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Yeqi"
      },
      {
        "surname": "Yu",
        "given_name": "Huihui"
      },
      {
        "surname": "Gong",
        "given_name": "Chuanyang"
      },
      {
        "surname": "Chen",
        "given_name": "Yingyi"
      }
    ]
  },
  {
    "title": "A new lossless secret color image sharing scheme with small shadow size",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102768",
    "abstract": "Though there have been many research works carried out on the grayscale secret image sharing schemes, there is not sufficient research work available on the field of color secret image sharing schemes, especially on the lossless secret image sharing schemes. In the current work, we propose a new lossless secret color image sharing scheme with small shadow size. We directly deal with the three components of the intensities of the RGB values of each color pixel of the secret image. Using a mathematical transformation, we first embed the RGB intensity values of each secret pixel as an element of a suitably chosen finite field F. Then the whole mathematical operations are carried over the finite field F to generate shares using k - 1 degree polynomials in the polynomial ring F [ x ] . Unlike most of the image sharing schemes, we do not have the preprocessing stage in which the secret image is transferred into random image either by using Arnold Cat map or by using some chaotic maps to avoid residual image effect. From experimental simulations, it is evident that the measure of randomness of the shares generated using our algorithm is same as that of the shares generated using the preprocessing stage in which Arnold Cat map or chaotic map is used, resulting our scheme simpler and efficient. Moreover, our scheme produces shares having smaller size than the secret image. Finally the recovery of the secret image by the qualified set of participants is lossless.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300183",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Binary image",
      "Color image",
      "Computer science",
      "Computer vision",
      "Cryptography",
      "Data compression",
      "Digital image",
      "Grayscale",
      "Homomorphic secret sharing",
      "Image (mathematics)",
      "Image processing",
      "Image sharing",
      "Lossless compression",
      "Mathematics",
      "Pixel",
      "RGB color model",
      "Secret sharing",
      "Shamir's Secret Sharing",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Sardar",
        "given_name": "Md Kutubuddin"
      },
      {
        "surname": "Adhikari",
        "given_name": "Avishek"
      }
    ]
  },
  {
    "title": "Salient object detection in video using deep non-local neural networks",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102769",
    "abstract": "Detection of salient objects in image and video is of great importance in many computer vision applications. In spite of the fact that the state of the art in saliency detection for still images has been changed substantially over the last few years, there have been few improvements in video saliency detection. This paper proposes a novel non-local fully convolutional network architecture for capturing global dependencies more efficiently and investigates the use of recently introduced non-local neural networks in video salient object detection. The effect of non-local operations is studied separately on static and dynamic saliency detection in order to exploit both appearance and motion features. A novel deep non-local fully convolutional network architecture is introduced for video salient object detection and tested on two well-known datasets DAVIS and FBMS. The experimental results show that the proposed algorithm outperforms state-of-the-art video saliency detection methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300195",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Convolutional neural network",
      "Exploit",
      "Image (mathematics)",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Salient"
    ],
    "authors": [
      {
        "surname": "Shokri",
        "given_name": "Mohammad"
      },
      {
        "surname": "Harati",
        "given_name": "Ahad"
      },
      {
        "surname": "Taba",
        "given_name": "Kimya"
      }
    ]
  },
  {
    "title": "Spatio-temporal metric learning for individual recognition from locomotion",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102753",
    "abstract": "Individual recognition from locomotion is a challenging task owing to large intra-class and small inter-class variations. In this article, we present a novel metric learning method for individual recognition from skeleton sequences. Firstly, we propose to model articulated body on Riemannian manifold to describe the essence of human motion, which can reflect biometric signatures of the enrolled individuals. Then two spatia-temporal metric learning approaches are proposed, namely Spatio-Temporal Large Margin Nearest Neighbor (ST-LMNN) and Spatio-Temporal Multi-Metric Learning (STMM), to learn discriminant bilinear metrics which can encode the spatio-temporal structure of human motion. Specifically, the ST-LMNN algorithm extends the bilinear model into classical Large Margin Nearest Neighbor method, which learns a low-dimensional local linear embedding in the spatial and temporal domain, respectively. To further capture the unique motion pattern for each individual, the proposed STMM algorithm learns a set of individual-specific spatio-temporal metrics, which make the projected features of the same person closer to its class mean than that of different classes by a large margin. Beyond that, we present a new publicly available dataset for locomotion recognition to evaluate the influence of both internal and external covariant factors. According to the experimental results from the three public datasets, we believe that the proposed approaches are both able to achieve competitive results in individual recognition.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300031",
    "keywords": [
      "Artificial intelligence",
      "Bilinear interpolation",
      "Biometrics",
      "Class (philosophy)",
      "Computer science",
      "Computer vision",
      "Economics",
      "Embedding",
      "Large margin nearest neighbor",
      "Machine learning",
      "Margin (machine learning)",
      "Mathematics",
      "Metric (unit)",
      "Motion (physics)",
      "Operations management",
      "Pattern recognition (psychology)",
      "k-nearest neighbors algorithm"
    ],
    "authors": [
      {
        "surname": "Su",
        "given_name": "Yong"
      },
      {
        "surname": "An",
        "given_name": "Simin"
      },
      {
        "surname": "Feng",
        "given_name": "Zhiyong"
      },
      {
        "surname": "Xing",
        "given_name": "Meng"
      },
      {
        "surname": "Zhang",
        "given_name": "Jianhai"
      }
    ]
  },
  {
    "title": "Detecting spatiotemporal irregularities in videos via a 3D convolutional autoencoder",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2019.102747",
    "abstract": "Spatiotemporal irregularities (i.e., the uncommon appearance and motion patterns) in videos are difficult to detect, as they are usually not well defined and appear rarely in videos. We tackle this problem by learning normal patterns from regular videos, while treating irregularities as deviations from normal patterns. To this end, we introduce a 3D fully convolutional autoencoder (3D-FCAE) that is trainable in an end-to-end manner to detect both temporal and spatiotemporal irregularities in videos using limited training data. Subsequently, temporal irregularities can be detected as frames with high reconstruction errors, and irregular spatiotemporal patterns can be detected as blurry regions that are not well reconstructed. Our approach can accurately locate temporal and spatiotemporal irregularities thanks to the 3D fully convolutional autoencoder and the explored effective architecture. We evaluate the proposed autoencoder for detecting irregular patterns on benchmark video datasets with weak supervision. Comparisons with state-of-the-art approaches demonstrate the effectiveness of our approach. Moreover, the learned autoencoder shows good generalizability across multiple datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320319303682",
    "keywords": [
      "Artificial intelligence",
      "Autoencoder",
      "Benchmark (surveying)",
      "Cartography",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Generalizability theory",
      "Geography",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Yan",
        "given_name": "Mengjia"
      },
      {
        "surname": "Meng",
        "given_name": "Jingjing"
      },
      {
        "surname": "Zhou",
        "given_name": "Chunluan"
      },
      {
        "surname": "Tu",
        "given_name": "Zhigang"
      },
      {
        "surname": "Tan",
        "given_name": "Yap-Peng"
      },
      {
        "surname": "Yuan",
        "given_name": "Junsong"
      }
    ]
  },
  {
    "title": "Parallel spatial-temporal convolutional neural networks for anomaly detection and location in crowded scenes",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102765",
    "abstract": "Anomaly detection and location in crowded scenes have attracted a lot of attention in computer vision research community recently due to the increased applications of intelligent surveillance improve security in public. We propose a novel parallel spatial-temporal convolution neural networks model to detect and localize the abnormal behavior in video surveillance. Our approach contains two main steps. Firstly, considering the typical position of camera and the large number of background information, we introduce a novel spatial-temporal cuboid of interest detection method with varied-size cell structure and optical flow algorithm. Then, we use the parallel 3D convolution neural networks to describe the same behavior in different temporal-lengths. That step ensures that the most of behavior information in cuboids could be captured, also insures the reduction of information unrelated to the major behavior. The evaluation results on benchmark datasets show the superiority of our method compared to the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300158",
    "keywords": [
      "Anomaly detection",
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Cartography",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Cuboid",
      "Economics",
      "Finance",
      "Geography",
      "Geometry",
      "Image (mathematics)",
      "Mathematics",
      "Optical flow",
      "Pattern recognition (psychology)",
      "Position (finance)"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Zheng-ping"
      },
      {
        "surname": "Zhang",
        "given_name": "Le"
      },
      {
        "surname": "Li",
        "given_name": "Shu-fang"
      },
      {
        "surname": "Sun",
        "given_name": "De-gang"
      }
    ]
  },
  {
    "title": "Matrix-variate variational auto-encoder with applications to image process",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2019.102750",
    "abstract": "Variational Auto-Encoder (VAE) is an important probabilistic technology to model 1D vectorial data. However, when applying VAE model to 2D image, vectorization is necessary. Vectorization process may lead to dimension curse and lose valuable spatial information. To avoid these problems, we propose a novel VAE model based on matrix variables named as Matrix-variate Variational Auto-Encoder (MVVAE). In this model, input, hidden and latent variables are all in matrix form, therefore inherent spatial structure of 2D images can be maintained and utilized better. Especially, the latent variable is assumed to follow matrix Gaussian distribution which is more suitable for describing 2D images. To solve the weights and the posterior of latent variable, the variational inference process is given. The experiments are designed for three real-world application: reconstruction, denoising and completion. The experimental results demonstrate that MVVAE shows better performance than VAE and other probabilistic methods for modeling and processing 2D data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320319303712",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Composite material",
      "Computer science",
      "Computer vision",
      "Dimension (graph theory)",
      "Latent variable",
      "Latent variable model",
      "Materials science",
      "Mathematics",
      "Matrix (chemical analysis)",
      "Parallel computing",
      "Pattern recognition (psychology)",
      "Probabilistic logic",
      "Pure mathematics",
      "Vectorization (mathematics)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Jinghua"
      },
      {
        "surname": "Yan",
        "given_name": "Huixia"
      },
      {
        "surname": "Gao",
        "given_name": "Junbin"
      },
      {
        "surname": "Kong",
        "given_name": "Dehui"
      },
      {
        "surname": "Wang",
        "given_name": "Lichun"
      },
      {
        "surname": "Wang",
        "given_name": "Shaofan"
      },
      {
        "surname": "Yin",
        "given_name": "Baocai"
      }
    ]
  },
  {
    "title": "Color image enhancement with high saturation using piecewise linear gamut mapping",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102759",
    "abstract": "Most of the color image enhancement algorithms are implemented in two stages: gray scale image enhancement, which finds the target intensity, and then gamut mapping of the original color coordinates to the target. Therefore, hue preserving gamut mapping is an essential and crucial step, which influences colorfulness. In conventional color mapping methods, color saturation is reduced after intensity modification, which deteriorates subjective image quality. In this paper, a new color enhancement algorithm resulting in high color saturation is proposed. The proposed method employs multiplicative and additive color mapping to improve color saturation without clipping of a color component for increased target intensity as well as decreased cases. This new scheme is fast and effective, therefore, it can be employed to real time applications such as video signal processing.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300092",
    "keywords": [
      "Artificial intelligence",
      "Color balance",
      "Color depth",
      "Color difference",
      "Color histogram",
      "Color image",
      "Color model",
      "Color space",
      "Computer science",
      "Computer vision",
      "Enhanced Data Rates for GSM Evolution",
      "Gamma correction",
      "Gamut",
      "Hue",
      "ICC profile",
      "Image (mathematics)",
      "Image processing",
      "Mathematics",
      "RGB color model",
      "RGB color space"
    ],
    "authors": [
      {
        "surname": "Park",
        "given_name": "Junhee"
      },
      {
        "surname": "Lee",
        "given_name": "Byung-Uk"
      }
    ]
  },
  {
    "title": "Saliency detection using adversarial learning networks",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102761",
    "abstract": "This paper proposes a novel model for saliency detection using the adversarial learning networks, in which the generator is used to generate the saliency map and the discriminator is deployed to guide the training process of overall network. Concretely, the training procedure of our model consists of three steps including the training of generator, the training of discriminator, and the training throughout the overall network. The key point of training process lies in the discriminator, which is designed to provide the feedback information for the acceleration of the generator and the refinement of saliency map. Therefore, during the training stage of overall network, the output of the generator, i.e. the coarse saliency map, is fed into the discriminator, yielding the corresponding feedback information. Following this way, we can obtain the final generator with a higher performance. For testing, the obtained generator is employed to perform saliency detection. Extensive experiments on four challenging saliency detection datasets show that our model not only achieves the favorable performance against the state-of-the-art saliency models, but also possesses the faster convergence speed when training the proposed model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300110",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Convergence (economics)",
      "Detector",
      "Discriminator",
      "Economic growth",
      "Economics",
      "Generator (circuit theory)",
      "Geometry",
      "Key (lock)",
      "Machine learning",
      "Mathematics",
      "Meteorology",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics",
      "Point (geometry)",
      "Power (physics)",
      "Process (computing)",
      "Quantum mechanics",
      "Telecommunications",
      "Training (meteorology)"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Yong"
      },
      {
        "surname": "Liu",
        "given_name": "Zhi"
      },
      {
        "surname": "Zhou",
        "given_name": "Xiaofei"
      }
    ]
  },
  {
    "title": "Exploiting the local temporal information for video captioning",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102751",
    "abstract": "Typical video captioning methods are developed based on the encoder-decoder architecture. To better exploit the local temporal information, e.g., details about objects and their corresponding actions, we propose a reinforcement learning based method to predict the adaptive sliding window size sequentially for better event exploration. More specifically, we introduce the single Monte-Carlo sample to approximate the gradient of reward-based loss function. And the self-critical strategy is employed to estimate baseline reward to diminish the variance of gradients. Moreover, temporal attention is utilized to selectively focus on a subset of temporal frame representations while generating each word. In addition, to better initialize the decoder’s state, we utilize the motion features extracted by 3D CNNs with mean pooling to endow the decoder with the prior knowledge of the entire video. To evaluate the proposed method, experiments are performed on three public benchmark datasets: Microsoft Video Description Corpus (MSVD), MSR Video to Text challenge (MSR-VTT) and Charades. The experimental results demonstrate the effectiveness of our method by comparing with state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300018",
    "keywords": [
      "Accounting",
      "Artificial intelligence",
      "Baseline (sea)",
      "Benchmark (surveying)",
      "Business",
      "Closed captioning",
      "Computer science",
      "Computer security",
      "Encoder",
      "Event (particle physics)",
      "Exploit",
      "Focus (optics)",
      "Frame (networking)",
      "Geodesy",
      "Geography",
      "Geology",
      "Image (mathematics)",
      "Linguistics",
      "Oceanography",
      "Operating system",
      "Optics",
      "Philosophy",
      "Physics",
      "Pooling",
      "Quantum mechanics",
      "Reinforcement learning",
      "Telecommunications",
      "Variance (accounting)",
      "Word (group theory)"
    ],
    "authors": [
      {
        "surname": "Wei",
        "given_name": "Ran"
      },
      {
        "surname": "Mi",
        "given_name": "Li"
      },
      {
        "surname": "Hu",
        "given_name": "Yaosi"
      },
      {
        "surname": "Chen",
        "given_name": "Zhenzhong"
      }
    ]
  },
  {
    "title": "A new pyramidal opponent color-shape model based video shot boundary detection",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102754",
    "abstract": "Video shot boundary detection (VSBD) is one of the most essential criteria for many intelligent video analysis-related applications, such as video retrieval, indexing, browsing, categorization and summarization. VSBD aims to segment big video data into meaningful fragments known as shots. This paper put forwards a new pyramidal opponent colour-shape (POCS) model which can detect abrupt transition (AT) and gradual transition (GT) simultaneously, even in the presence of illumination changes, huge object movement between frames, and fast camera motion. First, the content of frames in the video subjected to VSBD is represented by the proposed POCS model. Consequently, the temporal nature of the POCS model is subjected to a suitable segment (SS) selection procedure in order to minimize the complexity of VSBD method. The SS from the video frames is examined for transitions within it using a bagged-trees classifier (BTC) learned on a balanced training set via parallel processing. To prove the superiority of the proposed VSBD algorithm, it is evaluated on the TRECVID 2001, TRECVID2007 and VIDEOSEG2004 data sets for classifying the basic units of video according to no transition (NT), AT and GT. The experimental evaluation results in an F1-score of 95.13%, 98.13% and 97.11% on the TRECVID 2001, TRECVID2007 and VIDEOSEG2004 data sets, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300043",
    "keywords": [
      "Artificial intelligence",
      "Automatic summarization",
      "Categorization",
      "Chemistry",
      "Classifier (UML)",
      "Computer science",
      "Computer vision",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Programming language",
      "Search engine indexing",
      "Set (abstract data type)",
      "Shot (pellet)",
      "Video processing",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Sasithradevi",
        "given_name": "A."
      },
      {
        "surname": "Mohamed Mansoor Roomi",
        "given_name": "S."
      }
    ]
  },
  {
    "title": "Retraction notice to “Cross-camera multi-person tracking by leveraging fast graph mining algorithm” [J. Vis. Commun. Image R. 55 (2018) 711–719]",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102755",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300055",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Graph",
      "Image (mathematics)",
      "Law",
      "Notice",
      "Pedagogy",
      "Political science",
      "Psychology",
      "Theoretical computer science",
      "Tracking (education)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Caiyou"
      },
      {
        "surname": "Huang",
        "given_name": "Yuteng"
      },
      {
        "surname": "Wang",
        "given_name": "Zhiqiang"
      },
      {
        "surname": "Jiang",
        "given_name": "Hongcheng"
      },
      {
        "surname": "Yan",
        "given_name": "Dongfeng"
      }
    ]
  },
  {
    "title": "Parallel spatial-temporal convolutional neural networks for anomaly detection and location in crowded scenes",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102765",
    "abstract": "Anomaly detection and location in crowded scenes have attracted a lot of attention in computer vision research community recently due to the increased applications of intelligent surveillance improve security in public. We propose a novel parallel spatial-temporal convolution neural networks model to detect and localize the abnormal behavior in video surveillance. Our approach contains two main steps. Firstly, considering the typical position of camera and the large number of background information, we introduce a novel spatial-temporal cuboid of interest detection method with varied-size cell structure and optical flow algorithm. Then, we use the parallel 3D convolution neural networks to describe the same behavior in different temporal-lengths. That step ensures that the most of behavior information in cuboids could be captured, also insures the reduction of information unrelated to the major behavior. The evaluation results on benchmark datasets show the superiority of our method compared to the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300158",
    "keywords": [
      "Anomaly detection",
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Cartography",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Cuboid",
      "Economics",
      "Finance",
      "Geography",
      "Geometry",
      "Image (mathematics)",
      "Mathematics",
      "Optical flow",
      "Pattern recognition (psychology)",
      "Position (finance)"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Zheng-ping"
      },
      {
        "surname": "Zhang",
        "given_name": "Le"
      },
      {
        "surname": "Li",
        "given_name": "Shu-fang"
      },
      {
        "surname": "Sun",
        "given_name": "De-gang"
      }
    ]
  },
  {
    "title": "Blind tone mapped image quality assessment with image segmentation and visual perception",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102752",
    "abstract": "With tone mapping, high dynamic range (HDR) image contents can be displayed on low dynamic range (LDR) display devices, in which some important visual information may be distorted. Thus, the tone mapped image (TMI) quality assessment is one of important issues in HDR image/video processing fields. Considering the difference of visual distortion degrees between the flat and complex regions in TMI, and considering that high-quality TMI should preserve as much information as possible of its original HDR image especially in the high/low luminance regions, this paper proposes a new blind TMI quality assessment method with image segmentation and visual perception. First, we design different features to describe the distortion of TMI’s different regions with two kinds of TMI segmentation. Then, considering that there lacks an efficient algorithm to quantify the importance of features, a feature clustering scheme is designed to eliminate the poor effect feature components in the extracted features to improve the effectiveness of the selected features. Finally, considering the diversity of tone mapping operator (TMO), which may cause global and local distortion of TMI, some other global features are also combined. At last, a final feature vector is formed to synthetically describe the distortion in TMI and used to blindly predict the TMI’s quality. Experimental results in the public ESPL-LIVE HDR database show that the Pearson linear correlation coefficient and Spearman rank order correlation coefficient of the proposed method reach 0.8302 and 0.7887, respectively, which is superior to the state-of-the-art blind TMI quality assessment methods, and it means that the proposed method is highly consistent with human visual perception.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732032030002X",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Image quality",
      "Literature",
      "Neuroscience",
      "Perception",
      "Psychology",
      "Segmentation",
      "Tone (literature)"
    ],
    "authors": [
      {
        "surname": "Chi",
        "given_name": "Biwei"
      },
      {
        "surname": "Yu",
        "given_name": "Mei"
      },
      {
        "surname": "Jiang",
        "given_name": "Gangyi"
      },
      {
        "surname": "He",
        "given_name": "Zhouyan"
      },
      {
        "surname": "Peng",
        "given_name": "Zongju"
      },
      {
        "surname": "Chen",
        "given_name": "Fen"
      }
    ]
  },
  {
    "title": "Retraction notice to “A method of multi-criteria set recognition based on deep feature representation” [J. Vis. Commun. Image R. 55 (2018) 756–760]",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102757",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300079",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Law",
      "Linguistics",
      "Mathematics",
      "Notice",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Programming language",
      "Representation (politics)",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Caiyou"
      },
      {
        "surname": "Zhou",
        "given_name": "Man"
      },
      {
        "surname": "Yang",
        "given_name": "Hongzhen"
      },
      {
        "surname": "Shen",
        "given_name": "Xiaojun"
      },
      {
        "surname": "Wang",
        "given_name": "Yanbo"
      }
    ]
  },
  {
    "title": "Retraction notice to “Deep network for visual saliency prediction by encoding image composition” [J. Vis. Commun. Image R. 55 (2018) 789–794]",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102756",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300067",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Composition (language)",
      "Computer science",
      "Computer vision",
      "Encoding (memory)",
      "Image (mathematics)",
      "Law",
      "Literature",
      "Notice",
      "Pattern recognition (psychology)",
      "Political science"
    ],
    "authors": [
      {
        "surname": "Dai",
        "given_name": "Bo"
      },
      {
        "surname": "Ye",
        "given_name": "Weijing"
      },
      {
        "surname": "Zheng",
        "given_name": "Jing"
      },
      {
        "surname": "Chai",
        "given_name": "Qianyi"
      },
      {
        "surname": "Yao",
        "given_name": "Yiyang"
      }
    ]
  },
  {
    "title": "Sparsity adaptive matching pursuit for face recognition",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102764",
    "abstract": "Sparse representation methods have exhibited promising performance for pattern recognition. However, these methods largely rely on the data sparsity available in advance and are usually sensitive to noise in the training samples. To solve these problems, this paper presents sparsity adaptive matching pursuit based sparse representation for face recognition (SAMPSR). This method adaptively explores the valid training samples that exactly represent the test via iterative updating. Next, the test samples are reconstructed via the valid training samples, and classification is performed subsequently. The two-phase strategy helps to improve the discriminating power of class probability distribution, and thus alleviates effect of the noise from the training samples to some extent and correctly performs classification. In addition, the method solves the sparse coefficient by comparing the residual between the test sample and the reconstructed sample instead of using the sparsity. A large number of experiments show that our method achieves promising performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300146",
    "keywords": [
      "Artificial intelligence",
      "Compressed sensing",
      "Computer science",
      "Computer vision",
      "Face (sociological concept)",
      "Facial recognition system",
      "Linguistics",
      "Matching (statistics)",
      "Matching pursuit",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Speech recognition",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yuhong"
      },
      {
        "surname": "Peng",
        "given_name": "Yali"
      },
      {
        "surname": "Liu",
        "given_name": "Shigang"
      },
      {
        "surname": "Li",
        "given_name": "Jun"
      },
      {
        "surname": "Wang",
        "given_name": "Xili"
      }
    ]
  },
  {
    "title": "A novel multi-focus image fusion method based on distributed compressed sensing",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2020.102760",
    "abstract": "Multi-focus image fusion aims to produce an all-in-focus image by merging multiple partially focused images of the same scene. The main work is identifying the focused region and then composing all the focused regions. In this paper, a novel efficient multi-focus image fusion method based on distributed compressed sensing (DCS) is proposed. Firstly, the low-frequency and high-frequency images are obtained by comparing the variance of the source images, which are further utilized to get the low-frequency and high-frequency dictionaries. Secondly, DCS using joint sparsity model-1 (JSM-1) is applied to reconstruct the precise high-frequency images. Thirdly, the decision map is obtained based on all the high-frequency images and then improved by the morphological processing. Finally, the focused pixels are chosen from the source images through the decision map. Experimental results indicate that the proposed DCS-based method can be competitive with or even outperform some state-of-the-art methods in terms of both visual and quantitative metric evaluations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320320300109",
    "keywords": [
      "Accounting",
      "Artificial intelligence",
      "Business",
      "Compressed sensing",
      "Computer science",
      "Computer vision",
      "Economics",
      "Focus (optics)",
      "Fusion",
      "Image (mathematics)",
      "Image fusion",
      "Linguistics",
      "Metric (unit)",
      "Operations management",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Pixel",
      "Variance (accounting)"
    ],
    "authors": [
      {
        "surname": "Fu",
        "given_name": "Guan-Peng"
      },
      {
        "surname": "Hong",
        "given_name": "Shao-Hua"
      },
      {
        "surname": "Li",
        "given_name": "Fu-Lin"
      },
      {
        "surname": "Wang",
        "given_name": "Lin"
      }
    ]
  },
  {
    "title": "Face presentation attack detection based on chromatic co-occurrence of local binary pattern and ensemble learning",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2019.102746",
    "abstract": "To counter face presentation attacks in face recognition (FR), color texture has been successfully used for face presentation attack detection (PAD) in recent years. However, the existing research does not fully consider the correlation between different color channels as well as the optimization of classification for face PAD. To resolve these limitations, a face PAD scheme based on chromatic co-occurrence of local binary pattern (CCoLBP) and ensemble learning (EL) is proposed in this paper. A color distortion-based face PAD model is first built, and then the chromatic discrepancies between bona fide faces and artefacts are analyzed. After that, CCoLBP is extracted as the feature to characterize these discrepancies. Meanwhile, an EL based classifier is put forward to reduce the effect of class imbalance and to improve the generalization ability. Experimental results and analysis indicate that the proposed scheme can achieve an overall good performance. Moreover, it can achieve significant improvement in the cross-database test, and its computational complexity can meet the requirement of real time applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320319303670",
    "keywords": [
      "Amplifier",
      "Arithmetic",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Binary classification",
      "Binary number",
      "Chromatic scale",
      "Classifier (UML)",
      "Combinatorics",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Distortion (music)",
      "Face (sociological concept)",
      "Facial recognition system",
      "Feature (linguistics)",
      "Generalization",
      "Histogram",
      "Image (mathematics)",
      "Linguistics",
      "Local binary patterns",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Social science",
      "Sociology",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Peng",
        "given_name": "Fei"
      },
      {
        "surname": "Qin",
        "given_name": "Le"
      },
      {
        "surname": "Long",
        "given_name": "Min"
      }
    ]
  },
  {
    "title": "A novel dark channel prior guided variational framework for underwater image restoration",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2019.102732",
    "abstract": "Image captured underwater often suffers from low contrast, color distortion and noise problems, which is caused by absorbing and scattering before the light reaches the camera when traveling through water. Underwater image enhancement and restoration from a single image is known to be an ill-posed problem. To overcome these limitations, we establish an underwater total variation (UTV) model relying on underwater dark channel prior (UDCP), in which UDCP is used to estimate the transmission map. We design the data item and smooth item of the unified variational model based on the underwater image formation model. We further employ the alternating direction method of multipliers (ADMM) to accelerate the solving procedure. Numerical experiential results demonstrate that our underwater variational method obtains a good outcome on dehazing and denoising. Furthermore, compared with several other state-of-the-art algorithms, the proposed approach achieves better visual quality, which is illustrated by examples and statistics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320319303530",
    "keywords": [
      "Algorithm",
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Channel (broadcasting)",
      "Computer science",
      "Computer vision",
      "Distortion (music)",
      "Geology",
      "Image (mathematics)",
      "Image processing",
      "Image quality",
      "Image restoration",
      "Noise (video)",
      "Noise reduction",
      "Oceanography",
      "Telecommunications",
      "Transmission (telecommunications)",
      "Underwater"
    ],
    "authors": [
      {
        "surname": "Hou",
        "given_name": "Guojia"
      },
      {
        "surname": "Li",
        "given_name": "Jingming"
      },
      {
        "surname": "Wang",
        "given_name": "Guodong"
      },
      {
        "surname": "Yang",
        "given_name": "Huan"
      },
      {
        "surname": "Huang",
        "given_name": "Baoxiang"
      },
      {
        "surname": "Pan",
        "given_name": "Zhenkuan"
      }
    ]
  },
  {
    "title": "Objects and scenes classification with selective use of central and peripheral image content",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2019.102698",
    "abstract": "The human visual recognition system is more efficient than any current robotic vision setting. One reason for this superiority is that humans utilize different fields of vision, depending on the recognition task. For instance, experiments on human subjects show that the peripheral vision is more useful than the central vision in recognizing scenes. We tested our recently-developed model, that is, the elastic net-regularized hierarchical MAX (En-HMAX), in recognizing objects and scenes. In various experimental conditions, images were occluded with windows and scotomas of varying sizes. With this model, classification accuracies of up to 90 % for objects and scenes were possible. Modelling human experiments, window and scotoma analysis with the En-HMAX model revealed that object and scene recognition are sensitive to the availability of data in the centre and the periphery of the images, respectively. Similarly, results of deep learning models have shown that the classification accuracy diminishes dramatically in the absence of the peripheral vision. These differences led us to further analyse the performance of the En-HMAX model with the parafoveal versus peripheral areas of vision, in a second study. Results of the second study show that approximately 50 % of the visual field would be sufficient to achieve 96 % accuracy in the classification of unseen images. The En-HMAX model adopts a relative order of importance, similar to the human visual system, depending on the image category. We showed that utilizing the relevant regions of vision can significantly reduce the image processing time and size.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320319303190",
    "keywords": [
      "Artificial intelligence",
      "Blind spot",
      "Cognitive neuroscience of visual object recognition",
      "Computer science",
      "Computer vision",
      "Human visual system model",
      "Image (mathematics)",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Peripheral vision"
    ],
    "authors": [
      {
        "surname": "Alameer",
        "given_name": "Ali"
      },
      {
        "surname": "Degenaar",
        "given_name": "Patrick"
      },
      {
        "surname": "Nazarpour",
        "given_name": "Kianoush"
      }
    ]
  },
  {
    "title": "Sparseness embedding in bending of space and time; a case study on unsupervised 3D action recognition",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2019.102691",
    "abstract": "Human action recognition from skeletal data is one of the most popular topics in computer vision which has been widely studied in the literature, occasionally with some very promising results. However, being supervised, most of the existing methods suffer from two major drawbacks; (1) too much reliance on massive labeled data and (2) high sensitivity to outliers, which in turn hinder their applications in such real-world scenarios as recognizing long-term and complex movements. In this paper, we propose a novel unsupervised 3D action recognition method called Sparseness Embedding in which the spatiotemporal representation of action sequences is nonlinearly projected into an unwarped feature representation medium, where unlike the original curved space, one can easily apply the Euclidean metrics. Our strategy can simultaneously integrate the characteristics of nonlinearity, sparsity, and space curvature of sequences into a single objective function, leading to a more robust and highly compact representation of discriminative attributes without any need to label information. Moreover, we propose a joint learning strategy for dealing with the heterogeneity of the temporal and spatial characteristics of action sequences. A set of extensive experiments on six publicly available databases, including UTKinect, TST fall, UTD-MHAD, CMU, Berkeley MHAD, and NTU RGB+D demonstrates the superiority of our method compared with the state-of-the-art algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320319303128",
    "keywords": [
      "Action (physics)",
      "Action recognition",
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Embedding",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Space (punctuation)"
    ],
    "authors": [
      {
        "surname": "Mohammadzade",
        "given_name": "Hoda"
      },
      {
        "surname": "Tabejamaat",
        "given_name": "Mohsen"
      }
    ]
  },
  {
    "title": "A novel change-detection scheduler for a network of depth sensors",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2019.102733",
    "abstract": "In many monitoring applications such as smart home and surveillance, deployment of multiple depth sensors increases monitoring area and offers better occlusion handling which is not sensitive to illumination condition in comparison with RGB sensors. However, multiple sensors also increase the volume of data associated with signal processing alongside the associated computational complexity and power consumption. In order to address these drawbacks, this paper proposes a novel change detection algorithm that can be used as a part of a sensor scheduler in a centralized (e.g. star) network configuration. Initially, each sensor in the network performs a unique single scan of the common environment in order to detect any incremental changes in the sensed depth signal. This initial change detection is then used as a basis for several follow-up tasks such as foreground segmentation, background detection, target detection, and tracking for monitoring tasks. Here, instead of processing a complete depth frame, we proposed to utilize a collection of 1D scans of the depth frames. A confidence function is defined that can be used to estimate the reliability of the detected changes in each sensor and to reduce any false positive events which can be triggered by the noise and outliers. Analysis of the proposed confidence function is carried out through performance analysis in the presence of sensor noise and other parameters which can affect the reliability of the sensed data of each sensor. Finally, a score function is defined based on the confidence of the detected parameters and sensor resolution in order to rank and match sensors with the associated objects to be tracked. It results in tracking target(s) by a sensor (or sensors) that offer a high tracking score. This approach offers many advantages such as decreasing the overall system power consumption by placing the sensors with a low confidence value on standby mode and reducing the overall computational overheads.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320319303542",
    "keywords": [
      "Artificial intelligence",
      "Change detection",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Frame (networking)",
      "Image (mathematics)",
      "Key distribution in wireless sensor networks",
      "Noise (video)",
      "Outlier",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Real-time computing",
      "Reliability (semiconductor)",
      "Telecommunications",
      "Visual sensor network",
      "Wireless",
      "Wireless network",
      "Wireless sensor network"
    ],
    "authors": [
      {
        "surname": "Rasoulidanesh",
        "given_name": "Maryam S."
      },
      {
        "surname": "Payandeh",
        "given_name": "Shahram"
      }
    ]
  },
  {
    "title": "Convolutional sparse coding framework for compressive spectral imaging",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2019.102690",
    "abstract": "Spectral images (SI) can be represented as 3D-arrays of spatial information across multiple wavelengths. Compressive Spectral Imaging (CSI) reduces sensing costs by sensing compressed versions of the scene, recovering a suitable version of the original SI solving a sparsity-inducing inverse problem. On the other hand, Convolutional Sparse Coding (CSC) has been successfully proved for representing gray-scale images, however it misses any correlation between images. This work considers the spatial-spectral correlation within SIs introducing an extension of the CSC signal model describing the SI as the sum of convolutions of 3D sparse coefficient maps with their respective 3D dictionary filters. Furthermore, we use the proposed CSC framework for recovering SIs from CSI measurements. The simulations results, using two different CSI acquisition architectures, show that the proposed CSC framework yields better representations of the SIs than those obtained under the traditional sparse signal representation approach, improving the quality of the recovered SIs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320319303116",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Coding (social sciences)",
      "Compressed sensing",
      "Computer science",
      "Convolutional code",
      "Decoding methods",
      "Mathematics",
      "Neural coding",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Barajas-Solano",
        "given_name": "Crisostomo"
      },
      {
        "surname": "Ramirez",
        "given_name": "Juan-Marcos"
      },
      {
        "surname": "Arguello",
        "given_name": "Henry"
      }
    ]
  },
  {
    "title": "Anti-forensics of median filtering and contrast enhancement",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2019.102682",
    "abstract": "Digital images can be convincingly edited using image editing tools. In order to identify such image processing operations, various forensic techniques have been proposed. In response, anti-forensic operations designed as counter-measures have been devised. In this paper, we propose an anti-forensic technique to counter spatial domain forensic detectors and demonstrate its accuracy on popular image manipulation operations such as median filtering and contrast enhancement. The integrated anti-forensic attack is formulated as an optimization problem. The proposed optimization modifies the image so as to incorporate the median filtering or contrast enhancement operation while ensuring that its spatial characteristics do not change significantly. Through a series of experiments, we prove that the proposed algorithm can severely degrade the performance of median filtering and contrast enhancement detectors. The proposed algorithm also outperforms popular anti-forensic algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320319303037",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Contrast (vision)",
      "Contrast enhancement",
      "Data mining",
      "Detector",
      "Image (mathematics)",
      "Image enhancement",
      "Image manipulation",
      "Image processing",
      "Magnetic resonance imaging",
      "Median filter",
      "Medicine",
      "Radiology",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Sharma",
        "given_name": "Shishir"
      },
      {
        "surname": "Ravi",
        "given_name": "Hareesh"
      },
      {
        "surname": "Subramanyam",
        "given_name": "A.V."
      },
      {
        "surname": "Emmanuel",
        "given_name": "Sabu"
      }
    ]
  },
  {
    "title": "Superpixels extracted via region fusion with boundary constraint",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2019.102743",
    "abstract": "In this paper, we present an accurate superpixel algorithm by region fusion with boundary constraint (RFBC). Superpixels with regular shape and high boundary adherence can be generated in weak boundary and complex texture regions through our algorithm. RFBC includes two steps which are initial segmentation and region fusion respectively. In initial segmentation, broken Canny edges are connected through edge closing algorithm. Subsequently, the closed Canny edges and SLIC superpixel edges are combined together to form the incipient superpixels. In region fusion, gray Gaussian distribution and adjacent relation are used as priori to compute the degree of similarity across incipient superpixels in GBP algorithm. For concreteness, the information of similarity is propagated between regions and the most similar regions are fused, which are accomplished alternatingly to preserve accurate boundaries. Extensive experiments on the Berkeley segmentation benchmark show that the proposed algorithm outperforms the most state-of-the-art algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320319303645",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Boundary (topology)",
      "Computer science",
      "Computer vision",
      "Constraint (computer-aided design)",
      "Fusion",
      "Geometry",
      "Image (mathematics)",
      "Image segmentation",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Segmentation",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Li"
      },
      {
        "surname": "Li",
        "given_name": "Zhihui"
      },
      {
        "surname": "Men",
        "given_name": "Chaoguang"
      },
      {
        "surname": "Liu",
        "given_name": "Yongmei"
      }
    ]
  },
  {
    "title": "An efficient high-capacity reversible data hiding scheme for encrypted images",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2019.102744",
    "abstract": "Reversible data hiding in encrypted images is an effective technique to embed information in encrypted domain, without knowing the original content of the image or the encryption key. In this paper, a high-capacity reversible data hiding scheme for encrypted images based on MSB (most significant bit) prediction is proposed. Since the prediction is not always accurate, it is necessary to identify the prediction error and store this information in the location map. The stream cipher is then used to encrypt the original image directly. During the data hiding phase, up to three MSBs of each available pixel in the encrypted image are substituted by the bits of the secret message. At the receiving end, the embedded data can be extracted without any errors and the original image can be perfectly reconstructed by utilizing MSB prediction. Experimental results show that the scheme can achieve higher embedding capacity than most related methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320319303657",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer network",
      "Computer science",
      "Computer security",
      "Data mining",
      "Embedding",
      "Encryption",
      "Image (mathematics)",
      "Information hiding",
      "Key (lock)",
      "Mathematical analysis",
      "Mathematics",
      "Pixel",
      "Scheme (mathematics)",
      "Stream cipher",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Guan",
        "given_name": "Bo"
      },
      {
        "surname": "Xu",
        "given_name": "Dawen"
      }
    ]
  },
  {
    "title": "Spline-like Chebyshev polynomial model for compressive imaging",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2019.102731",
    "abstract": "This paper introduces a novel spline-like parametric model for an image representation obtained directly from compressive imaging (CI) measurements. As a representation basis we use Chebyshev polynomials. To avoid common problem of blocking artifacts in block-based reconstruction algorithms, a desired number of derivatives are equated on the block boundaries in a spline-like fashion. This introduces a new set of constraints that fits into CI setup. Unlike splines, the proposed system of equations is underdetermined to provide a necessary degree of freedom for achieving sparsity by solving an ℓ 1 optimization problem. Recovered coefficients of the parametric model can be further used for image processing where operations can be elegantly defined and calculated. This offers a new framework for acquisition and processing of analog signals without converting them into samples. Experiments on real measurements show that our model achieves sparse representation without visible blocking artifacts from a reduced set of CI measurements.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320319303529",
    "keywords": [
      "Algorithm",
      "Block (permutation group theory)",
      "Chebyshev filter",
      "Chebyshev polynomials",
      "Compressed sensing",
      "Computer science",
      "Computer vision",
      "Engineering",
      "Geometry",
      "Law",
      "Mathematical analysis",
      "Mathematics",
      "Parametric statistics",
      "Political science",
      "Politics",
      "Polynomial",
      "Representation (politics)",
      "Spline (mechanical)",
      "Statistics",
      "Structural engineering",
      "Underdetermined system"
    ],
    "authors": [
      {
        "surname": "Vlašić",
        "given_name": "Tin"
      },
      {
        "surname": "Ralašić",
        "given_name": "Ivan"
      },
      {
        "surname": "Tafro",
        "given_name": "Azra"
      },
      {
        "surname": "Seršić",
        "given_name": "Damir"
      }
    ]
  },
  {
    "title": "Video compressed sensing reconstruction based on structural group sparsity and successive approximation estimation model",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2019.102734",
    "abstract": "The existing video compressed sensing (CS) algorithms for inconsistent sampling ignore the joint correlations of video signals in space and time, and their reconstruction quality and speed need further improvement. To balance reconstruction quality with computational complexity, we introduce a structural group sparsity model for use in the initial reconstruction phase and propose a weight-based group sparse optimization algorithm acting in joint domains. Then, a coarse-to-fine optical flow estimation model with successive approximation is introduced for use in the interframe prediction stage to recover non-key frames through alternating optical flow estimation and residual sparse reconstruction. Experimental results show that, compared with the existing algorithms, the proposed algorithm achieves a peak signal-to-noise ratio gain of 1–3dB and a multi-scale structural similarity gain of 0.01–0.03 at a low time complexity, and the reconstructed frames not only have good edge contours but also retain textural details.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320319303554",
    "keywords": [
      "Algorithm",
      "Architectural engineering",
      "Artificial intelligence",
      "Compressed sensing",
      "Computational complexity theory",
      "Computer science",
      "Engineering",
      "Frame (networking)",
      "Image (mathematics)",
      "Inter frame",
      "Iterative reconstruction",
      "Joint (building)",
      "Noise (video)",
      "Radar",
      "Reconstruction algorithm",
      "Reference frame",
      "Residual",
      "Signal processing",
      "Signal reconstruction",
      "Similarity (geometry)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Jian"
      },
      {
        "surname": "Chen",
        "given_name": "Zhifeng"
      },
      {
        "surname": "Su",
        "given_name": "Kaixiong"
      },
      {
        "surname": "Peng",
        "given_name": "Zheng"
      },
      {
        "surname": "Ling",
        "given_name": "Nam"
      }
    ]
  },
  {
    "title": "Secure and robust host-adapted color image watermarking using inter-layered wavelet-packets",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2019.102726",
    "abstract": "This work presents a secure and robust color image watermarking for copyright protection applications, that is based on exploiting the multi-spectral properties of the primary color components of the RGB image. The proposed scheme employs the interconnection between the subbands of the primary color components in the wavelet-packet domain. The scheme is constructed to be adaptive, in the sense that the watermark bits are embedded in safe locations, depending on the inter-layer energy of coefficients in the wavelet-packets. The scheme immunity to attacks is improved by applying a two-level security procedure. To validate the high performance of the proposed scheme, several experimental tests were conducted and a comparative analysis was provided. The obtained results have shown improved watermarking robustness against a wide range of attacks while preserving a high watermarking imperceptibility.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320319303475",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Color image",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Digital watermarking",
      "Ecology",
      "Gene",
      "Host (biology)",
      "Image (mathematics)",
      "Image processing",
      "Network packet",
      "RGB color model",
      "Robustness (evolution)",
      "Watermark",
      "Wavelet",
      "Wavelet packet decomposition",
      "Wavelet transform"
    ],
    "authors": [
      {
        "surname": "Al-Otum",
        "given_name": "Hazem Munawer"
      }
    ]
  },
  {
    "title": "Visualization, Discriminability and Applications of Interpretable Saak Features",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2019.102699",
    "abstract": "In this work, we study the power of Saak features as an effort towards interpretable deep learning. Being inspired by the operations of convolutional layers of convolutional neural networks, multi-stage Saak transform was proposed. Based on this foundation, we provide an in-depth examination on Saak features, which are coefficients of the Saak transform, by analyzing their properties through visualization and demonstrating their applications in image classification. Being similar to CNN features, Saak features at later stages have larger receptive fields, yet they are obtained in a one-pass feedforward manner without backpropagation. The whole feature extraction process is transparent and is of extremely low complexity. The discriminant power of Saak features is demonstrated, and their classification performance in three well-known datasets (namely, MNIST, CIFAR-10 and STL-10) is shown by experimental results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320319303207",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Backpropagation",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Discriminant",
      "Feature (linguistics)",
      "Feature extraction",
      "Linguistics",
      "MNIST database",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Process (computing)",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Manimaran",
        "given_name": "Abinaya"
      },
      {
        "surname": "Ramanathan",
        "given_name": "Thiyagarajan"
      },
      {
        "surname": "You",
        "given_name": "Suya"
      },
      {
        "surname": "Kuo",
        "given_name": "C.-C. Jay"
      }
    ]
  },
  {
    "title": "Low complexity high efficiency coding of light fields using ensemble classifiers",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2019.102742",
    "abstract": "Light field images can be efficiently compressed using standard video codecs, such as the High Efficiency Video Coding (HEVC). However, the huge amount of data combined with the high computational complexity of HEVC, poses limitations on high-speed light field capturing and storage. This paper presents a contribution for low complexity encoding of light fields, in different formats using HEVC, based on a Random Forests ensemble classifier. Optimal features for training the classifier are found through a score fusion based approach. Using the HEVC still image profile, the proposed method gives speed-up of 56.23% for sub-aperture images. For pseudo video format, the proposed method outperforms others available in the literature, yielding an average speed-up of 62.18%, 56.54% and 44.73% for Random Access, Low-delay Main and All-Intra profiles respectively, with negligible decrease in RD performance. These are novel results in fast coding of light fields, which are useful for further research and benchmarking.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320319303633",
    "keywords": [
      "Algorithm",
      "Algorithmic efficiency",
      "Artificial intelligence",
      "Benchmarking",
      "Business",
      "Classifier (UML)",
      "Codec",
      "Coding (social sciences)",
      "Computational complexity theory",
      "Computer hardware",
      "Computer science",
      "Computer vision",
      "Light field",
      "Marketing",
      "Mathematics",
      "Operating system",
      "Random access",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Tahir",
        "given_name": "Muhammad"
      },
      {
        "surname": "Taj",
        "given_name": "Imtiaz A."
      },
      {
        "surname": "Assuncao",
        "given_name": "Pedro A."
      },
      {
        "surname": "Asif",
        "given_name": "Muhammad"
      }
    ]
  },
  {
    "title": "Identifying biometrics in the wild – A time, erosion and neural inspired framework for gait identification",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2019.102725",
    "abstract": "This paper evaluates the performance of contemporary gait identification systems. A time, erosion and neural inspired framework (TEN-FE) for gait identification was proposed to augment the performance of gait identification systems. Performance of TEN-FE framework was evaluated using CASIA and OU-ISIR large population dataset. Proposed framework relies on CNN and Reinforcement Learning to restrict the impact of confounding factors like baggage and bulky clothing on the accuracy of gait identification systems. Difference in gait signature due to time was also considered and normalized. The results observed a clear increase in system’s performance with minimal complexity and least hardware requirements.",
    "link": "https://www.sciencedirect.com/science/article/pii/S1047320319303463",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Biology",
      "Biometrics",
      "Botany",
      "Clothing",
      "Computer science",
      "Computer vision",
      "Environmental health",
      "Gait",
      "Geography",
      "Identification (biology)",
      "Medicine",
      "Pattern recognition (psychology)",
      "Physical medicine and rehabilitation",
      "Population"
    ],
    "authors": [
      {
        "surname": "Singh",
        "given_name": "Jaiteg"
      },
      {
        "surname": "Goyal",
        "given_name": "Gaurav"
      }
    ]
  },
  {
    "title": "IR saliency detection via a GCF-SB visual attention framework",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2019.102706",
    "abstract": "Infrared (IR) saliency detection with high detection accuracy is a challenging task due to the complex background and low contrast of IR images. In this paper, an IR saliency detection method via a new visual attention framework is proposed, which comprises two phases. In the first phase, a Gray & Contrast Features (GCF) model is established, in which the IR image is processed in two feature channels, a gray feature channel and a contrast feature channel. And then a primary feature map can be obtained by fusing the gray and contrast features from these two channels, which is the basis of the second phase. In the second phase, a Similarity-based Bayes (SB) model is established, in which two prior probabilities and two likelihood functions are calculated according to the previously obtained primary feature map. Finally, the saliency map is calculated with the obtained prior probabilities and likelihood functions by Bayes formula. Experimental results indicate that the proposed method can effectively reduce noise and enhance contrast of IR images with complex background and low contrast, and obtain a higher detection accuracy and robustness than seven state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732031930327X",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Channel (broadcasting)",
      "Chemistry",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Contrast (vision)",
      "Feature (linguistics)",
      "Gene",
      "Linguistics",
      "Mathematics",
      "Naive Bayes classifier",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Robustness (evolution)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Yufei"
      },
      {
        "surname": "Song",
        "given_name": "Yong"
      },
      {
        "surname": "Li",
        "given_name": "Xu"
      },
      {
        "surname": "Sulaman",
        "given_name": "Muhammad"
      },
      {
        "surname": "Guo",
        "given_name": "Zhengkun"
      },
      {
        "surname": "Yang",
        "given_name": "Xin"
      },
      {
        "surname": "Wang",
        "given_name": "Fengning"
      },
      {
        "surname": "Hao",
        "given_name": "Qun"
      }
    ]
  },
  {
    "title": "Unsupervised fisheye image correction through bidirectional loss with geometric prior",
    "journal": "Journal of Visual Communication and Image Representation",
    "year": "2020",
    "doi": "10.1016/j.jvcir.2019.102692",
    "abstract": "Neural network based methods for fisheye distortion correction are effective and increasingly popular, although training network require a high amount of labeled data. In this paper, we propose an unsupervised fisheye correction network to address the aforementioned issue. During the training process, the predicted parameters are employed to correct strong distortion that exists in the fisheye image and synthesize the corresponding distortion using the original distortion-free image. Thus, the network is constrained with bidirectional loss to obtain more accurate distortion parameters. We calculate the two losses at the image level as opposed to directly minimizing the difference between the predicted and ground truth of distortion parameters. Additionally, we leverage the geometric prior that the distortion distribution depends on the geometric regions of fisheye images and the straight line should be straight in the corrected images. The network focuses more on the geometric prior regions as opposed to equally perceiving the whole image without any attention mechanisms. To generate more appealing corrected results in visual appearance, we introduce a coarse-to-fine inpainting network to fill the hole regions caused by the irreversible mapping function using distortion parameters. Each module of the proposed network is differentiable, and thus the entire framework is completely end-to-end. When compared with the previous supervised methods, our method is more flexible and shows better practical applications for distortion rectification. The experiment results demonstrate that our proposed method outperforms state-of-the-art methods on the correction performance without any labeled distortion parameters.",
    "link": "https://www.sciencedirect.com/science/article/pii/S104732031930313X",
    "keywords": [
      "Algorithm",
      "Amplifier",
      "Artificial intelligence",
      "Artificial neural network",
      "Bandwidth (computing)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Decoding methods",
      "Distortion (music)",
      "Distortion function",
      "Image (mathematics)",
      "Image rectification",
      "Leverage (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Perspective distortion",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Rectification"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Shangrong"
      },
      {
        "surname": "Lin",
        "given_name": "Chunyu"
      },
      {
        "surname": "Liao",
        "given_name": "Kang"
      },
      {
        "surname": "Zhao",
        "given_name": "Yao"
      },
      {
        "surname": "Liu",
        "given_name": "Meiqin"
      }
    ]
  }
]